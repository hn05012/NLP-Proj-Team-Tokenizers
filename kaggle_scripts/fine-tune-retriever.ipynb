{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12136380,"sourceType":"datasetVersion","datasetId":7642928},{"sourceId":12252405,"sourceType":"datasetVersion","datasetId":7642488},{"sourceId":12403754,"sourceType":"datasetVersion","datasetId":7719677}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:14:59.725469Z","iopub.execute_input":"2025-07-08T00:14:59.725854Z","iopub.status.idle":"2025-07-08T00:16:29.976271Z","shell.execute_reply.started":"2025-07-08T00:14:59.725835Z","shell.execute_reply":"2025-07-08T00:16:29.975559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:16:29.977158Z","iopub.execute_input":"2025-07-08T00:16:29.977363Z","iopub.status.idle":"2025-07-08T00:16:33.981893Z","shell.execute_reply.started":"2025-07-08T00:16:29.977340Z","shell.execute_reply":"2025-07-08T00:16:33.981186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Install required packages if not available\ntry:\n    from sentence_transformers import SentenceTransformer\n    from sklearn.metrics.pairwise import cosine_similarity\nexcept ImportError:\n    import subprocess\n    import sys\n    print(\"Installing required packages...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\", \"scikit-learn\"])\n    from sentence_transformers import SentenceTransformer\n    from sklearn.metrics.pairwise import cosine_similarity\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:16:33.983526Z","iopub.execute_input":"2025-07-08T00:16:33.983736Z","iopub.status.idle":"2025-07-08T00:17:11.441116Z","shell.execute_reply.started":"2025-07-08T00:16:33.983716Z","shell.execute_reply":"2025-07-08T00:17:11.440484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\nimport os\nfrom sentence_transformers.losses import TripletLoss\n\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ndef preprocess_for_retriever(json_path):\n    with open(json_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)  # List of question objects\n\n    examples = []\n\n    for item in data:\n        question = item['question']\n        context_chunks = item.get('context_chunks', [])\n\n        # Positives only for contrastive loss (in-batch negatives used)\n        positives = [chunk['text'] for chunk in context_chunks if chunk.get('contains_answer', False)]\n\n        for pos_text in positives:\n            examples.append(InputExample(texts=[question, pos_text]))\n\n    print(f\"Created {len(examples)} InputExample instances\")\n    return examples\n\ndef preprocess_for_triplet_loss(json_path):\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    triplets = []\n    for item in data:\n        question = item['question']\n        chunks = item['context_chunks']\n        \n        # Anchor: Question\n        anchor = question\n        \n        # Positive: Chunk containing answer\n        positives = [c['text'] for c in chunks if c['contains_answer']]\n        \n        # Hard Negatives: Top irrelevant chunks (semantically similar but wrong)\n        negatives = [c['text'] for c in chunks \n                    if not c['contains_answer'] and c.get('score', 0) > 0.2]\n        \n        # Create triplets (anchor, positive, negative)\n        for pos in positives:\n            for neg in negatives[:3]:  # Use top 3 negatives per positive\n                triplets.append(InputExample(\n                    texts=[anchor, pos, neg],  # Order matters for TripletLoss!\n                    label=1.0\n                ))\n    \n    print(f\"Generated {len(triplets)} triplets\")\n    return triplets\n\ndef fine_tune_model(model_name, examples, output_path, num_epochs=2):\n    print(f\"\\nStarting training for model: {model_name}\")\n    model = SentenceTransformer(model_name)\n\n    train_dataloader = DataLoader(examples, shuffle=True, batch_size=16)\n    train_loss = losses.MultipleNegativesRankingLoss(model)\n\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # 10% warm-up\n\n    model.fit(\n        train_objectives=[(train_dataloader, train_loss)],\n        epochs=num_epochs,\n        warmup_steps=warmup_steps,\n        show_progress_bar=True,\n    )\n\n    model.save(output_path)\n    print(f\"Fine-tuned model saved at {output_path}\")\n\ndef train_with_triplet_loss(model_name, triplets, output_path, num_epochs=2):\n    model = SentenceTransformer(model_name)\n    \n    train_dataloader = DataLoader(triplets, shuffle=True, batch_size=16)\n    train_loss = losses.TripletLoss(\n        model=model,\n        distance_metric=losses.TripletDistanceMetric.COSINE,\n        triplet_margin=0.5  # How much better positives should be than negatives\n    )\n    \n    model.fit(\n        train_objectives=[(train_dataloader, train_loss)],\n        epochs=num_epochs,\n        warmup_steps=int(0.1 * len(triplets)),\n        show_progress_bar=True\n    )\n    \n    model.save(output_path)\n\n\n\n\ndef main():\n    json_path = '/kaggle/input/simple-python-qa-rag-dataset-v2/simple_python_qa_rag_dataset.json'  \n    \n    # Step 1: Prepare triplets (anchor=question, positive=answer, negative=hard distractor)\n    triplets = preprocess_for_triplet_loss(json_path)  # Use the new preprocessing function\n    \n    # Step 2: Train with TripletLoss instead of MNR\n    models_to_finetune = [\n        'intfloat/e5-base',\n        'thenlper/gte-base'\n    ]\n    \n    for model_name in models_to_finetune:\n        safe_model_name = model_name.replace('/', '_')\n        output_path = f'/kaggle/working/triplet_loss_finetuned_{safe_model_name}'\n        \n        train_with_triplet_loss(\n            model_name=model_name,\n            triplets=triplets,\n            output_path=output_path,\n            num_epochs=2  # Start with 2 epochs (TripletLoss may need fewer epochs)\n        )\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:17:11.441850Z","iopub.execute_input":"2025-07-08T00:17:11.442420Z","iopub.status.idle":"2025-07-08T00:23:35.773542Z","shell.execute_reply.started":"2025-07-08T00:17:11.442395Z","shell.execute_reply":"2025-07-08T00:23:35.772978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_test_data(json_path: str) -> List[Dict]:\n    \"\"\"Load test dataset from JSON file\"\"\"\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        print(f\"‚úÖ Loaded {len(data)} test questions from {json_path}\")\n        return data\n    except Exception as e:\n        print(f\"‚ùå Error loading test data: {e}\")\n        return []\n\ndef load_model_safely(model_path: str) -> SentenceTransformer:\n    \"\"\"Load model with error handling\"\"\"\n    try:\n        model = SentenceTransformer(model_path)\n        print(f\"‚úÖ Loaded model from {model_path}\")\n        return model\n    except Exception as e:\n        print(f\"‚ùå Error loading model from {model_path}: {e}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T23:23:41.767822Z","iopub.execute_input":"2025-06-25T23:23:41.768604Z","iopub.status.idle":"2025-06-25T23:23:41.773772Z","shell.execute_reply.started":"2025-06-25T23:23:41.768573Z","shell.execute_reply":"2025-06-25T23:23:41.773035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_retrieval_metrics(ranked_indices: np.ndarray, relevant_indices: List[int], \n                               top_k_values: List[int]) -> Dict:\n    \"\"\"Calculate standard information retrieval metrics\"\"\"\n    metrics = {}\n    \n    for k in top_k_values:\n        top_k_indices = set(ranked_indices[:k])\n        relevant_set = set(relevant_indices)\n        \n        # Recall@k: fraction of relevant items retrieved\n        relevant_in_top_k = len(top_k_indices & relevant_set)\n        recall_k = relevant_in_top_k / len(relevant_indices) if relevant_indices else 0\n        \n        # Precision@k: fraction of retrieved items that are relevant\n        precision_k = relevant_in_top_k / k if k > 0 else 0\n        \n        # Hit Rate@k: whether at least one relevant item is retrieved\n        hit_rate_k = 1 if relevant_in_top_k > 0 else 0\n        \n        metrics[f'recall@{k}'] = recall_k\n        metrics[f'precision@{k}'] = precision_k\n        metrics[f'hit_rate@{k}'] = hit_rate_k\n    \n    # Mean Reciprocal Rank (MRR)\n    mrr = 0\n    for rank, idx in enumerate(ranked_indices):\n        if idx in relevant_indices:\n            mrr = 1 / (rank + 1)\n            break\n    metrics['mrr'] = mrr\n    \n    # NDCG@10\n    dcg = 0\n    idcg = sum([1 / np.log2(i + 2) for i in range(min(len(relevant_indices), 10))])\n    for rank, idx in enumerate(ranked_indices[:10]):\n        if idx in relevant_indices:\n            dcg += 1 / np.log2(rank + 2)\n    metrics['ndcg@10'] = dcg / idcg if idcg > 0 else 0\n    \n    return metrics\n\ndef evaluate_retriever_performance(model: SentenceTransformer, test_data: List[Dict], \n                                 model_name: str = \"Model\", log_examples: int = 5) -> Dict:\n    \"\"\"Evaluate retriever performance on test data\"\"\"\n    \n    print(f\"\\nüîç Evaluating {model_name} (Logging first {log_examples} queries)...\")\n    print(\"-\" * 50)\n    \n    all_metrics = {metric: [] for metric in [f'recall@{k}' for k in TOP_K_VALUES] + \n                   [f'precision@{k}' for k in TOP_K_VALUES] + \n                   [f'hit_rate@{k}' for k in TOP_K_VALUES] + ['mrr', 'ndcg@10']}\n    \n    valid_questions = 0\n    \n    for idx, item in enumerate(test_data):\n        question = item.get('question', '')\n        context_chunks = item.get('context_chunks', [])\n        \n        if not question or not context_chunks:\n            continue\n            \n        # Get relevant chunks\n        relevant_indices = [i for i, chunk in enumerate(context_chunks) \n                          if chunk.get('contains_answer', False)]\n        \n        if not relevant_indices:\n            continue\n            \n        valid_questions += 1\n        \n        # Encode question and chunks\n        try:\n            all_chunk_texts = [chunk.get('text', '') for chunk in context_chunks]\n            question_embedding = model.encode([question], show_progress_bar=False)\n            chunk_embeddings = model.encode(all_chunk_texts, show_progress_bar=False)\n            \n            # Calculate similarities and rank\n            similarities = cosine_similarity(question_embedding, chunk_embeddings)[0]\n            ranked_indices = np.argsort(similarities)[::-1]\n\n\n            if idx < log_examples:\n                print(f\"\\nüìù Question {idx+1}: {question}\")\n                print(f\"   Gold Answer: {item['expected_answer']}...\")\n                for rank, chunk_idx in enumerate(ranked_indices[:3]):\n                    chunk = context_chunks[chunk_idx]\n                    print(f\"  Rank {rank+1} (Score={similarities[chunk_idx]:.3f}):\")\n                    print(f\"    Contains Answer: {chunk['contains_answer']}\")\n                    print(f\"    Text: {chunk['text']}...\")\n            \n            # Calculate metrics for this question\n            question_metrics = calculate_retrieval_metrics(ranked_indices, relevant_indices, TOP_K_VALUES)\n            \n            # Accumulate metrics\n            for metric, value in question_metrics.items():\n                all_metrics[metric].append(value)\n                \n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Error processing question {idx}: {e}\")\n            continue\n        \n        # Progress update\n        if valid_questions % 20 == 0:\n            print(f\"   Processed {valid_questions} questions...\")\n    \n    # Calculate averages\n    avg_metrics = {metric: np.mean(values) if values else 0 \n                   for metric, values in all_metrics.items()}\n    \n    print(f\"‚úÖ Evaluation complete! Processed {valid_questions} valid questions.\")\n    \n    return {\n        'model_name': model_name,\n        'total_questions': valid_questions,\n        'metrics': avg_metrics,\n        'raw_metrics': all_metrics\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T23:25:17.623682Z","iopub.execute_input":"2025-06-25T23:25:17.623955Z","iopub.status.idle":"2025-06-25T23:25:17.638085Z","shell.execute_reply.started":"2025-06-25T23:25:17.623935Z","shell.execute_reply":"2025-06-25T23:25:17.637202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your fine-tuned model (example for gte-base)\nmodel = SentenceTransformer('/kaggle/input/fine-tuned-retrievers/triplet_loss_finetuned_thenlper_gte-base')\n\n# Load your dataset\ntest_data = load_test_data('/kaggle/input/simple-python-qa-rag-dataset-v2/simple_python_qa_rag_dataset.json')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T23:25:21.112537Z","iopub.execute_input":"2025-06-25T23:25:21.112820Z","iopub.status.idle":"2025-06-25T23:25:40.601607Z","shell.execute_reply.started":"2025-06-25T23:25:21.112799Z","shell.execute_reply":"2025-06-25T23:25:40.600838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_models(ft_results: Dict, baseline_results: Dict) -> pd.DataFrame:\n    \"\"\"Create comparison DataFrame between fine-tuned and baseline models\"\"\"\n    \n    comparison_data = []\n    \n    for metric in ft_results['metrics'].keys():\n        ft_score = ft_results['metrics'][metric]\n        baseline_score = baseline_results['metrics'][metric]\n        improvement = ft_score - baseline_score\n        improvement_pct = (improvement / baseline_score * 100) if baseline_score > 0 else 0\n        \n        comparison_data.append({\n            'Metric': metric,\n            'Fine-tuned': round(ft_score, 4),\n            'Baseline': round(baseline_score, 4),\n            'Improvement': round(improvement, 4),\n            'Improvement %': round(improvement_pct, 2)\n        })\n    \n    return pd.DataFrame(comparison_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T23:29:58.206361Z","iopub.execute_input":"2025-06-25T23:29:58.206650Z","iopub.status.idle":"2025-06-25T23:29:58.212392Z","shell.execute_reply.started":"2025-06-25T23:29:58.206629Z","shell.execute_reply":"2025-06-25T23:29:58.211605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_qualitative_examples(model: SentenceTransformer, test_data: List[Dict], \n                            num_examples: int = 3, model_name: str = \"Model\"):\n    \"\"\"Display qualitative examples of retrieval results\"\"\"\n    \n    print(f\"\\nüîç QUALITATIVE ANALYSIS - {model_name}\")\n    print(\"=\" * 80)\n    \n    examples_shown = 0\n    \n    for idx, item in enumerate(test_data):\n        if examples_shown >= num_examples:\n            break\n            \n        question = item.get('question', '')\n        context_chunks = item.get('context_chunks', [])\n        \n        if not question or not context_chunks:\n            continue\n            \n        relevant_chunks = [chunk for chunk in context_chunks \n                         if chunk.get('contains_answer', False)]\n        \n        if not relevant_chunks:\n            continue\n            \n        examples_shown += 1\n        \n        print(f\"\\nüìù EXAMPLE {examples_shown}\")\n        print(\"-\" * 40)\n        print(f\"QUESTION: {question}\")\n        \n        print(f\"\\n‚úÖ GROUND TRUTH ({len(relevant_chunks)} relevant chunks):\")\n        for i, chunk in enumerate(relevant_chunks[:2]):  # Show max 2 for brevity\n            text_preview = chunk.get('text', '')[:150] + \"...\" if len(chunk.get('text', '')) > 150 else chunk.get('text', '')\n            print(f\"   {i+1}. {text_preview}\")\n        \n        # Get model predictions\n        try:\n            all_chunk_texts = [chunk.get('text', '') for chunk in context_chunks]\n            question_embedding = model.encode([question])\n            chunk_embeddings = model.encode(all_chunk_texts)\n            similarities = cosine_similarity(question_embedding, chunk_embeddings)[0]\n            ranked_indices = np.argsort(similarities)[::-1]\n            \n            print(f\"\\nü§ñ MODEL PREDICTIONS (Top 3):\")\n            for rank, idx in enumerate(ranked_indices[:3]):\n                is_relevant = context_chunks[idx].get('contains_answer', False)\n                status = \"‚úÖ RELEVANT\" if is_relevant else \"‚ùå NOT RELEVANT\"\n                score = similarities[idx]\n                text_preview = context_chunks[idx].get('text', '')[:150] + \"...\" if len(context_chunks[idx].get('text', '')) > 150 else context_chunks[idx].get('text', '')\n                \n                print(f\"   {rank+1}. [Score: {score:.3f}] {status}\")\n                print(f\"      {text_preview}\")\n                \n        except Exception as e:\n            print(f\"   ‚ùå Error getting predictions: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T23:30:00.184632Z","iopub.execute_input":"2025-06-25T23:30:00.184914Z","iopub.status.idle":"2025-06-25T23:30:00.194471Z","shell.execute_reply.started":"2025-06-25T23:30:00.184894Z","shell.execute_reply":"2025-06-25T23:30:00.193700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_performance_summary(results: Dict):\n    \"\"\"Print a formatted summary of performance metrics\"\"\"\n    \n    print(f\"\\nüìä PERFORMANCE SUMMARY - {results['model_name']}\")\n    print(\"=\" * 50)\n    print(f\"Total Questions Evaluated: {results['total_questions']}\")\n    print(\"\\nüéØ Key Metrics:\")\n    \n    # Group metrics for better readability\n    recall_metrics = {k: v for k, v in results['metrics'].items() if k.startswith('recall@')}\n    hit_rate_metrics = {k: v for k, v in results['metrics'].items() if k.startswith('hit_rate@')}\n    other_metrics = {k: v for k, v in results['metrics'].items() if not k.startswith(('recall@', 'hit_rate@', 'precision@'))}\n    \n    print(\"\\n   Recall (How much relevant content is retrieved):\")\n    for metric, score in recall_metrics.items():\n        print(f\"   ‚Ä¢ {metric:12}: {score:.4f}\")\n    \n    print(\"\\n   Hit Rate (Questions with at least one relevant result):\")\n    for metric, score in hit_rate_metrics.items():\n        print(f\"   ‚Ä¢ {metric:12}: {score:.4f}\")\n    \n    print(\"\\n   Ranking Quality:\")\n    for metric, score in other_metrics.items():\n        print(f\"   ‚Ä¢ {metric:12}: {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T23:29:29.821261Z","iopub.execute_input":"2025-06-25T23:29:29.821846Z","iopub.status.idle":"2025-06-25T23:29:29.828462Z","shell.execute_reply.started":"2025-06-25T23:29:29.821825Z","shell.execute_reply":"2025-06-25T23:29:29.827454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nFINE_TUNED_MODEL_PATHS = {\n    'e5-base': '/kaggle/input/fine-tuned-retrievers/triplet_loss_finetuned_intfloat_e5-base',\n    'gte-base': '/kaggle/input/fine-tuned-retrievers/triplet_loss_finetuned_thenlper_gte-base'  # Fix typo if needed\n}\n\nBASELINE_MODELS = {\n    'gte-base': 'thenlper/gte-base',\n    'e5-base': 'intfloat/e5-base',\n}\nTEST_DATA_PATH = '/kaggle/input/simple-python-qa-rag-dataset-v2/simple_python_qa_rag_dataset.json'\nOUTPUT_DIR = '/kaggle/working'\n\n# Evaluation settings\nTOP_K_VALUES = [1, 3]\nNUM_QUALITATIVE_EXAMPLES = 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T23:36:13.305296Z","iopub.execute_input":"2025-06-25T23:36:13.305628Z","iopub.status.idle":"2025-06-25T23:36:13.311108Z","shell.execute_reply.started":"2025-06-25T23:36:13.305605Z","shell.execute_reply":"2025-06-25T23:36:13.310238Z"}},"outputs":[],"execution_count":null}]}