{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up cache directory\n",
    "CACHE_DIR = Path(\"/kaggle/working/rag_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --- FAISS Installation Block ---\n",
    "print(\"Setting up FAISS...\")\n",
    "\n",
    "# First try GPU version (may fail in Kaggle)\n",
    "!pip install --quiet faiss-gpu --no-deps || echo \"GPU install failed, falling back to CPU\"\n",
    "\n",
    "# If GPU install failed, install CPU version\n",
    "if \"faiss-gpu\" not in sys.modules:\n",
    "    !pip install --quiet faiss-cpu\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"✅ FAISS imported - GPU version\" if \"faiss-gpu\" in sys.modules \n",
    "          else \"✅ FAISS imported - CPU version\")\n",
    "    \n",
    "    # Test basic functionality\n",
    "    dimension = 128\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Try to use GPU if available\n",
    "    USE_GPU = False\n",
    "    if \"faiss-gpu\" in sys.modules and torch.cuda.is_available():\n",
    "        try:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "            USE_GPU = True\n",
    "            print(\"FAISS GPU acceleration enabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"GPU acceleration failed, using CPU: {str(e)}\")\n",
    "    \n",
    "    print(f\"Test index created (dim={dimension}):\", index.is_trained)\n",
    "    HAS_FAISS = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"❌ FAISS installation failed completely:\", str(e))\n",
    "    HAS_FAISS = False\n",
    "\n",
    "# --- Core Package Installation ---\n",
    "print(\"\\nInstalling other requirements...\")\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'sentence-transformers',\n",
    "    'scikit-learn',\n",
    "    'transformers',\n",
    "    'numpy',\n",
    "    'torch'\n",
    "]\n",
    "\n",
    "!pip install --quiet {' '.join(REQUIRED_PACKAGES)}\n",
    "\n",
    "# --- Verification Block ---\n",
    "print(\"\\nVerifying imports...\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import sklearn\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    print(\"✅ All core packages imported!\")\n",
    "    \n",
    "    if HAS_FAISS:\n",
    "        if USE_GPU:\n",
    "            print(\"FAISS GPU available for accelerated search\")\n",
    "        else:\n",
    "            print(\"FAISS CPU available (no GPU acceleration)\")\n",
    "    else:\n",
    "        print(\"Using cosine similarity fallback\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(\"❌ Import failed:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:56:42.136174Z",
     "iopub.status.busy": "2025-07-10T11:56:42.135563Z",
     "iopub.status.idle": "2025-07-10T11:56:42.510486Z",
     "shell.execute_reply": "2025-07-10T11:56:42.509706Z",
     "shell.execute_reply.started": "2025-07-10T11:56:42.136146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()  # Clears unused cached memory\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:56:42.512459Z",
     "iopub.status.busy": "2025-07-10T11:56:42.511813Z",
     "iopub.status.idle": "2025-07-10T11:56:42.663058Z",
     "shell.execute_reply": "2025-07-10T11:56:42.662298Z",
     "shell.execute_reply.started": "2025-07-10T11:56:42.512427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:56:42.664734Z",
     "iopub.status.busy": "2025-07-10T11:56:42.664518Z",
     "iopub.status.idle": "2025-07-10T11:56:42.669225Z",
     "shell.execute_reply": "2025-07-10T11:56:42.668643Z",
     "shell.execute_reply.started": "2025-07-10T11:56:42.664716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import faiss\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "# Kaggle-specific paths\n",
    "CACHE_DIR = Path(\"/kaggle/working/rag_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:56:42.670252Z",
     "iopub.status.busy": "2025-07-10T11:56:42.670054Z",
     "iopub.status.idle": "2025-07-10T11:56:42.692130Z",
     "shell.execute_reply": "2025-07-10T11:56:42.691607Z",
     "shell.execute_reply.started": "2025-07-10T11:56:42.670235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', device='cuda'):\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "        \n",
    "        # Initialize FAISS availability\n",
    "        # self.has_faiss = self._initialize_faiss()\n",
    "        self.has_faiss = False\n",
    "        \n",
    "    def _initialize_faiss(self):\n",
    "        \"\"\"Check and initialize FAISS availability\"\"\"\n",
    "        try:\n",
    "            import faiss\n",
    "            self.faiss = faiss\n",
    "            # Test basic FAISS functionality\n",
    "            test_index = faiss.IndexFlatL2(128)\n",
    "            return True\n",
    "        except:\n",
    "            print(\"FAISS not available, falling back to cosine similarity\")\n",
    "            return False\n",
    "    \n",
    "    def save_state(self):\n",
    "        \"\"\"Save state to disk\"\"\"\n",
    "        state = {\n",
    "            'documents': self.documents,\n",
    "            'embeddings': self.embeddings,\n",
    "            'has_faiss': self.has_faiss  # Save FAISS availability state\n",
    "        }\n",
    "        \n",
    "        with open(CACHE_DIR/'retriever_state.pkl', 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "            \n",
    "        if self.has_faiss and self.index:\n",
    "            self.faiss.write_index(self.index, str(CACHE_DIR/'faiss_index.bin'))\n",
    "    \n",
    "    def load_state(self):\n",
    "        \"\"\"Load state from disk\"\"\"\n",
    "        try:\n",
    "            with open(CACHE_DIR/'retriever_state.pkl', 'rb') as f:\n",
    "                state = pickle.load(f)\n",
    "            \n",
    "            self.documents = state['documents']\n",
    "            self.embeddings = state['embeddings']\n",
    "            self.has_faiss = state.get('has_faiss', False)\n",
    "            \n",
    "            if self.has_faiss:\n",
    "                import faiss\n",
    "                self.faiss = faiss\n",
    "                faiss_file = CACHE_DIR/'faiss_index.bin'\n",
    "                if faiss_file.exists():\n",
    "                    self.index = self.faiss.read_index(str(faiss_file))\n",
    "            \n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "    \n",
    "    def add_documents(self, documents: list, batch_size=128, force_reindex=False):\n",
    "        \"\"\"Index documents with both methods\"\"\"\n",
    "        if not force_reindex and self.load_state():\n",
    "            print(\"Loaded cached document embeddings\")\n",
    "            return\n",
    "            \n",
    "        self.documents = []\n",
    "        \n",
    "        # Process documents\n",
    "        for entry in documents:\n",
    "            if not isinstance(entry, dict):\n",
    "                continue\n",
    "                \n",
    "            question = entry.get('question', '')\n",
    "            metadata = entry.get('metadata', {})\n",
    "            \n",
    "            for chunk in entry.get('context_chunks', []):\n",
    "                self.documents.append({\n",
    "                    'text': chunk.get('text', ''),\n",
    "                    'question': question,\n",
    "                    'contains_answer': chunk.get('contains_answer', False),\n",
    "                    'chunk_score': chunk.get('score', 0),\n",
    "                    'source': chunk.get('source', 'unknown'),\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "        \n",
    "        # Generate embeddings\n",
    "        pairs = [f\"{doc['question']} [SEP] {doc['text']}\" for doc in self.documents]\n",
    "        self.embeddings = self.model.encode(\n",
    "            pairs,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=False\n",
    "        )\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        self.embeddings = self.embeddings / np.linalg.norm(\n",
    "            self.embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Build FAISS index if available\n",
    "        if self.has_faiss:\n",
    "            dimension = self.embeddings.shape[1]\n",
    "            self.index = self.faiss.IndexFlatIP(dimension)\n",
    "            self.index.add(self.embeddings.astype('float32'))\n",
    "        \n",
    "        self.save_state()\n",
    "        print(f\"Indexed {len(self.documents)} chunks\")\n",
    "    \n",
    "    def query(self, question: str, k=5) -> List[dict]:\n",
    "        \"\"\"Unified query method that automatically uses the best available backend\"\"\"\n",
    "        if not hasattr(self, 'has_faiss'):  # Backward compatibility\n",
    "            self.has_faiss = self._initialize_faiss()\n",
    "            \n",
    "        if self.has_faiss and self.index is not None:\n",
    "            print(\"Using FAISS for accelerated search\")\n",
    "            return self._query_faiss(question, k)\n",
    "        else:\n",
    "            print(\"Using cosine similarity (FAISS not available)\")\n",
    "            return self._query_cosine(question, k)\n",
    "    \n",
    "    def _query_faiss(self, question: str, k=5) -> List[dict]:\n",
    "        \"\"\"Modified FAISS implementation with score thresholding\"\"\"\n",
    "        question_embedding = self.model.encode([question], convert_to_tensor=False)\n",
    "        question_embedding = question_embedding / np.linalg.norm(question_embedding)\n",
    "        question_embedding = question_embedding.astype('float32').reshape(1, -1)\n",
    "        \n",
    "        # Get more candidates initially\n",
    "        distances, indices = self.index.search(question_embedding, k*5)\n",
    "        \n",
    "        results = []\n",
    "        seen_texts = set()\n",
    "        \n",
    "        for idx, score in zip(indices[0], distances[0]):\n",
    "            if idx < 0 or score < 0.65:  # New score threshold\n",
    "                continue\n",
    "                \n",
    "            doc = self.documents[idx]\n",
    "            if doc['text'] not in seen_texts:\n",
    "                results.append({\n",
    "                    'text': doc['text'],\n",
    "                    'score': float(score),\n",
    "                    # ... rest of fields\n",
    "                })\n",
    "                seen_texts.add(doc['text'])\n",
    "                if len(results) >= k:\n",
    "                    break\n",
    "                    \n",
    "        return results if results else [{'text': \"NO_RELEVANT_CONTENT\", 'score': 0}]\n",
    "    \n",
    "    def _query_cosine(self, question: str, k=5) -> List[dict]:\n",
    "        \"\"\"Cosine similarity fallback\"\"\"\n",
    "        question_embedding = self.model.encode([question], convert_to_tensor=False)\n",
    "        question_embedding = question_embedding / np.linalg.norm(question_embedding)\n",
    "        \n",
    "        scores = np.dot(self.embeddings, question_embedding.T).flatten()\n",
    "        \n",
    "        results = []\n",
    "        seen_texts = set()\n",
    "        \n",
    "        for idx in np.argsort(scores)[-k*2:][::-1]:\n",
    "            doc = self.documents[idx]\n",
    "            if doc['text'] not in seen_texts:\n",
    "                results.append({\n",
    "                    'text': doc['text'],\n",
    "                    'score': float(scores[idx]),\n",
    "                    'question': doc['question'],\n",
    "                    'contains_answer': doc['contains_answer'],\n",
    "                    'source': doc['source'],\n",
    "                    'metadata': doc['metadata']\n",
    "                })\n",
    "                seen_texts.add(doc['text'])\n",
    "                if len(results) >= k:\n",
    "                    break\n",
    "                    \n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "    def benchmark_retrieval(self, question: str, k: int = 5, runs: int = 10) -> Dict:\n",
    "            \"\"\"Benchmark both retrieval methods and return timing results\"\"\"\n",
    "            results = {\n",
    "                'question': question,\n",
    "                'runs': runs,\n",
    "                'faiss': {'times': [], 'avg': 0, 'results': None},\n",
    "                'cosine': {'times': [], 'avg': 0, 'results': None}\n",
    "            }\n",
    "            \n",
    "            # Only benchmark FAISS if it's available\n",
    "            if self.has_faiss and self.index:\n",
    "                for _ in range(runs):\n",
    "                    start = time.perf_counter()\n",
    "                    faiss_results = self._query_faiss(question, k)\n",
    "                    elapsed = time.perf_counter() - start\n",
    "                    results['faiss']['times'].append(elapsed)\n",
    "                results['faiss']['avg'] = sum(results['faiss']['times'])/runs\n",
    "                results['faiss']['results'] = faiss_results[:2]  # Sample results\n",
    "            \n",
    "            # Always benchmark cosine\n",
    "            for _ in range(runs):\n",
    "                start = time.perf_counter()\n",
    "                cosine_results = self._query_cosine(question, k)\n",
    "                elapsed = time.perf_counter() - start\n",
    "                results['cosine']['times'].append(elapsed)\n",
    "            results['cosine']['avg'] = sum(results['cosine']['times'])/runs\n",
    "            results['cosine']['results'] = cosine_results[:2]  # Sample results\n",
    "            \n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:56:42.693234Z",
     "iopub.status.busy": "2025-07-10T11:56:42.692919Z",
     "iopub.status.idle": "2025-07-10T11:56:42.714620Z",
     "shell.execute_reply": "2025-07-10T11:56:42.714093Z",
     "shell.execute_reply.started": "2025-07-10T11:56:42.693206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model_name='deepseek-ai/deepseek-coder-6.7b-instruct'):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    def build_prompt(self, question: str, context_chunks: list[dict]) -> str:\n",
    "        \"\"\"More inclusive Python prompt covering ecosystem while staying strict\"\"\"\n",
    "        \n",
    "        # Filter chunks by score threshold\n",
    "        valid_chunks = [c for c in context_chunks if c['score'] >= 0.65]\n",
    "        \n",
    "        if not valid_chunks:\n",
    "            return f\"\"\"Answer the question following these rules:\n",
    "    1. If the question isn't related to Python (language, ecosystem, tools, or community), \n",
    "       respond EXACTLY with: \"<answer>I don't know</answer>\"\n",
    "    2. If no relevant context exists, respond EXACTLY with: \"<answer>I don't know</answer>\"\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    <answer>I don't know</answer>\"\"\"\n",
    "    \n",
    "        context_str = \"\\n\".join(\n",
    "            f\"### Context Block {i+1} [Relevance: {c['score']:.2f}]:\\n{c['text']}\\n\"\n",
    "            f\"----------------------------\"\n",
    "            for i, c in enumerate(valid_chunks[:3])\n",
    "        )\n",
    "    \n",
    "        return f\"\"\"You are a Python specialist assistant. Follow these rules STRICTLY:\n",
    "    \n",
    "    {context_str}\n",
    "    \n",
    "    ### Python Scope Includes:\n",
    "    - Python programming language (any version)\n",
    "    - Standard library and built-in functions\n",
    "    - Popular frameworks (Django, Flask, etc.)\n",
    "    - Scientific Python (NumPy, Pandas, etc.)\n",
    "    - Python packaging and distribution (pip, PyPI)\n",
    "    - Official Python documentation\n",
    "    - PEP standards\n",
    "    - Python community conventions\n",
    "    \n",
    "    ### Response Protocol:\n",
    "    1. FIRST determine if the question relates to ANY aspect of Python (see scope above)\n",
    "    2. THEN verify if the answer exists in the provided context\n",
    "    3. If both conditions are met:\n",
    "       - Cite which context block(s) you're using\n",
    "       - Provide a concise answer\n",
    "    4. If either condition fails:\n",
    "       - Respond EXACTLY with \"<answer>I don't know</answer>\"\n",
    "    5. ALWAYS wrap your final response in <answer></answer> tags\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    ### Step-by-Step Verification:\n",
    "    1. Python relevance check:\n",
    "    2. Context adequacy check: \n",
    "    3. Source context block(s):\n",
    "    4. Final answer:\n",
    "    \n",
    "    <answer>\"\"\"\n",
    "    \n",
    "    def generate_answer(self, prompt: str) -> str:\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:56:42.715637Z",
     "iopub.status.busy": "2025-07-10T11:56:42.715377Z",
     "iopub.status.idle": "2025-07-10T11:56:42.741459Z",
     "shell.execute_reply": "2025-07-10T11:56:42.740624Z",
     "shell.execute_reply.started": "2025-07-10T11:56:42.715602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, retriever_model='all-MiniLM-L6-v2', \n",
    "                 generator_model='deepseek-ai/deepseek-coder-6.7b-instruct'):\n",
    "        self.retriever = Retriever(model_name=retriever_model)\n",
    "        self.generator = Generator(model_name=generator_model)\n",
    "    \n",
    "    def load_dataset(self, dataset_path):\n",
    "        \"\"\"Load and parse the JSON dataset\"\"\"\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            return json.load(f)  # Returns list of QA objects\n",
    "    \n",
    "    def prepare_documents(self, dataset):\n",
    "        \"\"\"Pass through the data without transformation\"\"\"\n",
    "        return dataset  # Already in correct format    \n",
    "        \n",
    "    def add_documents_from_dataset(self, dataset_path: str, force_reindex=False, batch_size: int = 1):\n",
    "        \"\"\"Kaggle-friendly document loading\"\"\"\n",
    "        dataset = self.load_dataset(dataset_path)\n",
    "        documents = self.prepare_documents(dataset)\n",
    "        self.retriever.add_documents(documents, force_reindex=force_reindex, batch_size = batch_size)\n",
    "    \n",
    "    def query(self, question: str, k=5) -> dict:\n",
    "        \"\"\"End-to-end query with caching and formatted output\"\"\"\n",
    "        chunks = self.retriever.query(question, k=k)\n",
    "        prompt = self.generator.build_prompt(question, chunks)\n",
    "        raw_answer = self.generator.generate_answer(prompt)\n",
    "        \n",
    "        # Extract answer between tags\n",
    "        if \"<answer>\" in raw_answer and \"</answer>\" in raw_answer:\n",
    "            answer = raw_answer.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
    "        else:\n",
    "            answer = \"I don't know\"\n",
    "        \n",
    "        # Format the output\n",
    "        formatted_output = f\"\"\"\n",
    "{'='*50}\n",
    "QUESTION: {question}\n",
    "\n",
    "{'='*50}\n",
    "CONTEXT:\n",
    "{'-'*50}\n",
    "\"\"\" + \"\\n\".join([\n",
    "    f\"Source {i+1} [Relevance: {chunk['score']:.2f}]:\\n{chunk['text']}\\n{'-'*50}\"\n",
    "    for i, chunk in enumerate(chunks[:3])  # Show top 3 chunks\n",
    "]) + f\"\"\"\n",
    "\n",
    "{'='*50}\n",
    "ANSWER:\n",
    "{'-'*50}\n",
    "{answer if answer else \"I don't know\"}\n",
    "{'='*50}\n",
    "\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer if answer else \"I don't know\",\n",
    "            'context': [chunk['text'] for chunk in chunks[:5]],\n",
    "            'formatted': formatted_output  # Add this new field\n",
    "        }\n",
    "\n",
    "    def print_formatted_result(self, result: dict):\n",
    "        \"\"\"Prints the result in a human-readable format\"\"\"\n",
    "        print(result.get('formatted', \"No formatted output available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:56:42.742471Z",
     "iopub.status.busy": "2025-07-10T11:56:42.742197Z",
     "iopub.status.idle": "2025-07-10T11:58:02.698892Z",
     "shell.execute_reply": "2025-07-10T11:58:02.698191Z",
     "shell.execute_reply.started": "2025-07-10T11:56:42.742451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c345d16216c14a5ebd327627edfe1684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288396deb37b4a3aac5e0f0315851ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b1040565b54504a1b861b9166d6d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8812bb90d0454a4a9f775d51f9faa377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2f02793f8742eb97f1e1f9b1e432e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbae3e7078044eba5ff8f0ae022a999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be3f0d6660147a58da3e49a01e9f75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7625e0b5744a9ebed1a2b8477bfafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18316380c3d346e49f72525eb095d11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize ONCE (models load here)\n",
    "pipeline = RAGPipeline(\n",
    "    retriever_model='/kaggle/input/fine-tuned-retrievers/triplet_loss_finetuned_intfloat_e5-base',\n",
    "    generator_model='deepseek-ai/deepseek-coder-6.7b-instruct'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:58:02.699937Z",
     "iopub.status.busy": "2025-07-10T11:58:02.699703Z",
     "iopub.status.idle": "2025-07-10T11:58:03.182961Z",
     "shell.execute_reply": "2025-07-10T11:58:03.182318Z",
     "shell.execute_reply.started": "2025-07-10T11:58:02.699919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()  # Clears unused cached memory\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/kaggle/input/simple-python-qa-rag-dataset-v2/simple_python_qa_rag_dataset.json'\n",
    "pipeline.add_documents_from_dataset(DATASET_PATH, force_reindex=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "        \"ModuleNotFoundError with pytest\",\n",
    "        ]\n",
    "    \n",
    "# Run queries\n",
    "for question in questions:\n",
    "    result = pipeline.query(question)\n",
    "    pipeline.print_formatted_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "benchmark_results = pipeline.retriever.benchmark_retrieval(\n",
    "    question=\"How to format strings in Python\",\n",
    "    k=5,\n",
    "    runs=200  # More runs = more accurate results\n",
    ")\n",
    "\n",
    "print(f\"\\nBenchmark Results (avg of {benchmark_results['runs']} runs):\")\n",
    "if benchmark_results['faiss']['avg']:\n",
    "    print(f\"FAISS: {benchmark_results['faiss']['avg']*1000:.2f}ms\")\n",
    "print(f\"Cosine: {benchmark_results['cosine']['avg']*1000:.2f}ms\")\n",
    "\n",
    "if benchmark_results['faiss']['avg']:\n",
    "    speedup = benchmark_results['cosine']['avg']/benchmark_results['faiss']['avg']\n",
    "    print(f\"\\nFAISS is {speedup:.1f}x faster than cosine similarity\")\n",
    "else:\n",
    "    print(\"\\nFAISS not available for comparison\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7719677,
     "sourceId": 12403754,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7642488,
     "sourceId": 12404214,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
