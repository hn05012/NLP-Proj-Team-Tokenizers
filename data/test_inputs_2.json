[
  {
    "question": "VSCode Python extension loading forever, saying “Reactivating terminals”",
    "expected_answer": "These steps solved my issue: Open VS Code Settings search for Python Locator switch from native to js. restart the vs code",
    "context_chunks": [
      {
        "text": "After updating VS code to v1.92, the Python extension consistently fails to launch, indefinitely showing a spinner next to “Reactivating terminals…” on the status bar. Selecting OUTPUT &gt; Python reveals the error Failed to resolve env &quot;/mnt/data-linux/miniconda3&quot;. Here’s the error trace: 2024-08-07 18:35:35.873 [error] sendStartupTelemetry() failed. s [Error]: Failed to resolve env &quot;/mnt/data-linux/miniconda3&quot; at ae (/home/user/.vscode-insiders/extensions/ms-python.python-2024.12.2-linux-x64/out/client/extension.js:2:1968174) at oe (/home/user/.vscode-insiders/extensions/ms-python.python-2024.12.2-linux-x64/out/client/extension.js:2:1966134) at Immediate.&lt;anonymous&gt; (/home/user/.vscode-insiders/extensions/ms-python.python-2024.12.2-linux-x64/out/client/extension.js:2:1962428) at processImmediate (node:internal/timers:478:21) { code: -4, data: undefined } How do I fix this? Restarting worked, but that's not sustainable.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "These steps solved my issue: Open VS Code Settings search for Python Locator switch from native to js. restart the vs code",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This appears to be a bug related to the new &quot;native&quot; Python locator. You can go back to the old working version by adding the following line to the user settings JSON (until the bug in the native locator is fixed): &quot;python.locator&quot;: &quot;js&quot;, Note that this workaround pins you to the legacy version which is not something you'll want to have around forever so you might want to report your issue on Github at https://github.com/microsoft/vscode-python/issues. There've been many issues already filed and many solved but it's a work in progress. Example issues: https://github.com/microsoft/vscode-python/issues/23922 https://github.com/microsoft/vscode-python/issues/23963 https://github.com/microsoft/vscode-python/issues/23956",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code"
      ],
      "question_score": 81,
      "answer_score": 131,
      "created": "2024-08-19T02:31:16",
      "question_id": 78886125,
      "answer_id": 79002808
    }
  },
  {
    "question": "UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown plt.show()",
    "expected_answer": "I have the same issue. In my case, I installed the PyQt5==5.15.10. After that, I run my code successfully. pip install PyQt5==5.15.10 or pip install PyQt5 with python==3.11 But from 2024, you guys should install version PyQt6 or the last version with python==3.12 or later.",
    "context_chunks": [
      {
        "text": "I am using Windows 10 PyCharm 2021.3.3 Professional Edition python 3.11.5 matplotlib 3.8.1 How can I permanently resolve this issue in my development environment? import numpy as np import matplotlib matplotlib.use('Agg') import matplotlib.pyplot as plt # Read data from file, skipping the first row (header) data = np.loadtxt('cm.dat', skiprows=1) # Initialize reference point x0, y0, z0 = data[0] # Compute squared displacement for each time step SD = [(x - x0)**2 + (y - y0)**2 + (z - z0)**2 for x, y, z in data] # Compute the cumulative average of SD to get MSD at each time step MSD = np.cumsum(SD) / np.arange(1, len(SD) + 1) # Generate time steps t = np.arange(1, len(SD) + 1) # Create a log-log plot of MSD versus t plt.figure(figsize=(8, 6)) plt.loglog(t, MSD, marker='o') plt.title('Mean Squared Displacement vs Time') plt.xlabel('Time step') plt.ylabel('MSD') plt.grid(True, which=&quot;both&quot;, ls=&quot;--&quot;) plt.show() C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\python.exe C:/git/RouseModel/tau_plot.py C:\\git\\RouseModel\\tau_plot.py:29: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown plt.show() Process finished with exit code 0",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I have the same issue. In my case, I installed the PyQt5==5.15.10. After that, I run my code successfully. pip install PyQt5==5.15.10 or pip install PyQt5 with python==3.11 But from 2024, you guys should install version PyQt6 or the last version with python==3.12 or later.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This is a terrible answer, but I wasn't really sure where to put it. If you are using a more up to date version of Python, and are in the year 2024, you may have issues installing PyQT5, the solution is to install PyQt6, using Poetry it was: poetry add pyqt6 or using pip: pip install PyQt6 I was having issues with PyQt5, and when I used the newly released version, things worked immediately.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "matplotlib",
        "pycharm"
      ],
      "question_score": 66,
      "answer_score": 96,
      "created": "2023-11-18T15:40:11",
      "question_id": 77507580,
      "answer_id": 77644828
    }
  },
  {
    "question": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
    "expected_answer": "The reason is that pandas defines its numpy dependency freely as &quot;anything newer than certain version of numpy&quot;. The problem occured, when numpy==2.0.0 has been released on June 16th 2024, because it is no longer compatible with your pandas version. The solution is to pin down the numpy version to any before the 2.0.0. Today it could be (this is the most recent numpy 1 release): numpy==1.26.4 To be added in your requirements or to the pip command you use (but together with installing pandas). Nowadays pip is very flexible and can handle the issue flawesly. You just need to ask it to install both pandas and numpy of given versions in the same pip install invocation.",
    "context_chunks": [
      {
        "text": "I want to call my Python module from the Matlab. I received the error: Error using numpy_ops&gt;init thinc.backends.numpy_ops Python Error: ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject. The Python script is as follows import spacy def text_recognizer(model_path, text): try: # Load the trained model nlp = spacy.load(model_path) print(&quot;Model loaded successfully.&quot;) # Process the given text doc = nlp(text) ent_labels = [(ent.text, ent.label_) for ent in doc.ents] return ent_labels The Matlab script is as follows % Set up the Python environment pe = pyenv; py.importlib.import_module('final_output'); % Add the directory containing the Python script to the Python path path_add = fileparts(which('final_output.py')); if count(py.sys.path, path_add) == 0 insert(py.sys.path, int64(0), path_add); end % Define model path and text to process model_path = 'D:\\trained_model\\\\output\\\\model-best'; text = 'Roses are red'; % Call the Python function pyOut = py.final_output.text_recognizer(model_path, text); % Convert the output to a MATLAB cell array entity_labels = cell(pyOut); disp(entity_labels); I found one solution to update Numpy, what I did, but nothing changed. I am using Python 3.9 and Numpy version 2.0.0 The error was received when I tried to call the Python module using a Matlab script. How can I fix the issue?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The reason is that pandas defines its numpy dependency freely as &quot;anything newer than certain version of numpy&quot;. The problem occured, when numpy==2.0.0 has been released on June 16th 2024, because it is no longer compatible with your pandas version. The solution is to pin down the numpy version to any before the 2.0.0. Today it could be (this is the most recent numpy 1 release): numpy==1.26.4 To be added in your requirements or to the pip command you use (but together with installing pandas). Nowadays pip is very flexible and can handle the issue flawesly. You just need to ask it to install both pandas and numpy of given versions in the same pip install invocation.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "downgrade your numpy version to 1.26.4",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "matlab",
        "spacy"
      ],
      "question_score": 179,
      "answer_score": 260,
      "created": "2024-06-17T18:52:57",
      "question_id": 78634235,
      "answer_id": 78641304
    }
  },
  {
    "question": "pandas FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version",
    "expected_answer": "In case of following distinct examples: ser1 = pd.Series([False, True, float(&quot;nan&quot;)]) ser1 = ser1.fillna(False) and ser2 = pd.Series([&quot;Zero&quot;, 5, 2.3]) ser2 = ser2.replace(&quot;Zero&quot;, 0) the use of an option context combined with infer_objects at the end seems to be the most generic solution to get rid of the FutureWarning: with pd.option_context(&quot;future.no_silent_downcasting&quot;, True): ser1 = ser1.fillna(False).infer_objects(copy=False) and with pd.option_context(&quot;future.no_silent_downcasting&quot;, True): ser2 = ser2.replace(&quot;Zero&quot;, 0).infer_objects(copy=False) Probably better is to be more specific and use astype(bool) and astype(float) instead of infer_objects(copy=False) in the above. Remark that other proposed solutions don't work in this case: The use of infer_objects(copy=False) before fillna or replace: ser1.infer_objects(copy=False).fillna(False) ser2.infer_objects(copy=False).replace(&quot;Zero&quot;, 0) doesn't get rid of the FutureWarning. The use of astype before fillna or replace is even more dangerous as it returns the wrong result for the first example: ser1.astype(bool).fillna(False) and raises a ValueError for the second example: ser2.astype(float).replace(&quot;Zero&quot;, 0) I would not recommend setting pandas.set_option(&quot;future.no_silent_downcasting&quot;, True) as this may hide issues elsewhere.",
    "context_chunks": [
      {
        "text": "In order to print dataframes nicely using tabulate, so that NaN and NaT are printed as empty cells, I've been using this successfully: print(tabulate(df.astype(object).fillna(&quot;&quot;))) Now, this causes the following warning: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. I don't know what I should do instead now. I certainly don't see how infer_objects(copy=False) would help as the whole point here is indeed to force converting everything to a string representation and filling in missing values with empty strings.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In case of following distinct examples: ser1 = pd.Series([False, True, float(&quot;nan&quot;)]) ser1 = ser1.fillna(False) and ser2 = pd.Series([&quot;Zero&quot;, 5, 2.3]) ser2 = ser2.replace(&quot;Zero&quot;, 0) the use of an option context combined with infer_objects at the end seems to be the most generic solution to get rid of the FutureWarning: with pd.option_context(&quot;future.no_silent_downcasting&quot;, True): ser1 = ser1.fillna(False).infer_objects(copy=False) and with pd.option_context(&quot;future.no_silent_downcasting&quot;, True): ser2 = ser2.replace(&quot;Zero&quot;, 0).infer_objects(copy=False) Probably better is to be more specific and use astype(bool) and astype(float) instead of infer_objects(copy=False) in the above. Remark that other proposed solutions don't work in this case: The use of infer_objects(copy=False) before fillna or replace: ser1.infer_objects(copy=False).fillna(False) ser2.infer_objects(copy=False).replace(&quot;Zero&quot;, 0) doesn't get rid of the FutureWarning. The use of astype before fillna or replace is even more dangerous as it returns the wrong result for the first example: ser1.astype(bool).fillna(False) and raises a ValueError for the second example: ser2.astype(float).replace(&quot;Zero&quot;, 0) I would not recommend setting pandas.set_option(&quot;future.no_silent_downcasting&quot;, True) as this may hide issues elsewhere.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Convert the DataFrame/Series type first Example: df.astype(float).fillna(value) Infer the objects' type with infer_objects df.infer_objects(copy=False).fillna(value) Where value is a compatible type of the inferred objects: Setting pandas.set_option(&quot;future.no_silent_downcasting&quot;, True) seems to work to remove the warning too, but I don't know if this is the correct behavior (as also pointed out by @mrgou in the comments). Explanation The arrays are of the type object and, when you call fillna, it tries to infer the objects' type which issues this warning.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "downcast"
      ],
      "question_score": 65,
      "answer_score": 20,
      "created": "2024-01-29T15:54:50",
      "question_id": 77900971,
      "answer_id": 78066237
    }
  },
  {
    "question": "&quot;cannot import name &#39;DEFAULT_CIPHERS&#39; from &#39;urllib3.util.ssl_&#39;&quot; on AWS Lambda using a layer",
    "expected_answer": "cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' You are encountering this issue because you’re using botocore which does not support urllib3 2.0 yet. Since you’re deploying to AWS Lambda, you’ll need to explicitly pin to urllib3&lt;2 in your project to ensure urllib3 2.0 isn’t brought into your environment. (Source) urllib3&lt;2 Follow this guide for how to deploy Python Lambda functions with .zip file archives. If you cannot get it to work via the .zip file, consider deploying via a container image instead by following this guide.",
    "context_chunks": [
      {
        "text": "What I want to achieve To scrape an website using AWS Lambda and save the data on S3. The issues I'm having When I execute Lambda, the following error message appears. { &quot;errorMessage&quot;: &quot;Unable to import module 'lambda_function': cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/opt/python/urllib3/util/ssl_.py)&quot;, &quot;errorType&quot;: &quot;Runtime.ImportModuleError&quot;, &quot;requestId&quot;: &quot;fb66bea9-cbad-4bd3-bd4d-6125454e21be&quot;, &quot;stackTrace&quot;: [] } Code The minimum Lambda code is as follows. import requests import boto3 def lambda_handler(event, context): s3 = boto3.client('s3') upload_res = s3.put_object(Bucket='horserace-dx', Key='/raw/a.html', Body='testtext') return event An layer was added to the Lambda. Files were save in python folder using the commands below , frozen in a zip file, then uploaded to AWS Lambda as a layer. !pip install requests -t ./python --no-user !pip install pandas -t ./python --no-user !pip install beautifulsoup4 -t ./python --no-user The bucket horserace-dx exists The folder raw exists The role of the Lambda is properly set. It can read from and write to S3 The runtime of the Lambda is Python 3.9. The python version of the local computer is 3.9.13. What I did so far I google &quot;cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_'&quot; and found some suggestions. I made the layer with the following code and tried again in vain. !pip install requests -t ./python --no-user !pip install pandas -t ./python --no-user !pip install beautifulsoup4 -t ./python --no-user !pip install urllib3==1.26.15 -t ./python --no-user So what should I do to achieve what I want to achieve? Any suggestions would be greatly appreciated.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' You are encountering this issue because you’re using botocore which does not support urllib3 2.0 yet. Since you’re deploying to AWS Lambda, you’ll need to explicitly pin to urllib3&lt;2 in your project to ensure urllib3 2.0 isn’t brought into your environment. (Source) urllib3&lt;2 Follow this guide for how to deploy Python Lambda functions with .zip file archives. If you cannot get it to work via the .zip file, consider deploying via a container image instead by following this guide.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In my case I just specified requests version (runtime python3.9) - requests==2.28.2 and it worked.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "amazon-web-services",
        "amazon-s3",
        "aws-lambda",
        "boto3"
      ],
      "question_score": 70,
      "answer_score": 70,
      "created": "2023-06-06T12:10:43",
      "question_id": 76414514,
      "answer_id": 76430749
    }
  },
  {
    "question": "AttributeError: module &#39;pkgutil&#39; has no attribute &#39;ImpImporter&#39;. Did you mean: &#39;zipimporter&#39;?",
    "expected_answer": "Due to the removal of the long-deprecated pkgutil.ImpImporter class, the pip command may not work for Python 3.12. You just have to manually install pip for Python 3.12 python -m ensurepip --upgrade python -m pip install --upgrade setuptools python -m pip install &lt;module&gt; In your virtual environment: pip install --upgrade setuptools Python comes with an ensurepip, which can install pip in a Python environment. https://pip.pypa.io/en/stable/installation/ On Linux/macOS terminal: python -m ensurepip --upgrade On Windows: py -m ensurepip --upgrade also, make sure to upgrade pip: py -m pip install --upgrade pip To install numpy on Python 3.12, you must use numpy version 1.26.4 pip install numpy==1.26.4 https://github.com/numpy/numpy/issues/23808#issuecomment-1722440746 for Ubuntu sudo apt install python3.12-dev or python3.12 -m pip install --upgrade setuptools",
    "context_chunks": [
      {
        "text": "Earlier I installed some packages like Matplotlib, NumPy, pip (version 23.3.1), wheel (version 0.41.2), etc., and did some programming with those. I used the command C:\\Users\\UserName&gt;pip list to find the list of packages that I have installed, and I am using Python 3.12.0 (by employing code C:\\Users\\UserName&gt;py -V). I need to use pyspedas to analyse some data. I am following the instruction that that I received from site to install the package, with a variation (I am not sure whether it matters or not: I am using py, instead of python). The commands that I use, in the order, are: py -m venv pyspedas .\\pyspedas\\Scripts\\activate pip install pyspedas After the last step, I am getting the following output: Collecting pyspedas Using cached pyspedas-1.4.47-py3-none-any.whl.metadata (14 kB) Collecting numpy&gt;=1.19.5 (from pyspedas) Using cached numpy-1.26.1-cp312-cp312-win_amd64.whl.metadata (61 kB) Collecting requests (from pyspedas) Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB) Collecting geopack&gt;=1.0.10 (from pyspedas) Using cached geopack-1.0.10-py3-none-any.whl (114 kB) Collecting cdflib&lt;1.0.0 (from pyspedas) Using cached cdflib-0.4.9-py3-none-any.whl (72 kB) Collecting cdasws&gt;=1.7.24 (from pyspedas) Using cached cdasws-1.7.43.tar.gz (21 kB) Installing build dependencies ... done Getting requirements to build wheel ... done Preparing metadata (pyproject.toml) ... done Collecting netCDF4&gt;=1.6.2 (from pyspedas) Using cached netCDF4-1.6.5-cp312-cp312-win_amd64.whl.metadata (1.8 kB) Collecting pywavelets (from pyspedas) Using cached PyWavelets-1.4.1.tar.gz (4.6 MB) Installing build dependencies ... done Getting requirements to build wheel ... error error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; [33 lines of output] Traceback (most recent call last): File &quot;C:\\Users\\UserName\\pyspedas\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 353, in &lt;module&gt; main() File &quot;C:\\Users\\UserName\\pyspedas\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\UserName\\pyspedas\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 112, in get_requires_for_build_wheel backend = _build_backend() ^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\UserName\\pyspedas\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 77, in _build_backend obj = import_module(mod_path) ^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\UserName\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py&quot;, line 90, in import_module return _bootstrap._gcd_import(name[level:], package, level) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1381, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1354, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1304, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 488, in _call_with_frames_removed File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1381, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1354, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1325, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 929, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 994, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 488, in _call_with_frames_removed File &quot;C:\\Users\\UserName\\AppData\\Local\\Temp\\pip-build-env-_lgbq70y\\overlay\\Lib\\site-packages\\setuptools\\__init__.py&quot;, line 16, in &lt;module&gt; import setuptools.version File &quot;C:\\Users\\UserName\\AppData\\Local\\Temp\\pip-build-env-_lgbq70y\\overlay\\Lib\\site-packages\\setuptools\\version.py&quot;, line 1, in &lt;module&gt; import pkg_resources File &quot;C:\\Users\\UserName\\AppData\\Local\\Temp\\pip-build-env-_lgbq70y\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py&quot;, line 2191, in &lt;module&gt; register_finder(pkgutil.ImpImporter, find_on_path) ^^^^^^^^^^^^^^^^^^^ AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'? [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. After little bit of googling, I came to know that this issues was reported at multiple places, but none for this package. I did install wheel in the new environment as mentioned in the answer here, but the problem still persists. Instead of setting up a virtual environment, I simply executed the command py -m pip install pyspedas. But I am still getting the error. What I could gather is that the program has an issue with Collecting pywavelets (from pyspedas) Using cached PyWavelets-1.4.1.tar.gz (4.6 MB) Installing build dependencies ... done I am using IDLE in Windows 11.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Due to the removal of the long-deprecated pkgutil.ImpImporter class, the pip command may not work for Python 3.12. You just have to manually install pip for Python 3.12 python -m ensurepip --upgrade python -m pip install --upgrade setuptools python -m pip install &lt;module&gt; In your virtual environment: pip install --upgrade setuptools Python comes with an ensurepip, which can install pip in a Python environment. https://pip.pypa.io/en/stable/installation/ On Linux/macOS terminal: python -m ensurepip --upgrade On Windows: py -m ensurepip --upgrade also, make sure to upgrade pip: py -m pip install --upgrade pip To install numpy on Python 3.12, you must use numpy version 1.26.4 pip install numpy==1.26.4 https://github.com/numpy/numpy/issues/23808#issuecomment-1722440746 for Ubuntu sudo apt install python3.12-dev or python3.12 -m pip install --upgrade setuptools",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "My problem was listing older version of NumPy in requirements.txt, which may be obvious problem, but for people stuck with this: Check for the newer versions of NumPy. I needed to list it as: numpy~=1.26.4",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "numpy",
        "pip"
      ],
      "question_score": 229,
      "answer_score": 324,
      "created": "2023-10-26T06:22:24",
      "question_id": 77364550,
      "answer_id": 77364602
    }
  },
  {
    "question": "AttributeError: module &#39;PIL.Image&#39; has no attribute &#39;ANTIALIAS&#39;",
    "expected_answer": "ANTIALIAS was removed in Pillow 10.0.0 (after being deprecated through many previous versions). Now you need to use PIL.Image.LANCZOS or PIL.Image.Resampling.LANCZOS. (This is the exact same algorithm that ANTIALIAS referred to, you just can no longer access it through the name ANTIALIAS.) Reference: Pillow 10.0.0 release notes (with table of removed constants) Simple code example: import PIL import numpy as np # Gradient image with a sharp color boundary across the diagonal large_arr = np.fromfunction(lambda x, y, z: (x+y)//(z+1), (256, 256, 3)).astype(np.uint8) large_img = PIL.Image.fromarray(large_arr) # Resize it: PIL.Image.LANCZOS also works here small_img = large_img.resize((128, 128), PIL.Image.Resampling.LANCZOS) print(small_img.size) large_img.show() small_img.show()",
    "context_chunks": [
      {
        "text": "I am trying to have images in my Tkinter GUI, hence I am using PIL. Image.ANTIALAIS is not working. However, Image.BILINEAR works Here's some sample code: import tkinter as tk from PIL import Image, ImageTk window = tk.Tk() image = Image.open(r&quot;VC.png&quot;) image = image.resize((20, 20), Image.ANTIALIAS) tk_image = ImageTk.PhotoImage(image) image_label = tk.Label(window, image=tk_image) image_label.pack() window.mainloop() Here's the error: Traceback (most recent call last): File &quot;&lt;module1&gt;&quot;, line 19, in &lt;module&gt; AttributeError: module 'PIL.Image' has no attribute 'ANTIALIAS' I tried reinstalling pip and Pillow. It didn't work. I asked ChatGPT about this, and it advised me to upgrade to Pillow's latest version. I am on the latest version (10.0.0).",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "ANTIALIAS was removed in Pillow 10.0.0 (after being deprecated through many previous versions). Now you need to use PIL.Image.LANCZOS or PIL.Image.Resampling.LANCZOS. (This is the exact same algorithm that ANTIALIAS referred to, you just can no longer access it through the name ANTIALIAS.) Reference: Pillow 10.0.0 release notes (with table of removed constants) Simple code example: import PIL import numpy as np # Gradient image with a sharp color boundary across the diagonal large_arr = np.fromfunction(lambda x, y, z: (x+y)//(z+1), (256, 256, 3)).astype(np.uint8) large_img = PIL.Image.fromarray(large_arr) # Resize it: PIL.Image.LANCZOS also works here small_img = large_img.resize((128, 128), PIL.Image.Resampling.LANCZOS) print(small_img.size) large_img.show() small_img.show()",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The problem is with Pillow 10.0. Trying to uninstall Pillow might give some errors. Just put this in cmd: pip install Pillow==9.5.0",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-imaging-library"
      ],
      "question_score": 122,
      "answer_score": 213,
      "created": "2023-07-04T21:58:01",
      "question_id": 76616042,
      "answer_id": 76616129
    }
  },
  {
    "question": "Logging Error: Failed to initialize logging system. Log messages may be missing.?",
    "expected_answer": "To find IDEPreferLogStreaming, you need to go to Product -&gt; Scheme -&gt; Edit Scheme and then add it as a new Environment Variable yourself. IDEPreferLogStreaming=YES For me it didnt solve the issue though --- [Edit: it works for me now as well. Probably I was to quick saying it doesnt. Thanks for your feedback.]",
    "context_chunks": [
      {
        "text": "Logging Error: Failed to initialize logging system. Log messages may be missing. If this issue persists, try setting IDEPreferLogStreaming=YES in the active scheme actions environment variables. Has anyone else encountered this message? Where is IDEPreferLogStreaming located? I don't know what any of this means. It's building my app successfully but then loading it like its a computer using floppy discs (crazy slow). Any ideas? I tried wiping my OS and reinstalling. I've reinstalled Xcode twice now. Nothing. A colleague of mine is working on the same SwifUI project with no issues.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "To find IDEPreferLogStreaming, you need to go to Product -&gt; Scheme -&gt; Edit Scheme and then add it as a new Environment Variable yourself. IDEPreferLogStreaming=YES For me it didnt solve the issue though --- [Edit: it works for me now as well. Probably I was to quick saying it doesnt. Thanks for your feedback.]",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Set this in Product-&gt;Scheme-&gt;Edit Scheme-&gt;Run-&gt;Arguments-&gt;Environment Variable IDELogRedirectionPolicy oslogToStdio OS_ACTIVITY_MODE disable",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "swift",
        "xcode"
      ],
      "question_score": 86,
      "answer_score": 124,
      "created": "2024-03-08T19:52:20",
      "question_id": 78129981,
      "answer_id": 78204720
    }
  },
  {
    "question": "Why did Flask start failing with &quot;ImportError: cannot import name &#39;url_quote&#39; from &#39;werkzeug.urls&#39;&quot;?",
    "expected_answer": "I had the same problem. It is because Werkzeug 3.0.0 was released and Flask doesn't specify the dependency correctly (requirements says Werkzeug&gt;=2.2.0). This is why, Werkzeug 3.0.0 is still installed and Flask 2.2.2 isn't made for Werkzeug 3.0.0. Solution: Just set a fix version for Werkzeug such as Werkzeug==2.2.2 in your requirements.txt and it should work.",
    "context_chunks": [
      {
        "text": "Environment: Python 3.10.11 Flask==2.2.2 I run my Flask backend code in docker container, with BASE Image: FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime But when I run the pytest with version pytest 7.4.2, pip install pytest pytest it raised an Error, with logs: ==================================== ERRORS ==================================== _____________ ERROR collecting tests/test_fiftyone_utils_utils.py ______________ ImportError while importing test module '/builds/kw/data-auto-analysis-toolkit-backend/tests/test_fiftyone_utils_utils.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: /opt/conda/lib/python3.10/importlib/__init__.py:126: in import_module return _bootstrap._gcd_import(name[level:], package, level) tests/test_fiftyone_utils_utils.py:2: in &lt;module&gt; import daat # noqa: F401 /opt/conda/lib/python3.10/site-packages/daat-1.0.0-py3.10.egg/daat/__init__.py:1: in &lt;module&gt; from daat.app import app /opt/conda/lib/python3.10/site-packages/daat-1.0.0-py3.10.egg/daat/app/__init__.py:6: in &lt;module&gt; from flask import Flask, jsonify, request /opt/conda/lib/python3.10/site-packages/flask/__init__.py:5: in &lt;module&gt; from .app import Flask as Flask /opt/conda/lib/python3.10/site-packages/flask/app.py:30: in &lt;module&gt; from werkzeug.urls import url_quote E ImportError: cannot import name 'url_quote' from 'werkzeug.urls' (/opt/conda/lib/python3.10/site-packages/werkzeug/urls.py) My codes works well when I directly run it with python run.py run.py shown below from daat import app app.run(host='0.0.0.0') I guess it should be the pytest versions issue, because it used to work well without changing any related code, and I use pip install pytest without defined a specific version. And my backend runs well without pytest.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I had the same problem. It is because Werkzeug 3.0.0 was released and Flask doesn't specify the dependency correctly (requirements says Werkzeug&gt;=2.2.0). This is why, Werkzeug 3.0.0 is still installed and Flask 2.2.2 isn't made for Werkzeug 3.0.0. Solution: Just set a fix version for Werkzeug such as Werkzeug==2.2.2 in your requirements.txt and it should work.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The root cause of this is that Werkzeug 3.0.0 removed previously deprecated code: https://werkzeug.palletsprojects.com/en/3.0.x/changes/#version-3-0-0 Please update your Flask version, Flask 2.2.2 is unsupported: https://github.com/pallets/flask/releases Anyway, you need to pin Werkzeug yourself then if you insist on using a deprecated version of Flask, or if your code is using url_quote directly then you can switch to the built-in urllib: from urllib.parse import quote as url_quote",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "flask",
        "pytest",
        "werkzeug"
      ],
      "question_score": 202,
      "answer_score": 341,
      "created": "2023-10-02T03:02:00",
      "question_id": 77213053,
      "answer_id": 77214086
    }
  },
  {
    "question": "How can I migrate from Poetry to UV package manager?",
    "expected_answer": "Another very recently released tool is migrate-to-uv. Run: uvx migrate-to-uv to migrate from Poetry or pipenv to uv. This will rewrite your pyproject.toml file and remove poetry.lock. You still need to uv lock afterwards.",
    "context_chunks": [
      {
        "text": "I'm planning to switch from poetry to the uv Python package manager, but I can't find any migration guides. Currently, I'm using Poetry and already have a pyproject.toml file. What key(s) should be modified or added to migrate properly to uv? Here’s the current pyproject.toml structure: [tool.poetry] name = &quot;name&quot; version = &quot;1.6.0&quot; description = &quot;&quot; authors = [ &quot;...&quot;, ] maintainers = [ &quot;...&quot;, ] readme = &quot;README.md&quot; [tool.poetry.dependencies] python = &quot;^3.12&quot; fastapi = &quot;^0.115.2&quot; uvicorn = { version = &quot;^0.32.0&quot;, extras = [&quot;standard&quot;] } pydantic = &quot;^2.5.3&quot; pydantic-settings = &quot;^2&quot; [tool.poetry.group.dev.dependencies] pytest = &quot;^8.3.3&quot; flake8 = &quot;~7.1.1&quot; mypy = &quot;^1.12&quot; [tool.isort] profile = &quot;black&quot; multi_line_output = 3 [tool.mypy] strict = true ignore_missing_imports = true [tool.pytest.ini_options] filterwarnings = [ &quot;error&quot;, &quot;ignore::DeprecationWarning&quot;, &quot;ignore:.*unclosed.*:ResourceWarning&quot;, ] env = [ &quot;...=...&quot;, &quot;...=...&quot;, ] [tool.coverage.run] omit = [ &quot;...=...&quot;, &quot;...=...&quot;, ] [tool.coverage.report] exclude_lines = [ &quot;pragma: no cover&quot;, &quot;if TYPE_CHECKING&quot;, ] [build-system] requires = [&quot;poetry-core&gt;=1.0.0&quot;] build-backend = &quot;poetry.core.masonry.api&quot; Additionally, in the [build-system] section, I currently have poetry-core. Should this be replaced with something specific for uv during the migration?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Another very recently released tool is migrate-to-uv. Run: uvx migrate-to-uv to migrate from Poetry or pipenv to uv. This will rewrite your pyproject.toml file and remove poetry.lock. You still need to uv lock afterwards.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "First, you need to convert your Poetry-compatible pyproject.toml to be compatible with uv. You can actually do it quite easily with pdm (originally suggested here). uvx pdm import pyproject.toml Remove all poetry sections in the pyproject (i.e., [tool.poetry...] sections) Replace [tool.pdm.dev-dependencies] with [dependency-groups] Done. Your pyproject.toml file is now compatible with UV and you can do uv lock or uv sync. Having said that, it is likely that uv will resolve completely different versions of packages than what you already had in your poetry.lock. That doesn't have to be a problem, but can. It was a problem in my case. To work around this, I used a small script to hard code all the minor/patch versions of my packages in pyproject.toml before converting to the uv-compatible format. Then, after conversion, I would create the uv.lock with uv lock. Then, I'd undo the hard coded versions in the original pyproject.toml file and convert again. Because uv by default will prefer versions from the lock, you now have all your versions pinned as they were in Poetry and mostly the same packages in the lock (dependencies of dependencies can be different). It should be safe to do uv sync without worrying about things breaking due to versions change. Below is the script I used to hard code minor/patch versions in file pyproject.toml. It's not perfect and will create incorrect results for dependencies with extras/from Git, etc. but these are easy to tweak manually. from pathlib import Path import toml # Load poetry.lock and pyproject.toml lockfile_path = Path(&quot;poetry.lock&quot;) pyproject_path = Path(&quot;pyproject.toml&quot;) # Parse the lock file with lockfile_path.open(&quot;r&quot;) as lockfile: lock_data = toml.load(lockfile) # Parse pyproject.toml with pyproject_path.open(&quot;r&quot;) as pyproject_file: pyproject_data = toml.load(pyproject_file) # Build a map of package names to their specific versions from poetry.lock locked_versions = {package[&quot;name&quot;]: package[&quot;version&quot;] for package in lock_data[&quot;package&quot;]} # Helper function to update dependencies in a given section def update_dependencies(dependencies): for dep_name, dep_constraint in dependencies.items(): if dep_name in locked_versions: dependencies[dep_name] = locked_versions[dep_name] print(f&quot;Updated {dep_name} to version {locked_versions[dep_name]}&quot;) else: print(f&quot;{dep_name} not found in poetry.lock, skipping.&quot;) # Update dependencies in pyproject.toml update_dependencies(pyproject_data[&quot;tool&quot;][&quot;poetry&quot;][&quot;dependencies&quot;]) update_dependencies(pyproject_data[&quot;tool&quot;][&quot;poetry&quot;][&quot;group&quot;][&quot;dev&quot;][&quot;dependencies&quot;]) # Write the updated pyproject.toml with pyproject_path.open(&quot;w&quot;) as pyproject_file: toml.dump(pyproject_data, pyproject_file) print(&quot;pyproject.toml has been updated with specific versions.&quot;)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-poetry",
        "uv"
      ],
      "question_score": 55,
      "answer_score": 54,
      "created": "2024-10-23T16:29:39",
      "question_id": 79118841,
      "answer_id": 79316385
    }
  },
  {
    "question": "Why did I get an error ModuleNotFoundError: No module named &#39;distutils&#39;?",
    "expected_answer": "Python 3.12 does not come with a stdlib distutils module (changelog), because distutils was deprecated in 3.10 and removed in 3.12. See PEP 632 – Deprecate distutils module. You can still use distutils on Python 3.12+ by installing setuptools. When that doesn't work, you may need stay on Python &lt; 3.12 until the 3rd-party package (skfuzzy in this case) publishes an updated release for Python 3.12 support.",
    "context_chunks": [
      {
        "text": "I've installed scikit-fuzzy but when I import skfuzzy as fuzz I get an error ModuleNotFoundError: No module named 'distutils'&quot; I already tried to pip uninstall distutils and got this output Note: you may need to restart the kernel to use updated packages. WARNING: Skipping distutils as it is not installed. Then I tried to install it again pip install distutils Note: you may need to restart the kernel to use updated packages. ERROR: Could not find a version that satisfies the requirement distutils (from versions: none) ERROR: No matching distribution found for distutils Where did I go wrong? This question addresses the problem from the perspective of installing a library. For developing a library, see How can one fully replace distutils, which is deprecated in 3.10?.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Python 3.12 does not come with a stdlib distutils module (changelog), because distutils was deprecated in 3.10 and removed in 3.12. See PEP 632 – Deprecate distutils module. You can still use distutils on Python 3.12+ by installing setuptools. When that doesn't work, you may need stay on Python &lt; 3.12 until the 3rd-party package (skfuzzy in this case) publishes an updated release for Python 3.12 support.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "distutils module is used to install python packages. This module usually installed as part of python installation. In python v3.12, distutils module removed. This means your local doesn't have any tool in place to support python package installation. To resolve this issue, install setuptools for same purpose. Run the command python3 -m pip install setuptools",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "setuptools",
        "distutils",
        "skfuzzy",
        "python-3.12"
      ],
      "question_score": 60,
      "answer_score": 92,
      "created": "2023-10-05T01:54:27",
      "question_id": 77233855,
      "answer_id": 77233866
    }
  },
  {
    "question": "Alternative to .concat() of empty dataframe, now that it is being deprecated?",
    "expected_answer": "To be precise, concat is not deprecated (and won't be IMHO) but I can trigger this FutureWarning in 2.1.1 with the following example, while df2 being an empty DataFrame with a different dtypes than df1 : df1 = pd.DataFrame({&quot;A&quot;: [.1, .2, .3]}) df2 = pd.DataFrame(columns=[&quot;A&quot;], dtype=&quot;object&quot;) out = pd.concat([df1, df2]) print(out) A 0 0.1 1 0.2 2 0.3 As a solution in your case, you can try something like you did : out = (df1.copy() if df2.empty else df2.copy() if df1.empty else pd.concat([df1, df2]) # if both DataFrames non empty ) Or maybe even this one? : out = pd.concat([df1.astype(df2.dtypes), df2.astype(df1.dtypes)])",
    "context_chunks": [
      {
        "text": "I have two dataframes that can both be empty, and I want to concat them. Before I could just do : output_df= pd.concat([df1, df2]) But now I run into FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. An easy fix would be: if not df1.empty and not df2.empty: result_df = pd.concat([df1, df2], axis=0) elif not df1.empty: result_df = df1.copy() elif not df2.empty: result_df = df2.copy() else: result_df = pd.DataFrame() But that seems pretty ugly. Does anyone have a better solution ? FYI: this appeared after pandas released v2.1.0",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "To be precise, concat is not deprecated (and won't be IMHO) but I can trigger this FutureWarning in 2.1.1 with the following example, while df2 being an empty DataFrame with a different dtypes than df1 : df1 = pd.DataFrame({&quot;A&quot;: [.1, .2, .3]}) df2 = pd.DataFrame(columns=[&quot;A&quot;], dtype=&quot;object&quot;) out = pd.concat([df1, df2]) print(out) A 0 0.1 1 0.2 2 0.3 As a solution in your case, you can try something like you did : out = (df1.copy() if df2.empty else df2.copy() if df1.empty else pd.concat([df1, df2]) # if both DataFrames non empty ) Or maybe even this one? : out = pd.concat([df1.astype(df2.dtypes), df2.astype(df1.dtypes)])",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I found this solution based on @Timeless answer the most &quot;non-ugly&quot; for me. In [1]: import pandas as pd In [2]: df = pd.DataFrame([], columns=['A', 'B']) In [3]: df = pd.concat([ ...: df if not df.empty else None, ...: pd.DataFrame([{'A': 1.1, 'B': 2.2}]) ...: ]) In [4]: df Out[4]: A B 0 1.1 2.2",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "concatenation"
      ],
      "question_score": 48,
      "answer_score": 26,
      "created": "2023-10-08T18:12:04",
      "question_id": 77254777,
      "answer_id": 77254951
    }
  },
  {
    "question": "OpenAI Python Package Error: &#39;ChatCompletion&#39; object is not subscriptable",
    "expected_answer": "In the latest OpenAI package the response.choices object type is changed and in this way you must read the response: print(response.choices[0].message.content) The complete working code: from openai import OpenAI client = OpenAI(api_key='YourKey') GPT_MODEL = &quot;gpt-4-1106-preview&quot; #&quot;gpt-3.5-turbo-1106&quot; messages = [ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: 'You answer question about Web services.' }, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: 'the user message'}, ] response = client.chat.completions.create( model=GPT_MODEL, messages=messages, temperature=0 ) response_message = response.choices[0].message.content print(response_message ) See this example in the project README.",
    "context_chunks": [
      {
        "text": "After updating my OpenAI package to version 1.1.1, I got this error when trying to read the ChatGPT API response: 'ChatCompletion' object is not subscriptable Here is my code: messages = [ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: '''You answer question about some service''' }, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: 'The user question is ...'}, ] response = client.chat.completions.create( model=model, messages=messages, temperature=0 ) response_message = response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;] How can I resolve this error?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In the latest OpenAI package the response.choices object type is changed and in this way you must read the response: print(response.choices[0].message.content) The complete working code: from openai import OpenAI client = OpenAI(api_key='YourKey') GPT_MODEL = &quot;gpt-4-1106-preview&quot; #&quot;gpt-3.5-turbo-1106&quot; messages = [ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: 'You answer question about Web services.' }, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: 'the user message'}, ] response = client.chat.completions.create( model=GPT_MODEL, messages=messages, temperature=0 ) response_message = response.choices[0].message.content print(response_message ) See this example in the project README.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This error relates to the fact that since openai==1.0.0, the response objects (e.g. ChatCompletions, Assistant etc.) are pydantic models. Previously in openai&lt;1.0.0, the response objects were OpenAIObject which was sub-classed from a Python dictionary. However, if you want to deal with a dictionary, instead of the pydantic model, you can do so by first converting the response into a dictionary using .model_dump(). from openai import OpenAI client = OpenAI(api_key=&quot;YourKey&quot;) response = client.chat.completions.create( model=&quot;gpt-4o-mini&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You answer question about Web services.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi!&quot;} ], temperature=0 ) response_dict = response.model_dump() # &lt;--- convert to dictionary response_message = response_dict[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;] # &lt;--- lookup the dict The response looks like the following: ChatCompletion(id='chatcmpl-9yLjfDnwfXv48EPAVfUDFK9ZdO1iA', choices=[Choice(finish _reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hel lo! How can I assist you today?', role='assistant', function_call=None, tool_calls =None, refusal=None))], created=1724170259, model='gpt-4o-mini-2024-07-18', object ='chat.completion', service_tier=None, system_fingerprint='fp_48196bc89a', usage=C ompletionUsage(completion_tokens=9, prompt_tokens=20, total_tokens=29)) You can verify that it is a pydantic.BaseModel: from pydantic import BaseModel print(isinstance(response, BaseModel)) # True Using .model_dump(), we can convert the above object into a dictionary: { &quot;id&quot;: &quot;chatcmpl-9yLjfDnwfXv48EPAVfUDFK9ZdO1iA&quot;, &quot;choices&quot;: [ { &quot;finish_reason&quot;: &quot;stop&quot;, &quot;index&quot;: 0, &quot;logprobs&quot;: None, &quot;message&quot;: { &quot;content&quot;: &quot;Hello! How can I assist you today?&quot;, &quot;role&quot;: &quot;assistant&quot;, &quot;function_call&quot;: None, &quot;tool_calls&quot;: None, &quot;refusal&quot;: None } } ], &quot;created&quot;: 1724170259, &quot;model&quot;: &quot;gpt-4o-mini-2024-07-18&quot;, &quot;object&quot;: &quot;chat.completion&quot;, &quot;service_tier&quot;: None, &quot;system_fingerprint&quot;: &quot;fp_48196bc89a&quot;, &quot;usage&quot;: { &quot;completion_tokens&quot;: 9, &quot;prompt_tokens&quot;: 20, &quot;total_tokens&quot;: 29 } }",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "typeerror",
        "openai-api",
        "chatgpt-api"
      ],
      "question_score": 51,
      "answer_score": 100,
      "created": "2023-11-08T09:35:53",
      "question_id": 77444332,
      "answer_id": 77444334
    }
  },
  {
    "question": "Why list comprehensions create a function internally?",
    "expected_answer": "The main logic of creating a function is to isolate the comprehension’s iteration variablepeps.python.org. By creating a function, Comprehension iteration variables remain isolated and don’t overwrite a variable of the same name in the outer scope, nor are they visible after the comprehension However, this is inefficient at runtime. Due to this reason, python-3.12 implemented an optimization called comprehension inlining(PEP 709)peps.python.org which will no longer create a separate code objectpeps.python.org. Dictionary, list, and set comprehensions are now inlined, rather than creating a new single-use function object for each execution of the comprehension. This speeds up execution of a comprehension by up to two times. See PEP 709 for further details. Here is the output for the same code disassembled with python-3.12: >>> import dis >>> dis.dis(\"[True for _ in ()]\") 0 0 RESUME 0 1 2 LOAD_CONST 0 (()) 4 GET_ITER 6 LOAD_FAST_AND_CLEAR 0 (_) 8 SWAP 2 10 BUILD_LIST 0 12 SWAP 2 >> 14 FOR_ITER 4 (to 26) 18 STORE_FAST 0 (_) 20 LOAD_CONST 1 (True) 22 LIST_APPEND 2 24 JUMP_BACKWARD 6 (to 14) >> 26 END_FOR 28 SWAP 2 30 STORE_FAST 0 (_) 32 RETURN_VALUE >> 34 SWAP 2 36 POP_TOP 38 SWAP 2 40 STORE_FAST 0 (_) 42 RERAISE 0 ExceptionTable: 10 to 26 -> 34 [2] As you can see, there is no longer a MAKE_FUNCTION opcode nor a separate code object. Instead python-3.12 uses LOAD_FAST_AND_CLEARdocs.python.org(at offset 6) and STORE_FAST(at offset 30) opcodes to provide the isolation for the iteration variable. Quoting from the Specification sectionpeps.python.org of the PEP 709: Isolation of the x iteration variable is achieved by the combination of the new LOAD_FAST_AND_CLEAR opcode at offset 6, which saves any outer value of x on the stack before running the comprehension, and 30 STORE_FAST, which restores the outer value of x (if any) after running the comprehension. In addition to that, in python-3.12 there is no longer a separate frame for the comprehension in tracebacks. Traceback in &lt;python-3.12 Traceback in python-3.12 &gt;&gt;&gt; [1 / 0 for i in range(10)]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;listcomp&gt;ZeroDivisionError: division by zero &gt;&gt;&gt; [1 / 0 for i in range(10)]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;ZeroDivisionError: division by zero And here is the benchmark resultspeps.python.org(measured with MacOS M2): $ python3.10 -m pyperf timeit -s 'l = [1]' '[x for x in l]' Mean +- std dev: 108 ns +- 3 ns $ python3.12 -m pyperf timeit -s 'l = [1]' '[x for x in l]' Mean +- std dev: 60.9 ns +- 0.3 ns",
    "context_chunks": [
      {
        "text": "This is disassembly of a list comprehension in python-3.10: Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import dis &gt;&gt;&gt; &gt;&gt;&gt; dis.dis(&quot;[True for _ in ()]&quot;) 1 0 LOAD_CONST 0 (&lt;code object &lt;listcomp&gt; at 0x7fea68e0dc60, file &quot;&lt;dis&gt;&quot;, line 1&gt;) 2 LOAD_CONST 1 ('&lt;listcomp&gt;') 4 MAKE_FUNCTION 0 6 LOAD_CONST 2 (()) 8 GET_ITER 10 CALL_FUNCTION 1 12 RETURN_VALUE Disassembly of &lt;code object &lt;listcomp&gt; at 0x7fea68e0dc60, file &quot;&lt;dis&gt;&quot;, line 1&gt;: 1 0 BUILD_LIST 0 2 LOAD_FAST 0 (.0) &gt;&gt; 4 FOR_ITER 4 (to 14) 6 STORE_FAST 1 (_) 8 LOAD_CONST 0 (True) 10 LIST_APPEND 2 12 JUMP_ABSOLUTE 2 (to 4) &gt;&gt; 14 RETURN_VALUE From what I understand it creates a code object called listcomp which does the actual iteration and return the result list, and immediately call it. I can't figure out the need to create a separate function to execute this job. Is this kind of an optimization trick?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The main logic of creating a function is to isolate the comprehension’s iteration variablepeps.python.org. By creating a function, Comprehension iteration variables remain isolated and don’t overwrite a variable of the same name in the outer scope, nor are they visible after the comprehension However, this is inefficient at runtime. Due to this reason, python-3.12 implemented an optimization called comprehension inlining(PEP 709)peps.python.org which will no longer create a separate code objectpeps.python.org. Dictionary, list, and set comprehensions are now inlined, rather than creating a new single-use function object for each execution of the comprehension. This speeds up execution of a comprehension by up to two times. See PEP 709 for further details. Here is the output for the same code disassembled with python-3.12: >>> import dis >>> dis.dis(\"[True for _ in ()]\") 0 0 RESUME 0 1 2 LOAD_CONST 0 (()) 4 GET_ITER 6 LOAD_FAST_AND_CLEAR 0 (_) 8 SWAP 2 10 BUILD_LIST 0 12 SWAP 2 >> 14 FOR_ITER 4 (to 26) 18 STORE_FAST 0 (_) 20 LOAD_CONST 1 (True) 22 LIST_APPEND 2 24 JUMP_BACKWARD 6 (to 14) >> 26 END_FOR 28 SWAP 2 30 STORE_FAST 0 (_) 32 RETURN_VALUE >> 34 SWAP 2 36 POP_TOP 38 SWAP 2 40 STORE_FAST 0 (_) 42 RERAISE 0 ExceptionTable: 10 to 26 -> 34 [2] As you can see, there is no longer a MAKE_FUNCTION opcode nor a separate code object. Instead python-3.12 uses LOAD_FAST_AND_CLEARdocs.python.org(at offset 6) and STORE_FAST(at offset 30) opcodes to provide the isolation for the iteration variable. Quoting from the Specification sectionpeps.python.org of the PEP 709: Isolation of the x iteration variable is achieved by the combination of the new LOAD_FAST_AND_CLEAR opcode at offset 6, which saves any outer value of x on the stack before running the comprehension, and 30 STORE_FAST, which restores the outer value of x (if any) after running the comprehension. In addition to that, in python-3.12 there is no longer a separate frame for the comprehension in tracebacks. Traceback in &lt;python-3.12 Traceback in python-3.12 &gt;&gt;&gt; [1 / 0 for i in range(10)]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;listcomp&gt;ZeroDivisionError: division by zero &gt;&gt;&gt; [1 / 0 for i in range(10)]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;ZeroDivisionError: division by zero And here is the benchmark resultspeps.python.org(measured with MacOS M2): $ python3.10 -m pyperf timeit -s 'l = [1]' '[x for x in l]' Mean +- std dev: 108 ns +- 3 ns $ python3.12 -m pyperf timeit -s 'l = [1]' '[x for x in l]' Mean +- std dev: 60.9 ns +- 0.3 ns",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "List comprehensions in Python don't create a function internally. They are a concise way to create lists. If you're referring to lambda functions, they can be used within a list comprehension to apply a function to each element. However, list comprehensions themselves are not functions; they're a syntactic construct for creating lists.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "list-comprehension",
        "cpython",
        "python-internals"
      ],
      "question_score": 49,
      "answer_score": 69,
      "created": "2023-11-16T12:53:19",
      "question_id": 77494964,
      "answer_id": 77494995
    }
  },
  {
    "question": "TypeError: WebDriver.__init__() got an unexpected keyword argument &#39;executable_path&#39; in Selenium Python",
    "expected_answer": "This is due to changes in selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that executable_path was removed. If you want to pass in an executable_path, you'll have to use the service arg now. from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service(executable_path='./chromedriver.exe') options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... driver.quit()",
    "context_chunks": [
      {
        "text": "My code: from selenium import webdriver from selenium.webdriver.chrome.options import Options option = webdriver.ChromeOptions() driver = webdriver.Chrome(executable_path='./chromedriver.exe', options=option) driver.get('https://www.google.com/') Output: WebDriver.__init__() got an unexpected keyword argument 'executable_path' I'm trying to create a script to log in to a website. When I try to run this script, it gives me this error: WebDriver.__init__() got an unexpected keyword argument 'executable_path'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is due to changes in selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that executable_path was removed. If you want to pass in an executable_path, you'll have to use the service arg now. from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service(executable_path='./chromedriver.exe') options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... driver.quit()",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Note: Remove executable_url from the argument, because you have installed the latest version of Selenium if you have Selenium above the 4.6.0, you don't need to add executable_url, and in the latest version of Selenium, you don't need to download webdriver. Just copy the below code and run the your Python file simple. from selenium import webdriver driver=webdriver.Chrome() driver.get(&quot;https://www.facebook.com/&quot;)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "google-chrome",
        "selenium-webdriver",
        "selenium-chromedriver"
      ],
      "question_score": 54,
      "answer_score": 86,
      "created": "2023-06-25T12:55:13",
      "question_id": 76550506,
      "answer_id": 76550727
    }
  },
  {
    "question": "Deprecation Warning with groupby.apply",
    "expected_answer": "About include_groups parameter The include_groups parameter of DataFrameGroupBy.apply is new in pandas version 2.2.0. It is basically a transition period (2.2.0 -&gt; 3.0) parameter added to help communicating a changing behavior (with warnings) and to tackle pandas Issue 7155. In most cases you should be able to just set it to False to silent the warning (see below). Setup Let's say you have a pandas DataFrame df and a dummy function myfunc for apply, and you want to Group by column 'c' Apply myfunc on each group &gt;&gt;&gt; df a value c 0 foo 10 cat1 1 bar 20 cat2 2 baz 30 cat1 3 quux 40 cat2 &gt;&gt;&gt; def myfunc(x): print(x, '\\n') include_groups = True (Old behavior) This is the default behavior in pandas &lt;2.2.0 (there is no include_groups parameter) pandas 2.2.0 and above (likely until 3.0) will still default to this but issue a DeprecationWarning. The grouping column(s), here 'c' is included in the DataFrameGroupBy &gt;&gt;&gt; df.groupby('c').apply(myfunc) a value c 0 foo 10 cat1 2 baz 30 cat1 a value c 1 bar 20 cat2 3 quux 40 cat2 Now as mentioned in Issue 7155, keeping the grouping column c in the dataframe passed to apply is unwanted behavior. Most people will not expect c to be present here. The answer of bue has actually an example how this could lead to bugs; apply on np.mean and expect there be less columns (causes a bug if your grouping column is numerical). include_groups = False (new behavior) This will remove the warning in the pandas &gt; 2.2.0 (&lt;3.0) This will be the default in future version of pandas (likely 3.0) This is what you likely would want to have; drop the grouping column 'c': &gt;&gt;&gt; df.groupby('c').apply(myfunc, include_groups=False) a value 0 foo 10 2 baz 30 a value 1 bar 20 3 quux 40 Circumventing need to use include_groups at all Option 1: Explicitly giving column names You may also skip the need for using the include_groups parameter at all by explicitly giving the list of the columns (as pointed out by the warning itself; &quot;..or explicitly select the grouping columns after groupby to silence this warning..&quot;, and Cahit in their answer), like this: &gt;&gt;&gt; df.groupby('c')[['a', 'value', 'c']].apply(myfunc) a value c 0 foo 10 cat1 2 baz 30 cat1 a value c 1 bar 20 cat2 3 quux 40 cat2 Empty DataFrame Columns: [] Index: [] Option 2: Setting the index before groupby You may also set the groupby column to the index, as pointed out by Stefan in the comments. &gt;&gt;&gt; df.set_index('c').groupby(level='c').apply(myfunc) a value c cat1 foo 10 cat1 baz 30 a value c cat2 bar 20 cat2 quux 40 Empty DataFrame Columns: [] Index: [] Details just for this use case Your grouping columns are ['StartDate', 'Commodity', 'DealType'] In the apply function you use the following columns: ['MTMValue', 'FixedPriceStrike', 'Quantity'] i.e., you do not need any of the grouping columns in your apply, and therefore you can use include_groups=False which also removes the warning. fprice = df.groupby(['StartDate', 'Commodity', 'DealType']).apply(lambda group: -(group['MTMValue'].sum() - (group['FixedPriceStrike'] * group['Quantity']).sum()) / group['Quantity'].sum(), include_groups=False).reset_index(name='FloatPrice')",
    "context_chunks": [
      {
        "text": "I have a python script that reads in data from a csv file The code runs fine, but everytime it runs I get this Deprecation message: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning. the warning stems from this piece of code: fprice = df.groupby(['StartDate', 'Commodity', 'DealType']).apply(lambda group: -(group['MTMValue'].sum() - (group['FixedPriceStrike'] * group['Quantity']).sum()) / group['Quantity'].sum()).reset_index(name='FloatPrice') to my understanding, I am performing the apply function on my groupings,but then I am disregarding the groupings and not using them anymore to be apart of my dataframe. I am confused about the directions to silence the warning here is some sample data that this code uses: TradeID TradeDate Commodity StartDate ExpiryDate FixedPrice Quantity MTMValue -------- ---------- --------- --------- ---------- ---------- -------- --------- aaa 01/01/2024 (com1,com2) 01/01/2024 01/01/2024 10 10 100.00 bbb 01/01/2024 (com1,com2) 01/01/2024 01/01/2024 10 10 100.00 ccc 01/01/2024 (com1,com2) 01/01/2024 01/01/2024 10 10 100.00 and here is the expected output from this data: TradeID TradeDate Commodity StartDate ExpiryDate FixedPrice Quantity MTMValue FloatPrice -------- ---------- --------- --------- ---------- ---------- -------- --------- ---------- aaa 01/01/2024 (com1,com2) 01/01/2024 01/01/2024 10 10 100.00 0 bbb 01/01/2024 (com1,com2) 01/01/2024 01/01/2024 10 10 100.00 0 ccc 01/01/2024 (com1,com2) 01/01/2024 01/01/2024 10 10 100.00 0",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "About include_groups parameter The include_groups parameter of DataFrameGroupBy.apply is new in pandas version 2.2.0. It is basically a transition period (2.2.0 -&gt; 3.0) parameter added to help communicating a changing behavior (with warnings) and to tackle pandas Issue 7155. In most cases you should be able to just set it to False to silent the warning (see below). Setup Let's say you have a pandas DataFrame df and a dummy function myfunc for apply, and you want to Group by column 'c' Apply myfunc on each group &gt;&gt;&gt; df a value c 0 foo 10 cat1 1 bar 20 cat2 2 baz 30 cat1 3 quux 40 cat2 &gt;&gt;&gt; def myfunc(x): print(x, '\\n') include_groups = True (Old behavior) This is the default behavior in pandas &lt;2.2.0 (there is no include_groups parameter) pandas 2.2.0 and above (likely until 3.0) will still default to this but issue a DeprecationWarning. The grouping column(s), here 'c' is included in the DataFrameGroupBy &gt;&gt;&gt; df.groupby('c').apply(myfunc) a value c 0 foo 10 cat1 2 baz 30 cat1 a value c 1 bar 20 cat2 3 quux 40 cat2 Now as mentioned in Issue 7155, keeping the grouping column c in the dataframe passed to apply is unwanted behavior. Most people will not expect c to be present here. The answer of bue has actually an example how this could lead to bugs; apply on np.mean and expect there be less columns (causes a bug if your grouping column is numerical). include_groups = False (new behavior) This will remove the warning in the pandas &gt; 2.2.0 (&lt;3.0) This will be the default in future version of pandas (likely 3.0) This is what you likely would want to have; drop the grouping column 'c': &gt;&gt;&gt; df.groupby('c').apply(myfunc, include_groups=False) a value 0 foo 10 2 baz 30 a value 1 bar 20 3 quux 40 Circumventing need to use include_groups at all Option 1: Explicitly giving column names You may also skip the need for using the include_groups parameter at all by explicitly giving the list of the columns (as pointed out by the warning itself; &quot;..or explicitly select the grouping columns after groupby to silence this warning..&quot;, and Cahit in their answer), like this: &gt;&gt;&gt; df.groupby('c')[['a', 'value', 'c']].apply(myfunc) a value c 0 foo 10 cat1 2 baz 30 cat1 a value c 1 bar 20 cat2 3 quux 40 cat2 Empty DataFrame Columns: [] Index: [] Option 2: Setting the index before groupby You may also set the groupby column to the index, as pointed out by Stefan in the comments. &gt;&gt;&gt; df.set_index('c').groupby(level='c').apply(myfunc) a value c cat1 foo 10 cat1 baz 30 a value c cat2 bar 20 cat2 quux 40 Empty DataFrame Columns: [] Index: [] Details just for this use case Your grouping columns are ['StartDate', 'Commodity', 'DealType'] In the apply function you use the following columns: ['MTMValue', 'FixedPriceStrike', 'Quantity'] i.e., you do not need any of the grouping columns in your apply, and therefore you can use include_groups=False which also removes the warning. fprice = df.groupby(['StartDate', 'Commodity', 'DealType']).apply(lambda group: -(group['MTMValue'].sum() - (group['FixedPriceStrike'] * group['Quantity']).sum()) / group['Quantity'].sum(), include_groups=False).reset_index(name='FloatPrice')",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I made a very simple data frame, to understand what goes on. import pandas as pd import numpy as np df = pd.DataFrame([[1,1,1,2,2,2],[1,2,3,4,5,6]], index=['a','b']).T In []: print(df) Out[]: a b 0 1 1 1 1 2 2 1 3 3 2 4 4 2 5 5 2 6 we will now group by column a. have a look at the difference output with include_groups=True (the current default - pandas version 2.2.0) and include_groups=False which will be the only way allowed in the future. In []: df.groupby('a').apply(np.mean, include_groups=True) Out[]: a 1 1.5 2 3.5 dtype: float64 (1+1+1+1+2+3) / 6 = 1.5 (2+2+2+4+5+6) / 6 = 3.5 In []: df.groupby('a').apply(np.mean, include_groups=False) Out[]: a 1 2.0 2 5.0 dtype: float64 (1+2+3) / 3 = 2.0 (4+5+6) / 3 = 5.0 For me, this made things pretty clear. What the right thing is for your case, include_groups True or False, I don't know.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ],
      "question_score": 51,
      "answer_score": 84,
      "created": "2024-02-09T17:50:46",
      "question_id": 77969964,
      "answer_id": 78100669
    }
  },
  {
    "question": "&quot;ImportError: cannot import name &#39;triu&#39; from &#39;scipy.linalg&#39;&quot; when importing Gensim",
    "expected_answer": "I found the issue. The scipy.linalg functions tri, triu &amp; tril are deprecated and will be removed in SciPy 1.13. — SciPy 1.11.0 Release Notes § Deprecated features So, I installed SciPy v1.10.1 instead of the latest version and it was working well. pip install scipy==1.10.1",
    "context_chunks": [
      {
        "text": "I am trying to use Gensim, but running import gensim raises this error: Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/usr/local/lib/python3.10/dist-packages/gensim/__init__.py&quot;, line 11, in &lt;module&gt; from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils # noqa:F401 File &quot;/usr/local/lib/python3.10/dist-packages/gensim/corpora/__init__.py&quot;, line 6, in &lt;module&gt; from .indexedcorpus import IndexedCorpus # noqa:F401 must appear before the other classes File &quot;/usr/local/lib/python3.10/dist-packages/gensim/corpora/indexedcorpus.py&quot;, line 14, in &lt;module&gt; from gensim import interfaces, utils File &quot;/usr/local/lib/python3.10/dist-packages/gensim/interfaces.py&quot;, line 19, in &lt;module&gt; from gensim import utils, matutils File &quot;/usr/local/lib/python3.10/dist-packages/gensim/matutils.py&quot;, line 20, in &lt;module&gt; from scipy.linalg import get_blas_funcs, triu ImportError: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.10/dist-packages/scipy/linalg/__init__.py) Why is this happening and how can I fix it?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I found the issue. The scipy.linalg functions tri, triu &amp; tril are deprecated and will be removed in SciPy 1.13. — SciPy 1.11.0 Release Notes § Deprecated features So, I installed SciPy v1.10.1 instead of the latest version and it was working well. pip install scipy==1.10.1",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I wasn't able to use SciPy v1.10.1, but 1.12 seems to solve the problem too: pip install scipy==1.12",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "scipy",
        "gensim"
      ],
      "question_score": 48,
      "answer_score": 66,
      "created": "2024-04-05T10:01:46",
      "question_id": 78279136,
      "answer_id": 78279318
    }
  },
  {
    "question": "How to configure FastAPI logging so that it works both with Uvicorn locally and in production?",
    "expected_answer": "1. Setting up the uvicorn logger Straight from the documentation: Logging --log-config &lt;path&gt; - Logging configuration file. Options: dictConfig() formats: .json, .yaml. Any other format will be processed with fileConfig(). Set the formatters.default.use_colors and formatters.access.use_colors values to override the auto-detected behavior. If you wish to use a YAML file for your logging config, you will need to include PyYAML as a dependency for your project or install uvicorn with the [standard] optional extras. --log-level &lt;str&gt; - Set the log level. Options: 'critical', 'error', 'warning', 'info', 'debug', 'trace'. Default: 'info'. --no-access-log - Disable access log only, without changing log level. --use-colors / --no-use-colors - Enable / disable colorized formatting of the log records, in case this is not set it will be auto-detected. This option is ignored if the --log-config CLI option is used. Regarding the log level As shown above, the --log-level flag specifies the lowest severity log message the logger will handle, where trace is the lowest severity/level and critical is the highest one. For instance, if the level is set to info, the logger will only handle info, warning, error and critical messages, whereas debug and trace messages will be ignored. If the level is set to trace, the logger will handle all the messages. Running uvicorn from the command line When running uvicorn using the command line interface, you could set the log level as follows. On a side note, if one would like to disable the &quot;access log&quot; messages only, without changing the log level, they could use the --no-access-log flag (the --access-log flag is enabled by default). Moreover, in order to change the host and/or port, one could do that using --host 0.0.0.0 and/or --port 8000. In the example below, main refers to the filename of the application (e.g., main.py)—see this answer for more details. uvicorn main:app --log-level trace Running uvicorn programmatically To run uvicorn from within a Python program, you could use the following. One could set the logging level, using the log_level flag in uvicorn.run(), as shown below. Again, if one would like to disable the &quot;access log&quot; messages only, they could do that by setting the access_log argument to False (i.e., access_log=False). To change the host and/or port, one could use, for instance, host='0.0.0.0' and/or port=8000. uvicorn.run(app, log_level=&quot;trace&quot;) 2. Using the uvicorn logger to log custom messages too Uvicorn, as shown in its implementation here, internally uses various loggers such as uvicorn, uvicorn.access, uvicorn.error and uvicorn.asgi. The logger, however, that comes by the name uvicorn.error seems to be the one mostly used by Uvicorn, as shown here and here, for instance, to log various warnings, errors, as well as other type of information. On the other hand, uvicorn.access logger appears to be used for logging HTTP requests; for example, see here. For uvicorn.asgi logger, see here as well. Hence, one could use the uvicorn.error logger to log their own custom messages/errors, as shown in the example below, along with the uvicorn messages (again, the logging level could be changed using the log_level flag in uvicorn.run()) The uvicorn.error logger, as shown in the implementation here, will propagate a message by default to its ancestor logger, i.e., uvicorn. On a side note, the parent logger, in this case uvicorn, would normally pass on the message to the highest-level logger, known as the root logger, but the uvicorn logger seems to have propagate flag set to False (see the relevant implementation), meaning that its messages won't propagate to the root logger (which is perfectly fine—as described in the official Python documentation, it is strongly advised that you do not log to the root logger in your library). For the sake of completeness, it should be noted that in order to disable this behaviour—not that you have to—on uvicorn.error logger in the example below, one could set the propagate attribute to False for that logger as well, e.g., logger.propagate = False. main.py from fastapi import FastAPI import uvicorn import logging app = FastAPI(title='api') logger = logging.getLogger('uvicorn.error') @app.get('/') async def main(): logger.info('GET /') # or logger.debug(), logger.error(), etc. return 'success' if __name__ == '__main__': uvicorn.run(app, log_level=&quot;trace&quot;) 3. Using custom-formatted uvicorn loggers to log custom messages too This approach demonstrates how to customize the uvicorn loggers, as well as use them to log both uvicorn and custom messages. To define a custom format for the uvicorn loggers, one could use the log_config attribute in uvicorn.run() to pass a logging configuration dictionary (i.e., dictConfig()), as shown in the exmaple below, including the various schema details, such as formatters, handlers and loggers. You could then define the uvicorn.error logger in main.py, as demonstrated in the previous section, and use it across your application. For the file handler in the example below, RotatingFileHandler is used, in which: You can use the maxBytes and backupCount values to allow the file to rollover at a predetermined size. When the size is about to be exceeded, the file is closed and a new file is silently opened for output. Rollover occurs whenever the current log file is nearly maxBytes in length; but if either of maxBytes or backupCount is zero, rollover never occurs, so you generally want to set backupCount to at least 1, and have a non-zero maxBytes (by default, the file would grow indefinitely). When backupCount is non-zero, the system will save old log files by appending the extensions ‘.1’, ‘.2’ etc., to the filename. For example, with a backupCount of 5 and a base file name of app.log, you would get app.log, app.log.1, app.log.2, up to app.log.5. The file being written to is always app.log. When this file is filled, it is closed and renamed to app.log.1, and if files app.log.1, app.log.2, etc. exist, then they are renamed to app.log.2, app.log.3 etc. respectively. main.py from fastapi import FastAPI import uvicorn import logging import settings app = FastAPI(title='api') logger = logging.getLogger('uvicorn.error') @app.get('/') async def main(): logger.info('GET /') # or logger.debug(), logger.error(), etc. return 'success' if __name__ == '__main__': uvicorn.run(app, log_config=settings.LOGGING_CONFIG) settings.py LOGGING_CONFIG = { 'version': 1, 'disable_existing_loggers': True, 'formatters': { 'standard': { 'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s' }, 'custom_formatter': { 'format': &quot;%(asctime)s [%(processName)s: %(process)d] [%(threadName)s: %(thread)d] [%(levelname)s] %(name)s: %(message)s&quot; }, }, 'handlers': { 'default': { 'formatter': 'standard', 'class': 'logging.StreamHandler', 'stream': 'ext://sys.stdout', # Default is stderr }, 'stream_handler': { 'formatter': 'custom_formatter', 'class': 'logging.StreamHandler', 'stream': 'ext://sys.stdout', # Default is stderr }, 'file_handler': { 'formatter': 'custom_formatter', 'class': 'logging.handlers.RotatingFileHandler', 'filename': 'app.log', 'maxBytes': 1024 * 1024 * 1, # = 1MB 'backupCount': 3, }, }, 'loggers': { 'uvicorn': { 'handlers': ['default', 'file_handler'], 'level': 'TRACE', 'propagate': False }, 'uvicorn.access': { 'handlers': ['stream_handler', 'file_handler'], 'level': 'TRACE', 'propagate': False }, 'uvicorn.error': { 'handlers': ['stream_handler', 'file_handler'], 'level': 'TRACE', 'propagate': False }, 'uvicorn.asgi': { 'handlers': ['stream_handler', 'file_handler'], 'level': 'TRACE', 'propagate': False }, }, } Custom JSON Formatter (simple) One could have the log messages displayed and/or saved in JSON format, if they wish, by either using a simple JSON format such as: settings.py LOGGING_CONFIG = { 'version': 1, 'disable_existing_loggers': True, 'formatters': { 'standard': ..., # same as above or customize that as well 'custom_formatter': { 'format': &quot;{'time':'%(asctime)s', 'process_name': '%(processName)s', 'process_id': '%(process)s', 'thread_name': '%(threadName)s', 'thread_id': '%(thread)s','level': '%(levelname)s', 'logger_name': '%(name)s', 'message': '%(message)s'}&quot; }, }, ... # the rest is the same as in the original settings.py above } Custom JSON Formatter (elegant) Or, a more elegant version, as demonstrated previously in this answer and as shown below. Please refer to that answer and this one for further details, as well as the relevant middleware and methods for logging Request and Response information, which would go into the extra parameter when logging messages in the application, for example: logger.info(&quot;some msg&quot;, extra={'extra_info': get_extra_info(request, response)}) If you don't need that kind of information, please feel free not to use the extra parameter, as well as remove the extra_info part from the get_log() function below. settings.py import logging, json class CustomJSONFormatter(logging.Formatter): def __init__(self, fmt): logging.Formatter.__init__(self, fmt) def format(self, record): logging.Formatter.format(self, record) return json.dumps(get_log(record), indent=2) def get_log(record): d = { &quot;time&quot;: record.asctime, &quot;process_name&quot;: record.processName, &quot;process_id&quot;: record.process, &quot;thread_name&quot;: record.threadName, &quot;thread_id&quot;: record.thread, &quot;level&quot;: record.levelname, &quot;logger_name&quot;: record.name, &quot;pathname&quot;: record.pathname, &quot;line&quot;: record.lineno, &quot;message&quot;: record.message, } if hasattr(record, &quot;extra_info&quot;): d[&quot;req&quot;] = record.extra_info[&quot;req&quot;] d[&quot;res&quot;] = record.extra_info[&quot;res&quot;] return d LOGGING_CONFIG = { 'version': 1, 'disable_existing_loggers': True, 'formatters': { 'standard': ..., # same as above or customize that as well 'custom_formatter': { '()': lambda: CustomJSONFormatter(fmt='%(asctime)s') }, }, ... # the rest is the same as in the original settings.py above } Output example: { &quot;time&quot;: &quot;2024-10-27 11:05:00,300&quot;, &quot;process_name&quot;: &quot;MainProcess&quot;, &quot;process_id&quot;: 4102, &quot;thread_name&quot;: &quot;AnyIO worker thread&quot;, &quot;thread_id&quot;: 1147, &quot;level&quot;: &quot;INFO&quot;, &quot;logger_name&quot;: &quot;uvicorn.error&quot;, &quot;pathname&quot;: &quot;C:\\\\...&quot;, &quot;line&quot;: 33, &quot;message&quot;: &quot;GET /&quot;, &quot;req&quot;: { &quot;url&quot;: &quot;/&quot;, &quot;headers&quot;: { &quot;host&quot;: &quot;localhost:8000&quot;, &quot;user-agent&quot;: &quot;Mozilla...&quot;, &quot;accept&quot;: &quot;text/html,application/xhtml+xml,...&quot; }, &quot;method&quot;: &quot;GET&quot;, &quot;http_version&quot;: &quot;1.1&quot;, &quot;original_url&quot;: &quot;/&quot;, &quot;query&quot;: {} }, &quot;res&quot;: { &quot;status_code&quot;: 200, &quot;status&quot;: &quot;OK&quot; } } 4. Using a custom Python logger separate from uvicorn loggers In case one wished having a separate custom Python logger instead of customizing the existing uvicorn loggers, as demonstrated earlier, they would need to add a StreamHandler and/or FileHandler and set the desired level, i.e., DEBUG, INFO, WARNING, etc.—the lowest level offered by Python's logging module is DEBUG, with the default level being WARNING (if one is interested in adding a custom log level, see this post). You could either do that using a dictConfig(), as shown earlier, or directly using the logging's module functions and classes. The following example is based on this answer, which demonstrates how to customize the format of the logging messages in JSON (hence, see that answer, if you are looking for a similar format presented in the previous section), as well as this answer that shows how to log both the request and response bodies in the background. More details and examples can also be found in Python's official documentation page here. You may also want to have a look at all the available LogRecord attributes that can be used to format the logging records. Setting log_level=&quot;trace&quot; in uvicorn.run() would set the level of the uvicorn logger to trace, as described earlier—in case one needed that as well. Also, one could still customize the uvicorn loggers, if they wish, using the LOGGING_CONFIG dictionary provided in the previous section and passing it to the settings, i.e., uvicorn.run(..., log_config=settings.LOGGING_CONFIG). In that way, one could get the uvicorn logs in an elegant format and have them saved to a file on disk as well. Working Example from fastapi import FastAPI import logging import uvicorn import sys app = FastAPI() logger = logging.getLogger(__name__) logger.setLevel(logging.DEBUG) formatter = logging.Formatter(&quot;%(asctime)s [%(processName)s: %(process)d] [%(threadName)s: %(thread)d] [%(levelname)s] %(name)s: %(message)s&quot;) stream_handler = logging.StreamHandler(sys.stdout) stream_handler.setFormatter(formatter) file_handler = logging.FileHandler(&quot;info.log&quot;) file_handler.setFormatter(formatter) logger.addHandler(stream_handler) logger.addHandler(file_handler) logger.info('API is starting up') @app.get('/') async def main(): logger.info('GET /') return 'ok' if __name__ == '__main__': uvicorn.run(app, log_level=&quot;trace&quot;) # or `log_config=settings.LOGGING_CONFIG` 5. Final Notes In each of the above cases, one may wish to initialize the logger at startup inside a lifespan handler, and then add it to app.state or request.state, so that it can be accessed outside the main file of the application as well; for instance, from a submodule that uses APIRouter to create endpoints, lying inside a routers package, which is normally the case when building Bigger Applications. To do that, please have a look at this answer.",
    "context_chunks": [
      {
        "text": "I have the following FastAPI application: from fastapi import FastAPI import logging import uvicorn app = FastAPI(title=&quot;api&quot;) LOG = logging.getLogger(__name__) LOG.info(&quot;API is starting up&quot;) LOG.info(uvicorn.Config.asgi_version) @app.get(&quot;/&quot;) async def get_index(): LOG.info(&quot;GET /&quot;) return {&quot;Hello&quot;: &quot;Api&quot;} The application locally is run with: uvicorn api:app --reload INFO: Will watch for changes in these directories: ['/Users/user/code/backend/api'] INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [44258] using StatReload INFO: Started server process [44260] INFO: Waiting for application startup. INFO: Application startup complete. It is not logging any of the startup messages. Later on when sending an HTTP request to the API: INFO: 127.0.0.1:50538 - &quot;POST /api/v1/endpoint HTTP/1.1&quot; 200 OK in the function body, there is LOG.info(&quot;example&quot;) that does not get logged either. Is there a way to make FastAPI logging work with Uvicorn and also in production (independently of the execution environments like Uvicorn)?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "1. Setting up the uvicorn logger Straight from the documentation: Logging --log-config &lt;path&gt; - Logging configuration file. Options: dictConfig() formats: .json, .yaml. Any other format will be processed with fileConfig(). Set the formatters.default.use_colors and formatters.access.use_colors values to override the auto-detected behavior. If you wish to use a YAML file for your logging config, you will need to include PyYAML as a dependency for your project or install uvicorn with the [standard] optional extras. --log-level &lt;str&gt; - Set the log level. Options: 'critical', 'error', 'warning', 'info', 'debug', 'trace'. Default: 'info'. --no-access-log - Disable access log only, without changing log level. --use-colors / --no-use-colors - Enable / disable colorized formatting of the log records, in case this is not set it will be auto-detected. This option is ignored if the --log-config CLI option is used. Regarding the log level As shown above, the --log-level flag specifies the lowest severity log message the logger will handle, where trace is the lowest severity/level and critical is the highest one. For instance, if the level is set to info, the logger will only handle info, warning, error and critical messages, whereas debug and trace messages will be ignored. If the level is set to trace, the logger will handle all the messages. Running uvicorn from the command line When running uvicorn using the command line interface, you could set the log level as follows. On a side note, if one would like to disable the &quot;access log&quot; messages only, without changing the log level, they could use the --no-access-log flag (the --access-log flag is enabled by default). Moreover, in order to change the host and/or port, one could do that using --host 0.0.0.0 and/or --port 8000. In the example below, main refers to the filename of the application (e.g., main.py)—see this answer for more details. uvicorn main:app --log-level trace Running uvicorn programmatically To run uvicorn from within a Python program, you could use the following. One could set the logging level, using the log_level flag in uvicorn.run(), as shown below. Again, if one would like to disable the &quot;access log&quot; messages only, they could do that by setting the access_log argument to False (i.e., access_log=False). To change the host and/or port, one could use, for instance, host='0.0.0.0' and/or port=8000. uvicorn.run(app, log_level=&quot;trace&quot;) 2. Using the uvicorn logger to log custom messages too Uvicorn, as shown in its implementation here, internally uses various loggers such as uvicorn, uvicorn.access, uvicorn.error and uvicorn.asgi. The logger, however, that comes by the name uvicorn.error seems to be the one mostly used by Uvicorn, as shown here and here, for instance, to log various warnings, errors, as well as other type of information. On the other hand, uvicorn.access logger appears to be used for logging HTTP requests; for example, see here. For uvicorn.asgi logger, see here as well. Hence, one could use the uvicorn.error logger to log their own custom messages/errors, as shown in the example below, along with the uvicorn messages (again, the logging level could be changed using the log_level flag in uvicorn.run()) The uvicorn.error logger, as shown in the implementation here, will propagate a message by default to its ancestor logger, i.e., uvicorn. On a side note, the parent logger, in this case uvicorn, would normally pass on the message to the highest-level logger, known as the root logger, but the uvicorn logger seems to have propagate flag set to False (see the relevant implementation), meaning that its messages won't propagate to the root logger (which is perfectly fine—as described in the official Python documentation, it is strongly advised that you do not log to the root logger in your library). For the sake of completeness, it should be noted that in order to disable this behaviour—not that you have to—on uvicorn.error logger in the example below, one could set the propagate attribute to False for that logger as well, e.g., logger.propagate = False. main.py from fastapi import FastAPI import uvicorn import logging app = FastAPI(title='api') logger = logging.getLogger('uvicorn.error') @app.get('/') async def main(): logger.info('GET /') # or logger.debug(), logger.error(), etc. return 'success' if __name__ == '__main__': uvicorn.run(app, log_level=&quot;trace&quot;) 3. Using custom-formatted uvicorn loggers to log custom messages too This approach demonstrates how to customize the uvicorn loggers, as well as use them to log both uvicorn and custom messages. To define a custom format for the uvicorn loggers, one could use the log_config attribute in uvicorn.run() to pass a logging configuration dictionary (i.e., dictConfig()), as shown in the exmaple below, including the various schema details, such as formatters, handlers and loggers. You could then define the uvicorn.error logger in main.py, as demonstrated in the previous section, and use it across your application. For the file handler in the example below, RotatingFileHandler is used, in which: You can use the maxBytes and backupCount values to allow the file to rollover at a predetermined size. When the size is about to be exceeded, the file is closed and a new file is silently opened for output. Rollover occurs whenever the current log file is nearly maxBytes in length; but if either of maxBytes or backupCount is zero, rollover never occurs, so you generally want to set backupCount to at least 1, and have a non-zero maxBytes (by default, the file would grow indefinitely). When backupCount is non-zero, the system will save old log files by appending the extensions ‘.1’, ‘.2’ etc., to the filename. For example, with a backupCount of 5 and a base file name of app.log, you would get app.log, app.log.1, app.log.2, up to app.log.5. The file being written to is always app.log. When this file is filled, it is closed and renamed to app.log.1, and if files app.log.1, app.log.2, etc. exist, then they are renamed to app.log.2, app.log.3 etc. respectively. main.py from fastapi import FastAPI import uvicorn import logging import settings app = FastAPI(title='api') logger = logging.getLogger('uvicorn.error') @app.get('/') async def main(): logger.info('GET /') # or logger.debug(), logger.error(), etc. return 'success' if __name__ == '__main__': uvicorn.run(app, log_config=settings.LOGGING_CONFIG) settings.py LOGGING_CONFIG = { 'version': 1, 'disable_existing_loggers': True, 'formatters': { 'standard': { 'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s' }, 'custom_formatter': { 'format': &quot;%(asctime)s [%(processName)s: %(process)d] [%(threadName)s: %(thread)d] [%(levelname)s] %(name)s: %(message)s&quot; }, }, 'handlers': { 'default': { 'formatter': 'standard', 'class': 'logging.StreamHandler', 'stream': 'ext://sys.stdout', # Default is stderr }, 'stream_handler': { 'formatter': 'custom_formatter', 'class': 'logging.StreamHandler', 'stream': 'ext://sys.stdout', # Default is stderr }, 'file_handler': { 'formatter': 'custom_formatter', 'class': 'logging.handlers.RotatingFileHandler', 'filename': 'app.log', 'maxBytes': 1024 * 1024 * 1, # = 1MB 'backupCount': 3, }, }, 'loggers': { 'uvicorn': { 'handlers': ['default', 'file_handler'], 'level': 'TRACE', 'propagate': False }, 'uvicorn.access': { 'handlers': ['stream_handler', 'file_handler'], 'level': 'TRACE', 'propagate': False }, 'uvicorn.error': { 'handlers': ['stream_handler', 'file_handler'], 'level': 'TRACE', 'propagate': False }, 'uvicorn.asgi': { 'handlers': ['stream_handler', 'file_handler'], 'level': 'TRACE', 'propagate': False }, }, } Custom JSON Formatter (simple) One could have the log messages displayed and/or saved in JSON format, if they wish, by either using a simple JSON format such as: settings.py LOGGING_CONFIG = { 'version': 1, 'disable_existing_loggers': True, 'formatters': { 'standard': ..., # same as above or customize that as well 'custom_formatter': { 'format': &quot;{'time':'%(asctime)s', 'process_name': '%(processName)s', 'process_id': '%(process)s', 'thread_name': '%(threadName)s', 'thread_id': '%(thread)s','level': '%(levelname)s', 'logger_name': '%(name)s', 'message': '%(message)s'}&quot; }, }, ... # the rest is the same as in the original settings.py above } Custom JSON Formatter (elegant) Or, a more elegant version, as demonstrated previously in this answer and as shown below. Please refer to that answer and this one for further details, as well as the relevant middleware and methods for logging Request and Response information, which would go into the extra parameter when logging messages in the application, for example: logger.info(&quot;some msg&quot;, extra={'extra_info': get_extra_info(request, response)}) If you don't need that kind of information, please feel free not to use the extra parameter, as well as remove the extra_info part from the get_log() function below. settings.py import logging, json class CustomJSONFormatter(logging.Formatter): def __init__(self, fmt): logging.Formatter.__init__(self, fmt) def format(self, record): logging.Formatter.format(self, record) return json.dumps(get_log(record), indent=2) def get_log(record): d = { &quot;time&quot;: record.asctime, &quot;process_name&quot;: record.processName, &quot;process_id&quot;: record.process, &quot;thread_name&quot;: record.threadName, &quot;thread_id&quot;: record.thread, &quot;level&quot;: record.levelname, &quot;logger_name&quot;: record.name, &quot;pathname&quot;: record.pathname, &quot;line&quot;: record.lineno, &quot;message&quot;: record.message, } if hasattr(record, &quot;extra_info&quot;): d[&quot;req&quot;] = record.extra_info[&quot;req&quot;] d[&quot;res&quot;] = record.extra_info[&quot;res&quot;] return d LOGGING_CONFIG = { 'version': 1, 'disable_existing_loggers': True, 'formatters': { 'standard': ..., # same as above or customize that as well 'custom_formatter': { '()': lambda: CustomJSONFormatter(fmt='%(asctime)s') }, }, ... # the rest is the same as in the original settings.py above } Output example: { &quot;time&quot;: &quot;2024-10-27 11:05:00,300&quot;, &quot;process_name&quot;: &quot;MainProcess&quot;, &quot;process_id&quot;: 4102, &quot;thread_name&quot;: &quot;AnyIO worker thread&quot;, &quot;thread_id&quot;: 1147, &quot;level&quot;: &quot;INFO&quot;, &quot;logger_name&quot;: &quot;uvicorn.error&quot;, &quot;pathname&quot;: &quot;C:\\\\...&quot;, &quot;line&quot;: 33, &quot;message&quot;: &quot;GET /&quot;, &quot;req&quot;: { &quot;url&quot;: &quot;/&quot;, &quot;headers&quot;: { &quot;host&quot;: &quot;localhost:8000&quot;, &quot;user-agent&quot;: &quot;Mozilla...&quot;, &quot;accept&quot;: &quot;text/html,application/xhtml+xml,...&quot; }, &quot;method&quot;: &quot;GET&quot;, &quot;http_version&quot;: &quot;1.1&quot;, &quot;original_url&quot;: &quot;/&quot;, &quot;query&quot;: {} }, &quot;res&quot;: { &quot;status_code&quot;: 200, &quot;status&quot;: &quot;OK&quot; } } 4. Using a custom Python logger separate from uvicorn loggers In case one wished having a separate custom Python logger instead of customizing the existing uvicorn loggers, as demonstrated earlier, they would need to add a StreamHandler and/or FileHandler and set the desired level, i.e., DEBUG, INFO, WARNING, etc.—the lowest level offered by Python's logging module is DEBUG, with the default level being WARNING (if one is interested in adding a custom log level, see this post). You could either do that using a dictConfig(), as shown earlier, or directly using the logging's module functions and classes. The following example is based on this answer, which demonstrates how to customize the format of the logging messages in JSON (hence, see that answer, if you are looking for a similar format presented in the previous section), as well as this answer that shows how to log both the request and response bodies in the background. More details and examples can also be found in Python's official documentation page here. You may also want to have a look at all the available LogRecord attributes that can be used to format the logging records. Setting log_level=&quot;trace&quot; in uvicorn.run() would set the level of the uvicorn logger to trace, as described earlier—in case one needed that as well. Also, one could still customize the uvicorn loggers, if they wish, using the LOGGING_CONFIG dictionary provided in the previous section and passing it to the settings, i.e., uvicorn.run(..., log_config=settings.LOGGING_CONFIG). In that way, one could get the uvicorn logs in an elegant format and have them saved to a file on disk as well. Working Example from fastapi import FastAPI import logging import uvicorn import sys app = FastAPI() logger = logging.getLogger(__name__) logger.setLevel(logging.DEBUG) formatter = logging.Formatter(&quot;%(asctime)s [%(processName)s: %(process)d] [%(threadName)s: %(thread)d] [%(levelname)s] %(name)s: %(message)s&quot;) stream_handler = logging.StreamHandler(sys.stdout) stream_handler.setFormatter(formatter) file_handler = logging.FileHandler(&quot;info.log&quot;) file_handler.setFormatter(formatter) logger.addHandler(stream_handler) logger.addHandler(file_handler) logger.info('API is starting up') @app.get('/') async def main(): logger.info('GET /') return 'ok' if __name__ == '__main__': uvicorn.run(app, log_level=&quot;trace&quot;) # or `log_config=settings.LOGGING_CONFIG` 5. Final Notes In each of the above cases, one may wish to initialize the logger at startup inside a lifespan handler, and then add it to app.state or request.state, so that it can be accessed outside the main file of the application as well; for instance, from a submodule that uses APIRouter to create endpoints, lying inside a routers package, which is normally the case when building Bigger Applications. To do that, please have a look at this answer.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Looking at how uvicorn set its loggers in the recent versions I came up with this: logger = logging.getLogger('uvicorn.error') @app.get('/') async def main(): logger.debug('this is a debug message') return 'ok' The log level is controlled by the uvicorn command line option --log-level debug and produces: INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) DEBUG: this is a debug message",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "logging",
        "fastapi",
        "uvicorn"
      ],
      "question_score": 37,
      "answer_score": 46,
      "created": "2023-08-29T14:32:37",
      "question_id": 77001129,
      "answer_id": 77007723
    }
  },
  {
    "question": "How to invoke Github Copilot programmatically?",
    "expected_answer": "UPDATE GitHub released their offical language server for Copilot recently. Now instead of manually extract the dist/ directory from Copilot.vim, you can directly install @github/copilot-language-server via npm and invoke it via node ./node_modules/@github/copilot-language-server/dist/language-server.js --stdio true. The original answer still applies to the official language server. Below is the original answer. I just figured out how to invoke GitHub Copilot by the language server provided in Copilot.vim. I also referred to several community implementations like copilot.lua and LSP-copilot to understand that. TL;DR: Copilot.vim invokes GitHub Copilot via a language server, which is contained in the dist/ directory of its repository. Those vimscripts in Copilot.vim repository actually serve as a client to the language server. The language server itself is written in Node.js, and uses stdio to communicate with the client. You can just download the dist/ directory and run the language server by node dist/language-server.js. However, just running the server by hand is hard to use, so I wrote a simple client in Node.js to invoke the language server and get the results. This is a minimal example showing how to invoke GitHub Copilot programmatically via the language server extracted from Copilot.vim: // @ts-check const { spawn } = require(&quot;node:child_process&quot;); const server = spawn(&quot;node&quot;, [&quot;language-server.js&quot;, &quot;--stdio&quot;, &quot;true&quot;]); // use `fork` in `node:child_process` is also OK // const server = fork(&quot;language-server.js&quot;, { silent: true, execArgv: [&quot;--stdio&quot;, &quot;true&quot;] }); // `{ silent: true }` is to make sure that data sent to stdio will be returned normally /** * Send a LSP message to the server. */ const sendMessage = (/** @type {object} */ data) =&gt; { const dataString = JSON.stringify({ ...data, jsonrpc: &quot;2.0&quot; }); const contentLength = Buffer.byteLength(dataString, &quot;utf8&quot;); const rpcString = `Content-Length: ${contentLength}\\r\\n\\r\\n${dataString}`; server.stdin.write(rpcString); }; let requestId = 0; /** @type {Map&lt;number, (payload: object) =&gt; void | Promise&lt;void&gt;&gt;} */ const resolveMap = new Map(); /** @type {Map&lt;number, (payload: object) =&gt; void | Promise&lt;void&gt;&gt;} */ const rejectMap = new Map(); /** * Send a LSP request to the server. */ const sendRequest = (/** @type {string} */ method, /** @type {object} */ params) =&gt; { sendMessage({ id: ++requestId, method, params }); return new Promise((resolve, reject) =&gt; { resolveMap.set(requestId, resolve); rejectMap.set(requestId, reject); }); }; /** * Send a LSP notification to the server. */ const sendNotification = (/** @type {string} */ method, /** @type {object} */ params) =&gt; { sendMessage({ method, params }); }; /** * Handle received LSP payload. */ const handleReceivedPayload = (/** @type {object} */ payload) =&gt; { if (&quot;id&quot; in payload) { if (&quot;result&quot; in payload) { const resolve = resolveMap.get(payload.id); if (resolve) { resolve(payload.result); resolveMap.delete(payload.id); } } else if (&quot;error&quot; in payload) { const reject = rejectMap.get(payload.id); if (reject) { reject(payload.error); rejectMap.delete(payload.id); } } } }; server.stdout.on(&quot;data&quot;, (data) =&gt; { /** @type {string} */ const rawString = data.toString(&quot;utf-8&quot;); const payloadStrings = rawString.split(/Content-Length: \\d+\\r\\n\\r\\n/).filter((s) =&gt; s); for (const payloadString of payloadStrings) { /** @type {Record&lt;string, unknown&gt;} */ let payload; try { payload = JSON.parse(payloadString); } catch (e) { console.error(`Unable to parse payload: ${payloadString}`, e); continue; } handleReceivedPayload(payload); } }); const wait = (/** @type {number} */ ms) =&gt; new Promise((resolve) =&gt; setTimeout(resolve, ms)); /* Main */ const main = async () =&gt; { // Wait for server to start await wait(1000); // Send `initialize` request await sendRequest(&quot;initialize&quot;, { capabilities: { workspace: { workspaceFolders: true } }, }); // Send `initialized` notification sendNotification(&quot;initialized&quot;, {}); // Send `textDocument/didOpen` notification sendNotification(&quot;textDocument/didOpen&quot;, { textDocument: { uri: &quot;file:///home/fakeuser/my-project/test.py&quot;, languageId: &quot;python&quot;, version: 0, // The change count (i.e. version) of the document text: &quot;def hello():\\n&quot; + &quot; print('hello, w&quot;, }, }); // Send `getCompletions` request to get completions at line 1, character 19 // (i.e. after `print('hello, w`) const completions = await sendRequest(&quot;getCompletions&quot;, { doc: { version: 0, // Should be the same as the latest version of the document position: { line: 1, character: 19 }, uri: &quot;file:///home/fakeuser/my-project/test.py&quot;, }, }); console.log(&quot;Completions:&quot;, completions); }; void main(); This example assumes you’ve already logged in to GitHub Copilot via something also using the language server like Copilot.vim or LSP-copilot. You can view relevant implementations in LSP-copilot to see how to automate the login process programmatically (The signInInitiate request and signInConfirm request). Then you can see something like this in your console: If you’re not quite familiar with LSP, you can refer to LSP specification to understand the meaning of each request and notification. Requests like initialize and textDocument/didOpen here are defined in the LSP specification, while getCompletions is a custom request defined by the GitHub Copilot language server. You can also use textDocument/didChange or textDocument/didClose to tell the language server that the document has been changed or closed (don’t forget to update the version field on each change). Some other requests like getCompletionsCycling can provide more completions, you can also view Copilot.vim or relevant community implementations I mentioned above to see how to use them. It should also be possible to write a Python client to invoke the language server, but the language server itself is written in Node.js, so it’s easier to write a Node.js client to invoke it.",
    "context_chunks": [
      {
        "text": "I am currently exploring GitHub Copilot, and I am interested in using it programmatically, i.e., invoking it from code. As I understand, GitHub Copilot is an IDE plugin, which makes me wonder how it can be automated or controlled programmatically. We know Copilot uses OpenAI models behind the scene as an LLM. GitHub Copilot does not provide API access to control it programmatically. Clarification: It's important to note that the plugin, once downloaded and installed, completes my code automatically. Copilot used the OpenAI models such as gpt-3.5 or gpt-4 behind the scene. I know very well the OpenAI chat or text completion models.So that's not my question My question is how to capture the top three suggestions provided by Copilot in an automated fashion. For example, for any given autocomplete task to Copilot, the task is to record the code suggestions and save them into a file.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "UPDATE GitHub released their offical language server for Copilot recently. Now instead of manually extract the dist/ directory from Copilot.vim, you can directly install @github/copilot-language-server via npm and invoke it via node ./node_modules/@github/copilot-language-server/dist/language-server.js --stdio true. The original answer still applies to the official language server. Below is the original answer. I just figured out how to invoke GitHub Copilot by the language server provided in Copilot.vim. I also referred to several community implementations like copilot.lua and LSP-copilot to understand that. TL;DR: Copilot.vim invokes GitHub Copilot via a language server, which is contained in the dist/ directory of its repository. Those vimscripts in Copilot.vim repository actually serve as a client to the language server. The language server itself is written in Node.js, and uses stdio to communicate with the client. You can just download the dist/ directory and run the language server by node dist/language-server.js. However, just running the server by hand is hard to use, so I wrote a simple client in Node.js to invoke the language server and get the results. This is a minimal example showing how to invoke GitHub Copilot programmatically via the language server extracted from Copilot.vim: // @ts-check const { spawn } = require(&quot;node:child_process&quot;); const server = spawn(&quot;node&quot;, [&quot;language-server.js&quot;, &quot;--stdio&quot;, &quot;true&quot;]); // use `fork` in `node:child_process` is also OK // const server = fork(&quot;language-server.js&quot;, { silent: true, execArgv: [&quot;--stdio&quot;, &quot;true&quot;] }); // `{ silent: true }` is to make sure that data sent to stdio will be returned normally /** * Send a LSP message to the server. */ const sendMessage = (/** @type {object} */ data) =&gt; { const dataString = JSON.stringify({ ...data, jsonrpc: &quot;2.0&quot; }); const contentLength = Buffer.byteLength(dataString, &quot;utf8&quot;); const rpcString = `Content-Length: ${contentLength}\\r\\n\\r\\n${dataString}`; server.stdin.write(rpcString); }; let requestId = 0; /** @type {Map&lt;number, (payload: object) =&gt; void | Promise&lt;void&gt;&gt;} */ const resolveMap = new Map(); /** @type {Map&lt;number, (payload: object) =&gt; void | Promise&lt;void&gt;&gt;} */ const rejectMap = new Map(); /** * Send a LSP request to the server. */ const sendRequest = (/** @type {string} */ method, /** @type {object} */ params) =&gt; { sendMessage({ id: ++requestId, method, params }); return new Promise((resolve, reject) =&gt; { resolveMap.set(requestId, resolve); rejectMap.set(requestId, reject); }); }; /** * Send a LSP notification to the server. */ const sendNotification = (/** @type {string} */ method, /** @type {object} */ params) =&gt; { sendMessage({ method, params }); }; /** * Handle received LSP payload. */ const handleReceivedPayload = (/** @type {object} */ payload) =&gt; { if (&quot;id&quot; in payload) { if (&quot;result&quot; in payload) { const resolve = resolveMap.get(payload.id); if (resolve) { resolve(payload.result); resolveMap.delete(payload.id); } } else if (&quot;error&quot; in payload) { const reject = rejectMap.get(payload.id); if (reject) { reject(payload.error); rejectMap.delete(payload.id); } } } }; server.stdout.on(&quot;data&quot;, (data) =&gt; { /** @type {string} */ const rawString = data.toString(&quot;utf-8&quot;); const payloadStrings = rawString.split(/Content-Length: \\d+\\r\\n\\r\\n/).filter((s) =&gt; s); for (const payloadString of payloadStrings) { /** @type {Record&lt;string, unknown&gt;} */ let payload; try { payload = JSON.parse(payloadString); } catch (e) { console.error(`Unable to parse payload: ${payloadString}`, e); continue; } handleReceivedPayload(payload); } }); const wait = (/** @type {number} */ ms) =&gt; new Promise((resolve) =&gt; setTimeout(resolve, ms)); /* Main */ const main = async () =&gt; { // Wait for server to start await wait(1000); // Send `initialize` request await sendRequest(&quot;initialize&quot;, { capabilities: { workspace: { workspaceFolders: true } }, }); // Send `initialized` notification sendNotification(&quot;initialized&quot;, {}); // Send `textDocument/didOpen` notification sendNotification(&quot;textDocument/didOpen&quot;, { textDocument: { uri: &quot;file:///home/fakeuser/my-project/test.py&quot;, languageId: &quot;python&quot;, version: 0, // The change count (i.e. version) of the document text: &quot;def hello():\\n&quot; + &quot; print('hello, w&quot;, }, }); // Send `getCompletions` request to get completions at line 1, character 19 // (i.e. after `print('hello, w`) const completions = await sendRequest(&quot;getCompletions&quot;, { doc: { version: 0, // Should be the same as the latest version of the document position: { line: 1, character: 19 }, uri: &quot;file:///home/fakeuser/my-project/test.py&quot;, }, }); console.log(&quot;Completions:&quot;, completions); }; void main(); This example assumes you’ve already logged in to GitHub Copilot via something also using the language server like Copilot.vim or LSP-copilot. You can view relevant implementations in LSP-copilot to see how to automate the login process programmatically (The signInInitiate request and signInConfirm request). Then you can see something like this in your console: If you’re not quite familiar with LSP, you can refer to LSP specification to understand the meaning of each request and notification. Requests like initialize and textDocument/didOpen here are defined in the LSP specification, while getCompletions is a custom request defined by the GitHub Copilot language server. You can also use textDocument/didChange or textDocument/didClose to tell the language server that the document has been changed or closed (don’t forget to update the version field on each change). Some other requests like getCompletionsCycling can provide more completions, you can also view Copilot.vim or relevant community implementations I mentioned above to see how to use them. It should also be possible to write a Python client to invoke the language server, but the language server itself is written in Node.js, so it’s easier to write a Node.js client to invoke it.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "TLDR: I made an (unofficial) API: https://github.com/B00TK1D/copilot-api I was inspired by @Snowflyt at how simple their API interface was, so I reverse engineered the Copilot vim plugin and associated API. I matched the way they do OAuth (normal Github Apps aren't allowed Copilot access, so I just use Copilot's App ID). This way, you don't have to rely on installing vim and the plugin and going through the setup process. The repo linked includes a full self-hosted solution, that you can then call from whatever other functionality you want. The first time you start it, you'll have to complete OAuth (enter provided code into link), and then after that it will automatically refresh all the auth tokens. There are some more advanced features of Copilot that I haven't added yet, but it currently supports basic code completion prompting. To answer exactly the question you asked - if you want to record the prompts and top responses from the Copilot plugin as you're using it, you can just wrap the agent.js file in the plugin with a logger, such as by using the following commands (assuming linux, and that you have the Vim copilot plugin installed): mv ~/.config/nvim/pack/github/start/copilot.vim/dist/agent.js ~/.config/nvim/pack/github/start/copilot.vim/dist/agent.orig.js Edit the file ~/.config/nvim/pack/github/start/copilot.vim/dist/agent.js, and paste in the following code: const fs = require('fs'); const { spawn } = require('child_process'); const inLogStream = fs.createWriteStream('copilot-prompts.log', { flags: 'a' }); const outLogStream = fs.createWriteStream('copilot-suggestions.log', { flags: 'a' }); // Replace the path with the absolute path for agent.js const agentScriptPath = '/root/.config/nvim/pack/github/start/copilot.vim/dist/agent.orig.js'; // Spawn a new process running agent.js with the absolute path const agentProcess = spawn('node', [agentScriptPath]); // Pipe stdin from the main script to the new process and log it process.stdin.pipe(inLogStream); process.stdin.pipe(agentProcess.stdin); // Pipe stdout from the new process back to the main script's stdout and log it agentProcess.stdout.pipe(outLogStream); agentProcess.stdout.pipe(process.stdout); // Handle process exit agentProcess.on('exit', (code, signal) =&gt; { console.log(`Agent process exited with code ${code} and signal ${signal}`); inLogStream.end(); outLogStream.end(); process.exit(); }); // Handle errors agentProcess.on('error', (err) =&gt; { console.error(`Error in agent process: ${err.message}`); inLogStream.end(); outLogStream.end(); process.exit(1); }); // Handle main script stdin end process.stdin.on('end', () =&gt; { // Close the stdin stream for the spawned process when main script stdin ends agentProcess.stdin.end(); }); // Handle main script exit process.on('exit', () =&gt; { // Kill the spawned process when the main script exits agentProcess.kill(); }); // Handle main script termination process.on('SIGINT', () =&gt; { // Handle Ctrl+C to gracefully terminate both the main script and the spawned process process.exit(); }); Use copilot in vim as desired to generate logs. View the prompts and suggestions in ~/copilot-prompts.log and ~/copilot-suggestions.log. The output of these logs requires some parsing because they use JSON-RPC, but I'll let you decide exactly how you want to implement that.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "github-copilot"
      ],
      "question_score": 47,
      "answer_score": 27,
      "created": "2023-07-21T21:45:43",
      "question_id": 76741410,
      "answer_id": 77659136
    }
  },
  {
    "question": "Solve conda-libmamba-solver (libarchive.so.19) error after updating conda to 23.11.0 without reinstalling conda?",
    "expected_answer": "This works for me: $ conda install --solver=classic conda-forge::conda-libmamba-solver conda-forge::libmamba conda-forge::libmambapy conda-forge::libarchive Eventually install the necessary packages from the same channel (conda -forge).",
    "context_chunks": [
      {
        "text": "After conda update, I am getting the error below after running $ conda, even after setting the solver to classic (using conda config --set solver classic): Error while loading conda entry point: conda-libmamba-solver (libarchive.so.19: cannot open shared object file: No such file or directory) I have conda 23.11.0. On Github there is an issue https://github.com/conda/conda-libmamba-solver/issues/283 in which they mention that if libarchive and libmamba come from the same channel, it should be solved. But when I reinstall libarchive using channel main, it doesn't update. Does anyone know how to solve this? I do not want to reinstall Conda from scratch. Similar links online that do not solve the problem: After installing libmamba solver i get `warning libmamba Could not parse state file` How to Fix Entry Point Not Found while installing libraries in conda environment https://www.reddit.com/r/learnpython/comments/160kjz9/how_do_i_get_anaconda_to_work_the_way_i_want_it_to/",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This works for me: $ conda install --solver=classic conda-forge::conda-libmamba-solver conda-forge::libmamba conda-forge::libmambapy conda-forge::libarchive Eventually install the necessary packages from the same channel (conda -forge).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In this specific situation, to prevent walking in circles, you better specify the solver. As suggested, use the classic one: conda install -n base libarchive -c main --force-reinstall --solver classic The output you are looking for looks like the following: Error while loading conda entry point: conda-libmamba-solver (libarchive.so.19: cannot open shared object file: No such file or directory) Collecting package metadata (current_repodata.json): / WARNING conda.models.version:get_matcher(562): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1 done Solving environment: done \\## Package Plan ## environment location: /home/fsouto/anaconda3 added / updated specs: - libarchive The following packages will be downloaded: package | build ---------------------------|----------------- ca-certificates-2023.12.12 | h06a4308_0 133 KB main certifi-2024.2.2 | py311h06a4308_0 161 KB main conda-24.1.2 | py311h06a4308_0 1.3 MB main libarchive-3.6.2 | h6ac8c49_2 1.6 MB main ------------------------------------------------------------ Total: 3.1 MB The following packages will be UPDATED: libarchive conda-forge::libarchive-3.6.2-h3d5159~ --&gt; main::libarchive-3.6.2-h6ac8c49_2 The following packages will be SUPERSEDED by a higher-priority channel: ca-certificates conda-forge::ca-certificates-2024.2.2~ --&gt; main::ca-certificates-2023.12.12-h06a4308_0 certifi conda-forge/noarch::certifi-2024.2.2-~ --&gt; main/linux-64::certifi-2024.2.2-py311h06a4308_0 conda conda-forge::conda-24.1.2-py311h38be0~ --&gt; main::conda-24.1.2-py311h06a4308_0 Proceed ([y]/n)? y",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "conda",
        "libarchive"
      ],
      "question_score": 35,
      "answer_score": 60,
      "created": "2023-12-07T06:01:10",
      "question_id": 77617946,
      "answer_id": 78293971
    }
  },
  {
    "question": "langchain : ModuleNotFoundError: No module named &#39;langchain_community&#39;",
    "expected_answer": "pip install langchain-community langchain-core This is required. If still not working then: pip install --upgrade langchain Also, make sure that python_version=&gt;3.8.1",
    "context_chunks": [
      {
        "text": "Trying to execute this code: from langchain_community.vectorstores import FAISS Error it is showing: ModuleNotFoundError: No module named 'langchain_community' Thou I have executed the command: pip install langchain-community",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "pip install langchain-community langchain-core This is required. If still not working then: pip install --upgrade langchain Also, make sure that python_version=&gt;3.8.1",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Using pip install langchain-community or pip install --upgrade langchain did not work for me in spite of multiple tries. Using the PyCharm 'Interpreter Settings' GUI to manually install langchain-community instead, did the trick!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "langchain"
      ],
      "question_score": 39,
      "answer_score": 60,
      "created": "2024-02-15T05:24:26",
      "question_id": 77998568,
      "answer_id": 77998581
    }
  },
  {
    "question": "pip install RandomWords: &quot;No module named &#39;setuptools.command.test&#39;&quot;",
    "expected_answer": "It seems that their package is broken. This is because in setup.py they are doing from setuptools.command.test import test as TestCommand but setuptools.command.test was removed on 28 Jul 2024 (yesterday): https://setuptools.pypa.io/en/stable/history.html#v72-0-0 Here's a similar issue on the official GitHub repo: https://github.com/pypa/setuptools/issues/4519 To solve it locally do one of the following: downgrade your setuptools as @reza.safiyat suggested; use a project manager (like Poetry or Hatch) and pin setuptools = &quot;&lt;72.0.0&quot;. UPDATE setuptools 72.0.0 has been yanked (https://pypi.org/project/setuptools/72.0.0/#history) and the removal of command.test has been postponed to November 15. Everything should work as it did yesterday (with the addition of a new big warning, hopefully).",
    "context_chunks": [
      {
        "text": "I'm using pip 24.2 and Python 3.12.4. I want to install this: $ pip install RandomWords Preparing metadata (setup.py) ... error error: subprocess-exited-with-error python setup.py egg_info did not run successfully. exit code: 1 from setuptools.command.test import test as TestCommand ModuleNotFoundError: No module named 'setuptools.command.test' note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed × Encountered error while generating package metadata. ╰─&gt; See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Why is this and how can I fix it?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "It seems that their package is broken. This is because in setup.py they are doing from setuptools.command.test import test as TestCommand but setuptools.command.test was removed on 28 Jul 2024 (yesterday): https://setuptools.pypa.io/en/stable/history.html#v72-0-0 Here's a similar issue on the official GitHub repo: https://github.com/pypa/setuptools/issues/4519 To solve it locally do one of the following: downgrade your setuptools as @reza.safiyat suggested; use a project manager (like Poetry or Hatch) and pin setuptools = &quot;&lt;72.0.0&quot;. UPDATE setuptools 72.0.0 has been yanked (https://pypi.org/project/setuptools/72.0.0/#history) and the removal of command.test has been postponed to November 15. Everything should work as it did yesterday (with the addition of a new big warning, hopefully).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This module was removed from in this commit. Using setuptools&lt;72.0.0 will alleviate this problem. Steps you may use once you have sourced the relevant python environment: # Install setuptools below v72.0.0. Add `-U` after `install` in case you want to upgrade to a version below 72.0.0, if you already have setuptools installed. python -m pip install 'setuptools&lt;72.0.0' # Install RandomWords. python -m pip install RandomWords",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pip"
      ],
      "question_score": 35,
      "answer_score": 22,
      "created": "2024-07-29T08:31:32",
      "question_id": 78806100,
      "answer_id": 78806146
    }
  },
  {
    "question": "Only the first row of annotations displayed on seaborn heatmap",
    "expected_answer": "Just ran into the issue myself, I was on Seaborn 0.12.2. Ran pip install seaborn --upgrade and now have 0.13.0 Restarted vscode and annotations appeared.",
    "context_chunks": [
      {
        "text": "As it's usually advised, I have managed to reduce my problem to a minimal reproducible example: import numpy as np import seaborn as sns import matplotlib.pyplot as plt matrix = np.array([[0.1234, 1.4567, 0.7890, 0.1234], [0.9876, 0, 0.5432, 0.6789], [0.1111, 0.2222, 0, 0.3333], [0.4444, 0.5555, 0.6666, 0]]) sns.heatmap(matrix, annot=True) plt.show() Vaguely based on Seaborn official documentation. Unfortunately, unlike what would be expected (all numbers visible), I get only the numbers in the top row visible: As there is not really much room for error in this one, I'm out of ideas and google/SO doesn't seem to have this question asked before. Is this a bug? I am running: Seaborn 0.12.2 Matplotlib 3.8.0 PyCharm 2023.1.4 Windows 10",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Just ran into the issue myself, I was on Seaborn 0.12.2. Ran pip install seaborn --upgrade and now have 0.13.0 Restarted vscode and annotations appeared.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Thanks to a comment by @mwaskom, it seems like it's a Matplotlib issue with a documented solution: Looks like problem goes away if I downgrade MATPLOTLIB to version 3.7.3.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "matplotlib",
        "seaborn"
      ],
      "question_score": 47,
      "answer_score": 44,
      "created": "2023-09-23T22:19:13",
      "question_id": 77165100,
      "answer_id": 77231620
    }
  },
  {
    "question": "Theoretically can the Ackermann function be optimized?",
    "expected_answer": "Solution I recently wrote a bunch of solutions based on the same paper that templatetypedef mentioned. Many use generators, one for each m-value, yielding the values for n=0, n=1, n=2, etc. This one might be my favorite: def A_Stefan_generator_stack3(m, n): def a(m): if not m: yield from count(1) x = 1 for i, ai in enumerate(a(m-1)): if i == x: x = ai yield x return next(islice(a(m), n, None)) Explanation Consider the generator a(m). It yields A(m,0), A(m,1), A(m,2), etc. The definition of A(m,n) uses A(m-1, A(m, n-1)). So a(m) at its index n yields A(m,n), computed like this: A(m,n-1) gets yielded by the a(m) generator itself at index n-1. Which is just the previous value (x) yielded by this generator. A(m-1, A(m, n-1)) = A(m-1, x) gets yielded by the a(m-1) generator at index x. So the a(m) generator iterates over the a(m-1) generator and grabs the value at index i == x. Benchmark Here are times for computing all A(m,n) for m≤3 and n≤17, also including templatetypedef's solution: 1325 ms A_Stefan_row_class 1228 ms A_Stefan_row_lists 544 ms A_Stefan_generators 1363 ms A_Stefan_paper 459 ms A_Stefan_generators_2 866 ms A_Stefan_m_recursion 704 ms A_Stefan_function_stack 468 ms A_Stefan_generator_stack 945 ms A_Stefan_generator_stack2 582 ms A_Stefan_generator_stack3 467 ms A_Stefan_generator_stack4 1652 ms A_templatetypedef Note: Even faster (much faster) solutions using math insights/formulas are possible, see my comment and pts's answer. I intentionally didn't do that, as I was interested in coding techniques, for avoiding deep recursion and avoiding re-calculation. I got the impression that that's also what the question/OP wanted, and they confirmed that now (under a deleted answer, visible if you have enough reputation). Code def A_Stefan_row_class(m, n): class A0: def __getitem__(self, n): return n + 1 class A: def __init__(self, a): self.a = a self.n = 0 self.value = a[1] def __getitem__(self, n): while self.n &lt; n: self.value = self.a[self.value] self.n += 1 return self.value a = A0() for _ in range(m): a = A(a) return a[n] from collections import defaultdict def A_Stefan_row_lists(m, n): memo = defaultdict(list) def a(m, n): if not m: return n + 1 if m not in memo: memo[m] = [a(m-1, 1)] Am = memo[m] while len(Am) &lt;= n: Am.append(a(m-1, Am[-1])) return Am[n] return a(m, n) from itertools import count def A_Stefan_generators(m, n): a = count(1) def up(a, x=1): for i, ai in enumerate(a): if i == x: x = ai yield x for _ in range(m): a = up(a) return next(up(a, n)) def A_Stefan_paper(m, n): next = [0] * (m + 1) goal = [1] * m + [-1] while True: value = next[0] + 1 transferring = True i = 0 while transferring: if next[i] == goal[i]: goal[i] = value else: transferring = False next[i] += 1 i += 1 if next[m] == n + 1: return value def A_Stefan_generators_2(m, n): def a0(): n = yield while True: n = yield n + 1 def up(a): next(a) a = a.send i, x = -1, 1 n = yield while True: while i &lt; n: x = a(x) i += 1 n = yield x a = a0() for _ in range(m): a = up(a) next(a) return a.send(n) def A_Stefan_m_recursion(m, n): ix = [None] + [(-1, 1)] * m def a(m, n): if not m: return n + 1 i, x = ix[m] while i &lt; n: x = a(m-1, x) i += 1 ix[m] = i, x return x return a(m, n) def A_Stefan_function_stack(m, n): def a(n): return n + 1 for _ in range(m): def a(n, a=a, ix=[-1, 1]): i, x = ix while i &lt; n: x = a(x) i += 1 ix[:] = i, x return x return a(n) from itertools import count, islice def A_Stefan_generator_stack(m, n): a = count(1) for _ in range(m): a = ( x for a, x in [(a, 1)] for i, ai in enumerate(a) if i == x for x in [ai] ) return next(islice(a, n, None)) from itertools import count, islice def A_Stefan_generator_stack2(m, n): a = count(1) def up(a): i, x = 0, 1 while True: i, x = x+1, next(islice(a, x-i, None)) yield x for _ in range(m): a = up(a) return next(islice(a, n, None)) def A_Stefan_generator_stack3(m, n): def a(m): if not m: yield from count(1) x = 1 for i, ai in enumerate(a(m-1)): if i == x: x = ai yield x return next(islice(a(m), n, None)) def A_Stefan_generator_stack4(m, n): def a(m): if not m: return count(1) return ( x for x in [1] for i, ai in enumerate(a(m-1)) if i == x for x in [ai] ) return next(islice(a(m), n, None)) def A_templatetypedef(i, n): positions = [-1] * (i + 1) values = [0] + [1] * i while positions[i] != n: values[0] += 1 positions[0] += 1 j = 1 while j &lt;= i and positions[j - 1] == values[j]: values[j] = values[j - 1] positions[j] += 1 j += 1 return values[i] funcs = [ A_Stefan_row_class, A_Stefan_row_lists, A_Stefan_generators, A_Stefan_paper, A_Stefan_generators_2, A_Stefan_m_recursion, A_Stefan_function_stack, A_Stefan_generator_stack, A_Stefan_generator_stack2, A_Stefan_generator_stack3, A_Stefan_generator_stack4, A_templatetypedef, ] N = 18 args = ( [(0, n) for n in range(N)] + [(1, n) for n in range(N)] + [(2, n) for n in range(N)] + [(3, n) for n in range(N)] ) from time import time def print(*args, print=print, file=open('out.txt', 'w')): print(*args) print(*args, file=file, flush=True) expect = none = object() for _ in range(3): for f in funcs: t = time() result = [f(m, n) for m, n in args] # print(f'{(time()-t) * 1e3 :5.1f} ms ', f.__name__) print(f'{(time()-t) * 1e3 :5.0f} ms ', f.__name__) if expect is none: expect = result elif result != expect: raise Exception(f'{f.__name__} failed') del result print()",
    "context_chunks": [
      {
        "text": "I am wondering if there can be a version of Ackermann function with better time complexity than the standard variation. This is not a homework and I am just curious. I know the Ackermann function doesn't have any practical use besides as a performance benchmark, because of the deep recursion. I know the numbers grow very large very quickly, and I am not interested in computing it. Even though I use Python 3 and the integers won't overflow, I do have finite time, but I have implemented a version of it myself according to the definition found on Wikipedia, and computed the output for extremely small values, just to make sure the output is correct. def A(m, n): if not m: return n + 1 return A(m - 1, A(m, n - 1)) if n else A(m - 1, 1) The above code is a direct translation of the image, and is extremely slow, I don't know how it can be optimized, is it impossible to optimize it? One thing I can think of is to memoize it, but the recursion runs backwards, each time the function is recursively called the arguments were not encountered before, each successive function call the arguments decrease rather than increase, therefore each return value of the function needs to be calculated, memoization doesn't help when you call the function with different arguments the first time. Memoization can only help if you call it with the same arguments again, it won't compute the results and will retrieve cached result instead, but if you call the function with any input with (m, n) &gt;= (4, 2) it will crash the interpreter regardless. I also implemented another version according to this answer: def ack(x, y): for i in range(x, 0, -1): y = ack(i, y - 1) if y else 1 return y + 1 But it is actually slower: In [2]: %timeit A(3, 4) 1.3 ms ± 9.75 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) In [3]: %timeit ack(3, 4) 2 ms ± 59.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) Theoretically can Ackermann function be optimized? If not, can it be definitely proven that its time complexity cannot decrease? I have just tested A(3, 9) and A(4, 1) will crash the interpreter, and the performance of the two functions for A(3, 8): In [2]: %timeit A(3, 8) 432 ms ± 4.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) In [3]: %timeit ack(3, 8) 588 ms ± 10.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) I did some more experiments: from collections import Counter from functools import cache c = Counter() def A1(m, n): c[(m, n)] += 1 if not m: return n + 1 return A(m - 1, A(m, n - 1)) if n else A(m - 1, 1) def test(m, n): c.clear() A1(m, n) return c The arguments indeed repeat. But surprisingly caching doesn't help at all: In [9]: %timeit Ackermann = cache(A); Ackermann(3, 4) 1.3 ms ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) Caching only helps when the function is called with the same arguments again, as explained: In [14]: %timeit Ackermann(3, 2) 101 ns ± 0.47 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each) I have tested it with different arguments numerous times, and it always gives the same efficiency boost (which is none).",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Solution I recently wrote a bunch of solutions based on the same paper that templatetypedef mentioned. Many use generators, one for each m-value, yielding the values for n=0, n=1, n=2, etc. This one might be my favorite: def A_Stefan_generator_stack3(m, n): def a(m): if not m: yield from count(1) x = 1 for i, ai in enumerate(a(m-1)): if i == x: x = ai yield x return next(islice(a(m), n, None)) Explanation Consider the generator a(m). It yields A(m,0), A(m,1), A(m,2), etc. The definition of A(m,n) uses A(m-1, A(m, n-1)). So a(m) at its index n yields A(m,n), computed like this: A(m,n-1) gets yielded by the a(m) generator itself at index n-1. Which is just the previous value (x) yielded by this generator. A(m-1, A(m, n-1)) = A(m-1, x) gets yielded by the a(m-1) generator at index x. So the a(m) generator iterates over the a(m-1) generator and grabs the value at index i == x. Benchmark Here are times for computing all A(m,n) for m≤3 and n≤17, also including templatetypedef's solution: 1325 ms A_Stefan_row_class 1228 ms A_Stefan_row_lists 544 ms A_Stefan_generators 1363 ms A_Stefan_paper 459 ms A_Stefan_generators_2 866 ms A_Stefan_m_recursion 704 ms A_Stefan_function_stack 468 ms A_Stefan_generator_stack 945 ms A_Stefan_generator_stack2 582 ms A_Stefan_generator_stack3 467 ms A_Stefan_generator_stack4 1652 ms A_templatetypedef Note: Even faster (much faster) solutions using math insights/formulas are possible, see my comment and pts's answer. I intentionally didn't do that, as I was interested in coding techniques, for avoiding deep recursion and avoiding re-calculation. I got the impression that that's also what the question/OP wanted, and they confirmed that now (under a deleted answer, visible if you have enough reputation). Code def A_Stefan_row_class(m, n): class A0: def __getitem__(self, n): return n + 1 class A: def __init__(self, a): self.a = a self.n = 0 self.value = a[1] def __getitem__(self, n): while self.n &lt; n: self.value = self.a[self.value] self.n += 1 return self.value a = A0() for _ in range(m): a = A(a) return a[n] from collections import defaultdict def A_Stefan_row_lists(m, n): memo = defaultdict(list) def a(m, n): if not m: return n + 1 if m not in memo: memo[m] = [a(m-1, 1)] Am = memo[m] while len(Am) &lt;= n: Am.append(a(m-1, Am[-1])) return Am[n] return a(m, n) from itertools import count def A_Stefan_generators(m, n): a = count(1) def up(a, x=1): for i, ai in enumerate(a): if i == x: x = ai yield x for _ in range(m): a = up(a) return next(up(a, n)) def A_Stefan_paper(m, n): next = [0] * (m + 1) goal = [1] * m + [-1] while True: value = next[0] + 1 transferring = True i = 0 while transferring: if next[i] == goal[i]: goal[i] = value else: transferring = False next[i] += 1 i += 1 if next[m] == n + 1: return value def A_Stefan_generators_2(m, n): def a0(): n = yield while True: n = yield n + 1 def up(a): next(a) a = a.send i, x = -1, 1 n = yield while True: while i &lt; n: x = a(x) i += 1 n = yield x a = a0() for _ in range(m): a = up(a) next(a) return a.send(n) def A_Stefan_m_recursion(m, n): ix = [None] + [(-1, 1)] * m def a(m, n): if not m: return n + 1 i, x = ix[m] while i &lt; n: x = a(m-1, x) i += 1 ix[m] = i, x return x return a(m, n) def A_Stefan_function_stack(m, n): def a(n): return n + 1 for _ in range(m): def a(n, a=a, ix=[-1, 1]): i, x = ix while i &lt; n: x = a(x) i += 1 ix[:] = i, x return x return a(n) from itertools import count, islice def A_Stefan_generator_stack(m, n): a = count(1) for _ in range(m): a = ( x for a, x in [(a, 1)] for i, ai in enumerate(a) if i == x for x in [ai] ) return next(islice(a, n, None)) from itertools import count, islice def A_Stefan_generator_stack2(m, n): a = count(1) def up(a): i, x = 0, 1 while True: i, x = x+1, next(islice(a, x-i, None)) yield x for _ in range(m): a = up(a) return next(islice(a, n, None)) def A_Stefan_generator_stack3(m, n): def a(m): if not m: yield from count(1) x = 1 for i, ai in enumerate(a(m-1)): if i == x: x = ai yield x return next(islice(a(m), n, None)) def A_Stefan_generator_stack4(m, n): def a(m): if not m: return count(1) return ( x for x in [1] for i, ai in enumerate(a(m-1)) if i == x for x in [ai] ) return next(islice(a(m), n, None)) def A_templatetypedef(i, n): positions = [-1] * (i + 1) values = [0] + [1] * i while positions[i] != n: values[0] += 1 positions[0] += 1 j = 1 while j &lt;= i and positions[j - 1] == values[j]: values[j] = values[j - 1] positions[j] += 1 j += 1 return values[i] funcs = [ A_Stefan_row_class, A_Stefan_row_lists, A_Stefan_generators, A_Stefan_paper, A_Stefan_generators_2, A_Stefan_m_recursion, A_Stefan_function_stack, A_Stefan_generator_stack, A_Stefan_generator_stack2, A_Stefan_generator_stack3, A_Stefan_generator_stack4, A_templatetypedef, ] N = 18 args = ( [(0, n) for n in range(N)] + [(1, n) for n in range(N)] + [(2, n) for n in range(N)] + [(3, n) for n in range(N)] ) from time import time def print(*args, print=print, file=open('out.txt', 'w')): print(*args) print(*args, file=file, flush=True) expect = none = object() for _ in range(3): for f in funcs: t = time() result = [f(m, n) for m, n in args] # print(f'{(time()-t) * 1e3 :5.1f} ms ', f.__name__) print(f'{(time()-t) * 1e3 :5.0f} ms ', f.__name__) if expect is none: expect = result elif result != expect: raise Exception(f'{f.__name__} failed') del result print()",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You are asking these two questions in your text: Theoretically can Ackermann function be optimized? Sure, you can implement it naively, and then optimize that in technical ways (e.g. memoization, or storing intermediate values, and so on). But I think this is not the actual question you are asking. If not, can it be definitely proven that its time complexity cannot decrease? Optimization has nothing to do with time complexity. The &quot;O&quot; notation abstracts away from constant multiplicative factors. You can have two algorithms where the one calculates the Ackermann function in 1 millionth of the time or space than the other, but they would still have the same O complexity. To quote Wikipedia, the Ackermann function, named after Wilhelm Ackermann, is one of the simplest1 and earliest-discovered examples of a total computable function that is not primitive recursive. The function is not primitive recursive, and furthermore, it grows faster than any primitive recursive function. &quot;Primitive recursive&quot; means that you can implement the algorithm with loops where the bound is known beforehand; i.e. you do not need a possible infinitely repeating while loop. This is, granted, a bit of an abstract concept, and to quote Wikipedia again: The importance of primitive recursive functions lies in the fact that most computable functions that are studied in number theory (and more generally in mathematics) are primitive recursive. And yes, it has been proven that Ackermann is not primitive recursive. Discovering that it is actually not so would probably not make you any money, but certainly put your name on a pedestal, in the Theoretical Computer Science community. You cannot optimize this kind of complexity away - consider your program just being a different format of writing a proof that the Ackerman is, indeed, primitive recursive; you'd just have to convert it back into mathematical/logical terms, and voila. The fact that this has not happened over the many years, together with the existence of &quot;proof positive&quot; like the one linked tells you that it is, in fact, behaving as advertised. NB: finally, all of this has also to be seen in the light of the Ackerman function having likely been designed to have this property - as that Wikipedia page mentions, before it was discovered or created, some people thought that all computable functions were primitive recursive. While I don't know what drove Mr. Ackerman to do this back in the 1920's, the Ackerman function is now verily a corner-stone of complexity theory in Theoretical Computer Science; a very fascinating story.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "algorithm",
        "ackermann"
      ],
      "question_score": 43,
      "answer_score": 27,
      "created": "2023-06-26T18:13:11",
      "question_id": 76559257,
      "answer_id": 76561010
    }
  },
  {
    "question": "How can I process a pdf using OpenAI&#39;s APIs (GPTs)?",
    "expected_answer": "May 2025 edit: according to the official guide, using OpenAI GPT-4.1 allows to extract content of (or answer questions on) an input pdf file foobar.pdf stored locally, with a solution along the lines of from openai import OpenAI import os filename = &quot;foobar.pdf&quot; prompt = &quot;&quot;&quot;Extract the content from the file provided without altering it. Just output its exact content and nothing else.&quot;&quot;&quot; client = OpenAI(api_key=os.environ.get(&quot;MY_OPENAI_KEY&quot;)) file = client.files.create( file=open(filename, &quot;rb&quot;), purpose=&quot;user_data&quot; ) response = client.responses.create( model=&quot;gpt-4.1&quot;, input=[ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [ { &quot;type&quot;: &quot;input_file&quot;, &quot;file_id&quot;: file.id, }, { &quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: prompt, }, ] } ] ) The prompt can of course be replaced with the desired user request and I assume that the openai key is stored in a env var named MY_OPENAI_KEY. P.S. I have edited the answer as this approach is much more streamlined w.r.t to the assistants-based 2024 solution that you can see in the edit history, heavily inspired by https://medium.com/@erik-kokalj/effectively-analyze-pdfs-with-gpt-4o-api-378bd0f6be03",
    "context_chunks": [
      {
        "text": "The web interface for ChatGPT has an easy pdf upload. Is there an API from openAI that can receive pdfs? I know there are 3rd party libraries that can read pdf but given there are images and other important information in a pdf, it might be better if a model like GPT 4 Turbo was fed the actual pdf directly. I'll state my use case to add more context. I intent to do RAG. In the code below I handle the PDF and a prompt. Normally I'd append the text at the end of the prompt. I could still do that with a pdf if I extract its contents manually. The following code is taken from here https://platform.openai.com/docs/assistants/tools/code-interpreter. Is this how I'm supposed to do it? # Upload a file with an &quot;assistants&quot; purpose file = client.files.create( file=open(&quot;example.pdf&quot;, &quot;rb&quot;), purpose='assistants' ) # Create an assistant using the file ID assistant = client.beta.assistants.create( instructions=&quot;You are a personal math tutor. When asked a math question, write and run code to answer the question.&quot;, model=&quot;gpt-4-1106-preview&quot;, tools=[{&quot;type&quot;: &quot;code_interpreter&quot;}], file_ids=[file.id] ) There is an upload endpoint as well, but it seems the intent of those endpoints are for fine-tuning and assistants. I think the RAG use case is a normal one and not necessarily related to assistants.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "May 2025 edit: according to the official guide, using OpenAI GPT-4.1 allows to extract content of (or answer questions on) an input pdf file foobar.pdf stored locally, with a solution along the lines of from openai import OpenAI import os filename = &quot;foobar.pdf&quot; prompt = &quot;&quot;&quot;Extract the content from the file provided without altering it. Just output its exact content and nothing else.&quot;&quot;&quot; client = OpenAI(api_key=os.environ.get(&quot;MY_OPENAI_KEY&quot;)) file = client.files.create( file=open(filename, &quot;rb&quot;), purpose=&quot;user_data&quot; ) response = client.responses.create( model=&quot;gpt-4.1&quot;, input=[ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [ { &quot;type&quot;: &quot;input_file&quot;, &quot;file_id&quot;: file.id, }, { &quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: prompt, }, ] } ] ) The prompt can of course be replaced with the desired user request and I assume that the openai key is stored in a env var named MY_OPENAI_KEY. P.S. I have edited the answer as this approach is much more streamlined w.r.t to the assistants-based 2024 solution that you can see in the edit history, heavily inspired by https://medium.com/@erik-kokalj/effectively-analyze-pdfs-with-gpt-4o-api-378bd0f6be03",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "One solution: Convert the pdf to images and feed it to the vision model as multi image inputs https://platform.openai.com/docs/guides/vision. GPT-4 with vision is not a different model that does worse at text tasks because it has vision, it is simply GPT-4 with vision added Since its the same model with vision capabilities, this should be sufficient to do both text and image analysis. You could also choose to extract images from pdf and feed those separately making a multi-model architecture. I have a preference for the first. Ideally experiments should be run to see what produces better results. Text only + images only VS Images (containing both) Pdf to image can be done in python locally as can separating img from pdf. It isn't a difficult task requiring support from someone like openAI.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "machine-learning",
        "pdf",
        "openai-api",
        "chat-gpt-4"
      ],
      "question_score": 33,
      "answer_score": 21,
      "created": "2023-11-12T13:25:44",
      "question_id": 77469097,
      "answer_id": 78924474
    }
  },
  {
    "question": "Docker Alpine build fails on mysqlclient installation with error: Exception: Can not find valid pkg-config name",
    "expected_answer": "I'm using python:3.11.3-slim-bullseye instead of python:3.11-alpine image, but I had the same problem. So you have 2 options: Downgrade the mysqlclient to a previous version, ex: mysqlclient==2.1.1. And now, since the pkg-config is needed for mysqlclient==2.2.0 and on. Add pkg-config to the container. Would be something like this... FROM python:3.11.3-slim-bullseye RUN apt-get update \\ &amp;&amp; apt-get upgrade -y \\ &amp;&amp; apt-get install -y gcc default-libmysqlclient-dev pkg-config \\ &amp;&amp; rm -rf /var/lib/apt/lists/* WORKDIR /usr/src/app COPY . . RUN pip install --upgrade pip \\ &amp;&amp; pip install mysqlclient \\ &amp;&amp; pip install -r requirements.txt CMD [&quot;python&quot;, &quot;manage.py&quot;, &quot;runserver&quot;, &quot;0.0.0.0:8000&quot;] TIP: Maybe you're lacking the instalation of the default-libmysqlclient-dev or libmysqlclient in the container. Hope it helps. :D",
    "context_chunks": [
      {
        "text": "I'm encountering a problem when building a Docker image using a Python-based Dockerfile. I'm trying to use the mysqlclient library (version 2.2.0) and Django (version 4.2.2). Here is my Dockerfile: FROM python:3.11-alpine WORKDIR /usr/src/app COPY requirements.txt . RUN apk add --no-cache gcc musl-dev mariadb-connector-c-dev &amp;&amp; \\ pip install --no-cache-dir -r requirements.txt COPY . . CMD [&quot;python&quot;, &quot;manage.py&quot;, &quot;runserver&quot;, &quot;0.0.0.0:8000&quot;] The problem arises when the Docker build process reaches the point of installing the mysqlclient package. I get the following error: Exception: Can not find valid pkg-config name To address this issue, I tried adding pkgconfig to the apk add command, Unfortunately, this didn't help and the same error persists. I would appreciate any guidance on how to resolve this issue. Thank you in advance.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I'm using python:3.11.3-slim-bullseye instead of python:3.11-alpine image, but I had the same problem. So you have 2 options: Downgrade the mysqlclient to a previous version, ex: mysqlclient==2.1.1. And now, since the pkg-config is needed for mysqlclient==2.2.0 and on. Add pkg-config to the container. Would be something like this... FROM python:3.11.3-slim-bullseye RUN apt-get update \\ &amp;&amp; apt-get upgrade -y \\ &amp;&amp; apt-get install -y gcc default-libmysqlclient-dev pkg-config \\ &amp;&amp; rm -rf /var/lib/apt/lists/* WORKDIR /usr/src/app COPY . . RUN pip install --upgrade pip \\ &amp;&amp; pip install mysqlclient \\ &amp;&amp; pip install -r requirements.txt CMD [&quot;python&quot;, &quot;manage.py&quot;, &quot;runserver&quot;, &quot;0.0.0.0:8000&quot;] TIP: Maybe you're lacking the instalation of the default-libmysqlclient-dev or libmysqlclient in the container. Hope it helps. :D",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Install build dependencies ( specially - pkg-config ) sudo apt-get install python3-dev default-libmysqlclient-dev build-essential pkg-config",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "docker",
        "alpine-linux",
        "mysql-connector-python"
      ],
      "question_score": 31,
      "answer_score": 36,
      "created": "2023-06-22T15:18:10",
      "question_id": 76533384,
      "answer_id": 76560124
    }
  },
  {
    "question": "selenium.common.exceptions.SessionNotCreatedException: This version of ChromeDriver only supports Chrome version 114. LATEST_RELEASE_115 doesn&#39;t exist",
    "expected_answer": "For Chrome/chromedriver 116+, the solution is similar to https://stackoverflow.com/a/76731553/7058266, but now you need a minimum selenium version of 4.11.2. Then selenium gets the correct driver for you at runtime. from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service() options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... Automate something here driver.quit() For more info on the new Chrome-for-Testing, see https://googlechromelabs.github.io/chrome-for-testing/ (this also includes chromedrivers for testing).",
    "context_chunks": [
      {
        "text": "#Once the zip has finished downloading, extract the folder and copy the path of the chromedriver exe file (should be the #first one), add it to your code like this, from selenium import webdriver from selenium.webdriver.chrome.service import Service url = &quot;somewebsite.com&quot; service_obj = Service(&quot;D:\\\\Users\\\\eggman\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe&quot;) driver = webdriver.Chrome(service=service_obj) driver.get(url) Returns the error: selenium.common.exceptions.SessionNotCreatedException: This version of ChromeDriver only supports Chrome version 114. LATEST_RELEASE_115 doesn't exist I'm guessing to avoid this in the future I can just turn off automatic updates? I originally used the following code which worked fine driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "For Chrome/chromedriver 116+, the solution is similar to https://stackoverflow.com/a/76731553/7058266, but now you need a minimum selenium version of 4.11.2. Then selenium gets the correct driver for you at runtime. from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service() options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... Automate something here driver.quit() For more info on the new Chrome-for-Testing, see https://googlechromelabs.github.io/chrome-for-testing/ (this also includes chromedrivers for testing).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Please use the below command to upgrade Selenium. It helped me out to get resolve the issue. This version of Selenium will start recognizing the correct browser version. (Environment: Windows 10, PyCharm, and venv) pip install -U selenium==4.11.2",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "google-chrome",
        "selenium-webdriver",
        "selenium-chromedriver",
        "chromium"
      ],
      "question_score": 33,
      "answer_score": 46,
      "created": "2023-08-16T13:13:14",
      "question_id": 76913935,
      "answer_id": 76914590
    }
  },
  {
    "question": "Selecting default search engine is needed for Chrome version 127",
    "expected_answer": "You need to add this Chrome Option to disable the 'choose your search engine' screen: options.addArguments(&quot;--disable-search-engine-choice-screen&quot;); If you are using selenium with Python, you'll have to use: options.add_argument(&quot;--disable-search-engine-choice-screen&quot;)",
    "context_chunks": [
      {
        "text": "All of my Selenium scripts are raising errors after Chrome updated to version 127 because I always have to select a default search engine when the browser is being launched. I use ChromeDriver 127.0.6533.72. How to fix it?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You need to add this Chrome Option to disable the 'choose your search engine' screen: options.addArguments(&quot;--disable-search-engine-choice-screen&quot;); If you are using selenium with Python, you'll have to use: options.add_argument(&quot;--disable-search-engine-choice-screen&quot;)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "David's solution worked for me. You can add the arguments like so: from selenium import webdriver from selenium.webdriver.chrome.options import Options chrome_options = Options() chrome_options.add_argument(&quot;--disable-search-engine-choice-screen&quot;) driver = webdriver.Chrome(options=chrome_options)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver"
      ],
      "question_score": 33,
      "answer_score": 61,
      "created": "2024-07-24T09:00:03",
      "question_id": 78787332,
      "answer_id": 78788229
    }
  },
  {
    "question": "Selenium WebDriver Chrome 115 stopped working",
    "expected_answer": "Selenium Manager is now fully included with selenium 4.10.0 (and newer), so this is all you need: from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service() options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... driver.quit() If the driver isn't found on your system PATH, Selenium Manager will automatically download it. If you're wondering why you're now seeing this error for ChromeDriverManager, it's because developer.chrome.com/docs/chromedriver/downloads only goes up to version 114 due to driver restructuring by the Chromium Team for the new Chrome-for-Testing.",
    "context_chunks": [
      {
        "text": "I have Chrome 115.0.5790.99 installed on Windows, and I use Selenium 4.10.0. In my Python code, I call service = Service(ChromeDriverManager().install()) and it returns the error: ValueError: There is no such driver by url [sic] https://chromedriver.storage.googleapis.com/LATEST_RELEASE_115.0.5790. I use ChromeDriverManager().install() in order to ensure the use of last stable version of webdriver. How to solve the issue? My simple code: from selenium import webdriver from selenium.webdriver.chrome.service import Service from webdriver_manager.chrome import ChromeDriverManager import time # Install Webdriver service = Service(ChromeDriverManager().install()) # Create Driver Instance driver = webdriver.Chrome(service=service) # Get Web Page driver.get('https://www.crawler-test.com') time.sleep(5) driver.quit() Error output: Traceback (most recent call last): File &quot;C:\\Users\\Administrator\\Documents\\...\\test.py&quot;, line 7, in &lt;module&gt; service = Service(ChromeDriverManager().install()) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\webdriver_manager\\chrome.py&quot;, line 39, in install driver_path = self._get_driver_path(self.driver) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\webdriver_manager\\core\\manager.py&quot;, line 30, in _get_driver_path file = self._download_manager.download_file(driver.get_driver_download_url()) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\webdriver_manager\\drivers\\chrome.py&quot;, line 40, in get_driver_download_url driver_version_to_download = self.get_driver_version_to_download() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\webdriver_manager\\core\\driver.py&quot;, line 51, in get_driver_version_to_download self._driver_to_download_version = self._version if self._version not in (None, &quot;latest&quot;) else self.get_latest_release_version() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\webdriver_manager\\drivers\\chrome.py&quot;, line 62, in get_latest_release_version resp = self._http_client.get(url=latest_release_url) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\webdriver_manager\\core\\http.py&quot;, line 37, in get self.validate_response(resp) File &quot;C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\webdriver_manager\\core\\http.py&quot;, line 16, in validate_response raise ValueError(f&quot;There is no such driver by url {resp.url}&quot;) ValueError: There is no such driver by url https://chromedriver.storage.googleapis.com/LATEST_RELEASE_115.0.5790 I tried the following but no success: to disable Chrome auto-update, but Chrome manages to update itself anyway (https://www.minitool.com/news/disable-automatic-chrome-updates.html and https://www.webnots.com/7-ways-to-disable-automatic-chrome-update-in-windows-and-mac); to install Chrome 114 and webdriver for version 114, than it works till Chrome get updated automatically; to follow instructions https://chromedriver.chromium.org/downloads/version-selection but when generating URL and running link https://chromedriver.storage.googleapis.com/LATEST_RELEASE_115.0.5790 I get error No such object: chromedriver/LATEST_RELEASE_115.0.5790 How can I solve the issue till webdriver for Chrome 115 will be finally released at the download location?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Selenium Manager is now fully included with selenium 4.10.0 (and newer), so this is all you need: from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service() options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... driver.quit() If the driver isn't found on your system PATH, Selenium Manager will automatically download it. If you're wondering why you're now seeing this error for ChromeDriverManager, it's because developer.chrome.com/docs/chromedriver/downloads only goes up to version 114 due to driver restructuring by the Chromium Team for the new Chrome-for-Testing.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This worked for me: service = Service(ChromeDriverManager(version=&quot;114.0.5735.90&quot;).install())",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "google-chrome",
        "selenium-webdriver"
      ],
      "question_score": 30,
      "answer_score": 37,
      "created": "2023-07-20T07:55:42",
      "question_id": 76727774,
      "answer_id": 76731553
    }
  },
  {
    "question": "Why is builtin sorted() slower for a list containing descending numbers if each number appears twice consecutively?",
    "expected_answer": "As alluded to in the comments by btilly and Amadan, this is due to how the Timsort sorting algorithm works. Detailed description of the algorithm is here. Timsort speeds up operation on partially sorted arrays by identifying runs of sorted elements. A run is either &quot;ascending&quot;, which means non-decreasing: a0 &lt;= a1 &lt;= a2 &lt;= ... or &quot;descending&quot;, which means strictly decreasing: a0 &gt; a1 &gt; a2 &gt; ... Note that a run is always at least 2 long, unless we start at the array's last element. Your arrays a, b and c each consist of just one run. The array d has 1 million runs. The reason why the descending run cannot be &gt;= is to make the sort stable, i.e. keep the order of equal elements: The definition of descending is strict, because the main routine reverses a descending run in-place, transforming a descending run into an ascending run. Reversal is done via the obvious fast &quot;swap elements starting at each end, and converge at the middle&quot; method, and that can violate stability if the slice contains any equal elements. Using a strict definition of descending ensures that a descending run contains distinct elements. Python 3.11 has slightly improved version of timsort, sometimes called powersort, but it uses the same run detection and thus has the same performance.",
    "context_chunks": [
      {
        "text": "I sorted four similar lists. List d consistently takes much longer than the others, which all take about the same time: a: 33.5 ms b: 33.4 ms c: 36.4 ms d: 110.9 ms Why is that? Test script (Attempt This Online!): from timeit import repeat n = 2_000_000 a = [i // 1 for i in range(n)] # [0, 1, 2, 3, ..., 1_999_999] b = [i // 2 for i in range(n)] # [0, 0, 1, 1, 2, 2, ..., 999_999] c = a[::-1] # [1_999_999, ..., 3, 2, 1, 0] d = b[::-1] # [999_999, ..., 2, 2, 1, 1, 0, 0] for name in 'abcd': lst = globals()[name] time = min(repeat(lambda: sorted(lst), number=1)) print(f'{name}: {time*1e3 :5.1f} ms')",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "As alluded to in the comments by btilly and Amadan, this is due to how the Timsort sorting algorithm works. Detailed description of the algorithm is here. Timsort speeds up operation on partially sorted arrays by identifying runs of sorted elements. A run is either &quot;ascending&quot;, which means non-decreasing: a0 &lt;= a1 &lt;= a2 &lt;= ... or &quot;descending&quot;, which means strictly decreasing: a0 &gt; a1 &gt; a2 &gt; ... Note that a run is always at least 2 long, unless we start at the array's last element. Your arrays a, b and c each consist of just one run. The array d has 1 million runs. The reason why the descending run cannot be &gt;= is to make the sort stable, i.e. keep the order of equal elements: The definition of descending is strict, because the main routine reverses a descending run in-place, transforming a descending run into an ascending run. Reversal is done via the obvious fast &quot;swap elements starting at each end, and converge at the middle&quot; method, and that can violate stability if the slice contains any equal elements. Using a strict definition of descending ensures that a descending run contains distinct elements. Python 3.11 has slightly improved version of timsort, sometimes called powersort, but it uses the same run detection and thus has the same performance.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As @jpa's answer explained, a, b and c have a single &quot;run&quot; while d has a million. We can also see the effect of that by counting the comparisons: a: 1999999 comparisons, 1.000 per element b: 1999999 comparisons, 1.000 per element c: 1999999 comparisons, 1.000 per element d: 10710650 comparisons, 5.355 per element A single long run means we only compare each pair of neighbors once, that's all. For n elements, that's n-1 comparisons. Since a and b are ascending, they're already sorted, and nothing further is done. Since c is descending, it simply gets reversed, and then nothing further is done. For d, the two-element runs first get combined into runs of 32 to 64 elements using binary insertion sort. And then these longer runs get merged again and again into larger and larger runs until the whole list is sorted. I tried various values of n. The number of comparisons for d hovered around 5 per element, going up to ~5.3 until n reached a power of 2, after which the number of comparisons fell to ~4.7 again. In general, d requires O(n) comparisons. It doesn't need n log n comparisons, because the merging uses &quot;galloping&quot; (an exponential search), which in this case needs only logarithmically many comparisons. So unlike ordinary merge sort in general cases, the recurrence relation for the number of comparisons here isn't T(n) = 2T(n/2) + n but T(n) = 2T(n/2) + log(n), which is O(n). The runtime however isn't O(n), as there are still n log n moves of elements. But that's low-level and very fast, relatively fast compared to the O(n) slower comparisons. If you measure time for various n, you might very well think it's O(n) time. The script for counting (Attempt This Online!): n = 2_000_000 a = [i // 1 for i in range(n)] b = [i // 2 for i in range(n)] c = a[::-1] d = b[::-1] class Int: def __init__(self, value): self.value = value def __lt__(self, other): global comparisons comparisons += 1 return self.value &lt; other.value for name in 'abcd': lst = globals()[name] comparisons = 0 sorted(map(Int, lst)) print(f'{name}: {comparisons} comparisons, {comparisons/len(lst) :5.3f} per element')",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "performance",
        "sorting",
        "time-complexity"
      ],
      "question_score": 32,
      "answer_score": 43,
      "created": "2024-03-05T15:21:50",
      "question_id": 78108792,
      "answer_id": 78108912
    }
  },
  {
    "question": "How to create a langchain doc from an str?",
    "expected_answer": "This worked for me: from langchain.docstore.document import Document doc = Document(page_content=&quot;text&quot;, metadata={&quot;source&quot;: &quot;local&quot;})",
    "context_chunks": [
      {
        "text": "I've searched all over langchain documentation on their official website but I didn't find how to create a langchain doc from a str variable in python so I searched in their GitHub code and I found this : doc=Document( page_content=&quot;text&quot;, metadata={&quot;source&quot;: &quot;local&quot;} ) PS: I added the metadata attribute then I tried using that doc with my chain: Memory and Chain: memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, input_key=&quot;human_input&quot;) chain = load_qa_chain( llm, chain_type=&quot;stuff&quot;, memory=memory, prompt=prompt ) the call method: chain({&quot;input_documents&quot;: doc, &quot;human_input&quot;: query}) prompt template: template = &quot;&quot;&quot;You are a senior financial analyst analyzing the below document and having a conversation with a human. {context} {chat_history} Human: {human_input} senior financial analyst:&quot;&quot;&quot; prompt = PromptTemplate( input_variables=[&quot;chat_history&quot;, &quot;human_input&quot;, &quot;context&quot;], template=template ) but I am getting the following error: AttributeError: 'tuple' object has no attribute 'page_content' when I tried to check the type and the page content of the Document object before using it with the chain I got this print(type(doc)) &lt;class 'langchain.schema.Document'&gt; print(doc.page_content) &quot;text&quot;",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This worked for me: from langchain.docstore.document import Document doc = Document(page_content=&quot;text&quot;, metadata={&quot;source&quot;: &quot;local&quot;})",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "First, some context. From what I've learned so far, a Document is a list of Document objects. If you run type(doc[0]) you get langchain.schema.document.Document. This Document object is a dictionary made of two keys: one is page_content: which accepts string values, and the second key is metadata: which only accepts dictionaries. {page_content: str, metadata: dict}. By default (don't quote me on this: it's been lots of trial and error and, as you mentioned, there is no documentation), an &quot;empty&quot; Document contains the two mentioned keys, and a single dictionary in its metadata: with one key: {source:} that only accepts strings. You can create a multiple &quot;page&quot; Document object by creating a list of Document objects like so: First, you must have a list of string texts: text_list below, and a list of dictionaries for the metadata: text_list below. You must ensure both lists are the same length. from langchain.docstore.document import Document document = [] for item in range(len(text_string)): page = Document(page_content=doc_text_splits[item], metadata = metadata_string[item]) doc.append(page) Additionally, you can also create Document object using any splitter from LangChain: from langchain.text_splitter import CharacterTextSplitter doc_creator = CharacterTextSplitter(parameters) document = doc_creator.create_documents(texts = text_list, metadatas = metadata_list)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "nlp",
        "langchain",
        "large-language-model"
      ],
      "question_score": 31,
      "answer_score": 51,
      "created": "2023-06-25T15:09:42",
      "question_id": 76551067,
      "answer_id": 76651436
    }
  },
  {
    "question": "TypeError: WebDriver.__init__() got multiple values for argument &#39;options&#39;",
    "expected_answer": "This is due to changes in selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that the first argument is no longer executable_path, but options. (That's why it complains that you're passing it in twice.) If you want to pass in an executable_path, you'll have to use the service arg now. from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service(executable_path=r'/usr/bin/chromedriver') options = webdriver.ChromeOptions() options.add_argument('--headless') options.add_argument('--no-sandbox') options.add_argument('--disable-dev-shm-usage') driver = webdriver.Chrome(service=service, options=options) # ... driver.quit()",
    "context_chunks": [
      {
        "text": "Error is: TypeError: WebDriver.__init__() got multiple values for argument 'options' ` The code is: chrome_options = Options() chrome_options.add_argument('--headless') chrome_options.add_argument('--no-sandbox') chrome_options.add_argument('--disable-dev-shm-usage') browser = webdriver.Chrome(r'/usr/bin/chromedriver', options=chrome_options) This is the error: TypeError Traceback (most recent call last) &lt;ipython-input-5-9a7e59e392ae&gt; in &lt;cell line: 6&gt;() 4 chrome_options.add_argument('--headless') 5 ----&gt; 6 browser = webdriver.Chrome(r'/usr/bin/chromedriver', options=chrome_options) TypeError: WebDriver.__init__() got multiple values for argument 'options'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is due to changes in selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that the first argument is no longer executable_path, but options. (That's why it complains that you're passing it in twice.) If you want to pass in an executable_path, you'll have to use the service arg now. from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service(executable_path=r'/usr/bin/chromedriver') options = webdriver.ChromeOptions() options.add_argument('--headless') options.add_argument('--no-sandbox') options.add_argument('--disable-dev-shm-usage') driver = webdriver.Chrome(service=service, options=options) # ... driver.quit()",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I got this answer from this blog and it worked! https://nariyoo.com/python-how-to-run-selenium-in-google-colab/ [Python] How to run selenium in Google Colab !pip install chromedriver-autoinstaller import sys sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver') import time import pandas as pd from bs4 import BeautifulSoup from selenium import webdriver import chromedriver_autoinstaller # setup chrome options chrome_options = webdriver.ChromeOptions() chrome_options.add_argument('--headless') # ensure GUI is off chrome_options.add_argument('--no-sandbox') chrome_options.add_argument('--disable-dev-shm-usage') # set path to chromedriver as per your configuration chromedriver_autoinstaller.install() # set the target URL url = &quot;put-url-here-to-scrape&quot; # set up the webdriver driver = webdriver.Chrome(options=chrome_options) Now you can easily import other selenium library that you need and the driver will be worked. I hope it help!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "selenium-webdriver",
        "selenium-chromedriver",
        "webdriver"
      ],
      "question_score": 30,
      "answer_score": 66,
      "created": "2023-06-08T04:11:08",
      "question_id": 76428561,
      "answer_id": 76432322
    }
  },
  {
    "question": "&#39;super&#39; object has no attribute &#39;__sklearn_tags__&#39;",
    "expected_answer": "Scikit-learn version 1.6.0 modified the API around its &quot;tags&quot;, and that's the cause of this error. XGBoost made the necessary changes in version 2.1.4 (specifically in PR11021). In sklearn 1.6.1, the error was downgraded to a warning (to be returned to an error in 1.7). So you should be OK with any of: xgboost &gt;=2.1.4 sklearn &gt;=1.6.1,&lt;1.7, and expect DeprecationWarnings sklearn &lt;1.6 See also sklearn Issue#30479 and 1.6.1 release notes, and xgboost 2.1.4 release notes.",
    "context_chunks": [
      {
        "text": "I am encountering an AttributeError while fitting an XGBRegressor using RandomizedSearchCV from Scikit-learn. The error message states: 'super' object has no attribute '__sklearn_tags__'. This occurs when I invoke the fit method on the RandomizedSearchCV object. I suspect it could be related to compatibility issues between Scikit-learn and XGBoost or Python version. I am using Python 3.12, and both Scikit-learn and XGBoost are installed with their latest versions. I attempted to tune the hyperparameters of an XGBRegressor using RandomizedSearchCV from Scikit-learn. I expected the model to fit the training data without issues and provide the best parameters after cross-validation. I also checked for compatibility issues, ensured the libraries were up-to-date, and reinstalled Scikit-learn and XGBoost, but the error persists.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Scikit-learn version 1.6.0 modified the API around its &quot;tags&quot;, and that's the cause of this error. XGBoost made the necessary changes in version 2.1.4 (specifically in PR11021). In sklearn 1.6.1, the error was downgraded to a warning (to be returned to an error in 1.7). So you should be OK with any of: xgboost &gt;=2.1.4 sklearn &gt;=1.6.1,&lt;1.7, and expect DeprecationWarnings sklearn &lt;1.6 See also sklearn Issue#30479 and 1.6.1 release notes, and xgboost 2.1.4 release notes.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I also had a similiar error,Xg boost too. But what the guy above me said, uninstall and reinstalling a lower version of sklearn (i used version 1.5.2) fixed this issue for me! !pip uninstall -y scikit-learn !pip install scikit-learn==1.5.2",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "machine-learning",
        "scikit-learn",
        "xgboost"
      ],
      "question_score": 29,
      "answer_score": 30,
      "created": "2024-12-18T11:45:52",
      "question_id": 79290968,
      "answer_id": 79291260
    }
  },
  {
    "question": "SSL: CERTIFICATE_VERIFY_FAILED certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)&#39;)))",
    "expected_answer": "Try this in the command line: pip install pip-system-certs I've been struggling with the CERTIFICATE_VERIFY_FAILED error as well when using the requests module. I tried installing certifi but that didn't help. The only solution to my problem was to install pip-system-certs. For some reason that allowed requests to access a local certificate. Also: requests.get(url, verify=False) is not recommended for production environment because you basically turn safety off.",
    "context_chunks": [
      {
        "text": "Working on scripts to connect to AWS and recently started getting this error when I try to install a python module or execute a script I get the following error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129) It appears I have defined the certificate in the environment path so not sure what else to use to troubleshoot the issue.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Try this in the command line: pip install pip-system-certs I've been struggling with the CERTIFICATE_VERIFY_FAILED error as well when using the requests module. I tried installing certifi but that didn't help. The only solution to my problem was to install pip-system-certs. For some reason that allowed requests to access a local certificate. Also: requests.get(url, verify=False) is not recommended for production environment because you basically turn safety off.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Try to set SSL_CERT_DIR environment variable: export SSL_CERT_DIR=/etc/ssl/certs/ or alternatively the SSL_CERT_FILE env variable export SSL_CERT_FILE=/etc/ssl/certs/your_cert.pem",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 27,
      "answer_score": 58,
      "created": "2023-11-08T00:14:45",
      "question_id": 77442172,
      "answer_id": 77491061
    }
  },
  {
    "question": "MultiQC: ModuleNotFoundError: No module named &#39;imp&#39;",
    "expected_answer": "Python 3.12 is new and the consequences of the changes it introduces (like dropping the imp module) need to propagate to the community. Stay on Python 3.11 for now.",
    "context_chunks": [
      {
        "text": "I am running fastqc and multiqc in ubuntu linux terminal. fastqc runs perfectly without any issues but multiqc fails to run, showing the message. No idea how to fix the missing 'imp' module. I used the command conda install multiqc to install multiqc in the existing conda environment. I tried to install it in a new conda environment. Still, its showing the same message.The python version currently running is 3.12.0. How to fix the issue?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Python 3.12 is new and the consequences of the changes it introduces (like dropping the imp module) need to propagate to the community. Stay on Python 3.11 for now.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "If you are using python3.12, you should use importlib instead of imp module. up to python 3.11 =&gt; imp python 3.12+ ==&gt; importlib",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "module",
        "conda",
        "imp"
      ],
      "question_score": 27,
      "answer_score": 51,
      "created": "2023-10-11T15:48:42",
      "question_id": 77274572,
      "answer_id": 77281165
    }
  },
  {
    "question": "A module that was compiled using NumPy 1.x cannot be run in NumPy 2.0.0 as it may crash",
    "expected_answer": "Try using: pip uninstall numpy pip install numpy==1.26.4",
    "context_chunks": [
      {
        "text": "Traceback (most recent call last): File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\tut2.py&quot;, line 8, in &lt;module&gt; import imutils File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\.venv\\Lib\\site-packages\\imutils\\__init__.py&quot;, line 8, in &lt;module&gt; from .convenience import translate File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\.venv\\Lib\\site-packages\\imutils\\convenience.py&quot;, line 6, in &lt;module&gt; import cv2 File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\.venv\\Lib\\site-packages\\cv2\\__init__.py&quot;, line 181, in &lt;module&gt; bootstrap() File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\.venv\\Lib\\site-packages\\cv2\\__init__.py&quot;, line 153, in bootstrap native_module = importlib.import_module(&quot;cv2&quot;) File &quot;C:\\Program Files\\Python312\\Lib\\importlib\\__init__.py&quot;, line 90, in import_module return _bootstrap._gcd_import(name[level:], package, level) AttributeError: _ARRAY_API not found Traceback (most recent call last): File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\tut2.py&quot;, line 8, in &lt;module&gt; import imutils File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\.venv\\Lib\\site-packages\\imutils\\__init__.py&quot;, line 8, in &lt;module&gt; from .convenience import translate File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\.venv\\Lib\\site-packages\\imutils\\convenience.py&quot;, line 6, in &lt;module&gt; import cv2 File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\.venv\\Lib\\site-packages\\cv2\\__init__.py&quot;, line 181, in &lt;module&gt; bootstrap() File &quot;C:\\Users\\mohit\\OneDrive\\Desktop\\Front End\\Flask\\pythonProject1\\.venv\\Lib\\site-packages\\cv2\\__init__.py&quot;, line 153, in bootstrap native_module = importlib.import_module(&quot;cv2&quot;) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Program Files\\Python312\\Lib\\importlib\\__init__.py&quot;, line 90, in import_module return _bootstrap._gcd_import(name[level:], package, level) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ImportError: numpy.core.multiarray failed to import A module that was compiled using NumPy 1.x cannot be run in NumPy 2.0.0 as it may crash. To support both 1.x and 2.x versions of NumPy, modules must be compiled with NumPy 2.0. Some module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'. If you are a user of the module, the easiest solution will be to downgrade to 'numpy&lt;2' or try to upgrade the affected module. We expect that some modules will need time to support NumPy 2. I was trying to make a flask app that uses cv2 and imutils libraries of python in Pycharm. But there seems to be a problem in Pycharm. The error seems to be on the lines of import statement : import imutils import cv2 I watched a tutorial online about how to download cv2 and imutils , but even after downloading there seems to be a problem. The error is about the upgraded version of Numpy which I don't understand why it is happening , as I am not using Numpy. Still I upgraded the version of Numpy but It does not work. I have tried multiple things the people have suggested but nothing is working I think i am not sure how to make a docker. The problem can be something related to Pycharm. I am not sure.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Try using: pip uninstall numpy pip install numpy==1.26.4",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "pip uninstall numpy # uninstall existing numpy pip install &quot;numpy&lt;2.0&quot; # install the latest numpy 1.x version",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "opencv",
        "flask"
      ],
      "question_score": 26,
      "answer_score": 35,
      "created": "2024-06-18T10:49:29",
      "question_id": 78636947,
      "answer_id": 78637197
    }
  },
  {
    "question": "How can i solve ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1` when using Huggingface&#39;s TrainArguments?",
    "expected_answer": "If you're not particular about which transformers and accelerate version to tie to, then do this to use the most up-to-date version in Google Colab: ! pip install -U accelerate ! pip install -U transformers Then the issue you are having with accelerate should auto-resolve itself. Note: Underspecifying pip install -U transformers instead of pip install transformers[pytorch] might be easier since that's what most of the users do and the developers of the library will make sure that the basic pip works with the common functions and class like TrainingArguments Instead of specifying accelerate to the pip install accelerate&gt;=0.20.1, if you have no particular need to fixed the version, automatically upgrading to the latest version might get you more stability when using the library, esp. with &quot;hot&quot;/&quot;trending&quot; libraries that are constantly changing (almost) daily. If further debugging is necessary, i.e. if the above didn't work. To check your transformers and accelerate version, do this: import accelerate accelerate.__version__ Most probably you might have an ImportError at the first line if accelerate is not already installed when you installed transformers. And then if the first line works and the 2nd line is not outputting a version &gt;=0.20.1, then that is the cause of your issue. The current versions to-date (July 2023) are: import accelerate import transformers transformers.__version__, accelerate.__version__ [out]: ('4.30.1', '0.21.0') Here's an example notebook with the model that you wish to use as per the comments in your question, https://colab.research.google.com/drive/1D79AjHMeE6HAZC-g2S83baTgsHtDUu5i?usp=sharing If the error persist after the pip install ..., try restarting the runtime. If you can't find the buttons to press to restart, try this in the cell Restart kernel in Google Colab then re-run the cells for import ... import os os._exit(00)",
    "context_chunks": [
      {
        "text": "I'm using the transformers library in Google colab, and When i am using TrainingArguments from transformers library i'm getting Import error with this code: from transformers import TrainingArguments training_args = TrainingArguments( output_dir = &quot;/content/our-model&quot;, learning_rate=2e-5, per_device_train_batch_size= 64, per_device_eval_batch_size = 16, num_train_epochs = 2, weight_decay = 0.01, evaluation_strategy = &quot;epoch&quot;, save_strategy = &quot;epoch&quot;, load_best_model_at_end = True, push_to_hub = False ) This is the error i'm getting: &lt;ipython-input-28-0518ea5ff407&gt; in &lt;cell line: 2&gt;() 1 from transformers import TrainingArguments ----&gt; 2 training_args = TrainingArguments( 3 output_dir = &quot;/content/our-model&quot;, 4 learning_rate=2e-5, 5 per_device_train_batch_size= 64, 4 frames /usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self) 1670 if not is_sagemaker_mp_enabled(): 1671 if not is_accelerate_available(min_version=&quot;0.20.1&quot;): -&gt; 1672 raise ImportError( 1673 &quot;Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`&quot; 1674 ) ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U I already tried pip install for 0.20.1 version of accelerate and pip install transformers[torch] and both didn't worked.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "If you're not particular about which transformers and accelerate version to tie to, then do this to use the most up-to-date version in Google Colab: ! pip install -U accelerate ! pip install -U transformers Then the issue you are having with accelerate should auto-resolve itself. Note: Underspecifying pip install -U transformers instead of pip install transformers[pytorch] might be easier since that's what most of the users do and the developers of the library will make sure that the basic pip works with the common functions and class like TrainingArguments Instead of specifying accelerate to the pip install accelerate&gt;=0.20.1, if you have no particular need to fixed the version, automatically upgrading to the latest version might get you more stability when using the library, esp. with &quot;hot&quot;/&quot;trending&quot; libraries that are constantly changing (almost) daily. If further debugging is necessary, i.e. if the above didn't work. To check your transformers and accelerate version, do this: import accelerate accelerate.__version__ Most probably you might have an ImportError at the first line if accelerate is not already installed when you installed transformers. And then if the first line works and the 2nd line is not outputting a version &gt;=0.20.1, then that is the cause of your issue. The current versions to-date (July 2023) are: import accelerate import transformers transformers.__version__, accelerate.__version__ [out]: ('4.30.1', '0.21.0') Here's an example notebook with the model that you wish to use as per the comments in your question, https://colab.research.google.com/drive/1D79AjHMeE6HAZC-g2S83baTgsHtDUu5i?usp=sharing If the error persist after the pip install ..., try restarting the runtime. If you can't find the buttons to press to restart, try this in the cell Restart kernel in Google Colab then re-run the cells for import ... import os os._exit(00)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In my case, I tried with: ! pip install -U accelerate ! pip install -U transformers But, it didn't work and then I checked the current version of accelerate by, import accelerate accelerate.__version__ and found the version is 0.31.0 but it was expecting 0.21.0. Then I installed the exact version and successfully worked: ! pip install -U 'accelerate==0.21.0' You must have to restart the kernel",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "nlp",
        "importerror",
        "huggingface-transformers",
        "huggingface"
      ],
      "question_score": 27,
      "answer_score": 39,
      "created": "2023-06-10T21:51:03",
      "question_id": 76448287,
      "answer_id": 76452964
    }
  },
  {
    "question": "Why is &quot;dict[int, int]&quot; incompatible with &quot;dict[int, int | str]&quot;?",
    "expected_answer": "This code may seem correct on first sight if you think of only reading from the dicts: a: dict[int, int] = {} b: dict[int, int | str] = a However, if you ever write to them, you can see how it would be wrong to allow that: b[1] = &quot;x&quot; assert isinstance(a[1], int) # fails The difference with Mapping type is that it does not support modifications. (Chukwujiobi's answer explains it well in legal language, if you want a more precise explanation)",
    "context_chunks": [
      {
        "text": "import typing a: dict[int, int] = {} b: dict[int, int | str] = a c: typing.Mapping[int, int | str] = a d: typing.Mapping[int | str, int] = a Pylance reports an error for b: dict[int, int | str] = a: Expression of type &quot;dict[int, int]&quot; is incompatible with declared type &quot;dict[int, int | str]&quot; &quot;dict[int, int]&quot; is incompatible with &quot;dict[int, int | str]&quot; Type parameter &quot;_VT@dict&quot; is invariant, but &quot;int&quot; is not the same as &quot;int | str&quot; Consider switching from &quot;dict&quot; to &quot;Mapping&quot; which is covariant in the value type But c: typing.Mapping[int, int | str] = a is OK. Additionally, d: typing.Mapping[int | str, int] = a also gets an error: Expression of type &quot;dict[int, int]&quot; is incompatible with declared type &quot;Mapping[int | str, int]&quot; &quot;dict[int, int]&quot; is incompatible with &quot;Mapping[int | str, int]&quot; Type parameter &quot;_KT@Mapping&quot; is invariant, but &quot;int&quot; is not the same as &quot;int | str&quot; Why are these types hint incompatible? If a function declares a parameter of type dict[int, int | str], how can I pass a dict[int, int] object as its parameter?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This code may seem correct on first sight if you think of only reading from the dicts: a: dict[int, int] = {} b: dict[int, int | str] = a However, if you ever write to them, you can see how it would be wrong to allow that: b[1] = &quot;x&quot; assert isinstance(a[1], int) # fails The difference with Mapping type is that it does not support modifications. (Chukwujiobi's answer explains it well in legal language, if you want a more precise explanation)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "dict type was designed to be completely invariant on key and value. Hence when you assign dict[int, int] to dict[int, int | str], you make the type system raise errors. [1] Mapping type on the other hand wasn’t designed to be completely invariant but rather is invariant on key and covariant on value. Hence you can assign one Mapping type (dict[int, int]) to another (Mapping[int, int | str]) if they are both covariant on value. if they are invariant on key, you can assign them else you cannot. Hence when you assign dict[int, int] to Mapping[int | str, int], you make the type system raise errors. [2][3] There is a good reason for the above design in the type system and I will give a few: 1. dict type is a concrete type so it will actually get used in a program. 2. Because of the above mentioned, it was designed the way it was to avoid things like this: a: dict[int, int] = {} b: dict[int, int | str] = a b[0] = 0xDEADBEEF b[1] = \"Bull\" dicts are assigned by reference [4] hence any mutation to b is actually a mutation to a. So if one reads a as follows: x: int = a[0] assert isinstance(x, int) y: int = a[1] assert isinstance(y, int) One gets unexpected results. x passes but y doesn’t. It then seems like the type system is contradicting itself. This can cause worse problems in a program. For posterity, to correctly type a dictionary in Python, use Mapping type to denote a readonly dictionary and use MutableMapping type to denote a read-write dictionary. [1] Of course Python’s type system doesn’t influence program’s running behaviour but at least linters have some use of this. [2] dict type is a Mapping type but Mapping type is not a dict type. [3] Keep in mind that the ordering of types is important in type theory. [4] All variable names in Python are references to values.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-typing",
        "type-theory"
      ],
      "question_score": 29,
      "answer_score": 52,
      "created": "2024-05-27T03:25:43",
      "question_id": 78537075,
      "answer_id": 78537805
    }
  },
  {
    "question": "NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported",
    "expected_answer": "Try doing: pip install -U datasets This error stems from a breaking change in fsspec. It has been fixed in the latest datasets release (2.14.6). Updating the installation with pip install -U datasets should fix the issue. git link : https://github.com/huggingface/datasets/issues/6352 If you are using fsspec, then do: pip install fsspec==2023.9.2 There is a problem with fsspec==2023.10.0 git link : https://github.com/huggingface/datasets/issues/6330 Edit: Looks like it broken again in 2.17 and 2.18 downgrading to 2.16 should work.",
    "context_chunks": [
      {
        "text": "I try to load a dataset using the datasets python module in my local Python Notebook. I am running a Python 3.10.13 kernel as I do for my virtual environment. I cannot load the datasets I am following from a tutorial. Here's the error: --------------------------------------------------------------------------- NotImplementedError Traceback (most recent call last) /Users/ari/Downloads/00-fine-tuning.ipynb Celda 2 line 3 1 from datasets import load_dataset ----&gt; 3 data = load_dataset( 4 &quot;jamescalam/agent-conversations-retrieval-tool&quot;, 5 split=&quot;train&quot; 6 ) 7 data File ~/Documents/fastapi_language_tutor/env/lib/python3.10/site-packages/datasets/load.py:2149, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs) 2145 # Build dataset for splits 2146 keep_in_memory = ( 2147 keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size) 2148 ) -&gt; 2149 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory) 2150 # Rename and cast features to match task schema 2151 if task is not None: 2152 # To avoid issuing the same warning twice File ~/Documents/fastapi_language_tutor/env/lib/python3.10/site-packages/datasets/builder.py:1173, in DatasetBuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory) 1171 is_local = not is_remote_filesystem(self._fs) 1172 if not is_local: -&gt; 1173 raise NotImplementedError(f&quot;Loading a dataset cached in a {type(self._fs).__name__} is not supported.&quot;) 1174 if not os.path.exists(self._output_dir): 1175 raise FileNotFoundError( 1176 f&quot;Dataset {self.dataset_name}: could not find data in {self._output_dir}. Please make sure to call &quot; 1177 &quot;builder.download_and_prepare(), or use &quot; 1178 &quot;datasets.load_dataset() before trying to access the Dataset object.&quot; 1179 ) NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported. How do I resolve this? I don't understand how this error is applicable, given that the dataset is something I am fetching and thus cannot be cached in my LocalFileSystem in the first place.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Try doing: pip install -U datasets This error stems from a breaking change in fsspec. It has been fixed in the latest datasets release (2.14.6). Updating the installation with pip install -U datasets should fix the issue. git link : https://github.com/huggingface/datasets/issues/6352 If you are using fsspec, then do: pip install fsspec==2023.9.2 There is a problem with fsspec==2023.10.0 git link : https://github.com/huggingface/datasets/issues/6330 Edit: Looks like it broken again in 2.17 and 2.18 downgrading to 2.16 should work.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I managed to get round it by deleting the files from the cached hugging face datasets folder. This is not the best way to go about solving this but it managed to work afterwards. Do bare in mind that I was only using datasets for one dataset though, so it didn't affect anything else.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pip",
        "openai-api",
        "huggingface-datasets"
      ],
      "question_score": 27,
      "answer_score": 66,
      "created": "2023-11-06T17:29:38",
      "question_id": 77433096,
      "answer_id": 77433141
    }
  },
  {
    "question": "Import vaex error: PydanticImportError: `BaseSettings` has been moved to the `pydantic-settings` package",
    "expected_answer": "What i have done Migration Guide pip install pydantic-settings I replaced in my code: # from pydantic import BaseSettings # OLD from pydantic_settings import BaseSettings # NEW",
    "context_chunks": [
      {
        "text": "I am using Sagemaker notebook and when importing vaex, I am getting the below error. the version of vaex I'm using is 4.16.0 PydanticImportError: BaseSettings has been moved to the pydantic-settings package. See https://docs.pydantic.dev/2.0.2/migration/#basesettings-has-moved-to-pydantic-settings for more details. For further information visit https://errors.pydantic.dev/2.0.2/u/import-error Anyone knows how to solve this? I tried downgrading the pydantic library while installing vaex but that didn't help.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "What i have done Migration Guide pip install pydantic-settings I replaced in my code: # from pydantic import BaseSettings # OLD from pydantic_settings import BaseSettings # NEW",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In pydantic-V2, Settings management has been moved to a separate package named pydantic-settings. You can install it by pip install pydantic-settings. If you still want to have V1-style settings management in V2, you can import it like: from pydantic.v1 import BaseSettings",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "amazon-sagemaker",
        "pydantic",
        "vaex"
      ],
      "question_score": 29,
      "answer_score": 45,
      "created": "2023-07-11T16:57:32",
      "question_id": 76664231,
      "answer_id": 76670657
    }
  },
  {
    "question": ".corr results in ValueError: could not convert string to float",
    "expected_answer": "since pandas version 2.0.0 now you need to add numeric_only=True param to avoid the issue",
    "context_chunks": [
      {
        "text": "I'm getting this very strange error when trying to follow the following exercise on using the corr() method in Python https://www.geeksforgeeks.org/python-pandas-dataframe-corr/ Specifically, when I try to run the following code: df.corr(method ='pearson') The error message offers no clue. I thought the corr() method was supposed to automatically ignore strings and empty values etc. Traceback (most recent call last): File &quot;&lt;pyshell#6&gt;&quot;, line 1, in &lt;module&gt; df.corr(method='pearson') File &quot;C:\\Users\\d.o\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py&quot;, line 10059, in corr mat = data.to_numpy(dtype=float, na_value=np.nan, copy=False) File &quot;C:\\Users\\d.o\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py&quot;, line 1838, in to_numpy result = self._mgr.as_array(dtype=dtype, copy=copy, na_value=na_value) File &quot;C:\\Users\\d.o\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py&quot;, line 1732, in as_array arr = self._interleave(dtype=dtype, na_value=na_value) File &quot;C:\\Users\\d.o\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py&quot;, line 1794, in _interleave result[rl.indexer] = arr ValueError: could not convert string to float: 'Avery Bradley'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "since pandas version 2.0.0 now you need to add numeric_only=True param to avoid the issue",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Try to use df.corr(numeric_only=True) You'll get through it",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "correlation",
        "valueerror"
      ],
      "question_score": 25,
      "answer_score": 61,
      "created": "2023-06-22T14:53:50",
      "question_id": 76533178,
      "answer_id": 76717659
    }
  },
  {
    "question": "Python zip magic for classes instead of tuples",
    "expected_answer": "You can define custom __iter__ magic function in your dataclass: from dataclasses import dataclass @dataclass class XY: &quot;2d point&quot; x: float | int y: float | int def __iter__(self): yield self.x yield self.y points = [XY(1,2), XY(3,4), XY(5,6), XY(7,8)] xs, ys = zip(*points) print(xs) print(ys) Prints: (1, 3, 5, 7) (2, 4, 6, 8)",
    "context_chunks": [
      {
        "text": "Python zip function is its own inverse (in a way), thus we can do this: points = [(1,2), (3,4), (5,6), (7,8)] xs, ys = zip(*points) and now xs=[1,3,5,7] and ys=[2,4,6,8]. I wonder if something similar can be done with data class instances instead of tuples: from dataclasses import dataclass @dataclass class XY: &quot;2d point&quot; x: float | int y: float | int points = [XY(1,2), XY(3,4), XY(5,6), XY(7,8)] xs, ys = zip(*[(p.x,p.y) for p in points]) but without an explicit list comprehension. Of course, the result would not be a tuple (xs,ys) but a dict with keys x and y because, without an explicit list comprehension, we would be collecting all fields.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can define custom __iter__ magic function in your dataclass: from dataclasses import dataclass @dataclass class XY: &quot;2d point&quot; x: float | int y: float | int def __iter__(self): yield self.x yield self.y points = [XY(1,2), XY(3,4), XY(5,6), XY(7,8)] xs, ys = zip(*points) print(xs) print(ys) Prints: (1, 3, 5, 7) (2, 4, 6, 8)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "With astuple: from dataclasses import dataclass, astuple @dataclass class XY: &quot;2d point&quot; x: float | int y: float | int def __iter__(self): return iter(astuple(self)) points = [XY(1,2), XY(3,4), XY(5,6), XY(7,8)] xs, ys = zip(*points) Or instead map it: xs, ys = zip(*map(astuple, points))",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 26,
      "answer_score": 36,
      "created": "2023-07-07T21:42:39",
      "question_id": 76640378,
      "answer_id": 76640399
    }
  },
  {
    "question": "PydanticUserError: If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`",
    "expected_answer": "In my env, I have pip list | grep pydantic pydantic 2.2.1 I fix the problem, by downgrading pydantic version pip install pydantic==1.10.9",
    "context_chunks": [
      {
        "text": "I want to execute this code in google colab but I get following error: from llama_index.prompts.prompts import SimpleInputPrompt # Create a system prompt system_prompt = &quot;&quot;&quot;[INST] &lt;&gt; more string here.&lt;&gt; &quot;&quot;&quot; query_wrapper_prompt = SimpleInputPrompt(&quot;{query_str} [/INST]&quot;) Error: /usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2: * 'allow_population_by_field_name' has been renamed to 'populate_by_name' warnings.warn(message, UserWarning) --------------------------------------------------------------------------- PydanticUserError Traceback (most recent call last) &lt;ipython-input-36-c45796b371fe&gt; in &lt;cell line: 3&gt;() 1 # Import the prompt wrapper... 2 # but for llama index ----&gt; 3 from llama_index.prompts.prompts import SimpleInputPrompt 4 # Create a system prompt 5 system_prompt = &quot;&quot;&quot;[INST] &lt;&gt; 6 frames /usr/local/lib/python3.10/dist-packages/pydantic/deprecated/class_validators.py in root_validator(pre, skip_on_failure, allow_reuse, *__args) 226 mode: Literal['before', 'after'] = 'before' if pre is True else 'after' 227 if pre is False and skip_on_failure is not True: --&gt; 228 raise PydanticUserError( 229 'If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.' 230 ' Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.', PydanticUserError: If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`. For further information visit https://errors.pydantic.dev/2.1.1/u/root-validator-pre-skip If I follow the link, there is no solution for my case. How can I solve that problem?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In my env, I have pip list | grep pydantic pydantic 2.2.1 I fix the problem, by downgrading pydantic version pip install pydantic==1.10.9",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Instead of downgrading pydantic, consider upgrading langchain or llama-index. For example, the error in the title doesn't occur with pydantic==2.8.2 and llama_index.core==0.10.67. Similar errors involving langchain also doesn't occur for newer versions of langchain, e.g. langchain&gt;=0.3.0 and langchain-core&gt;=0.3.0.1 So pip install langchain langchain-core -U should really solve the issue. 1 Some background: For langchain&gt;=0.3.0 and langchain-core&gt;=0.3.0, langchain internally uses pydantic v2, so importing from pydantic should pose no issue. For langchain&gt;=0.0.267,&lt;0.3.0, it internally used pydantic.v1 (the change in the repo), so explicitly importing from pydantic sometimes caused an issue due to mixing pydantic v1 and v2. To avoid this issue, if you're explicitly importing from pydantic (perhaps to use v2 functionality such as field_validator), then you shouldn't delegate other definitions related to pydantic to langchain; define all of those yourself. An easier solution for these langchain versions was to import from langchain_core.pydantic_v1 and use the objects from therein. For langchain&lt;0.0267, the error in the title commonly occurred because langchain was internally importing from pydantic when it actually wanted to use pydantic v1 (and that name became pydantic v2 which had some breaking changes).",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "prompt",
        "pydantic",
        "langchain",
        "llama-index"
      ],
      "question_score": 23,
      "answer_score": 36,
      "created": "2023-08-19T10:11:55",
      "question_id": 76934579,
      "answer_id": 76958769
    }
  },
  {
    "question": "A module that was compiled using NumPy 1.x cannot be run in NumPy 2.0.0",
    "expected_answer": "There are two solutions for this error: 1. downgrade your numpy to 1.26.4 pip install numpy==1.26.4 or pip install &quot;numpy&lt;2.0&quot; Make sure to restart your kernel after downgrading numpy Another option is: 2. install the latest version of the module which is failing* I had an old version of opencv-python 4.8.0.76 I was able to get this working by installing the latest version of opencv-python by pip install opencv-python==4.10.0.84 *Some modules may still not work with numpy 2.0 'We expect that some modules will need time to support NumPy 2'",
    "context_chunks": [
      {
        "text": "I installed numpy 2.0.0 pip install numpy==2.0.0 import numpy as np np.__version__ #2.0.0 then I installed: pip install opencv-python Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76) Requirement already satisfied: numpy&gt;=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (2.0.0) Then I did: import cv2 I am getting this error: A module that was compiled using NumPy 1.x cannot be run in NumPy 2.0.0 as it may crash. To support both 1.x and 2.x versions of NumPy, modules must be compiled with NumPy 2.0. Some module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'. If you are a user of the module, the easiest solution will be to downgrade to 'numpy&lt;2' or try to upgrade the affected module. We expect that some modules will need time to support NumPy 2. Traceback (most recent call last): File &quot;/usr/lib/python3.10/runpy.py&quot;, line 196, in _run_module_as_main return _run_code(code, main_globals, None, File &quot;/usr/lib/python3.10/runpy.py&quot;, line 86, in _run_code exec(code, run_globals) File &quot;/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py&quot;, line 37, in &lt;module&gt; ColabKernelApp.launch_instance() File &quot;/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py&quot;, line 992, in launch_instance app.start() File &quot;/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py&quot;, line 619, in start self.io_loop.start() File &quot;/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py&quot;, line 195, in start self.asyncio_loop.run_forever() File &quot;/usr/lib/python3.10/asyncio/base_events.py&quot;, line 603, in run_forever self._run_once() File &quot;/usr/lib/python3.10/asyncio/base_events.py&quot;, line 1909, in _run_once handle._run() File &quot;/usr/lib/python3.10/asyncio/events.py&quot;, line 80, in _run self._context.run(self._callback, *self._args) File &quot;/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py&quot;, line 685, in &lt;lambda&gt; lambda f: self._run_callback(functools.partial(callback, future)) File &quot;/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py&quot;, line 738, in _run_callback ret = callback() File &quot;/usr/local/lib/python3.10/dist-packages/tornado/gen.py&quot;, line 825, in inner self.ctx_run(self.run) File &quot;/usr/local/lib/python3.10/dist-packages/tornado/gen.py&quot;, line 786, in run yielded = self.gen.send(value) File &quot;/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py&quot;, line 361, in process_one yield gen.maybe_future(dispatch(*args)) File &quot;/usr/local/lib/python3.10/dist-packages/tornado/gen.py&quot;, line 234, in wrapper yielded = ctx_run(next, result) File &quot;/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py&quot;, line 261, in dispatch_shell yield gen.maybe_future(handler(stream, idents, msg)) File &quot;/usr/local/lib/python3.10/dist-packages/tornado/gen.py&quot;, line 234, in wrapper yielded = ctx_run(next, result) File &quot;/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py&quot;, line 539, in execute_request self.do_execute( File &quot;/usr/local/lib/python3.10/dist-packages/tornado/gen.py&quot;, line 234, in wrapper yielded = ctx_run(next, result) File &quot;/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py&quot;, line 302, in do_execute res = shell.run_cell(code, store_history=store_history, silent=silent) File &quot;/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py&quot;, line 539, in run_cell return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs) File &quot;/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py&quot;, line 2975, in run_cell result = self._run_cell( File &quot;/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py&quot;, line 3030, in _run_cell return runner(coro) File &quot;/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py&quot;, line 78, in _pseudo_sync_runner coro.send(None) File &quot;/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py&quot;, line 3257, in run_cell_async has_raised = await self.run_ast_nodes(code_ast.body, cell_name, File &quot;/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py&quot;, line 3473, in run_ast_nodes if (await self.run_code(code, result, async_=asy)): File &quot;/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py&quot;, line 3553, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File &quot;&lt;ipython-input-4-c8ec22b3e787&gt;&quot;, line 1, in &lt;cell line: 1&gt; import cv2 File &quot;/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py&quot;, line 78, in load_module cv_module = imp.load_module(name, *module_info) File &quot;/usr/lib/python3.10/imp.py&quot;, line 245, in load_module return load_package(name, filename) File &quot;/usr/lib/python3.10/imp.py&quot;, line 217, in load_package return _load(spec) File &quot;/usr/local/lib/python3.10/dist-packages/cv2/__init__.py&quot;, line 181, in &lt;module&gt; bootstrap() File &quot;/usr/local/lib/python3.10/dist-packages/cv2/__init__.py&quot;, line 153, in bootstrap native_module = importlib.import_module(&quot;cv2&quot;) File &quot;/usr/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py&quot;, line 78, in load_module cv_module = imp.load_module(name, *module_info) File &quot;/usr/lib/python3.10/imp.py&quot;, line 243, in load_module return load_dynamic(name, filename, file) File &quot;/usr/lib/python3.10/imp.py&quot;, line 343, in load_dynamic return _load(spec) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) AttributeError: _ARRAY_API not found --------------------------------------------------------------------------- ImportError Traceback (most recent call last) &lt;ipython-input-4-c8ec22b3e787&gt; in &lt;cell line: 1&gt;() ----&gt; 1 import cv2 8 frames /usr/lib/python3.10/imp.py in load_dynamic(name, path, file) 341 spec = importlib.machinery.ModuleSpec( 342 name=name, loader=loader, origin=path) --&gt; 343 return _load(spec) 344 345 else: ImportError: numpy.core.multiarray failed to import --------------------------------------------------------------------------- NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the &quot;Open Examples&quot; button below.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "There are two solutions for this error: 1. downgrade your numpy to 1.26.4 pip install numpy==1.26.4 or pip install &quot;numpy&lt;2.0&quot; Make sure to restart your kernel after downgrading numpy Another option is: 2. install the latest version of the module which is failing* I had an old version of opencv-python 4.8.0.76 I was able to get this working by installing the latest version of opencv-python by pip install opencv-python==4.10.0.84 *Some modules may still not work with numpy 2.0 'We expect that some modules will need time to support NumPy 2'",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "NOTE: I know the op question is not what im writing but since searching the error led me here, I just wanted to make sure people see this. For Those who face this problem during import of fresh installed tensorflow: A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.4 as it may crash. To support both 1.x and 2.x versions of NumPy, modules must be compiled with NumPy 2.0. Some module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'. i can confirm that this is an easy working sample for having latest windows native tensorflow supporting gpu. environment is as follow: windows 11 pro 24H2 build 26100.3624 rtx3060 GPU with insatalled gpu driver (latest preferebelly) Anaconda3-2024.10-1 so i try to be newbie friendly(as I'm one of those) After that open Anaconda Navigator head over to Environments section. From menu on the bottom of the screen click Create, choose a name for your environment, check the pyhton language and select Pyhton3.10.X-(in my case was 3.10.16 but should be ok if your X is different) and Press green button Create. NOTE: According to Tensorflow windows-native installation guide and Tested GPU Build Configurations the latest python supported is 3.10 and Tensorflow GPU will NOT SUPPORT PYTHON &gt; 3.11 and 3.12 and later ON WINDOWS NATIVELY! (You can Install it using WSL2 following this guide. After Environment Creation, you see its activated and there is a little green play button click on it and select Open Terminal to open a cmd (or whatever command line) inside that environment, you can tell by the name of the environment inside a pair of prantheses before the path like below. (my-tensorflow-env) C:\\Users\\someone&gt; Use this command to install cudatoolkit and cudnn easilly inside your isolated environment. (for me was two ~650 MB files to download since the versions are fixed, you probabely see similar) conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0 Now Before installing tensorflow downgrade Numpy to version 1.X. pip install &quot;numpy&lt;2.0&quot; Now for the last part install tensorflow.(it may take a while also prepare a reliable connection as pip is vulrenble to connection issue, also dont panic if it connection timed out or whatever, just the command again nothing is broken) python -m pip install tensorflow==2.10.0 to test the redemption inter this command: python -c &quot;import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))&quot; if you see something like: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] Congrats! enjoy GPU.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "numpy",
        "pip",
        "numpy-2.x"
      ],
      "question_score": 23,
      "answer_score": 54,
      "created": "2024-06-19T07:49:51",
      "question_id": 78641150,
      "answer_id": 78641405
    }
  },
  {
    "question": "Unable to use Selenium Webdriver. Getting two exceptions",
    "expected_answer": "If the Selenium version you are using is v4.6.0 or above (which I think it is as I see SeleniumManger in the error trace), then you don't really have to set the driver.exe path. Selenium can handle the browser and drivers by itself. So your code can be simplified as below: from selenium import webdriver driver = webdriver.Chrome() driver.get(&quot;https://www.google.com/&quot;) driver.quit() A few references: Purpose of webdriver manager Introducing Selenium Manager",
    "context_chunks": [
      {
        "text": "I am getting the following error when trying to create an object with Selenium Webdriver. &quot;\\selenium\\webdriver\\common\\driver_finder.py&quot;, line 42, in get_path path = SeleniumManager().driver_location(options) if path is None else path &quot;\\selenium\\webdriver\\common\\selenium_manager.py&quot;, line 74, in driver_location browser = options.capabilities[&quot;browserName&quot;] AttributeError: 'str' object has no attribute 'capabilities' During handling of the above exception, another exception occurred: Traceback (most recent call last): &quot;\\selenium_webdriver_webscraping.py&quot;, line 4, in &lt;module&gt; driver = webdriver.Chrome(chrome_driver_path) &quot;\\selenium\\webdriver\\chrome\\webdriver.py&quot;, line 47, in __init__ self.service.path = DriverFinder.get_path(self.service, self.options) &quot;\\selenium\\webdriver\\common\\driver_finder.py&quot;, line 44, in get_path raise NoSuchDriverException(f&quot;Unable to obtain {service.path} using Selenium Manager; {err}&quot;) selenium.common.exceptions.NoSuchDriverException: Message: Unable to obtain chromedriver using Selenium Manager; 'str' object has no attribute 'capabilities'; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location This is the code I used: from selenium import webdriver chrome_driver_path = &lt;chrome drive .exe path&gt; driver = webdriver.Chrome(chrome_driver_path)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "If the Selenium version you are using is v4.6.0 or above (which I think it is as I see SeleniumManger in the error trace), then you don't really have to set the driver.exe path. Selenium can handle the browser and drivers by itself. So your code can be simplified as below: from selenium import webdriver driver = webdriver.Chrome() driver.get(&quot;https://www.google.com/&quot;) driver.quit() A few references: Purpose of webdriver manager Introducing Selenium Manager",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This is due to changes in Selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that the first argument is no longer executable_path, and that desired_capabilities has been removed, but there is now another way of passing it in. See Upgrade to Selenium 4 for the documentation on how to pass in desired capabilities when using Selenium 4.10.0 (or newer). Also, if you want to set an executable_path, it can be passed in via the service, but it is no longer necessary, as selenium manager is included. Here's a code snippet with everything you need: from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service() options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... driver.quit()",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver",
        "exception",
        "selenium-chromedriver"
      ],
      "question_score": 23,
      "answer_score": 64,
      "created": "2023-06-13T03:52:30",
      "question_id": 76461596,
      "answer_id": 76463081
    }
  },
  {
    "question": "Accelerate and bitsandbytes is needed to install but I did",
    "expected_answer": "I downgraded transformers library to version 4.30 using the following command: pip install transformers==4.30",
    "context_chunks": [
      {
        "text": "I'm trying to load quantization like from transformers import LlamaForCausalLM from transformers import BitsAndBytesConfig model = '/model/' model = LlamaForCausalLM.from_pretrained(model, quantization_config=BitsAndBytesConfig(load_in_8bit=True)) but I get the error ImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` But I've installed both, and I get the same error. I shut down and restarted the jupyter kernel I was using this on.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I downgraded transformers library to version 4.30 using the following command: pip install transformers==4.30",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I am on a M1 Mac and also have similar problems. After installing accelerate and bitsandbytes I still get ImportError: Using load_in_8bit=True requires Accelerate: pip install accelerate and the latest version of bitsandbytes pip install -i https://test.pypi.org/simple/ bitsandbytes or pip install bitsandbytes I looked around a bit in the Transformers source code and found a function called is_bitsandbytes_available() which only returns true if bitsandbytes is installed and torch.cuda.is_available(), which is not the case on an Apple Silicon machine. Also, https://github.com/TimDettmers/bitsandbytes/issues/252 confirms that there is no MPS (Metal) support in bitsandbytes. I don't know why Huggingface's documentation doesn't mention (unless I've missed it somewhere) that their examples won't work on Apple Silicon.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "huggingface-transformers"
      ],
      "question_score": 25,
      "answer_score": 30,
      "created": "2023-08-17T18:32:45",
      "question_id": 76924239,
      "answer_id": 76976563
    }
  },
  {
    "question": "streamlit: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 &gt;= 3.35.0",
    "expected_answer": "I did this and it worked for me: Install pysqlite3-binary using: pip3 install pysqlite3-binary Then add below 3 lines in your code file where you have defined Chromadb Credits: I got this answer from here: Issues with chroma and sqlite Note: Doesn't matter if you are using django, flask or fastapi. It should work regardless. __import__('pysqlite3') import sys sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')",
    "context_chunks": [
      {
        "text": "I previously deployed an app on Streamlit Cloud that utilized chromadb. The app worked fine in the past. However, today I encountered a new error (as indicated in the title) and the app has stopped functioning. I attempted to troubleshoot based on solutions from the Streamlit forum and performed the following steps sequentially: Updated the requirements.txt file by adding pysqlite3-binary. Added the following three lines of code at the top of app.py: __import__('pysqlite3') import sys sys.modules['sqlite3'] = sys.modules.pop('pysqlite3') After rebooting my app, I discovered the new error: ModuleNotFoundError: No module named 'pysqlite3' Traceback: File &quot;/home/adminuser/venv/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py&quot;, line 552, in _run_script exec(code, module.__dict__) File &quot;/mount/src/docgpt-streamlit/app.py&quot;, line 2, in &lt;module&gt; import pysqlite3 Subsequently, I tried adding pysqlite3 again to requirements.txt, but the error persisted. According to the logs from manage app, I observed that Streamlit did not perform a re-pip install action. Could this be causing the pysqlite error? If so, how can I correctly enable the Streamlit app to automatically pip install due to my updated requirements.txt?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I did this and it worked for me: Install pysqlite3-binary using: pip3 install pysqlite3-binary Then add below 3 lines in your code file where you have defined Chromadb Credits: I got this answer from here: Issues with chroma and sqlite Note: Doesn't matter if you are using django, flask or fastapi. It should work regardless. __import__('pysqlite3') import sys sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This was helpful, worked for me ! step 1. pip install pysqlite3-binary step 2. add three lines in this file /venv/lib/python3.10/site-packages/chromadb/ __init__.py __import__('pysqlite3') import sys sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "streamlit",
        "chromadb"
      ],
      "question_score": 23,
      "answer_score": 18,
      "created": "2023-08-23T06:31:40",
      "question_id": 76958817,
      "answer_id": 78237141
    }
  },
  {
    "question": "ModuleNotFoundError: No module named &#39;imp&#39;",
    "expected_answer": "I encountered this as well. As far as I understand its a deprecation issue. awsebcli will install with Python 3.12 but imp will not. If you type import imp into Python 3.11 you will get the following response DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses At the time of writing this, Elastic Beanstalk is only supporting 3.8, 3.9 &amp; 3.11 https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.python",
    "context_chunks": [
      {
        "text": "I need to install the eb command on windows. I would like to try to deploy an application on AWS using the elasticbeanstalk service, and through this command you can configure and deploy an environment directly with a configuration file. To do this I followed the guide. I first installed python via the site (Python version 3.12.0), and then all the steps described in the guide link. Now if I run the eb command from cmd I always get this error. Traceback (most recent call last): File &quot;&lt;frozen runpy&gt;&quot;, line 198, in _run_module_as_main File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code File &quot;C:\\Users\\Utente\\.ebcli-virtual-env\\Scripts\\eb.exe\\__main__.py&quot;, line 4, in &lt;module&gt; File &quot;C:\\Users\\Utente\\.ebcli-virtual-env\\Lib\\site-packages\\ebcli\\core\\ebcore.py&quot;, line 16, in &lt;module&gt; from cement.core import foundation, handler, hook File &quot;C:\\Users\\Utente\\.ebcli-virtual-env\\Lib\\site-packages\\cement\\core\\foundation.py&quot;, line 11, in &lt;module&gt; from ..core import output, extension, arg, controller, meta, cache, mail File &quot;C:\\Users\\Utente\\.ebcli-virtual-env\\Lib\\site-packages\\cement\\core\\extension.py&quot;, line 8, in &lt;module&gt; from imp import reload # pragma: no cover ^^^^^^^^^^^^^^^^^^^^^^ ModuleNotFoundError: No module named 'imp' I've tried several things but can't come to a conclusion. Does anyone know how to help me? I also tried installing previous versions of python, even though I didn't like it as a solution, but still I still have the problem.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I encountered this as well. As far as I understand its a deprecation issue. awsebcli will install with Python 3.12 but imp will not. If you type import imp into Python 3.11 you will get the following response DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses At the time of writing this, Elastic Beanstalk is only supporting 3.8, 3.9 &amp; 3.11 https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.python",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It solved for me after installing below package. pip install cement==2.10.14",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "amazon-web-services",
        "amazon-elastic-beanstalk"
      ],
      "question_score": 25,
      "answer_score": 26,
      "created": "2023-11-01T09:57:02",
      "question_id": 77401730,
      "answer_id": 77411659
    }
  },
  {
    "question": "Warning while using tensorflow : tensorflow/core/util/port.cc:113] oneDNN custom operations are on",
    "expected_answer": "import os os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' import keras It should work then without issues, at least for me it worked!",
    "context_chunks": [
      {
        "text": "When using tensorflow, I get this warning import tensorflow tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. I tried to change the environmental variable to 0 by set TF_ENABLE_ONEDNN_OPTS=0. But still the issue persists.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "import os os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' import keras It should work then without issues, at least for me it worked!",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Question 1: tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0 Solutions: 1. import os os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' import tensorflow as tf export TF_ENABLE_ONEDNN_OPTS=0; Here, TF_ENABLE_ONEDNN_OPTS=0 should be above import tensorflow as tf as shown above. Question 2: To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. Solutions: 1. import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' import tensorflow as tf export TF_CPP_MIN_LOG_LEVEL=2 Here, TF_CPP_MIN_LOG_LEVEL=2 should be above import tensorflow as tf as shown above.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "tensorflow"
      ],
      "question_score": 23,
      "answer_score": 26,
      "created": "2024-02-01T15:19:42",
      "question_id": 77921357,
      "answer_id": 77984103
    }
  },
  {
    "question": "How do I install Python dev-dependencies using uv?",
    "expected_answer": "To expand on the other good answers by Phil: Dependency Groups With uv &gt;= 0.4.27 we can use the new Python packaging feature dependency groups to make pyproject uv-agnostic: [project] name = &quot;my-project&quot; dynamic = [&quot;version&quot;] requires-python = &quot;&gt;=3.10&quot; dependencies = [&quot;django&quot;] [dependency-groups] dev = [&quot;factory-boy&quot;] Python Pinned Also can make use of .python-version to pin the project Python version, which be created manually or by uv python: uv python pin 3.10 Check version written to file: cat .python-version Output: 3.10 Dev venv With the above configuration in place, a developer virtualenv can be created with: uv sync Finally, activate the venv: source .venv/bin/activate",
    "context_chunks": [
      {
        "text": "I'm trying out uv to manage my Python project's dependencies and virtualenv, but I can't see how to install all my dependencies for local development, including the development dependencies. In my pyproject.toml file, I have this kind of thing: [project] name = &quot;my-project&quot; dependencies = [ &quot;django&quot;, ] [tool.uv] dev-dependencies = [ &quot;factory-boy&quot;, ] [tool.uv.pip] python-version = &quot;3.10&quot; I can do the following to create a virtualenv and then generate a requirements.txt lockfile, which does not contain dev-dependencies (which is OK, because that is for production): uv venv --python 3.10 uv pip compile pyproject.toml -o requirements.txt But how can I install all the dependencies in my virtualenv? uv pip sync will use the requirements.txt. There's also uv sync, but I don't understand how that differs, and trying that generates an error: error: Multiple top-level packages discovered in a flat-layout: ['conf', 'hines', 'docker', 'assets', 'node_modules'].",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "To expand on the other good answers by Phil: Dependency Groups With uv &gt;= 0.4.27 we can use the new Python packaging feature dependency groups to make pyproject uv-agnostic: [project] name = &quot;my-project&quot; dynamic = [&quot;version&quot;] requires-python = &quot;&gt;=3.10&quot; dependencies = [&quot;django&quot;] [dependency-groups] dev = [&quot;factory-boy&quot;] Python Pinned Also can make use of .python-version to pin the project Python version, which be created manually or by uv python: uv python pin 3.10 Check version written to file: cat .python-version Output: 3.10 Dev venv With the above configuration in place, a developer virtualenv can be created with: uv sync Finally, activate the venv: source .venv/bin/activate",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "TLDR. Any dependency (such as pytest) added to a project by running uv add --dev pytest will be added to the corresponding dev dependency group in pyproject.toml. pyproject.toml [dependency-groups] dev = [ &quot;pytest&quot; ] Any such dependencies will be installed when running uv sync (and are excluded when running uv sync --no-dev). Such dependencies are local-only and will not be included in the project requirements of the published project. General Dependency Management When working with project dependencies, it is helpful to differentiate between published dependencies that are used when publishing to PyPI or other indexes, and when building a wheel package; optional published dependencies that are required for an optional (extra) feature of the project. For example, Polars has a Pydantic extra that enables conversion from Pydantic models to Polar models. It can be installed with the polars[pydantic] syntax, but Pydantic is not installed unless explicitly specified. Still, these dependencies will be included as optional requirements when publishing to PyPI or other indexes; development (local) dependencies are local-only and will not be included when publishing a project. Such dependencies are the concern of this question. Development (local) dependencies Unlike published (required or optional) dependencies, development dependencies will not be included when publishing a project. Especially, they are not included in the [project] table (i.e., section) of the pyproject.toml. Instead, they are added to the [dependency-groups] table (see PEP 735). This table may contain arbitrary dependency groups, such as dev, test, and funky. Adding Dependencies to a Dependency Group With uv, a dependency can be added to any given group in the [dependency-groups] table by running uv add --group GROUP DEPENDENCY e.g. uv add --group test pytest This will create a corresponding list-entry in the [dependency-groups] table of pyproject.toml (if non-existent) and append the dependency. [dependency-groups] test = [ &quot;pytest&quot; ] Installing Dependencies from a Dependency Group With uv, the dependencies from a given dependency group can be included when updating an environment by using the --group GROUP option for uv sync. uv sync --group test Tricks and Defaults The aforementioned information is sufficient to use development dependencies with uv. However, uv also provides sensible defaults and gives privileged treatment to the dev dependency-group. Especially, uv add --dev pytest is syntactic sugar for uv add --group dev pytest and will make the same changes to pyproject.toml. Moreover, all dependencies in the dev dependency-group will be installed by default, when running uv sync. They can be explicitly excluded by running uv sync --no-group dev or uv sync --no-dev Moreover, the default groups can be changed to be something other than [&quot;dev&quot;] by adjusting the default-groups setting in the [tool.uv] table. [tool.uv] default-groups = [&quot;dev&quot;, &quot;funky&quot;] Note. More information can be found in the documentation.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "uv"
      ],
      "question_score": 22,
      "answer_score": 18,
      "created": "2024-08-22T16:20:22",
      "question_id": 78902565,
      "answer_id": 79226411
    }
  },
  {
    "question": "Python does not find module installed with pipx",
    "expected_answer": "From Python 3.11 onward, Debian encourages the users to create a separate Python virtual environment to install Python packages. Because Debian declares its Python install to be externally-managed, pip (and other installers) will refuse to install packages system-wide. Installation is only possible in virtual environments or separate Python installs. This is because Python package installers (like pip) are unaware of the constraints that APT-managed packages have on libraries and versions. See PEP 668 for a full discussion of the problems that can occur when multiple installers operate on the same Python install. Therefore, the optimal way is to create a virtual environment, say MyEnv, and install packages therein: mkdir -p $HOME/.venvs # Create a folder for all virtual environments python3 -m venv $HOME/.venvs/MyEnv # Create MyEnv This will create a directory $HOME/.venvs/MyEnv with a configuration file pyvenv.cfg which includes some details for this virtual environment, such as the Python executable and Python version. Verify the version of the Python in the virtual environment: $HOME/.venvs/MyEnv/bin/python --version The executables of the created virtual environment are found under $HOME/.venvs/MyEnv/bin. To install a package into the virtual environment, use $HOME/.venvs/MyEnv/bin/python -m pip install &lt;some-package&gt; To 'activate' the virtual environment, i.e. adding its configuration variables into the shell environment, use source $HOME/.venvs/MyEnv/bin/activate Consult Python's guide to virtualenv and pip at Install packages in a virtual environment using pip and venv.",
    "context_chunks": [
      {
        "text": "Debian Stable wants me to install Python modules using pipx. So I do pipx install auditwheel pipx ensurepath python3 -m pipx ensurepath python3 Output: Python 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. And: import auditwheel Output: Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'auditwheel' What am I doing wrong?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "From Python 3.11 onward, Debian encourages the users to create a separate Python virtual environment to install Python packages. Because Debian declares its Python install to be externally-managed, pip (and other installers) will refuse to install packages system-wide. Installation is only possible in virtual environments or separate Python installs. This is because Python package installers (like pip) are unaware of the constraints that APT-managed packages have on libraries and versions. See PEP 668 for a full discussion of the problems that can occur when multiple installers operate on the same Python install. Therefore, the optimal way is to create a virtual environment, say MyEnv, and install packages therein: mkdir -p $HOME/.venvs # Create a folder for all virtual environments python3 -m venv $HOME/.venvs/MyEnv # Create MyEnv This will create a directory $HOME/.venvs/MyEnv with a configuration file pyvenv.cfg which includes some details for this virtual environment, such as the Python executable and Python version. Verify the version of the Python in the virtual environment: $HOME/.venvs/MyEnv/bin/python --version The executables of the created virtual environment are found under $HOME/.venvs/MyEnv/bin. To install a package into the virtual environment, use $HOME/.venvs/MyEnv/bin/python -m pip install &lt;some-package&gt; To 'activate' the virtual environment, i.e. adding its configuration variables into the shell environment, use source $HOME/.venvs/MyEnv/bin/activate Consult Python's guide to virtualenv and pip at Install packages in a virtual environment using pip and venv.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "pipx is not exactly the same as pip. pipx installs applications in an isolated environment. It will not help if you want to compile and import a module. You can use a virtual environment as AlQuemist suggests.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "debian",
        "pipx"
      ],
      "question_score": 23,
      "answer_score": 17,
      "created": "2023-06-18T08:02:30",
      "question_id": 76499565,
      "answer_id": 76506829
    }
  },
  {
    "question": "Django built in Logout view `Method Not Allowed (GET): /users/logout/`",
    "expected_answer": "Since django-5, you need to do this through a POST request, since it has side-effects. The fact that it worked with a GET request was (likely) a violation of the HTTP protocol: it made it possible for certain scripts to log out users, without the user wanting to. So a POST request also protects against cross-site request forgery (CSRF) [wiki]. So in the template, work with a mini-form: &lt;form method=&quot;post&quot; action=&quot;{% url 'logout' %}&quot;&gt; {% csrf_token %} &lt;button type=&quot;submit&quot;&gt;logout&lt;/button&gt; &lt;/form&gt; Also note that Django's LogoutView [Django-doc] does not render a page: it only works with a POST request that logs out the user that is making the request, and redirect to the page provided by the ?next_page=… parameter, or if such parameter is absent with LOGOUT_REDIRECT_URL [Django-doc]. Visiting the page will thus not work and return a 405 Method Not Allowed. If you thus want to make a page to logout, you add an extra view: from django.views.generic import TemplateView urlpatterns = [ # &hellip;, path( 'do-logout/', TemplateView.as_view(template_name='my_template.html'), name='do-logout', ) ] where the my_template.html then thus contains such miniform. You can thus visit /do-logout/ (or use another path) to render a page with a miniform to log out.",
    "context_chunks": [
      {
        "text": "Method Not Allowed (GET): /users/logout/ Method Not Allowed: /users/logout/ [10/Dec/2023 12:46:21] &quot;GET /users/logout/ HTTP/1.1&quot; 405 0 This is happening when I went to url http://127.0.0.1:8000/users/logout/ urls.py: from django.contrib.auth import views as auth_views urlpatterns = [ ...other urls... path('users/logout/', auth_views.LogoutView.as_view(), name='logout'), ] I am expecting user to logout",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Since django-5, you need to do this through a POST request, since it has side-effects. The fact that it worked with a GET request was (likely) a violation of the HTTP protocol: it made it possible for certain scripts to log out users, without the user wanting to. So a POST request also protects against cross-site request forgery (CSRF) [wiki]. So in the template, work with a mini-form: &lt;form method=&quot;post&quot; action=&quot;{% url 'logout' %}&quot;&gt; {% csrf_token %} &lt;button type=&quot;submit&quot;&gt;logout&lt;/button&gt; &lt;/form&gt; Also note that Django's LogoutView [Django-doc] does not render a page: it only works with a POST request that logs out the user that is making the request, and redirect to the page provided by the ?next_page=… parameter, or if such parameter is absent with LOGOUT_REDIRECT_URL [Django-doc]. Visiting the page will thus not work and return a 405 Method Not Allowed. If you thus want to make a page to logout, you add an extra view: from django.views.generic import TemplateView urlpatterns = [ # &hellip;, path( 'do-logout/', TemplateView.as_view(template_name='my_template.html'), name='do-logout', ) ] where the my_template.html then thus contains such miniform. You can thus visit /do-logout/ (or use another path) to render a page with a miniform to log out.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Since Django-5, you need to logout through a POST request &lt;li&gt; &lt;form id=&quot;logout-form&quot; action=&quot;{% url 'logout' %}&quot; method=&quot;post&quot;&gt; &lt;a href=&quot;#&quot; onclick=&quot;document.getElementById('logout-form').submit(); return false;&quot;&gt;Sign out&lt;/a&gt; {% csrf_token %} &lt;/form&gt; &lt;/li&gt;",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "django",
        "django-forms",
        "django-authentication"
      ],
      "question_score": 22,
      "answer_score": 22,
      "created": "2023-12-20T10:12:19",
      "question_id": 77690729,
      "answer_id": 77690772
    }
  },
  {
    "question": "OpenAI API error: &quot;You tried to access openai.ChatCompletion, but this is no longer supported in openai&gt;=1.0.0&quot;",
    "expected_answer": "Try updating to the latest and using: from openai import OpenAI client = OpenAI( # defaults to os.environ.get(&quot;OPENAI_API_KEY&quot;) api_key=&quot;private&quot;, ) def chat_gpt(prompt): response = client.chat.completions.create( model=&quot;gpt-3.5-turbo&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}] ) return response.choices[0].message.content.strip() Link EDIT: message.['content'] -&gt; message.content on the return of this function, as a message object is not subscriptable error is thrown while using message.['content']. Also, update link from pointing to the README (subject to change) to migration guide specific to this code.",
    "context_chunks": [
      {
        "text": "I am currently working on a chatbot, and as I am using Windows 11 it does not let me migrate to newer OpenAI library or downgrade it. Could I replace the ChatCompletion function with something else to work on my version? This is the code: import openai openai.api_key = &quot;private&quot; def chat_gpt(prompt): response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}] ) return response.choices[0].message['content'].strip() if __name__ == &quot;__main__&quot;: while True: user_input = input(&quot;You: &quot;) if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;bye&quot;]: break response = chat_gpt(user_input) print(&quot;Bot:&quot;, response) And this is the full error: ... You tried to access openai.ChatCompletion, but this is no longer supported in openai&gt;=1.0.0 - see the README at https://github.com/openai/openai-python for the API. You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface. Alternatively, you can pin your installation to the old version, e.g. &lt;pip install openai==0.28&gt; A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742 I tried both upgrading and downgrading through pip.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Try updating to the latest and using: from openai import OpenAI client = OpenAI( # defaults to os.environ.get(&quot;OPENAI_API_KEY&quot;) api_key=&quot;private&quot;, ) def chat_gpt(prompt): response = client.chat.completions.create( model=&quot;gpt-3.5-turbo&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}] ) return response.choices[0].message.content.strip() Link EDIT: message.['content'] -&gt; message.content on the return of this function, as a message object is not subscriptable error is thrown while using message.['content']. Also, update link from pointing to the README (subject to change) to migration guide specific to this code.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Problem The method you're trying to use doesn't work with the OpenAI Python SDK &gt;=v1.0.0 (if you're using Python) or OpenAI Node.js SDK &gt;=v4.0.0 (if you're using Node.js). See the Python SDK migration guide or the Node.js SDK migration guide. Python The old SDK (i.e., v0.28.1) works with the following method: client.ChatCompletion.create() The new SDK (i.e., &gt;=v1.0.0) works with the following method: client.chat.completions.create() Note: Be careful because the API is case-sensitive (i.e., client.Chat.Completions.create() will not work with the new SDK version). Node.js The old SDK (i.e., v3.3.0) works with the following method: client.createChatCompletion() The new SDK (i.e., &gt;=v4.0.0) works with the following method: client.chat.completions.create() Note: Be careful because the API is case-sensitive (i.e., client.Chat.Completions.create() will not work with the new SDK version). Solution Python SDK v1.0.0 working example If you run test.py, the OpenAI API will return the following completion: Hello! How can I assist you today? test.py import os from openai import OpenAI client = OpenAI( api_key = os.getenv(&quot;OPENAI_API_KEY&quot;), ) completion = client.chat.completions.create( model = &quot;gpt-3.5-turbo&quot;, messages = [ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}, ] ) print(completion.choices[0].message.content.strip()) Node.js SDK v4.0.0 working example If you run test.js, the OpenAI API will return the following completion: Hello! How can I assist you today? test.js const OpenAI = require(&quot;openai&quot;); const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY, }); async function main() { const completion = await client.chat.completions.create({ model: &quot;gpt-3.5-turbo&quot;, messages: [ { role: &quot;system&quot;, content: &quot;You are a helpful assistant.&quot; }, { role: &quot;user&quot;, content: &quot;Hello!&quot; }, ], }); console.log(completion.choices[0].message.content.trim()); } main();",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pip",
        "artificial-intelligence",
        "openai-api",
        "chatgpt-api"
      ],
      "question_score": 21,
      "answer_score": 37,
      "created": "2023-11-17T23:09:08",
      "question_id": 77505030,
      "answer_id": 77505042
    }
  },
  {
    "question": "Why does llama-index still require an OpenAI key when using Hugging Face local embedding model?",
    "expected_answer": "Turns out I had to set the embed_model to &quot;local&quot; on the ServiceContext. ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=&quot;local&quot;) Also, when I was loading the vector index from disk I wasn't setting the llm predictor again which cause a secondary issue. So I decided to make the vector index a global variable. Here is my final code that works. from pathlib import Path import gradio as gr import sys import logging import os from llama_index.llms import HuggingFaceLLM from llama_index.prompts.prompts import SimpleInputPrompt logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext storage_path = &quot;storage&quot; docs_path=&quot;docs&quot; print(storage_path) max_input_size = 4096 num_outputs = 512 #max_chunk_overlap = 20 chunk_overlap_ratio = 0.1 chunk_size_limit = 600 system_prompt = &quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version) - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI. - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user. - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes. - StableLM will refuse to participate in anything that could harm a human. &quot;&quot;&quot; # This will wrap the default prompts that are internal to llama-index query_wrapper_prompt = SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;) llm = HuggingFaceLLM( context_window=4096, max_new_tokens=256, generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;: False}, system_prompt=system_prompt, query_wrapper_prompt=query_wrapper_prompt, tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, device_map=&quot;auto&quot;, stopping_ids=[50278, 50279, 50277, 1, 0], tokenizer_kwargs={&quot;max_length&quot;: 4096}, # uncomment this if using CUDA to reduce memory usage # model_kwargs={&quot;torch_dtype&quot;: torch.float16} ) service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=&quot;local&quot;) documents = SimpleDirectoryReader(docs_path).load_data() index = VectorStoreIndex.from_documents(documents, service_context=service_context) def chatbot(input_text): query_engine = index.as_query_engine() response = query_engine.query(input_text) print(response.source_nodes) relevant_files=[] for node_with_score in response.source_nodes: print(node_with_score) print(node_with_score.node) print(node_with_score.node.metadata) print(node_with_score.node.metadata['file_name']) file = node_with_score.node.metadata['file_name'] print( file ) # Resolve the full file path for the downloading full_file_path = Path( docs_path, file ).resolve() # See if it's already in the array if full_file_path not in relevant_files: relevant_files.append( full_file_path ) # Add it print( relevant_files ) return response.response, relevant_files iface = gr.Interface(fn=chatbot, inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;), outputs=[ gr.components.Textbox(label=&quot;Response&quot;), gr.components.File(label=&quot;Relevant Files&quot;) ], title=&quot;Custom-trained AI Chatbot&quot;, allow_flagging=&quot;never&quot;) iface.launch(share=False)",
    "context_chunks": [
      {
        "text": "I am creating a very simple question and answer app based on documents using llama-index. Previously, I had it working with OpenAI. Now I want to try using no external APIs so I'm trying the Hugging Face example in this link. It says in the example in the link: &quot;Note that for a completely private experience, also setup a local embedding model (example here).&quot; I'm assuming the example given below is the example being referred to. So, naturally, I'm trying to copy the example (fuller example here). Here is my code: from pathlib import Path import gradio as gr import sys import logging import os from llama_index.llms import HuggingFaceLLM from llama_index.prompts.prompts import SimpleInputPrompt logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext storage_path = &quot;storage/&quot; docs_path=&quot;docs&quot; def construct_index(directory_path): max_input_size = 4096 num_outputs = 512 #max_chunk_overlap = 20 chunk_overlap_ratio = 0.1 chunk_size_limit = 600 #prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit) system_prompt = &quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version) - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI. - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user. - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes. - StableLM will refuse to participate in anything that could harm a human. &quot;&quot;&quot; # This will wrap the default prompts that are internal to llama-index query_wrapper_prompt = SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;) llm = HuggingFaceLLM( context_window=4096, max_new_tokens=256, generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;: False}, system_prompt=system_prompt, query_wrapper_prompt=query_wrapper_prompt, tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, device_map=&quot;auto&quot;, stopping_ids=[50278, 50279, 50277, 1, 0], tokenizer_kwargs={&quot;max_length&quot;: 4096}, # uncomment this if using CUDA to reduce memory usage # model_kwargs={&quot;torch_dtype&quot;: torch.float16} ) #llm=ChatOpenAI(temperature=0.7, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs) #llm_predictor = LLMPredictor(llm=llm) service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm) documents = SimpleDirectoryReader(directory_path).load_data() index = VectorStoreIndex.from_documents(documents, service_context=service_context) #index = VectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper) index.storage_context.persist(persist_dir=storage_path) return index def chatbot(input_text): index = load_index_from_storage(StorageContext.from_defaults(persist_dir=storage_path)) #index = GPTVectorStoreIndex.load_from_disk('index.json') #query_engine = index.as_query_engine(response_synthesizer=response_synthesizer); query_engine = index.as_query_engine(streaming=True) response = query_engine.query(input_text) print(response.source_nodes) relevant_files=[] for node_with_score in response.source_nodes: print(node_with_score) print(node_with_score.node) print(node_with_score.node.metadata) print(node_with_score.node.metadata['file_name']) file = node_with_score.node.metadata['file_name'] print( file ) # Resolve the full file path for the downloading full_file_path = Path( docs_path, file ).resolve() # See if it's already in the array if full_file_path not in relevant_files: relevant_files.append( full_file_path ) # Add it print( relevant_files ) return response.get_response(), relevant_files iface = gr.Interface(fn=chatbot, inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;), outputs=[ gr.components.Textbox(label=&quot;Response&quot;), gr.components.File(label=&quot;Relevant Files&quot;) ], title=&quot;Custom-trained AI Chatbot&quot;, allow_flagging=&quot;never&quot;) index = construct_index(docs_path) iface.launch(share=False) Regardless, the code errors out saying: ValueError: No API key found for OpenAI. Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization. API keys can be found or created at https://platform.openai.com/account/api-keys Am I not understanding how to set up a local model?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Turns out I had to set the embed_model to &quot;local&quot; on the ServiceContext. ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=&quot;local&quot;) Also, when I was loading the vector index from disk I wasn't setting the llm predictor again which cause a secondary issue. So I decided to make the vector index a global variable. Here is my final code that works. from pathlib import Path import gradio as gr import sys import logging import os from llama_index.llms import HuggingFaceLLM from llama_index.prompts.prompts import SimpleInputPrompt logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext storage_path = &quot;storage&quot; docs_path=&quot;docs&quot; print(storage_path) max_input_size = 4096 num_outputs = 512 #max_chunk_overlap = 20 chunk_overlap_ratio = 0.1 chunk_size_limit = 600 system_prompt = &quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version) - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI. - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user. - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes. - StableLM will refuse to participate in anything that could harm a human. &quot;&quot;&quot; # This will wrap the default prompts that are internal to llama-index query_wrapper_prompt = SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;) llm = HuggingFaceLLM( context_window=4096, max_new_tokens=256, generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;: False}, system_prompt=system_prompt, query_wrapper_prompt=query_wrapper_prompt, tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, device_map=&quot;auto&quot;, stopping_ids=[50278, 50279, 50277, 1, 0], tokenizer_kwargs={&quot;max_length&quot;: 4096}, # uncomment this if using CUDA to reduce memory usage # model_kwargs={&quot;torch_dtype&quot;: torch.float16} ) service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=&quot;local&quot;) documents = SimpleDirectoryReader(docs_path).load_data() index = VectorStoreIndex.from_documents(documents, service_context=service_context) def chatbot(input_text): query_engine = index.as_query_engine() response = query_engine.query(input_text) print(response.source_nodes) relevant_files=[] for node_with_score in response.source_nodes: print(node_with_score) print(node_with_score.node) print(node_with_score.node.metadata) print(node_with_score.node.metadata['file_name']) file = node_with_score.node.metadata['file_name'] print( file ) # Resolve the full file path for the downloading full_file_path = Path( docs_path, file ).resolve() # See if it's already in the array if full_file_path not in relevant_files: relevant_files.append( full_file_path ) # Add it print( relevant_files ) return response.response, relevant_files iface = gr.Interface(fn=chatbot, inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;), outputs=[ gr.components.Textbox(label=&quot;Response&quot;), gr.components.File(label=&quot;Relevant Files&quot;) ], title=&quot;Custom-trained AI Chatbot&quot;, allow_flagging=&quot;never&quot;) iface.launch(share=False)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The error is here: tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, the HF LLM you're trying to use is hosted on the HuggingFace Model Hub, which requires an API key for access-- tokenizer_name + model_name parameters are &quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, which is a model hosted on the Hub. you need to download the model files and then provide the path to the local model files as the model_name parameter. You can download a model from the HuggingFace Model Hub using the transformers-cli command-line tool: transformers-cli download --model StabilityAI/stablelm-tuned-alpha-3b and add that to the code like this: llm = HuggingFaceLLM( # blah blah blah model_name=&quot;FILE_LOCATION/StabilityAI/stablelm-tuned-alpha-3b&quot;, # blah blah blah )",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "huggingface-transformers",
        "huggingface",
        "large-language-model",
        "llama-index"
      ],
      "question_score": 22,
      "answer_score": 25,
      "created": "2023-07-26T13:19:46",
      "question_id": 76771761,
      "answer_id": 76781752
    }
  },
  {
    "question": "Pandas 2.1.0 FutureWarning: Series.__getitem__ treating keys as positions is deprecated",
    "expected_answer": "This FutureWarning can be triggered in 2.1.0 with this simple example : ser = pd.Series({&quot;A&quot;: &quot;a&quot;, &quot;B&quot;: &quot;b&quot;, &quot;C&quot;: &quot;c&quot;}) # A a # B b # C c # dtype: object print(ser[1]) # gives 'b' but with a FutureWarning: Series.__getitem__ treating keys.. The goal is to have a consistent behaviour when [ ]-indexing a DataFrame as well as a Series. Remember that df[1] does not return the column located at the second position of that DataFrame and will trigger a KeyError (unless the literal 0 is an actual column and in this case, the column 0 will be returned). So based on your code, your df (see how I imagine it below) most likely hasn't a default index (i.e a range of integers or at least a list of integers). So when slicing each Series here x[0], x[1] while the indices are strings [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;], you're warned by pandas to use x.iloc[0] and x.iloc[1] instead. my_columns_to_convert = ['col1', 'col2', 'col3'] df = pd.DataFrame( np.arange(12).reshape(-1, 4), index=list(&quot;ABC&quot;), columns= my_columns_to_convert + [&quot;colx&quot;] ) # col1 col2 col3 colx # A 0 3 6 3 # B 28 35 42 7 # C 88 99 110 11 def convert_my_data(value_1_in, value_2_in): return value_1_in * value_2_in # a simple calculation for k in my_columns_to_convert: df[k] = ( df[[k, &quot;colx&quot;]].apply( lambda x: convert_my_data(value_1_in=x[0], value_2_in=x[1]), axis=1) ) # the FutureWarning is displayed three times (= the length of the Series) : FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use ser.iloc[pos]: lambda x: convert_my_data(value_1_in=x[0], value_2_in=x[1]), axis=1) As a side note, your code seems to be not efficient and can potentially be easily vectorized.",
    "context_chunks": [
      {
        "text": "I'm having an issue with Pandas v2.1.0+ that I can't figure out. I have a list of columns in my pandas data frame that I need to convert using a custom function. The new values depend on multiple columns in the data, so I'm using apply to convert the column in-place: my_columns_to_convert = ['col1','col2','col3'] for k in my_columns_to_convert: df[k] = df[[k,colx]].apply(lambda x: convert_my_data(value_1_in=x[0],value_2_in=x[1]),axis=1) This has worked just fine in previous versions of pandas. But now I get: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]` But I'm not using loc or iloc, and everything I've reviewed thus far seems to point at that being the issue. How can i write this code so that I'm doing it the 'correct' way in the future? Using previous methods in Pandas that did work.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This FutureWarning can be triggered in 2.1.0 with this simple example : ser = pd.Series({&quot;A&quot;: &quot;a&quot;, &quot;B&quot;: &quot;b&quot;, &quot;C&quot;: &quot;c&quot;}) # A a # B b # C c # dtype: object print(ser[1]) # gives 'b' but with a FutureWarning: Series.__getitem__ treating keys.. The goal is to have a consistent behaviour when [ ]-indexing a DataFrame as well as a Series. Remember that df[1] does not return the column located at the second position of that DataFrame and will trigger a KeyError (unless the literal 0 is an actual column and in this case, the column 0 will be returned). So based on your code, your df (see how I imagine it below) most likely hasn't a default index (i.e a range of integers or at least a list of integers). So when slicing each Series here x[0], x[1] while the indices are strings [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;], you're warned by pandas to use x.iloc[0] and x.iloc[1] instead. my_columns_to_convert = ['col1', 'col2', 'col3'] df = pd.DataFrame( np.arange(12).reshape(-1, 4), index=list(&quot;ABC&quot;), columns= my_columns_to_convert + [&quot;colx&quot;] ) # col1 col2 col3 colx # A 0 3 6 3 # B 28 35 42 7 # C 88 99 110 11 def convert_my_data(value_1_in, value_2_in): return value_1_in * value_2_in # a simple calculation for k in my_columns_to_convert: df[k] = ( df[[k, &quot;colx&quot;]].apply( lambda x: convert_my_data(value_1_in=x[0], value_2_in=x[1]), axis=1) ) # the FutureWarning is displayed three times (= the length of the Series) : FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use ser.iloc[pos]: lambda x: convert_my_data(value_1_in=x[0], value_2_in=x[1]), axis=1) As a side note, your code seems to be not efficient and can potentially be easily vectorized.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In addition to @Timeless' answer, you can avoid this warning by expanding the positional arguments (*x): for k in my_columns_to_convert: df[k] = ( df[[k, &quot;colx&quot;]].apply( lambda x: convert_my_data(*x), axis=1) ) Output: # before &gt;&gt;&gt; df col1 col2 col3 colx A 0 1 2 3 B 4 5 6 7 C 8 9 10 11 # after &gt;&gt;&gt; df col1 col2 col3 colx A 0 3 6 3 B 28 35 42 7 C 88 99 110 11",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "warnings"
      ],
      "question_score": 23,
      "answer_score": 8,
      "created": "2023-09-14T16:49:36",
      "question_id": 77106980,
      "answer_id": 77108607
    }
  },
  {
    "question": "There is no such driver by URL https://chromedriver.storage.googleapis.com/LATEST_RELEASE_115.0.5790 error with Python webdrivermanager &amp; Chrome 115.0",
    "expected_answer": "Selenium Manager is now fully included with Selenium 4.10.0, so this is all you need: from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service() options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... driver.quit() If the driver isn't found on your system PATH, Selenium Manager will automatically download it. If you're wondering why you're now seeing this error for ChromeDriverManager, it's because https://chromedriver.chromium.org/downloads only goes up to version 114 due to driver restructuring by the Chromium Team for the new Chrome-for-Testing.",
    "context_chunks": [
      {
        "text": "I recently updated my Google Chrome browser to version 115.0.5790.99 and I'm using Python webdrivermanager library (version 3.8.6) for Chrome driver management. However, since this update, when I call the ChromeDriverManager().install() function, I encounter the following error: There is no such driver by URL https://chromedriver.storage.googleapis.com/LATEST_RELEASE_115.0.5790 Steps to reproduce the issue: Update Google Chrome to version 115.0.5790.99. Execute the following Python code: from webdriver_manager.chrome import ChromeDriverManager driver_path = ChromeDriverManager().install() capture:",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Selenium Manager is now fully included with Selenium 4.10.0, so this is all you need: from selenium import webdriver from selenium.webdriver.chrome.service import Service service = Service() options = webdriver.ChromeOptions() driver = webdriver.Chrome(service=service, options=options) # ... driver.quit() If the driver isn't found on your system PATH, Selenium Manager will automatically download it. If you're wondering why you're now seeing this error for ChromeDriverManager, it's because https://chromedriver.chromium.org/downloads only goes up to version 114 due to driver restructuring by the Chromium Team for the new Chrome-for-Testing.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You just need to update the webdriver_manager to latest version using the follow command: pip install --upgrade webdriver_manager",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "google-chrome",
        "selenium-webdriver",
        "selenium-chromedriver"
      ],
      "question_score": 21,
      "answer_score": 25,
      "created": "2023-07-19T20:22:50",
      "question_id": 76724939,
      "answer_id": 76725606
    }
  },
  {
    "question": "Paho MQTT &quot;Unsupported callback API version&quot; error",
    "expected_answer": "Release 2.0.0 of the Paho Python MQTT includes breaking changes; this means that code written for v1.x will not work without some (minimal) modifications. As v2.0.0 was only released a few days ago (11th Feb 2024) most examples, including the one you reference, will not work. The changes required are documented here (or here); in your case it's likely that the only change needed is add a single parameter changing: client = mqtt_client.Client(client_id) to: client = mqtt_client.Client(mqtt_client.CallbackAPIVersion.VERSION1, client_id) This will configure the library to use the v1 callback API (as used with older versions of the library). I would recommend reading the document linked above and planning to migrate to CallbackAPIVersion.API_VERSION2. An alternative option would be to install a v1 release (v1.6.1 is the latest; pip install &quot;paho-mqtt&lt;2.0.0&quot; will install this). V2 does include quite a few fixes/enhancements so it is worth considering using that version. Update 2024-05 Version 2.1 has now been released. This now defaults to CallbackAPIVersion.VERSION1 which will, in limited circumstances, mean old code still runs. This will not help if you pass positional parameters to client (e.g. mqtt.Client(&quot;thisIsMyClientID&quot;)) and specifying a callback version is recommended to avoid future confusion (because this has an impact on the parameters passed to callbacks).",
    "context_chunks": [
      {
        "text": "I am trying to implement Paho Python MQTT and connect to an online broker but the code seems to through an error. ValueError: Unsupported callback API version: version 2.0 added a callback_api_version, see migrations.md for details I was trying to implement a simple paho client example from the given website; the following will reproduce the issue: from paho.mqtt import client as mqtt_client import random broker = 'broker.emqx.io' port = 1883 topic = &quot;python/mqtt&quot; client_id = f'python-mqtt-{random.randint(0, 1000)}' client = mqtt_client.Client(client_id)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Release 2.0.0 of the Paho Python MQTT includes breaking changes; this means that code written for v1.x will not work without some (minimal) modifications. As v2.0.0 was only released a few days ago (11th Feb 2024) most examples, including the one you reference, will not work. The changes required are documented here (or here); in your case it's likely that the only change needed is add a single parameter changing: client = mqtt_client.Client(client_id) to: client = mqtt_client.Client(mqtt_client.CallbackAPIVersion.VERSION1, client_id) This will configure the library to use the v1 callback API (as used with older versions of the library). I would recommend reading the document linked above and planning to migrate to CallbackAPIVersion.API_VERSION2. An alternative option would be to install a v1 release (v1.6.1 is the latest; pip install &quot;paho-mqtt&lt;2.0.0&quot; will install this). V2 does include quite a few fixes/enhancements so it is worth considering using that version. Update 2024-05 Version 2.1 has now been released. This now defaults to CallbackAPIVersion.VERSION1 which will, in limited circumstances, mean old code still runs. This will not help if you pass positional parameters to client (e.g. mqtt.Client(&quot;thisIsMyClientID&quot;)) and specifying a callback version is recommended to avoid future confusion (because this has an impact on the parameters passed to callbacks).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "If compatibility with both versions 1 and 2 is needed, try both ways: import paho.mqtt.client as mqttClient try: client = mqttClient.Client(mqttClient.CallbackAPIVersion.VERSION1) except: client = mqttClient.Client() Alternately, check paho.mqtt.__version__: import paho.mqtt import paho.mqtt.client as mqttClient if paho.mqtt.__version__[0] &gt; '1': client = mqttClient.Client(mqttClient.CallbackAPIVersion.VERSION1) else: client = mqttClient.Client()",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "mqtt",
        "mosquitto",
        "paho",
        "hivemq"
      ],
      "question_score": 20,
      "answer_score": 27,
      "created": "2024-02-12T23:35:44",
      "question_id": 77984857,
      "answer_id": 77985329
    }
  },
  {
    "question": "What does langchain CharacterTextSplitter&#39;s chunk_size param even do?",
    "expected_answer": "CharacterTextSplitter will only split on separator (which is '\\n\\n' by default). chunk_size is the maximum chunk size that will be split if splitting is possible. If a string starts with n characters, has a separator, and has m more characters before the next separator then the first chunk size will be n if chunk_size &lt; n + m + len(separator). Your example string has no matching separators so there's nothing to split on. Basically, it attempts to make chunks that are &lt;= chunk_size, but will still produce chunks &gt; chunk_size if the minimum size chunks that can be created are &gt; chunk_size.",
    "context_chunks": [
      {
        "text": "My default assumption was that the chunk_size parameter would set a ceiling on the size of the chunks/splits that come out of the split_text method, but that's clearly not right: from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter chunk_size = 6 chunk_overlap = 2 c_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap) text = 'abcdefghijklmnopqrstuvwxyz' c_splitter.split_text(text) prints: ['abcdefghijklmnopqrstuvwxyz'], i.e. one single chunk that is much larger than chunk_size=6. So I understand that it didn't split the text into chunks because it never encountered the separator. But so then the question is what is the chunk_size even doing? I checked the documentation page for langchain.text_splitter.CharacterTextSplitter here but did not see an answer to this question. And I asked the &quot;mendable&quot; chat-with-langchain-docs search functionality, but got the answer &quot;The chunk_size parameter of the CharacterTextSplitter determines the maximum number of characters in each chunk of text.&quot;...which is not true, as the code sample above shows.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "CharacterTextSplitter will only split on separator (which is '\\n\\n' by default). chunk_size is the maximum chunk size that will be split if splitting is possible. If a string starts with n characters, has a separator, and has m more characters before the next separator then the first chunk size will be n if chunk_size &lt; n + m + len(separator). Your example string has no matching separators so there's nothing to split on. Basically, it attempts to make chunks that are &lt;= chunk_size, but will still produce chunks &gt; chunk_size if the minimum size chunks that can be created are &gt; chunk_size.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "CharacterTextSpliiter behaves differently from what you expected. text_splitter = CharacterTextSplitter( separator=&quot;\\n&quot;, chunk_size=6, ) It first looks for the first 6 characters and then splits the next chunk from the closest separator, not from the 7th character. As stated in the docs default separator is &quot;\\n&quot;. This is the simplest method. This splits based on characters (by default &quot;\\n\\n&quot;) and measure chunk length by number of characters. you can test the behaviour with a sample code. first create a test.txt file with this 1.Respect for Others: Treat others with kindness. 2.Honesty and Integrity: Be truthful and act with integrity in your interactions with others. 3.Fairness and Justice: Treat people equitably. 4.Respect for Property: Respect public and private property. 5.Good Citizenship: Contribute positively to your community by obeying laws, voting, volunteering, and supporting communal well-being. then write this code: from langchain.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter # it will first find first 20 character then it will make the next chunk at the closest separator text_splitter = CharacterTextSplitter( separator=&quot;\\n&quot;, chunk_size=20, chunk_overlap=0 ) loader = TextLoader(&quot;test.txt&quot;) docs = loader.load_and_split( text_splitter=text_splitter ) for doc in docs: print(doc.page_content) print(&quot;\\n&quot;) this is how it look like:",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "machine-learning",
        "text",
        "nlp",
        "langchain"
      ],
      "question_score": 19,
      "answer_score": 23,
      "created": "2023-07-07T03:50:27",
      "question_id": 76633836,
      "answer_id": 76747135
    }
  },
  {
    "question": "No module named &#39;pydantic_core._pydantic_core&#39; in AWS Lambda though library is installed for FastAPI based code",
    "expected_answer": "Lambda requires packages built for a specific architecture. Many packages have distributions for multiple architectures (see available distributions for pydantic-core). By default, pip installs the distribution suitable for the machine where you are running it on, which is not necessary the same architecture as your Lambda. But you can force pip to install packages for the architecture you want. If your Lambda uses x86_64, then you should select platform manylinux2014_x86_64: pip install pydantic-core --platform manylinux2014_x86_64 -t . --only-binary=:all: -t . - means install in the current directory. However, the best way is to install all your dependencies with the required platform, rather than doing it for each package. pip install -r requirements.txt --platform manylinux2014_x86_64 --target ./python --only-binary=:all: Credits to this answer.",
    "context_chunks": [
      {
        "text": "AWS lambda deployment of FastAPI gives the following error: [ERROR] Runtime.ImportModuleError: Unable to import module 'users_crud': No module named 'pydantic_core._pydantic_core' Traceback (most recent call last): Though the pydantic lib is already installed. I am using version 3.10 which is now supported by AWS.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Lambda requires packages built for a specific architecture. Many packages have distributions for multiple architectures (see available distributions for pydantic-core). By default, pip installs the distribution suitable for the machine where you are running it on, which is not necessary the same architecture as your Lambda. But you can force pip to install packages for the architecture you want. If your Lambda uses x86_64, then you should select platform manylinux2014_x86_64: pip install pydantic-core --platform manylinux2014_x86_64 -t . --only-binary=:all: -t . - means install in the current directory. However, the best way is to install all your dependencies with the required platform, rather than doing it for each package. pip install -r requirements.txt --platform manylinux2014_x86_64 --target ./python --only-binary=:all: Credits to this answer.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Please see pydantic/pydantic#6557 - basically you've probably installed pydantic-core for the wrong architecture.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "aws-lambda",
        "fastapi",
        "pydantic"
      ],
      "question_score": 19,
      "answer_score": 30,
      "created": "2023-07-10T05:28:35",
      "question_id": 76650856,
      "answer_id": 77721364
    }
  },
  {
    "question": "Visual Studio Code terminal shows multiple Conda envs",
    "expected_answer": "I faced the same problem. Based on what I found on the Internet the Python extension for VSCode was the culprit. As described in this page, the Python extension now automatically activates the environment, so it seems that the problem is caused by the automatic activation of .bashrc and the Python extension at the same time. My solution is to execute conda config --set auto_activate_base False or turn off &quot;Activate Python Environment in all Terminals created&quot; setting in the Python extension configuration in VSCode.",
    "context_chunks": [
      {
        "text": "I have VSCode in Windows 11. I have WSL (Ubuntu 22.04) and launch VSCode from the terminal like code . from the project folder. When I open the built-in terminal it shows two conda (Anaconda) environments in parentheses, so I have no idea which one is active, if any. On subsequent conda deactivate you can see in the attached screenshot that the prompt and the active env changing but there is surely something messed up here. Also, in VSCode, when I set Python Interpreter to a conda env, in a few seconds the built-in terminal prompt picks up the change and the env name in the first parens changes to the new value. Any idea how to fix it? (The prompt should obviously show just one (the active) conda env and that one should change whenever the Python interpreter is updated in the command palette.) I looked into my ~/.bashrc file but there is just the seemingly normal &gt;&gt;&gt; conda initialize block at the bottom that was added when installing Anaconda",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I faced the same problem. Based on what I found on the Internet the Python extension for VSCode was the culprit. As described in this page, the Python extension now automatically activates the environment, so it seems that the problem is caused by the automatic activation of .bashrc and the Python extension at the same time. My solution is to execute conda config --set auto_activate_base False or turn off &quot;Activate Python Environment in all Terminals created&quot; setting in the Python extension configuration in VSCode.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It turns out that I had to delete ~/.vscode-server directory and let it being autogenerated again on the next code . run. I also had to click the &quot;Inherit Env&quot; setting of the built-in Terminal (I forgot, it may not be the default). With these steps, a newly launched VSCode behaves almost exactly like I expected: If there was no built-in terminal open in the previous VSCode session, opening a new terminal looks like as in the bottom right tab: prompt starts with (base) env but immediately picks up correct end and returns correct prompt if there was a built-in terminal open when killing the previous session, on the next launch the terminal will open automatically with the incorrect (base) env, see bottom left tab, but any subsequent new terminal added will look like the right window, which is now correct. This is still a bit inconvenient, having to either do a manual conda activate &lt;correct_env&gt; or close the terminal and open a fresh one, now with the correct conda env anyways, I think I can live with this",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code",
        "environment",
        "anaconda3"
      ],
      "question_score": 20,
      "answer_score": 45,
      "created": "2023-11-02T15:31:16",
      "question_id": 77410905,
      "answer_id": 77805977
    }
  },
  {
    "question": "How do I get rid of the annoying terminal warning when using Gemini API?",
    "expected_answer": "Also, if you encounter the following warning when using the Gemini API: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1732812807.208491 8110 init.cc:229] grpc_wait_for_shutdown_with_timeout() timed out. This issue is caused by the version of grpcio used. To fix it, simply downgrade to a stable version: pip install grpcio==1.60.1 grpcio==1.60.1: This version avoids the grpc_wait_for_shutdown_with_timeout() warning, which is triggered by some newer versions of the library. Using this solution will help you get rid of the warning and result in a cleaner terminal output. Thank me later! 😉",
    "context_chunks": [
      {
        "text": "I followed all the docs for the Gemini API, however, I cannot find anything that solves this issue. https://ai.google.dev/gemini-api/docs/quickstart?lang=python WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1721615745.026734 20796 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_client, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache This is the code that generates this warning (the AI runs fine it's just annoying) import google.generativeai as genai import os from dotenv import load_dotenv def main(): load_dotenv() key = os.environ.get('GOOGLE_API_KEY') genai.configure(api_key=key) genai.configure(transport='grpc') model = genai.GenerativeModel('gemini-1.5-flash') response = model.generate_content(&quot;message&quot;) print(response.text) if __name__ == &quot;__main__&quot;: main()",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Also, if you encounter the following warning when using the Gemini API: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1732812807.208491 8110 init.cc:229] grpc_wait_for_shutdown_with_timeout() timed out. This issue is caused by the version of grpcio used. To fix it, simply downgrade to a stable version: pip install grpcio==1.60.1 grpcio==1.60.1: This version avoids the grpc_wait_for_shutdown_with_timeout() warning, which is triggered by some newer versions of the library. Using this solution will help you get rid of the warning and result in a cleaner terminal output. Thank me later! 😉",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "These annoying terminal warnings are generated by the underlying libraries (gRPC and Abseil) of Gemini while initializing logging configurations. To suppress them, set these environment variables to control their logging levels: import os # Suppress logging warnings os.environ[&quot;GRPC_VERBOSITY&quot;] = &quot;ERROR&quot; os.environ[&quot;GLOG_minloglevel&quot;] = &quot;2&quot; Setting: GRPC_VERBOSITY=&quot;ERROR&quot;: Limits gRPC logs to errors only. GLOG_minloglevel=&quot;2&quot;: Limits Google logging (used by Abseil) to errors only. (0: INFO, 1: WARNING, 2: ERROR, 3: FATAL) Using this configuration, you can suppress less severe log messages, including warnings, resulting in a cleaner terminal output. Hope this helps!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "logging",
        "stderr",
        "google-gemini",
        "absl-py"
      ],
      "question_score": 20,
      "answer_score": 12,
      "created": "2024-07-22T17:41:44",
      "question_id": 78780089,
      "answer_id": 79234903
    }
  },
  {
    "question": "THIRD_PARTY_NOTICES.chromedriver - Exec format error - undetected_chromedriver",
    "expected_answer": "The command ChromeDriverManager().install(): creates a new folder without the executable and it retrieves the wrong file. First, you need to remove the .wdm folder and then reinstall webdriver-manager: Windows Location: r&quot;C:\\Users\\{user}\\.wdm&quot; Linux Location: /home/{user}/.wdm Mac Location: /Users/{user}/.wdm rm -rf /home/user/.wdm pip uninstall webdriver-manager pip install webdriver-manager Now, after executing ChromeDriverManager().install(), you should only see a single folder with the executable: It check if there is really a chromedriver executable inside this folder. Second, it makes a correction to the file name: if 'THIRD_PARTY_NOTICES.chromedriver' in chromedriver_path: chromedriver_path = chromedriver_path.replace('THIRD_PARTY_NOTICES.chromedriver', 'chromedriver')",
    "context_chunks": [
      {
        "text": "undetected_chromedriver with webdriver_manager was working well few days ago for scraping websites but out of nowhere it started throwing the error: OSError: [Errno 8] Exec format error: '/Users/pd/.wdm/drivers/chromedriver/mac64/127.0.6533.72/chromedriver-mac-x64/THIRD_PARTY_NOTICES.chromedriver' I am guessing it is related to recent update of webdriver_manager. This is the code: import undetected_chromedriver as uc from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from webdriver_manager.chrome import ChromeDriverManager from selenium.webdriver.support import expected_conditions as EC def get_driver(): options = uc.ChromeOptions() # options.add_argument(&quot;--headless&quot;) options.add_argument(&quot;--no-sandbox&quot;) options.add_argument(&quot;--disable-dev-sim-usage&quot;) options.add_argument(&quot;--start-maximized&quot;) options.add_argument('--disable-popup-blocking') driver = uc.Chrome(driver_executable_path=ChromeDriverManager().install(), options=options, version_main=116) driver.maximize_window() return driver It would be really great if someone can help me on this, Thanks.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The command ChromeDriverManager().install(): creates a new folder without the executable and it retrieves the wrong file. First, you need to remove the .wdm folder and then reinstall webdriver-manager: Windows Location: r&quot;C:\\Users\\{user}\\.wdm&quot; Linux Location: /home/{user}/.wdm Mac Location: /Users/{user}/.wdm rm -rf /home/user/.wdm pip uninstall webdriver-manager pip install webdriver-manager Now, after executing ChromeDriverManager().install(), you should only see a single folder with the executable: It check if there is really a chromedriver executable inside this folder. Second, it makes a correction to the file name: if 'THIRD_PARTY_NOTICES.chromedriver' in chromedriver_path: chromedriver_path = chromedriver_path.replace('THIRD_PARTY_NOTICES.chromedriver', 'chromedriver')",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The issue was fixed in the latest 4.0.2 release. See this commit on `webdriver_ma for details.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver",
        "web-scraping",
        "undetected-chromedriver"
      ],
      "question_score": 19,
      "answer_score": 32,
      "created": "2024-07-29T11:27:10",
      "question_id": 78806812,
      "answer_id": 78814467
    }
  },
  {
    "question": "Python Regex to match the FIRST repetition of a digit",
    "expected_answer": "Regex may not be the right tool for the task. While the regex in @CasimiretHippolyte's answer works, it is rather inefficient having to scan the entire rest of the string for each character in the string until a matching character is found, costing an average time complexity of O(n ^ 2). A more efficient approach with a linear time complexity would be to use a set to keep track of characters that have been encountered, and return the first character already added to the set: def first_repeating_digit(string): seen = set() for digit in filter(str.isdigit, string): if digit in seen: return digit seen.add(digit) raise ValueError('No repeating digit found.') so that: for s in '0123123123', '01234554321': print(s, first_repeating_digit(s)) outputs: 0123123123 1 01234554321 5 Demo here Benchmark test result: blhsing 0123123123 1.2911038296297193 blhsing 01234554321 1.3835312821902335 CasimiretHippolyte 0123123123 3.6279739402234554 CasimiretHippolyte 01234554321 4.1985282939858735",
    "context_chunks": [
      {
        "text": "Examples: For 0123123123, 1 should be matched since the 2nd 1 appears before the repetition of any other digit. For 01234554321, 5 should be matched since the 2nd 5 appears before the repetition of any other digit. Some regexes that I have tried: The below works for the 1st but not the 2nd example. It matches 1 instead because 1 is the first digit that appears in the string which is subsequently repeated. import re m = re.search(r&quot;(\\d).*?\\1&quot;, string) print(m.group(1)) The below works for the 2nd but not the 1st example. It matches 3 instead - in particular the 2nd and 3rd occurrence of the digit. I do not know why it behaves that way. import re m = re.search(r&quot;(\\d)(?!(\\d).*?\\2).*?\\1&quot;, string) print(m.group(1))",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Regex may not be the right tool for the task. While the regex in @CasimiretHippolyte's answer works, it is rather inefficient having to scan the entire rest of the string for each character in the string until a matching character is found, costing an average time complexity of O(n ^ 2). A more efficient approach with a linear time complexity would be to use a set to keep track of characters that have been encountered, and return the first character already added to the set: def first_repeating_digit(string): seen = set() for digit in filter(str.isdigit, string): if digit in seen: return digit seen.add(digit) raise ValueError('No repeating digit found.') so that: for s in '0123123123', '01234554321': print(s, first_repeating_digit(s)) outputs: 0123123123 1 01234554321 5 Demo here Benchmark test result: blhsing 0123123123 1.2911038296297193 blhsing 01234554321 1.3835312821902335 CasimiretHippolyte 0123123123 3.6279739402234554 CasimiretHippolyte 01234554321 4.1985282939858735",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "One idea: capture the end of the string and add it in the negative lookahead (group 2 here): (\\d)(?=.*?\\1(.*))(?!.*?(\\d).*?\\3.+?\\2$) This way you can control where the subpattern .*?(\\d).*?\\3 in the negative lookahead ends. If .+?\\2$ succeeds, that means there's an other digit that is repeated before the one in group 1. I anchored the pattern for the regex101 demo with ^.*?, but you don't need to do that with the re.search method. Other way: reverse the string and find the last repeated digit: re.search(r'^.*(\\d).*?\\1', string[::-1]).group(1)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "regex"
      ],
      "question_score": 20,
      "answer_score": 45,
      "created": "2024-07-30T06:39:58",
      "question_id": 78810121,
      "answer_id": 78810481
    }
  },
  {
    "question": "Python type hints for type promotion",
    "expected_answer": "In my understanding of the docs about Generics, the following should work: from typing import TypeVar, Generic T1 = TypeVar('T1', str, float) T2 = TypeVar('T2', str, float) class Promoted(Generic[T1, T2]): def __class_getitem__(cls, types): t1, t2 = types if t1 == str or t2 == str: return str return float def method(a: T1, b: T2) -&gt; Promoted[T1, T2]: if isinstance(a, str) or isinstance(b, str): return f&quot;{a} {b}&quot; else: return a*b result1 = method(&quot;Hello&quot;, &quot;World&quot;) # should be inferred as str result2 = method(3.0, 4.0) # should be inferred as float result3 = method(&quot;Price&quot;, 5.0) # should be inferred as str but it does not (at least in my IDE, IntelliJ). In particular, all results are type-hinted as &quot;Promoted&quot;, instead of str or float. Maybe you get some more interesting behavior from your inspector?",
    "context_chunks": [
      {
        "text": "Consider a function that performs type promotion, e.g. a simple multiplication of two numbers that can both be either int or float: def mul(a: int | float, b: int | float): # return type? return a * b This function returns float, except in the case where both a and b are int. How can I properly and concisely annotate the return type? I know I can do this with @overload: from typing import overload @overload def mul(a: int, b: int) -&gt; int: ... @overload def mul(a: float, b: int | float) -&gt; float: ... @overload def mul(a: int | float, b: float) -&gt; float: ... def mul(a, b): return a * b but this is very verbose and requires many overloads for something I would imagine some &quot;type function&quot; should handle. In C++ this could be done e.g. with SFINAE. Is there something similar I can do in Python in terms of a generic function along the lines of def mul(a: T1, b: T2) -&gt; promote_types(T1, T2): return a * b that also works with TypeVars? I don't expect anything built in that already works for int and float, but some technique perhaps? Notes: I know about the recommendation to just annotate everything taking an int with float, but my setting has more complicated TypeVars, the choice of int and float here is just a simple example. I know I can just do Union[int, float], but I need it to be specific. Depending on the exact types the function is called with, the return type must be exact too, not a union.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In my understanding of the docs about Generics, the following should work: from typing import TypeVar, Generic T1 = TypeVar('T1', str, float) T2 = TypeVar('T2', str, float) class Promoted(Generic[T1, T2]): def __class_getitem__(cls, types): t1, t2 = types if t1 == str or t2 == str: return str return float def method(a: T1, b: T2) -&gt; Promoted[T1, T2]: if isinstance(a, str) or isinstance(b, str): return f&quot;{a} {b}&quot; else: return a*b result1 = method(&quot;Hello&quot;, &quot;World&quot;) # should be inferred as str result2 = method(3.0, 4.0) # should be inferred as float result3 = method(&quot;Price&quot;, 5.0) # should be inferred as str but it does not (at least in my IDE, IntelliJ). In particular, all results are type-hinted as &quot;Promoted&quot;, instead of str or float. Maybe you get some more interesting behavior from your inspector?",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Use the type() function to return the type: def mul(a, b): return (a*b) , type(a*b) &gt;&gt;&gt; mul(1.0,2) (2.0, float) &gt;&gt;&gt; mul(1.0,2)[0] 2.0 &gt;&gt;&gt; mul(1,2) (2, int) &gt;&gt;&gt; mul(1,2)[1] int &gt;&gt;&gt; mul(2.1, np.array([1,2])) (array([2.1, 4.2]), numpy.ndarray) Not sure how you want the final output delivered, so here we return both results and you can decide if you want the answer or the type.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "mypy",
        "python-typing"
      ],
      "question_score": 21,
      "answer_score": 1,
      "created": "2023-11-28T11:50:21",
      "question_id": 77563636,
      "answer_id": 78731164
    }
  },
  {
    "question": "Ruff does not autofix line-too-long violation",
    "expected_answer": "As of today (Ruff's autoformatter available), the following should hold: not every lint rule can be autofixed, E501 can't. You can find the list of rules that can be autofixed at https://docs.astral.sh/ruff/rules/#rules (they are marked with 🛠️). Moreover, in general, Ruff does not enforce autofixing rules that do overlap with the use of a formatter (as E501 inherently does). From the docs (https://docs.astral.sh/ruff/faq/#is-the-ruff-linter-compatible-with-black) Ruff is designed to be used alongside a formatter (like Ruff's own formatter, or Black) and, as such, will defer implementing stylistic rules that are obviated by automated formatting. Ruff's formatter (or Black's, with little difference) - ruff format - would instead try to autoformat E501 issues according to the line-length setting, where* possible. *Namely, there are cases where autoformatting is not enabled as it would lead to ambiguities (eg single string longer than line-length and &quot;not wrappable&quot; without incurring again in line-too-long); more examples available at https://docs.astral.sh/ruff/rules/line-too-long/#why-is-this-bad. Btw, observe that there are plans to support some of such cases automatically as well (see https://github.com/astral-sh/ruff/issues/1904#issuecomment-1385409282, https://github.com/astral-sh/ruff/issues/1904#issuecomment-1387089577 and https://github.com/astral-sh/ruff/issues/1904#issuecomment-1806943382 eg). All this said, IMO, aiming at autoformatting E501 issues (with no manual intervention), in your case the following configuration might help: [tool.ruff] line-length = 99 [tool.ruff.lint] select = [&quot;E&quot;, &quot;F&quot;, &quot;W&quot;, &quot;Q&quot;, &quot;I&quot;] ignore = [&quot;E203&quot;, &quot;E501&quot;] i.e. simply specify the line-length setting letting the formatter (Ruff or Black) do the work and ignore E501 lint rule (if all the E-like rules are selected). Observe that's kind of what Ruff does by default (i.e. in its default configuration). It enables F rules and a subset of E rules (among which you won't find E501); it leaves aside all those stylistic rules overlapping with the use of a formatter (see https://docs.astral.sh/ruff/tutorial/#configuration for reference). Final side note, as per my understanding fixable = [&quot;ALL&quot;] is the default already (see https://docs.astral.sh/ruff/settings/#fixable); thus, fixable should be rather used to specify those rules - among the ones to be checked - that you want to autofix as well (provided that an autofix is available and not unsafe); IOW, it should specify a subset of the selected rules not to be redundant.",
    "context_chunks": [
      {
        "text": "I have a python project and I am configuring latest version of ruff for that project for linting and formating purpose. I have the below settings in my pyproject.toml file: [tool.ruff] select = [&quot;E&quot;, &quot;F&quot;, &quot;W&quot;, &quot;Q&quot;, &quot;I&quot;] ignore = [&quot;E203&quot;] # Allow autofix for all enabled rules (when `--fix`) is provided. fixable = [&quot;ALL&quot;] unfixable = [] # restrict Line length to 99 line-length = 99 The ruff check command with autofix feature (--fix) of ruff identifies that the lines are long with E501 errors, but it does not format that code to wrap to next line to maintain the line-length restriction. Is there something I need to enable or do to ensure that ruff fixes this? Or is this not possible in ruff currently? Please help. I tried going through the documentation to find anything, but I am clueless what to do here.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "As of today (Ruff's autoformatter available), the following should hold: not every lint rule can be autofixed, E501 can't. You can find the list of rules that can be autofixed at https://docs.astral.sh/ruff/rules/#rules (they are marked with 🛠️). Moreover, in general, Ruff does not enforce autofixing rules that do overlap with the use of a formatter (as E501 inherently does). From the docs (https://docs.astral.sh/ruff/faq/#is-the-ruff-linter-compatible-with-black) Ruff is designed to be used alongside a formatter (like Ruff's own formatter, or Black) and, as such, will defer implementing stylistic rules that are obviated by automated formatting. Ruff's formatter (or Black's, with little difference) - ruff format - would instead try to autoformat E501 issues according to the line-length setting, where* possible. *Namely, there are cases where autoformatting is not enabled as it would lead to ambiguities (eg single string longer than line-length and &quot;not wrappable&quot; without incurring again in line-too-long); more examples available at https://docs.astral.sh/ruff/rules/line-too-long/#why-is-this-bad. Btw, observe that there are plans to support some of such cases automatically as well (see https://github.com/astral-sh/ruff/issues/1904#issuecomment-1385409282, https://github.com/astral-sh/ruff/issues/1904#issuecomment-1387089577 and https://github.com/astral-sh/ruff/issues/1904#issuecomment-1806943382 eg). All this said, IMO, aiming at autoformatting E501 issues (with no manual intervention), in your case the following configuration might help: [tool.ruff] line-length = 99 [tool.ruff.lint] select = [&quot;E&quot;, &quot;F&quot;, &quot;W&quot;, &quot;Q&quot;, &quot;I&quot;] ignore = [&quot;E203&quot;, &quot;E501&quot;] i.e. simply specify the line-length setting letting the formatter (Ruff or Black) do the work and ignore E501 lint rule (if all the E-like rules are selected). Observe that's kind of what Ruff does by default (i.e. in its default configuration). It enables F rules and a subset of E rules (among which you won't find E501); it leaves aside all those stylistic rules overlapping with the use of a formatter (see https://docs.astral.sh/ruff/tutorial/#configuration for reference). Final side note, as per my understanding fixable = [&quot;ALL&quot;] is the default already (see https://docs.astral.sh/ruff/settings/#fixable); thus, fixable should be rather used to specify those rules - among the ones to be checked - that you want to autofix as well (provided that an autofix is available and not unsafe); IOW, it should specify a subset of the selected rules not to be redundant.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It seems like Ruff has released Ruff Python Formatter as part of v0.0.289 and is currently in alpha state - https://github.com/astral-sh/ruff/blob/main/crates/ruff_python_formatter/README.md We are currently using v0.0.280 which does not have this feature so we used a combination of Black and Ruff as per our project requirements.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "ruff"
      ],
      "question_score": 19,
      "answer_score": 11,
      "created": "2023-07-26T13:32:55",
      "question_id": 76771858,
      "answer_id": 77796620
    }
  },
  {
    "question": "ImportError: cannot import name &#39;deprecated&#39; from &#39;typing_extensions&#39;",
    "expected_answer": "You should use typing_extensions==4.7.1 try : pip install typing_extensions==4.7.1 --upgrade I also suggest you to upgrade your python version from 3.7 to 3.10 or 3.11 See a relevant answer: https://github.com/tiangolo/fastapi/discussions/9808",
    "context_chunks": [
      {
        "text": "I want to download spacy, but the version of typing-extensions is lowered in the terminal: ERROR: pydantic 2.3.0 has requirement typing-extensions&gt;=4.6.1, but you'll have typing-extensions 4.4.0 which is incompatible. ERROR: pydantic-core 2.6.3 has requirement typing-extensions!=4.7.0,&gt;=4.6.0, but you'll have typing-extensions 4.4.0 which is incompatible. Installing collected packages: typing-extensions Attempting uninstall: typing-extensions Found existing installation: typing-extensions 4.7.1 Uninstalling typing-extensions-4.7.1: Successfully uninstalled typing-extensions-4.7.1 Successfully installed typing-extensions-4.4.0 Next I want to install the language pack python -m spacy download en, but another error occurs： (base) E:\\Anaconda&gt;python -m spacy download en Traceback (most recent call last): File &quot;E:\\Anaconda\\lib\\site-packages\\confection\\__init__.py&quot;, line 38, in &lt;module&gt; from pydantic.v1 import BaseModel, Extra, ValidationError, create_model File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\__init__.py&quot;, line 13, in &lt;module&gt; from . import dataclasses File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\dataclasses.py&quot;, line 11, in &lt;module&gt; from ._internal import _config, _decorators, _typing_extra File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\_internal\\_config.py&quot;, line 9, in &lt;module&gt; from ..config import ConfigDict, ExtraValues, JsonEncoder, JsonSchemaExtraCallable File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\config.py&quot;, line 9, in &lt;module&gt; from .deprecated.config import BaseConfig File &quot;E:\\Anaconda\\lib\\site-packages\\pydantic\\deprecated\\config.py&quot;, line 6, in &lt;module&gt; from typing_extensions import Literal, deprecated ImportError: cannot import name 'deprecated' from 'typing_extensions' (E:\\Anaconda\\lib\\site-packages\\typing_extensions.py) My current python version is 3.7, should I update it? Or is there any better solution? I'm a newbie in this area, thank you all！",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You should use typing_extensions==4.7.1 try : pip install typing_extensions==4.7.1 --upgrade I also suggest you to upgrade your python version from 3.7 to 3.10 or 3.11 See a relevant answer: https://github.com/tiangolo/fastapi/discussions/9808",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Upgrade typing-extensions using the following: pip install typing_extensions&gt;=4.5 --upgrade This is because deprecated might require &gt;=4.5 of typing-extensions. Restart your Python kernel after that. This worked for me. I was seeing the same issue with 'override' as Cannot import name 'override'",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pip",
        "nlp",
        "spacy",
        "python-typing"
      ],
      "question_score": 18,
      "answer_score": 23,
      "created": "2023-09-10T02:58:29",
      "question_id": 77074676,
      "answer_id": 77075595
    }
  },
  {
    "question": "How can I select the proper openai.api_version?",
    "expected_answer": "The API Version property depends on the method you are calling in the API: all methods are not supported in all API versions. Details are listed here: https://learn.microsoft.com/en-US/azure/cognitive-services/openai/reference And preview API lifecycle is described here: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation =&gt; check those pages for up-to-date references As of March 7th, 2024: Example: &quot;completions&quot; endpoint is available in the following versions (ordered by date): 2024-02-15-preview 2023-12-01-preview (retiring April 2, 2024) 2023-09-01-preview (retiring April 2, 2024) 2023-08-01-preview (retiring April 2, 2024) 2023-07-01-preview (retiring April 2, 2024) 2023-06-01-preview (still supported, due to DALL-E 2) 2023-05-15 2023-03-15-preview (retiring April 2, 2024) 2022-12-01 But &quot;chat completions&quot; endpoint is available only in the following versions (ordered by date): 2024-02-15-preview 2023-12-01-preview (retiring April 2, 2024) 2023-09-01-preview (retiring April 2, 2024) 2023-08-01-preview (retiring April 2, 2024) 2023-07-01-preview (retiring April 2, 2024) 2023-06-01-preview (still supported, due to DALL-E 2) 2023-05-15 2023-03-15-preview (retiring April 2, 2024) Because basically it was not offered in the initial API. Generally, use the latest &quot;not preview&quot; version for production when possible, as the preview versions might be retired on a more frequent basis.",
    "context_chunks": [
      {
        "text": "I read on https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions: openai.api_version = &quot;2023-05-15&quot; and on https://learn.microsoft.com/en-us/answers/questions/1193969/how-to-integrate-tiktoken-library-with-azure-opena: openai.api_version = &quot;2023-03-15-preview&quot; This makes me wonder: How can I select the proper openai.api_version? Does that depend on my Azure OpenAI instance or deployed models or which features I use in my Python code? Or something else? I couldn't find the info in my deployed models:",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The API Version property depends on the method you are calling in the API: all methods are not supported in all API versions. Details are listed here: https://learn.microsoft.com/en-US/azure/cognitive-services/openai/reference And preview API lifecycle is described here: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation =&gt; check those pages for up-to-date references As of March 7th, 2024: Example: &quot;completions&quot; endpoint is available in the following versions (ordered by date): 2024-02-15-preview 2023-12-01-preview (retiring April 2, 2024) 2023-09-01-preview (retiring April 2, 2024) 2023-08-01-preview (retiring April 2, 2024) 2023-07-01-preview (retiring April 2, 2024) 2023-06-01-preview (still supported, due to DALL-E 2) 2023-05-15 2023-03-15-preview (retiring April 2, 2024) 2022-12-01 But &quot;chat completions&quot; endpoint is available only in the following versions (ordered by date): 2024-02-15-preview 2023-12-01-preview (retiring April 2, 2024) 2023-09-01-preview (retiring April 2, 2024) 2023-08-01-preview (retiring April 2, 2024) 2023-07-01-preview (retiring April 2, 2024) 2023-06-01-preview (still supported, due to DALL-E 2) 2023-05-15 2023-03-15-preview (retiring April 2, 2024) Because basically it was not offered in the initial API. Generally, use the latest &quot;not preview&quot; version for production when possible, as the preview versions might be retired on a more frequent basis.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As a complement to Nicolas R's answer: the API version property also depends on the model you are calling in the API. For example: Support for the o1 series models was added in API version 2024-09-01-preview. Note that, confusingly, if one tries to use an API version that doesn't exist (e.g., 2024-09-01-previewwww), one gets this error message: openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "azure",
        "azure-openai"
      ],
      "question_score": 19,
      "answer_score": 19,
      "created": "2023-06-14T15:52:48",
      "question_id": 76475419,
      "answer_id": 76476951
    }
  },
  {
    "question": "ModuleNotFoundError: No module named &#39;jupyter_server.contents&#39;",
    "expected_answer": "Edit: https://github.com/jupyter/notebook/issues/7048#issuecomment-1724637960 https://github.com/jupyter/notebook/issues/7048#issuecomment-1720815902 pip install notebook==6.5.6 Or as @West commented, use: pip install --upgrade --no-cache-dir notebook==6.* Old Answer : The Workaround: Uninstall the Recent Problematic Release (v5.10.0) and Install the Prior Version (v5.9.0). Command Line: pip uninstall traitlets pip install traitlets==5.9.0 Git links: https://github.com/microsoft/azuredatastudio/issues/24436#issuecomment-1723328100 https://github.com/jupyter/notebook/issues/7048",
    "context_chunks": [
      {
        "text": "I got this error: Traceback (most recent call last): File &quot;C:\\ProgramData\\anaconda3\\Lib\\site-packages\\notebook\\traittypes.py&quot;, line 235, in _resolve_classes klass = self._resolve_string(klass) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py&quot;, line 2025, in _resolve_string return import_item(string) ^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\utils\\importstring.py&quot;, line 31, in import_item module = __import__(package, fromlist=[obj]) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ModuleNotFoundError: No module named 'jupyter_server.contents' During handling of the above exception, another exception occurred: Traceback (most recent call last): File &quot;C:\\ProgramData\\anaconda3\\Scripts\\jupyter-notebook-script.py&quot;, line 10, in sys.exit(main()) ^^^^^^ File &quot;C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\jupyter_core\\application.py&quot;, line 280, in launch_instance super().launch_instance(argv=argv, **kwargs) File &quot;C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py&quot;, line 1051, in launch_instance app = cls.instance(**kwargs) ^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\configurable.py&quot;, line 575, in instance inst = cls(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py&quot;, line 1311, in __new__ inst.setup_instance(*args, **kwargs) File &quot;C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py&quot;, line 1354, in setup_instance super(HasTraits, self).setup_instance(*args, **kwargs) File &quot;C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py&quot;, line 1330, in setup_instance init(self) File &quot;C:\\ProgramData\\anaconda3\\Lib\\site-packages\\notebook\\traittypes.py&quot;, line 226, in instance_init self._resolve_classes() File &quot;C:\\ProgramData\\anaconda3\\Lib\\site-packages\\notebook\\traittypes.py&quot;, line 238, in _resolve_classes warn(f&quot;{klass} is not importable. Is it installed?&quot;, ImportWarning) TypeError: warn() missing 1 required keyword-only argument: 'stacklevel' Thank you for help ! :) I have tried installing and uninstalling and it didnt work.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Edit: https://github.com/jupyter/notebook/issues/7048#issuecomment-1724637960 https://github.com/jupyter/notebook/issues/7048#issuecomment-1720815902 pip install notebook==6.5.6 Or as @West commented, use: pip install --upgrade --no-cache-dir notebook==6.* Old Answer : The Workaround: Uninstall the Recent Problematic Release (v5.10.0) and Install the Prior Version (v5.9.0). Command Line: pip uninstall traitlets pip install traitlets==5.9.0 Git links: https://github.com/microsoft/azuredatastudio/issues/24436#issuecomment-1723328100 https://github.com/jupyter/notebook/issues/7048",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It turns out the problem is not with traitlets, but with notebook. Update that to a version that is newer than 6.5.6 will fix this problem: https://github.com/jupyter/notebook/issues/7048#issuecomment-1724851639",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "jupyter-notebook",
        "pip",
        "jupyter"
      ],
      "question_score": 18,
      "answer_score": 32,
      "created": "2023-11-25T20:28:44",
      "question_id": 77549493,
      "answer_id": 77549554
    }
  },
  {
    "question": "How to find Base-line of Curved Text?",
    "expected_answer": "I found an approach which is a possibility to find your lines in „pure“ opencv. The suggested solution is not perfect, but demonstrates a first direction. Maybe you should use pytesseract to follow up your overall goal ? In general the suggested solution below is quite sensitive to the parameters of the first filter A. The basics pseudo code steps are: A) apply filters to merge letters to words B) select contours of words (filter by: ratio heights vs widths , area size) C) get random points from word-contours using gaussian distribution and the center point centroid of contour D) use linear regression to find middle line of word-contours E) merge all word-contours which are neighbors to line-contours (outer middle line points are close together) F) do polynomial regression 2nd order to estimate middle line of line-contours G) write the found merged lines from our estimaded group line The main output for example 2 shows robust output but still has some artifacts from step 1 merge all letter to words. import cv2 import math import uuid import numpy as np from scipy import stats def resizeImageByPercentage(img,scalePercent = 60): width = int(img.shape[1] * scalePercent / 100) height = int(img.shape[0] * scalePercent / 100) dim = (width, height) # resize image return cv2.resize(img, dim, interpolation = cv2.INTER_AREA) def calcMedianContourWithAndHeigh(contourList): hs = list() ws = list() for cnt in contourList: (x, y, w, h) = cv2.boundingRect(cnt) ws.append(w) hs.append(h) return np.median(ws),np.median(hs) def calcCentroid(contour): houghMoments = cv2.moments(contour) # calculate x,y coordinate of centroid if houghMoments[&quot;m00&quot;] != 0: #case no contour could be calculated cX = int(houghMoments[&quot;m10&quot;] / houghMoments[&quot;m00&quot;]) cY = int(houghMoments[&quot;m01&quot;] / houghMoments[&quot;m00&quot;]) else: # set values as what you need in the situation cX, cY = -1, -1 return cX,cY def applyDilateImgFilter(img,kernelSize= 3,iterations=1): img_bin = 255 - img #invert kernel = np.ones((kernelSize,kernelSize),np.uint8) img_dilated = cv2.dilate(img_bin, kernel, iterations = iterations) return (255- img_dilated) #invert back def randomColor(): return tuple(np.random.randint(0, 255, 3).tolist()) def drawGaussianValuesInsideRange(start, end, center, stdDev, amountValues): values = [] if center &lt; 0: return values if start &gt; end: return values while len(values) &lt; amountValues: valueListPotencial = np.random.normal(center, stdDev, amountValues) valueListFiltered = [value for value in valueListPotencial if start &lt;= value &lt;= end] values.extend(valueListFiltered) return values[:amountValues] def drawRandomPointsInPolygon(amountPoints, cntFactObj): pointList = list() if not isinstance(cntFactObj, ContourFacts): return pointList #we calc basic parameter from random point selection horizontalStart = cntFactObj.x horizontalEnd = cntFactObj.x + cntFactObj.w verticalStart = cntFactObj.y verticalEnd = cntFactObj.y + cntFactObj.h #calc std deviation connected to length and ratio horitonalStdDeviation = 1 / cntFactObj.ratioHeightoWidth * (horizontalEnd-horizontalStart) verticalStdDeviation = 1 / cntFactObj.ratioHeightoWidth * (verticalEnd-verticalStart) while len(pointList)&lt;amountPoints: if cntFactObj.centoird[0] &lt; 0 or cntFactObj.centoird[1] &lt; 0: return pointList drawXValues = drawGaussianValuesInsideRange(horizontalStart, horizontalEnd, cntFactObj.centoird[0], horitonalStdDeviation, amountPoints) drawYValues = drawGaussianValuesInsideRange(verticalStart, verticalEnd, cntFactObj.centoird[1], verticalStdDeviation, amountPoints) #we create the points and check if they are inside the polygon for i in range(0,len(drawXValues)): #create points point = (drawXValues[i],drawYValues[i]) # check if the point is inside the polygon if cv2.pointPolygonTest(cntFactObj.contour, point, False) &gt; 0: pointList.append(point) return pointList[:amountPoints] def drawCountourOn(img,contours,color=None): imgContour = img.copy() for i in range(len(contours)): if color is None: color = randomColor() cv2.drawContours(imgContour, contours, i, color, 2) return imgContour DEBUGMODE = True fileIn = &quot;bZzzEeCU.jpg&quot;#&quot;269aSnEM.jpg&quot; img = cv2.imread(fileIn) ## A) apply filters to merge letters to words # prepare img load imgGrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #gaussian filter imgGaussianBlur = cv2.GaussianBlur(imgGrey,(3,3),1) #make binary img, black and white via filter _, imgBinThres = cv2.threshold(imgGaussianBlur, 140, 230, cv2.THRESH_BINARY) if DEBUGMODE: cv2.imwrite(&quot;img01bw.jpg&quot;,resizeImageByPercentage(imgBinThres,30)) ## 3 steps merged by helper class ContourFacts ## B) select contours of words (filter by: ratio heights vs widths , area size) ## C) get random points from wordcontours with gaussian distribution and center point centroid of contour ## D) use linear regression to find middle line of wordcontours #apply dilate filter to merge letter to words imgDilated = applyDilateImgFilter(imgBinThres,5,3) if DEBUGMODE: cv2.imwrite(&quot;img02dilated.jpg&quot;,resizeImageByPercentage(imgDilated,30)) # detect contours contourList, _ = cv2.findContours(imgDilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) if DEBUGMODE: imgContour = drawCountourOn(img,contourList) cv2.imwrite(&quot;img03contourAll.jpg&quot;,resizeImageByPercentage(imgContour,30)) #do a selection of contours by rule #A) ratio h vs w #B) area size mediaWordWidth, medianWordHigh = calcMedianContourWithAndHeigh(contourList) print(&quot;median word width: &quot;, mediaWordWidth) print(&quot;median word high: &quot;, medianWordHigh) contourSelectedByRatio=list() #we calc for every contour ratio h vs w ratioThresholdHeightToWidth = 1.1 #thresold ratio should be a least be 1 to 1 # e.g word to --&gt; 10 pixel / 13 pixel #helper class for contour atrributess class ContourFacts: def __init__(self,contour): if contour is None: return self.uid = uuid.uuid4() (self.x, self.y, self.w, self.h) = cv2.boundingRect(contour) self.minRect = cv2.minAreaRect(contour) self.angle = self.minRect[-1] _, (rectWidth, rectHeight), _ = self.minRect self.minRectArea = rectWidth * rectHeight self.ratioHeightoWidth = self.h / self.w self.contour = contour self.centoird = calcCentroid(contour) self.randomPoinsInCnt = self.DrawRandomPoints() if len(self.randomPoinsInCnt) &gt; 0: (self.bottomSlope, self.bottomIntercept) = self.EstimateCenterLineViaLinearReg() self.bottomMinX = min([x for x,y in self.randomPoinsInCnt]) self.bottomMaxX = max([x for x,y in self.randomPoinsInCnt]) def EstimateCenterLineViaLinearReg(self): if self.contour is None: return (0,0) slope = 0 intercept = 0 #model = slope (x) + intercept xValues = [x for x,y in self.randomPoinsInCnt] yValues = [y for x,y in self.randomPoinsInCnt] if len(xValues) &lt; 2: return (0,0) elif len(xValues) ==2: #we calc a line with 2 points # y = m*x + b deltaX = xValues[1]-xValues[0] if deltaX == 0: return (0,0) slope = (yValues[1]-yValues[0])/(deltaX) intercept = yValues[0] - (slope*xValues[0]) else: #normal linear regression above 2 points slope, intercept, r, p, std_err = stats.linregress(xValues, yValues) #TODO check std_err return slope, intercept def DrawRandomPoints(self,pointFactor=2): pointList = list() #calc area to amount point relation -&gt; bigger area more points amountPointsNeeded = int(self.minRectArea/pointFactor) pointList = drawRandomPointsInPolygon(amountPointsNeeded,self) return pointList def GetCenterLineLeftCorner(self): if self.contour is None or len(self.randomPoinsInCnt) == 0: return (0,0) # calc via y = m*x + b with min return (int(self.bottomMinX), int(self.bottomSlope*self.bottomMinX + self.bottomIntercept)) def GetCenterLineRightCorner(self): if self.contour is None or len(self.randomPoinsInCnt) == 0: return (0,0) # calc via via y = m*x + b with max return (int(self.bottomMaxX), int(self.bottomSlope*self.bottomMaxX + self.bottomIntercept)) def __eq__(self, other): if isinstance(other, ContourFacts): return self.uid == other.uid return False def __hash__(self): return hash(self.uid) #calc mean area size from area size vectorOfAreaSize = np.array([cv2.contourArea(cnt) for cnt in contourList]) meanAreaSize = np.mean(vectorOfAreaSize) print(&quot;mean area size: &quot;, meanAreaSize) stdDevAreaSize = np.std(vectorOfAreaSize) print(&quot;std dev area size: &quot;, stdDevAreaSize) thresoldDiffAreaSize = stdDevAreaSize/4 #we iterate all contours and select by ratio and size for cnt in contourList: #construct helper class instance contourFactObj = ContourFacts(cnt) #calc abs diff to mean area size diffArea = abs(cv2.contourArea(cnt) - meanAreaSize) if contourFactObj.ratioHeightoWidth &lt; ratioThresholdHeightToWidth and diffArea &lt; (thresoldDiffAreaSize): contourSelectedByRatio.append(contourFactObj) #debug print if DEBUGMODE: #we print words imgContourSelection = img.copy() for cnt in contourSelectedByRatio: contourColor = randomColor() imgContourSelection = drawCountourOn(imgContourSelection,[cnt.contour],contourColor) #we print centroid cv2.circle(imgContourSelection, cnt.centoird, 5, (0, 0, 255), -1) p1 = cnt.GetCenterLineLeftCorner() p2 = cnt.GetCenterLineRightCorner() if p1 != (0,0) or p2 != (0,0): cv2.circle(imgContourSelection, p1, 5, (0, 0, 255), -1) cv2.circle(imgContourSelection, p2, 5, (0, 0, 255), -1) cv2.line(imgContourSelection, p1, p2, (0, 255, 0), 2) cv2.imwrite(&quot;img04contourSelection.jpg&quot;,resizeImageByPercentage(imgContourSelection,30)) ## E) merge all wordcontours which are neighbours to linecontours (outer middle line points are close together) #define distance function, differences in height is negativ weighted def euclidianDistanceWithNegativHeightWeight(cnt1,cnt2,negativeHeightWeight=2.0): if cnt1 is None or cnt2 is None: return 1000000 if not isinstance(cnt1, ContourFacts) or not isinstance(cnt2, ContourFacts): return 1000000 p1 = cnt1.GetCenterLineRightCorner() p2 = cnt2.GetCenterLineLeftCorner() return math.sqrt((p2[0] - p1[0])**2 + (negativeHeightWeight*(p2[1] - p1[1]))**2) # helper class to group contours class ContourGroup: def __init__(self): self.uuid = uuid.uuid4() self.contourList = list() def GetLastElement(self): if len(self.contourList) == 0: return None return self.contourList[-1] def Add(self,cnt): self.contourList.append(cnt) def __eq__(self, other): if isinstance(other, ContourGroup): return self.uuid == other.uuid return False groupMap = dict() lineGroupList = list() ## we grouping the contours to lines maxDistanceThresholNextWord= medianWordHigh *0.9 #TODO get better estimate #recursive function to get nearest neighbors def getNearestNeighbors(cnt1,depthCounter,contourSelectedByRatio,maxDistanceThresholNextWord): maxDepth = 10 #var for max recursion depth nearestCnt = None nearestDist = maxDistanceThresholNextWord for j in range(0,len(contourSelectedByRatio)): cnt2 = contourSelectedByRatio[j] if cnt1 == cnt2:#skip same continue dist = euclidianDistanceWithNegativHeightWeight(cnt1,cnt2) if dist &lt; nearestDist: nearestDist = dist nearestCnt = cnt2 if nearestCnt is not None:#call recursive nearaestListWeHave = [nearestCnt] #new list depthCounter += 1 if depthCounter &lt; maxDepth:# all to call nearListWeGet =getNearestNeighbors(nearestCnt,depthCounter,contourSelectedByRatio,maxDistanceThresholNextWord) if nearListWeGet is None: return nearaestListWeHave else: nearListWeGet.extend(nearaestListWeHave) return nearListWeGet else:#limit reached of recursion skip return nearaestListWeHave else: return None ## E) merge all wordcontours which are neighbours to linecontours (outer middle line points are close together) #we group all contours for i in range(0,len(contourSelectedByRatio)): cnt1 = contourSelectedByRatio[i] if cnt1 in groupMap: continue lineGroup = ContourGroup() lineGroup.Add(cnt1) groupMap[cnt1] = lineGroup depthCounter = 0 nearaestList = getNearestNeighbors(cnt1,depthCounter, contourSelectedByRatio,maxDistanceThresholNextWord) if nearaestList is None: lineGroupList.append(lineGroup) #no neighbor found continue for cnt in nearaestList: groupMap[cnt] = lineGroup lineGroup.Add(cnt) lineGroupList.append(lineGroup) if DEBUGMODE: imgContourGroup = img.copy() for group in lineGroupList: #print(f&quot;group({group.uuid} size: {len(group.contourList)}&quot;) #we print all corner points for cnt in group.contourList: leftCorner = cnt.GetCenterLineLeftCorner() rigthCorner = cnt.GetCenterLineRightCorner() cv2.circle(imgContourGroup, leftCorner, 5, (0, 0, 255), -1) cv2.circle(imgContourGroup, rigthCorner, 5, (140, 0, 0), -1) #we print estimated underlines for cnt in group.contourList: leftCorner = cnt.GetCenterLineLeftCorner() rigthCorner = cnt.GetCenterLineRightCorner() cv2.line(imgContourGroup, leftCorner, rigthCorner, (0, 255, 0), 2) # we print all contours groupColor = randomColor() cntList = [cnt.contour for cnt in group.contourList] imgContourGroup = drawCountourOn(imgContourGroup,cntList,groupColor) cv2.imwrite(&quot;img05contourGroup.jpg&quot;,resizeImageByPercentage(imgContourGroup,30)) ## F) do polynomial regression 2nd order to estimate middle line of linecontours # calc line from stable group points minAmountRegressionElements = 12 movingWindowSize = 3 letterCenterOffset = medianWordHigh * 0.5 lineListCollection = list() for group in lineGroupList: stablePoints = list() for cnt in group.contourList: stablePoints.extend(cnt.randomPoinsInCnt) if len(stablePoints) &gt;= minAmountRegressionElements : xValues = [x for x,y in stablePoints] yValues = [y for x,y in stablePoints] # perform polynomial regression of degree 2 coefffientValues = np.polyfit(np.array(xValues), np.array(yValues), 2) # create a polynomial function with the coefficients polynomial = np.poly1d(coefffientValues) #we filter to build something like a line xValuesNewLineFilter = list() xMin =int( min(xValues)) xMax = int(max(xValues)) for xNew in range(xMin,xMax,movingWindowSize): xValuesNewLineFilter.append(xNew) #we predict new points with all old x values yValuesNew = polynomial(xValuesNewLineFilter) yValuesNewHighCorrect =np.array(yValuesNew) + letterCenterOffset lineList = list() #we create a list of points for i in range(0,len(xValuesNewLineFilter)): pointInt = (int(xValuesNewLineFilter[i]),int(yValuesNewHighCorrect[i])) lineList.append(pointInt) lineListCollection.append(lineList) ## G) write the lines imgLines = img.copy() for lineList in lineListCollection: p1 = lineList[0] for j in range(1,len(lineList)): p2 = lineList[j] #cv2.circle(imgLines, p2Int, 5, (0, 0, 255), -1) cv2.line(imgLines, p1, p2, (0, 255, 0), 2) p1 = p2 cv2.imwrite(&quot;img06Lines.jpg&quot;,resizeImageByPercentage(imgLines,30)) if DEBUGMODE: cv2.waitKey(0) more debug output is: The picture below shows word contours with green middle lines and red outer points for neighborhood analysis.",
    "context_chunks": [
      {
        "text": "Attached is a picture with curved lines, how can you find the Baseline of the text? The goal is to get lines like I drew by hand in the following picture: I tried the following code, but letters like g p q y and similar break the line. import cv2 as cv import numpy as np src = cv.imread(&quot;boston_cooking_a.jpg&quot;, cv.IMREAD_GRAYSCALE) src = cv.adaptiveThreshold(src=src, maxValue=255, blockSize=55, C=11, thresholdType=cv.THRESH_BINARY, adaptiveMethod=cv.ADAPTIVE_THRESH_MEAN_C) src = cv.dilate(src, cv.getStructuringElement(ksize=(3, 3), shape=cv.MORPH_RECT)) src = cv.erode(src, cv.getStructuringElement(ksize=(50, 3), shape=cv.MORPH_RECT)) src = cv.Sobel(src, ddepth=0, dx=0, dy=1, ksize=5) cv.imwrite(&quot;test.jpg&quot;, src) cv.imshow(&quot;src&quot;, src) cv.waitKey(0) EDIT: Attached is another image to test your answer on, so we can make sure the answer doesn't suffer from &quot;overfitting&quot; to a single image.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I found an approach which is a possibility to find your lines in „pure“ opencv. The suggested solution is not perfect, but demonstrates a first direction. Maybe you should use pytesseract to follow up your overall goal ? In general the suggested solution below is quite sensitive to the parameters of the first filter A. The basics pseudo code steps are: A) apply filters to merge letters to words B) select contours of words (filter by: ratio heights vs widths , area size) C) get random points from word-contours using gaussian distribution and the center point centroid of contour D) use linear regression to find middle line of word-contours E) merge all word-contours which are neighbors to line-contours (outer middle line points are close together) F) do polynomial regression 2nd order to estimate middle line of line-contours G) write the found merged lines from our estimaded group line The main output for example 2 shows robust output but still has some artifacts from step 1 merge all letter to words. import cv2 import math import uuid import numpy as np from scipy import stats def resizeImageByPercentage(img,scalePercent = 60): width = int(img.shape[1] * scalePercent / 100) height = int(img.shape[0] * scalePercent / 100) dim = (width, height) # resize image return cv2.resize(img, dim, interpolation = cv2.INTER_AREA) def calcMedianContourWithAndHeigh(contourList): hs = list() ws = list() for cnt in contourList: (x, y, w, h) = cv2.boundingRect(cnt) ws.append(w) hs.append(h) return np.median(ws),np.median(hs) def calcCentroid(contour): houghMoments = cv2.moments(contour) # calculate x,y coordinate of centroid if houghMoments[&quot;m00&quot;] != 0: #case no contour could be calculated cX = int(houghMoments[&quot;m10&quot;] / houghMoments[&quot;m00&quot;]) cY = int(houghMoments[&quot;m01&quot;] / houghMoments[&quot;m00&quot;]) else: # set values as what you need in the situation cX, cY = -1, -1 return cX,cY def applyDilateImgFilter(img,kernelSize= 3,iterations=1): img_bin = 255 - img #invert kernel = np.ones((kernelSize,kernelSize),np.uint8) img_dilated = cv2.dilate(img_bin, kernel, iterations = iterations) return (255- img_dilated) #invert back def randomColor(): return tuple(np.random.randint(0, 255, 3).tolist()) def drawGaussianValuesInsideRange(start, end, center, stdDev, amountValues): values = [] if center &lt; 0: return values if start &gt; end: return values while len(values) &lt; amountValues: valueListPotencial = np.random.normal(center, stdDev, amountValues) valueListFiltered = [value for value in valueListPotencial if start &lt;= value &lt;= end] values.extend(valueListFiltered) return values[:amountValues] def drawRandomPointsInPolygon(amountPoints, cntFactObj): pointList = list() if not isinstance(cntFactObj, ContourFacts): return pointList #we calc basic parameter from random point selection horizontalStart = cntFactObj.x horizontalEnd = cntFactObj.x + cntFactObj.w verticalStart = cntFactObj.y verticalEnd = cntFactObj.y + cntFactObj.h #calc std deviation connected to length and ratio horitonalStdDeviation = 1 / cntFactObj.ratioHeightoWidth * (horizontalEnd-horizontalStart) verticalStdDeviation = 1 / cntFactObj.ratioHeightoWidth * (verticalEnd-verticalStart) while len(pointList)&lt;amountPoints: if cntFactObj.centoird[0] &lt; 0 or cntFactObj.centoird[1] &lt; 0: return pointList drawXValues = drawGaussianValuesInsideRange(horizontalStart, horizontalEnd, cntFactObj.centoird[0], horitonalStdDeviation, amountPoints) drawYValues = drawGaussianValuesInsideRange(verticalStart, verticalEnd, cntFactObj.centoird[1], verticalStdDeviation, amountPoints) #we create the points and check if they are inside the polygon for i in range(0,len(drawXValues)): #create points point = (drawXValues[i],drawYValues[i]) # check if the point is inside the polygon if cv2.pointPolygonTest(cntFactObj.contour, point, False) &gt; 0: pointList.append(point) return pointList[:amountPoints] def drawCountourOn(img,contours,color=None): imgContour = img.copy() for i in range(len(contours)): if color is None: color = randomColor() cv2.drawContours(imgContour, contours, i, color, 2) return imgContour DEBUGMODE = True fileIn = &quot;bZzzEeCU.jpg&quot;#&quot;269aSnEM.jpg&quot; img = cv2.imread(fileIn) ## A) apply filters to merge letters to words # prepare img load imgGrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #gaussian filter imgGaussianBlur = cv2.GaussianBlur(imgGrey,(3,3),1) #make binary img, black and white via filter _, imgBinThres = cv2.threshold(imgGaussianBlur, 140, 230, cv2.THRESH_BINARY) if DEBUGMODE: cv2.imwrite(&quot;img01bw.jpg&quot;,resizeImageByPercentage(imgBinThres,30)) ## 3 steps merged by helper class ContourFacts ## B) select contours of words (filter by: ratio heights vs widths , area size) ## C) get random points from wordcontours with gaussian distribution and center point centroid of contour ## D) use linear regression to find middle line of wordcontours #apply dilate filter to merge letter to words imgDilated = applyDilateImgFilter(imgBinThres,5,3) if DEBUGMODE: cv2.imwrite(&quot;img02dilated.jpg&quot;,resizeImageByPercentage(imgDilated,30)) # detect contours contourList, _ = cv2.findContours(imgDilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) if DEBUGMODE: imgContour = drawCountourOn(img,contourList) cv2.imwrite(&quot;img03contourAll.jpg&quot;,resizeImageByPercentage(imgContour,30)) #do a selection of contours by rule #A) ratio h vs w #B) area size mediaWordWidth, medianWordHigh = calcMedianContourWithAndHeigh(contourList) print(&quot;median word width: &quot;, mediaWordWidth) print(&quot;median word high: &quot;, medianWordHigh) contourSelectedByRatio=list() #we calc for every contour ratio h vs w ratioThresholdHeightToWidth = 1.1 #thresold ratio should be a least be 1 to 1 # e.g word to --&gt; 10 pixel / 13 pixel #helper class for contour atrributess class ContourFacts: def __init__(self,contour): if contour is None: return self.uid = uuid.uuid4() (self.x, self.y, self.w, self.h) = cv2.boundingRect(contour) self.minRect = cv2.minAreaRect(contour) self.angle = self.minRect[-1] _, (rectWidth, rectHeight), _ = self.minRect self.minRectArea = rectWidth * rectHeight self.ratioHeightoWidth = self.h / self.w self.contour = contour self.centoird = calcCentroid(contour) self.randomPoinsInCnt = self.DrawRandomPoints() if len(self.randomPoinsInCnt) &gt; 0: (self.bottomSlope, self.bottomIntercept) = self.EstimateCenterLineViaLinearReg() self.bottomMinX = min([x for x,y in self.randomPoinsInCnt]) self.bottomMaxX = max([x for x,y in self.randomPoinsInCnt]) def EstimateCenterLineViaLinearReg(self): if self.contour is None: return (0,0) slope = 0 intercept = 0 #model = slope (x) + intercept xValues = [x for x,y in self.randomPoinsInCnt] yValues = [y for x,y in self.randomPoinsInCnt] if len(xValues) &lt; 2: return (0,0) elif len(xValues) ==2: #we calc a line with 2 points # y = m*x + b deltaX = xValues[1]-xValues[0] if deltaX == 0: return (0,0) slope = (yValues[1]-yValues[0])/(deltaX) intercept = yValues[0] - (slope*xValues[0]) else: #normal linear regression above 2 points slope, intercept, r, p, std_err = stats.linregress(xValues, yValues) #TODO check std_err return slope, intercept def DrawRandomPoints(self,pointFactor=2): pointList = list() #calc area to amount point relation -&gt; bigger area more points amountPointsNeeded = int(self.minRectArea/pointFactor) pointList = drawRandomPointsInPolygon(amountPointsNeeded,self) return pointList def GetCenterLineLeftCorner(self): if self.contour is None or len(self.randomPoinsInCnt) == 0: return (0,0) # calc via y = m*x + b with min return (int(self.bottomMinX), int(self.bottomSlope*self.bottomMinX + self.bottomIntercept)) def GetCenterLineRightCorner(self): if self.contour is None or len(self.randomPoinsInCnt) == 0: return (0,0) # calc via via y = m*x + b with max return (int(self.bottomMaxX), int(self.bottomSlope*self.bottomMaxX + self.bottomIntercept)) def __eq__(self, other): if isinstance(other, ContourFacts): return self.uid == other.uid return False def __hash__(self): return hash(self.uid) #calc mean area size from area size vectorOfAreaSize = np.array([cv2.contourArea(cnt) for cnt in contourList]) meanAreaSize = np.mean(vectorOfAreaSize) print(&quot;mean area size: &quot;, meanAreaSize) stdDevAreaSize = np.std(vectorOfAreaSize) print(&quot;std dev area size: &quot;, stdDevAreaSize) thresoldDiffAreaSize = stdDevAreaSize/4 #we iterate all contours and select by ratio and size for cnt in contourList: #construct helper class instance contourFactObj = ContourFacts(cnt) #calc abs diff to mean area size diffArea = abs(cv2.contourArea(cnt) - meanAreaSize) if contourFactObj.ratioHeightoWidth &lt; ratioThresholdHeightToWidth and diffArea &lt; (thresoldDiffAreaSize): contourSelectedByRatio.append(contourFactObj) #debug print if DEBUGMODE: #we print words imgContourSelection = img.copy() for cnt in contourSelectedByRatio: contourColor = randomColor() imgContourSelection = drawCountourOn(imgContourSelection,[cnt.contour],contourColor) #we print centroid cv2.circle(imgContourSelection, cnt.centoird, 5, (0, 0, 255), -1) p1 = cnt.GetCenterLineLeftCorner() p2 = cnt.GetCenterLineRightCorner() if p1 != (0,0) or p2 != (0,0): cv2.circle(imgContourSelection, p1, 5, (0, 0, 255), -1) cv2.circle(imgContourSelection, p2, 5, (0, 0, 255), -1) cv2.line(imgContourSelection, p1, p2, (0, 255, 0), 2) cv2.imwrite(&quot;img04contourSelection.jpg&quot;,resizeImageByPercentage(imgContourSelection,30)) ## E) merge all wordcontours which are neighbours to linecontours (outer middle line points are close together) #define distance function, differences in height is negativ weighted def euclidianDistanceWithNegativHeightWeight(cnt1,cnt2,negativeHeightWeight=2.0): if cnt1 is None or cnt2 is None: return 1000000 if not isinstance(cnt1, ContourFacts) or not isinstance(cnt2, ContourFacts): return 1000000 p1 = cnt1.GetCenterLineRightCorner() p2 = cnt2.GetCenterLineLeftCorner() return math.sqrt((p2[0] - p1[0])**2 + (negativeHeightWeight*(p2[1] - p1[1]))**2) # helper class to group contours class ContourGroup: def __init__(self): self.uuid = uuid.uuid4() self.contourList = list() def GetLastElement(self): if len(self.contourList) == 0: return None return self.contourList[-1] def Add(self,cnt): self.contourList.append(cnt) def __eq__(self, other): if isinstance(other, ContourGroup): return self.uuid == other.uuid return False groupMap = dict() lineGroupList = list() ## we grouping the contours to lines maxDistanceThresholNextWord= medianWordHigh *0.9 #TODO get better estimate #recursive function to get nearest neighbors def getNearestNeighbors(cnt1,depthCounter,contourSelectedByRatio,maxDistanceThresholNextWord): maxDepth = 10 #var for max recursion depth nearestCnt = None nearestDist = maxDistanceThresholNextWord for j in range(0,len(contourSelectedByRatio)): cnt2 = contourSelectedByRatio[j] if cnt1 == cnt2:#skip same continue dist = euclidianDistanceWithNegativHeightWeight(cnt1,cnt2) if dist &lt; nearestDist: nearestDist = dist nearestCnt = cnt2 if nearestCnt is not None:#call recursive nearaestListWeHave = [nearestCnt] #new list depthCounter += 1 if depthCounter &lt; maxDepth:# all to call nearListWeGet =getNearestNeighbors(nearestCnt,depthCounter,contourSelectedByRatio,maxDistanceThresholNextWord) if nearListWeGet is None: return nearaestListWeHave else: nearListWeGet.extend(nearaestListWeHave) return nearListWeGet else:#limit reached of recursion skip return nearaestListWeHave else: return None ## E) merge all wordcontours which are neighbours to linecontours (outer middle line points are close together) #we group all contours for i in range(0,len(contourSelectedByRatio)): cnt1 = contourSelectedByRatio[i] if cnt1 in groupMap: continue lineGroup = ContourGroup() lineGroup.Add(cnt1) groupMap[cnt1] = lineGroup depthCounter = 0 nearaestList = getNearestNeighbors(cnt1,depthCounter, contourSelectedByRatio,maxDistanceThresholNextWord) if nearaestList is None: lineGroupList.append(lineGroup) #no neighbor found continue for cnt in nearaestList: groupMap[cnt] = lineGroup lineGroup.Add(cnt) lineGroupList.append(lineGroup) if DEBUGMODE: imgContourGroup = img.copy() for group in lineGroupList: #print(f&quot;group({group.uuid} size: {len(group.contourList)}&quot;) #we print all corner points for cnt in group.contourList: leftCorner = cnt.GetCenterLineLeftCorner() rigthCorner = cnt.GetCenterLineRightCorner() cv2.circle(imgContourGroup, leftCorner, 5, (0, 0, 255), -1) cv2.circle(imgContourGroup, rigthCorner, 5, (140, 0, 0), -1) #we print estimated underlines for cnt in group.contourList: leftCorner = cnt.GetCenterLineLeftCorner() rigthCorner = cnt.GetCenterLineRightCorner() cv2.line(imgContourGroup, leftCorner, rigthCorner, (0, 255, 0), 2) # we print all contours groupColor = randomColor() cntList = [cnt.contour for cnt in group.contourList] imgContourGroup = drawCountourOn(imgContourGroup,cntList,groupColor) cv2.imwrite(&quot;img05contourGroup.jpg&quot;,resizeImageByPercentage(imgContourGroup,30)) ## F) do polynomial regression 2nd order to estimate middle line of linecontours # calc line from stable group points minAmountRegressionElements = 12 movingWindowSize = 3 letterCenterOffset = medianWordHigh * 0.5 lineListCollection = list() for group in lineGroupList: stablePoints = list() for cnt in group.contourList: stablePoints.extend(cnt.randomPoinsInCnt) if len(stablePoints) &gt;= minAmountRegressionElements : xValues = [x for x,y in stablePoints] yValues = [y for x,y in stablePoints] # perform polynomial regression of degree 2 coefffientValues = np.polyfit(np.array(xValues), np.array(yValues), 2) # create a polynomial function with the coefficients polynomial = np.poly1d(coefffientValues) #we filter to build something like a line xValuesNewLineFilter = list() xMin =int( min(xValues)) xMax = int(max(xValues)) for xNew in range(xMin,xMax,movingWindowSize): xValuesNewLineFilter.append(xNew) #we predict new points with all old x values yValuesNew = polynomial(xValuesNewLineFilter) yValuesNewHighCorrect =np.array(yValuesNew) + letterCenterOffset lineList = list() #we create a list of points for i in range(0,len(xValuesNewLineFilter)): pointInt = (int(xValuesNewLineFilter[i]),int(yValuesNewHighCorrect[i])) lineList.append(pointInt) lineListCollection.append(lineList) ## G) write the lines imgLines = img.copy() for lineList in lineListCollection: p1 = lineList[0] for j in range(1,len(lineList)): p2 = lineList[j] #cv2.circle(imgLines, p2Int, 5, (0, 0, 255), -1) cv2.line(imgLines, p1, p2, (0, 255, 0), 2) p1 = p2 cv2.imwrite(&quot;img06Lines.jpg&quot;,resizeImageByPercentage(imgLines,30)) if DEBUGMODE: cv2.waitKey(0) more debug output is: The picture below shows word contours with green middle lines and red outer points for neighborhood analysis.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I can give you another approach, it is shorter but @t2solve's code may give you better result. Here is the method: Otsu Threshold the image, close with a custom kernel that connects horizontal blobs Find contours of each word in text and add their star&amp;end points to a list Extract all the lines in the text by grouping words that are on the same line Fit a polygon to the points in each line This is thresholded image: This is closed image with the custom kernel: This is te poly fitted image with baselines: This is the result for your other image: Here is the complete code: import cv2 import numpy import math #Funciton to create custom kernel def xAxisKernel(size): size = size if size%2 else size+1 xkernel = numpy.zeros((size,size),dtype=numpy.uint8) center = size//2 for j in range(size): xkernel[center][j] = 1 xkernel[center-1][j] = 1 xkernel[center+1][j] = 1 return xkernel #Put each word inside a line def extractLines(words): lines = [] while len(words): line = [] line.append(words[0][0]) #add a word to a line line.append(words[0][1]) #add a word to a line line_start,line_end = words[0][0],words[0][1] words.remove(words[0]) for ww in line: for word in words: start,end = word if math.dist(line_end,start) &lt; 100 and abs(line_end[1]-start[1])&lt;30 and line_end[0]&lt;start[0]: line_end = end line.append([word[0][0],word[0][1]+3]) line.append([word[1][0],word[1][1]+3]) words.remove(word) if math.dist(line_start,end) &lt; 100 and abs(line_start[1]-end[1])&lt;30 and line_start[0]&gt;end[0]: line_start = start line.append([word[0][0],word[0][1]+3]) line.append([word[1][0],word[1][1]+3]) words.remove(word) lines.append(line) return lines image = cv2.imread('curved_book.jpg') h,w,c = image.shape gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) _,thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU) closed = cv2.morphologyEx(thresh,cv2.MORPH_CLOSE,xAxisKernel(13)) #Connecting the letters of a word contours,hierarchy = cv2.findContours(closed,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) #From contours extract start and end points of each word words = [] #list to contain start and end point of each word for cnt in contours: area = cv2.contourArea(cnt) if area&gt;500 and area&lt;20000: rect = cv2.minAreaRect(cnt) box = cv2.boxPoints(rect) box = numpy.intp(box) box = [[p[0],p[1]] for p in box] box.sort() start = box[0] if box[0][1]&gt;box[1][1] else box[1] end = box[2] if box[2][1]&gt;box[3][1] else box[3] word = [start,end] words.append(word) lines = extractLines(words) # list that contains start-end points of every word in each line lines = [numpy.array(line,numpy.int32) for line in lines] #Draw baselines for line in lines: #side parabola coeffs coeffs = numpy.polyfit(line[:,0], line[:,1], 2) poly = numpy.poly1d(coeffs) line_start_x = min(line[:,0]) line_end_x = max(line[:,0]) xarr = numpy.arange(line_start_x, line_end_x) yarr = poly(xarr) parab_pts = numpy.array([xarr, yarr],dtype=numpy.int32).T cv2.polylines(image, [parab_pts], False, (255,0,0), 8) for p in line: cv2.circle(image,p,10,(0,0,255),-1) cv2.imshow('thresh',cv2.resize(thresh,(w*720//h,720))) cv2.imshow('closed',cv2.resize(closed,(w*720//h,720))) cv2.imshow('image',cv2.resize(image,(w*720//h,720))) cv2.waitKey() When grouping the words to extract the lines the idea is: Choose a word remove it from the main list and add it to a fresh line list Set chosen word's start point as line_start, its end point as line_end Check remaining words. Compare their start points with line_end. If they are close and on the same y-level add that word to line Also remove that word from words list Change the line_end as the added word's end point Do the above process for the line_start On the remaining words repeat the full process to find another line",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "opencv",
        "image-processing",
        "ocr"
      ],
      "question_score": 19,
      "answer_score": 10,
      "created": "2024-06-04T10:46:09",
      "question_id": 78574898,
      "answer_id": 78601209
    }
  },
  {
    "question": "pandas or Polars: find index of previous element larger than current one",
    "expected_answer": "It's hard to vectorize these kind of problems, but you can use numba module to speed-up the task. Also this problem can be parallelized very easily: from numba import njit, prange @njit(parallel=True) def get_values(values): out = np.zeros_like(values, dtype=np.float64) for i in prange(len(values)): idx = np.int64(i) v = values[idx] while idx &gt; -1 and values[idx] &lt;= v: idx -= 1 if idx &gt; -1: out[i] = i - idx out[0] = np.nan return out data = { &quot;value&quot;: [1, 9, 6, 7, 3, 2, 4, 5, 1, 9], &quot;out&quot;: [None, 0, 1, 2, 1, 1, 3, 4, 1, 0], } df = pd.DataFrame(data) df[&quot;out2&quot;] = get_values(df[&quot;value&quot;].values) print(df) Prints: value out out2 0 1 NaN NaN 1 9 0.0 0.0 2 6 1.0 1.0 3 7 2.0 2.0 4 3 1.0 1.0 5 2 1.0 1.0 6 4 3.0 3.0 7 5 4.0 4.0 8 1 1.0 1.0 9 9 0.0 0.0 Benchmark (with 1_000_000 items from 1-100): from timeit import timeit data = { &quot;value&quot;: np.random.randint(1, 100, size=1_000_000), } df = pd.DataFrame(data) t = timeit('df[&quot;out&quot;] = get_values(df[&quot;value&quot;].values)', globals=globals(), number=1) print(t) Prints on my machine (AMD 5700x): 0.3559090679627843",
    "context_chunks": [
      {
        "text": "Suppose my data looks like this: data = { 'value': [1,9,6,7,3, 2,4,5,1,9] } For each row, I would like to find the row number of the latest previous element larger than the current one. So, my expected output is: [None, 0, 1, 2, 1, 1, 3, 4, 1, 0] the first element 1 has no previous element, so I want None in the result the next element 9 is at least as large than all its previous elements, so I want 0 in the result the next element 6, has its previous element 9 which is larger than it. The distance between them is 1. So, I want 1 in the result here. I'm aware that I can do this in a loop in Python (or in C / Rust if I write an extension). My question: is it possible to solve this using entirely dataframe operations? pandas or Polars, either is fine. But only dataframe operations. So, none of the following please: apply map_elements map_rows iter_rows Python for loops which loop over the rows and extract elements one-by-one from the dataframes",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "It's hard to vectorize these kind of problems, but you can use numba module to speed-up the task. Also this problem can be parallelized very easily: from numba import njit, prange @njit(parallel=True) def get_values(values): out = np.zeros_like(values, dtype=np.float64) for i in prange(len(values)): idx = np.int64(i) v = values[idx] while idx &gt; -1 and values[idx] &lt;= v: idx -= 1 if idx &gt; -1: out[i] = i - idx out[0] = np.nan return out data = { &quot;value&quot;: [1, 9, 6, 7, 3, 2, 4, 5, 1, 9], &quot;out&quot;: [None, 0, 1, 2, 1, 1, 3, 4, 1, 0], } df = pd.DataFrame(data) df[&quot;out2&quot;] = get_values(df[&quot;value&quot;].values) print(df) Prints: value out out2 0 1 NaN NaN 1 9 0.0 0.0 2 6 1.0 1.0 3 7 2.0 2.0 4 3 1.0 1.0 5 2 1.0 1.0 6 4 3.0 3.0 7 5 4.0 4.0 8 1 1.0 1.0 9 9 0.0 0.0 Benchmark (with 1_000_000 items from 1-100): from timeit import timeit data = { &quot;value&quot;: np.random.randint(1, 100, size=1_000_000), } df = pd.DataFrame(data) t = timeit('df[&quot;out&quot;] = get_values(df[&quot;value&quot;].values)', globals=globals(), number=1) print(t) Prints on my machine (AMD 5700x): 0.3559090679627843",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I guess you are looking for the algorithm part for implementation in Rust, so I propose you the following: import pandas as pd import time import numpy as np data = { 'value': [1, 9, 6, 7, 3, 2, 4, 5, 1, 9] } df = pd.DataFrame(data) values = df['value'].tolist() start = time.time() ### Algorithm for implementation in Rust, C ... _max = values[0] r = [None] for i in range(1, len(values)): prev = values[:i+1][:-1] last = values[:i+1][-1] dist=0 _max = max(prev) if last &gt;= _max else _max for j in range(len(prev)-1, -1, -1): if last &lt; _max: dist+=1 else: r.append(dist) break if last &lt; prev[j]: r.append(dist) break end = time.time() print(end-start) print(r) [None, 0, 1, 2, 1, 1, 3, 4, 1, 0] To implement the calculation in a dataframe after calculation in Rust, Python or whatever : df['out'] = r print(df) value out 0 1 NaN 1 9 0.0 2 6 1.0 3 7 2.0 4 3 1.0 5 2 1.0 6 4 3.0 7 5 4.0 8 1 1.0 9 9 0.0 Rust implementation (see PyO3) : (A priori the logic of the Python algorithm should be preserved) Online compiler : https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021 use std::time::Instant; fn main() { let values = vec![1, 9, 6, 7, 3, 2, 4, 5, 1, 9]; let mut r: Vec&lt;Option&lt;usize&gt;&gt; = Vec::new(); let mut _max = values[0]; r.push(None); let start = Instant::now(); for i in 1..values.len() { let last = values[i]; let mut dist = 0; // Calculate _max if last is greater than or equal to previous _max _max = if last &gt;= _max { last } else { *values[..i].iter().max().unwrap() }; for &amp;value in values[..i].iter().rev() { if last &lt; _max { dist += 1; if last &lt; value { r.push(Some(dist)); break; } } else { r.push(Some(dist)); break; } } } let duration = start.elapsed(); println!(&quot;Time elapsed is: {:?}&quot;, duration); println!(&quot;{:?}&quot;, r); } Result (tested online) : Time elapsed is: 5.02µs [None, Some(0), Some(1), Some(2), Some(1), Some(1), Some(3), Some(4), Some(1), Some(0)] Note : To parallelize tasks in Rust, you can use radius.rayon which provides parallel iterators that to parallelize many data processing tasks.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "python-polars"
      ],
      "question_score": 18,
      "answer_score": 12,
      "created": "2024-02-25T17:22:09",
      "question_id": 78056934,
      "answer_id": 78058031
    }
  },
  {
    "question": "In Python is there a way to get the code object of top level code?",
    "expected_answer": "The code that is executed at the top level of a module is not directly accessible as a code object in the same way that functions' code objects are, because the top-level code is executed immediately when the module is imported or run, and it doesn't exist as a separate entity like a function does. But when Python runs a script, it compiles it first to bytecode and stores it in a code object. The top-level code (__main__ module), have a code object, but it is not directly exposed, so you need to use inspect module to dig deeper: import inspect def get_top_level_code_object(): frame = inspect.currentframe() # Go back to the top-level frame while frame.f_back: frame = frame.f_back # The code object is stored in f_code return frame.f_code if __name__ == &quot;__main__&quot;: top_level_code_obj = get_top_level_code_object() print(top_level_code_obj.co_consts) would yield (0, None, &lt;code object get_top_level_code_object at 0x7f970ad658f0, file &quot;/tmp/test.py&quot;, line 3&gt;, '__main__')",
    "context_chunks": [
      {
        "text": "Is it possible to get the code object of top level code within a module? For example, if you have a python file like this: myvar = 1 print('hello from top level') def myfunction(): print('hello from function') and you want to access the code object for myfunction, then you can use myfunction.__code__. For example, myfunction.__code__.co_consts will contain the string 'hello from function' etc... Is there a way to get the code object for the top level code? That is, for the code: myvar = 1 print('hello from top level') I would like something like __main__.__code__.co_consts that will contain 'hello from top level', but I cannot find any way to get this. Does such a thing exist?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The code that is executed at the top level of a module is not directly accessible as a code object in the same way that functions' code objects are, because the top-level code is executed immediately when the module is imported or run, and it doesn't exist as a separate entity like a function does. But when Python runs a script, it compiles it first to bytecode and stores it in a code object. The top-level code (__main__ module), have a code object, but it is not directly exposed, so you need to use inspect module to dig deeper: import inspect def get_top_level_code_object(): frame = inspect.currentframe() # Go back to the top-level frame while frame.f_back: frame = frame.f_back # The code object is stored in f_code return frame.f_code if __name__ == &quot;__main__&quot;: top_level_code_obj = get_top_level_code_object() print(top_level_code_obj.co_consts) would yield (0, None, &lt;code object get_top_level_code_object at 0x7f970ad658f0, file &quot;/tmp/test.py&quot;, line 3&gt;, '__main__')",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Option 1: You can always create a code object from the module source code using compile. To do that within the module itself: myvar = 1 print('hello from top level') def myfunction(): print('hello from function') import inspect, sys c = compile(inspect.getsource(sys.modules[__name__]), &quot;mymodule&quot;, &quot;exec&quot;) print(c) print(c.co_consts[1].upper()) Output: hello from top level &lt;code object &lt;module&gt; at 0xcafef00d, file &quot;mymodule&quot;, line 1&gt; HELLO FROM TOP LEVEL This is probably your most sensible option, assuming the original source code still exists and is available to inspect. Option 2: To access the identical code object of the module itself, rather than recreating a new one, I've found an &quot;off-label&quot; way by intentionally causing an exception within the module and handling it: myvar = 1 print('hello from top level') def myfunction(): print('hello from function') try: errorerrorerror except NameError as e: c = e.__traceback__.tb_frame.f_code print(c) print(c.co_consts[:2]) Output: hello from top level &lt;code object &lt;module&gt; at 0xdeadbeef, file &quot;/path/to/mymodule.py&quot;, line 1&gt; (1, 'hello from top level') This is sort of a diabolical trick and I wouldn't be surprised if it breaks on other implementations of Python, or even in future versions of CPython. But it works in some additional cases where inspecting the source code will not. See also Marcin's answer, which does similar using inspect.currentframe(). Option 3: The import system caches the bytecode of an imported source file when creating the module instance, so that it doesn't need to be recompiled unless necessary. Option 3 involves loading the bytecode of an imported module from that cache. It only works for modules that can be imported, i.e. it won't work for scripts (files executed as __main__ like python myfile.py) nor in several other situations where the cached bytecode is unavailable for whatever reason (e.g. PYTHONDONTWRITEBYTECODE is enabled, Python was run with the -B option, the filesystem for laying down a cached .pyc file was read-only, etc) In your file: # mymodule.py myvar = 1 print('hello from top level') def myfunction(): print('hello from function') From &quot;outside&quot;: &gt;&gt;&gt; import mymodule hello from top level &gt;&gt;&gt; import marshal &gt;&gt;&gt; with open(mymodule.__cached__, 'rb') as f: ... f.read(16) ... c = marshal.load(f) ... b'\\xcb\\r\\r\\n\\x00\\x00\\x00\\x00\\xe3\\xa7|ej\\x00\\x00\\x00' &gt;&gt;&gt; c.co_consts (1, 'hello from top level', &lt;code object myfunction at 0xdefeca7e, file &quot;/path/to/mymodule.py&quot;, line 5&gt;, None) The statement import mymodule will compile the module and lay down a bytecode cache at /path/to/__pycache__/mymodule.cpython-312.pyc, if a valid cache wasn't already existing there, or if the existing cache was stale (i.e. the source was since modified). This bytecode filename is specific to the Python minor-version and implementation, so the filename will be different if you're not on CPython 3.12. It also may be in a different location if PYTHONPYCACHEPREFIX was set. Those 16 bytes &quot;discarded&quot; with f.read(16) are the .pyc header: 4 bytes magic number, indicating the Python minor version. 4 bytes flags, specifying the bytecode invalidation mode. Either a serialized version of the original .py file's info (8 bytes mtime + 8 bytes size) if using the TIMESTAMP invalidation mode (which is the default), otherwise a 16 bytes SipHash of the original .py file's source code. The marshaled code object follows the .pyc header. It can be easily deserialized into a code object as demonstrated.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "toplevel"
      ],
      "question_score": 19,
      "answer_score": 19,
      "created": "2023-12-15T18:20:08",
      "question_id": 77668149,
      "answer_id": 77668290
    }
  },
  {
    "question": "How to load a fine-tuned peft/lora model based on llama with Huggingface transformers?",
    "expected_answer": "To load a fine-tuned peft/lora model, take a look at the guanco example, https://stackoverflow.com/a/76372390/610569 import torch from peft import PeftModel from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer model_name = &quot;decapoda-research/llama-7b-hf&quot; adapters_name = &quot;lucas0/empath-llama-7b&quot; print(f&quot;Starting to load the model {model_name} into memory&quot;) m = AutoModelForCausalLM.from_pretrained( model_name, #load_in_4bit=True, torch_dtype=torch.bfloat16, device_map={&quot;&quot;: 0} ) m = PeftModel.from_pretrained(m, adapters_name) m = m.merge_and_unload() tok = LlamaTokenizer.from_pretrained(model_name) tok.bos_token_id = 1 stop_token_ids = [0] print(f&quot;Successfully loaded the model {model_name} into memory&quot;) You will need an A10g GPU runtime minimally to load the model properly. For more details see https://github.com/artidoro/qlora#tutorials-and-demonstrations Inference notebook: https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing Training notebook: https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing",
    "context_chunks": [
      {
        "text": "I've followed this tutorial (colab notebook) in order to finetune my model. Trying to load my locally saved model model = AutoModelForCausalLM.from_pretrained(&quot;finetuned_model&quot;) yields Killed. Trying to load model from hub: yields import torch from peft import PeftModel, PeftConfig from transformers import AutoModelForCausalLM, AutoTokenizer peft_model_id = &quot;lucas0/empath-llama-7b&quot; config = PeftConfig.from_pretrained(peft_model_id) model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto') tokenizer = AutoTokenizer.from_pretrained(cwd+&quot;/tokenizer.model&quot;) # Load the Lora model model = PeftModel.from_pretrained(model, peft_model_id) yields AttributeError: /home/ubuntu/empath/lora/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cget_col_row_stats full stacktrace Model Creation: I have finetuned a model using PEFT and LoRa: model = AutoModelForCausalLM.from_pretrained( &quot;decapoda-research/llama-7b-hf&quot;, torch_dtype=torch.float16, device_map='auto', ) I had to download and manually specify the llama tokenizer. tokenizer = LlamaTokenizer(cwd+&quot;/tokenizer.model&quot;) tokenizer.pad_token = tokenizer.eos_token to the training: from peft import LoraConfig, get_peft_model config = LoraConfig( r=8, lora_alpha=16, target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;], lora_dropout=0.05, bias=&quot;none&quot;, task_type=&quot;CAUSAL_LM&quot; ) model = get_peft_model(model, config) data = pd.read_csv(&quot;my_csv.csv&quot;) dataset = Dataset.from_pandas(data) tokenized_dataset = dataset.map(lambda samples: tokenizer(samples[&quot;text&quot;])) trainer = transformers.Trainer( model=model, train_dataset=tokenized_dataset, args=transformers.TrainingArguments( per_device_train_batch_size=4, gradient_accumulation_steps=4, warmup_steps=100, max_steps=100, learning_rate=1e-3, fp16=True, logging_steps=1, output_dir='outputs', ), data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False) ) model.config.use_cache = True # silence the warnings. Please re-enable for inference! trainer.train() and saved it locally with: trainer.save_model(cwd+&quot;/finetuned_model&quot;) print(&quot;saved trainer locally&quot;) as well as to the hub: model.push_to_hub(&quot;lucas0/empath-llama-7b&quot;, create_pr=1) How can I load my finetuned model?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "To load a fine-tuned peft/lora model, take a look at the guanco example, https://stackoverflow.com/a/76372390/610569 import torch from peft import PeftModel from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer model_name = &quot;decapoda-research/llama-7b-hf&quot; adapters_name = &quot;lucas0/empath-llama-7b&quot; print(f&quot;Starting to load the model {model_name} into memory&quot;) m = AutoModelForCausalLM.from_pretrained( model_name, #load_in_4bit=True, torch_dtype=torch.bfloat16, device_map={&quot;&quot;: 0} ) m = PeftModel.from_pretrained(m, adapters_name) m = m.merge_and_unload() tok = LlamaTokenizer.from_pretrained(model_name) tok.bos_token_id = 1 stop_token_ids = [0] print(f&quot;Successfully loaded the model {model_name} into memory&quot;) You will need an A10g GPU runtime minimally to load the model properly. For more details see https://github.com/artidoro/qlora#tutorials-and-demonstrations Inference notebook: https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing Training notebook: https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can load like this after pushing. I did using the following snippet successfully . # pip install peft transformers import torch from peft import PeftModel, PeftConfig from transformers import LlamaTokenizer, LlamaForCausalLM from accelerate import infer_auto_device_map, init_empty_weights peft_model_id = &quot;--path--&quot; config = PeftConfig.from_pretrained(peft_model_id) model1 = LlamaForCausalLM.from_pretrained( config.base_model_name_or_path, torch_dtype='auto', device_map='auto', offload_folder=&quot;offload&quot;, offload_state_dict = True ) tokenizer = LlamaTokenizer.from_pretrained(config.base_model_name_or_path) # Load the Lora model model1 = PeftModel.from_pretrained(model, peft_model_id)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "huggingface-transformers",
        "llama-index",
        "peft"
      ],
      "question_score": 19,
      "answer_score": 16,
      "created": "2023-06-12T17:34:46",
      "question_id": 76459034,
      "answer_id": 76469875
    }
  },
  {
    "question": "How to stop numpy floats being displayed as &quot;np.float64&quot;?",
    "expected_answer": "numpy allows you to control this without downgrading: np.set_printoptions(legacy='1.25') See numpy.set_printoptions for details. This is also stated here: Representation of NumPy scalars changed - NumPy 2.0.0 Release Notes",
    "context_chunks": [
      {
        "text": "I have a large library with many doctests. All doctests pass on my computer. When I push changes to GitHub, GitHub Actions runs the same tests in Python 3.8, 3.9, 3.10 and 3.11. All tests run correctly on on Python 3.8; however, on Python 3.9, 3.10 and 3.11, I get many errors of the following type: Expected: [13.0, 12.0, 7.0] Got: [np.float64(13.0), np.float64(12.0), np.float64(7.0)] I.e., the results are correct, but for some reason, they are displayed inside &quot;np.float64&quot;. In my code, I do not use np.float64 at all, so I do not know why this happens. Also, as the tests pass on my computer, I do not know how to debug the error, and it is hard to produce a minimal working example. Is there a way I can make the doctests pass again, without changing each individual test?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "numpy allows you to control this without downgrading: np.set_printoptions(legacy='1.25') See numpy.set_printoptions for details. This is also stated here: Representation of NumPy scalars changed - NumPy 2.0.0 Release Notes",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This is due to a change in how scalars are printed in numpy 2: numpy 1.x.x: &gt;&gt;&gt; repr(np.array([1.0])[0]) '1.0' numpy 2.x.x: &gt;&gt;&gt; repr(np.array([1.0])[0]) 'np.float64(1.0)' You should restrict the version of numpy to be 1.x.x in your requirements file to make sure you don't end up installing numpy 2.x.x: numpy ~&gt; 1.26 (same as numpy &gt;= 1.26, == 1.*, see this answer) or update your code to work with numpy 2 and change it to numpy ~&gt; 2.0.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "doctest",
        "numpy-2.x"
      ],
      "question_score": 17,
      "answer_score": 24,
      "created": "2024-06-16T19:19:38",
      "question_id": 78630047,
      "answer_id": 78793583
    }
  },
  {
    "question": "How to solve HTTP Error 400: Bad Request in PyTube?",
    "expected_answer": "I tried this and it worked fine for me pip install pytubefix from pytubefix import YouTube from pytubefix.cli import on_progress url = &quot;url&quot; yt = YouTube(url, on_progress_callback = on_progress) print(yt.title) ys = yt.streams.get_highest_resolution() ys.download() that writes the same command as the pytube library #This is a link to the library. https://pypi.org/project/pytubefix/",
    "context_chunks": [
      {
        "text": "I made a simple 3 line project with pyTube library. All it does is download a video from YT. With it I usually download videos of handball games which are around 100 minutes long. It all worked fine 7 days ago when i last used it but now it throws an &quot;HTTP Error 400: Bad Request&quot; error. from pytube import YouTube youtubeObject = YouTube('https://www.youtube.com/watch?v=DASMWPUFFP4') youtubeObject = youtubeObject.streams.get_highest_resolution() youtubeObject.download('D:\\\\Utakmice') It works with a shorter videos but it doesnt work with any other videos of the similar length (~100mins). I tried upgrading pyTube library and cleaning browser cache but it didnt help. Tried to dig dipper into the urllib but couldnt find anything there either. The error I am getting is: urllib.error.HTTPError: HTTP Error 400: Bad Request Couldnt find any solution online so any help is appreciated. Thanks in advance. EDIT-FIXED PROBLEM I found an issue about this on the pytube GitHub. The fix requiered the change of certain client versions in innertube.py file. The link to the issue: https://github.com/pytube/pytube/issues/1894#issue-2180600881",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I tried this and it worked fine for me pip install pytubefix from pytubefix import YouTube from pytubefix.cli import on_progress url = &quot;url&quot; yt = YouTube(url, on_progress_callback = on_progress) print(yt.title) ys = yt.streams.get_highest_resolution() ys.download() that writes the same command as the pytube library #This is a link to the library. https://pypi.org/project/pytubefix/",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This is a recent issue that has been raised in the Pytube project: issue link In the meantime, you can use a new package as suggested by Tomtunn. It serves a similar purpose named pytubefix I hope this solves your issue",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "http-error",
        "pytube"
      ],
      "question_score": 17,
      "answer_score": 18,
      "created": "2024-03-14T11:07:02",
      "question_id": 78160027,
      "answer_id": 78898726
    }
  },
  {
    "question": "Sort imports alphabetically with ruff",
    "expected_answer": "You can sort the imports &amp; format the code by running the following commands (source): ruff check --select I --fix ruff format Note that sorting is done at linting stage, which can be a little surprising. There are discussions to address it. Thus, if you only need to sort the imports, you can skip ruff format command. Note that the above command is equivalent to running isort with profile = &quot;black&quot;. Depending on your expectations you might need to fiddle with ruff configuration options. You can find more information in the docs: How does Ruff's import sorting compare to isort?",
    "context_chunks": [
      {
        "text": "Trying ruff for the first time and I'm not being able to sort imports alphabetically, using default settings. According to docs ruff should be very similar to isort. Here is a short example with unsorted imports import os import collections Run ruff command $ ruff format file.py 1 file left unchanged But if I run isort the imports are properly sorted $ isort file.py Fixing .../file.py What am I doing wrong?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can sort the imports &amp; format the code by running the following commands (source): ruff check --select I --fix ruff format Note that sorting is done at linting stage, which can be a little surprising. There are discussions to address it. Thus, if you only need to sort the imports, you can skip ruff format command. Note that the above command is equivalent to running isort with profile = &quot;black&quot;. Depending on your expectations you might need to fiddle with ruff configuration options. You can find more information in the docs: How does Ruff's import sorting compare to isort?",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "According to https://github.com/astral-sh/ruff/issues/8926#issuecomment-1834048218: In Ruff, import sorting and re-categorization is part of the linter, not the formatter. The formatter will re-format imports, but it won't rearrange or regroup them, because the formatter maintains the invariant that it doesn't modify the program's AST (i.e., its semantics and behavior). To get isort-like behavior, you'd want to run ruff check --fix with --select I or adding extend-select = [&quot;I&quot;] to your pyproject.toml or ruff.toml.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "isort",
        "ruff"
      ],
      "question_score": 17,
      "answer_score": 25,
      "created": "2024-01-24T21:12:26",
      "question_id": 77876253,
      "answer_id": 78467524
    }
  },
  {
    "question": "ImportError: cannot import name &#39;VectorStoreIndex&#39; from &#39;llama_index&#39; (unknown location)",
    "expected_answer": "The llama index library was recently updated so I was able to solve the issue by using updating the import according to the documentation from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate from llama_index.llms.huggingface import HuggingFaceLLM https://docs.llamaindex.ai/en/stable/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html",
    "context_chunks": [
      {
        "text": "I ran into this problem when I was trying to import the following libraries and it is giving the error &quot;ImportError: cannot import name 'VectorStoreIndex' from 'llama_index' (unknown location)&quot; I ran this exact same code in the morning and it worked perfectly. I did !pip install llama_index from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext from llama_index.llms import HuggingFaceLLM from llama_index.prompts.prompts import SimpleInputPrompt I tried commenting out first line and faced same issue for HuggingFaceLLM module Same issue for SimpleInputPrompt, got error &quot;ModuleNotFoundError: No module named 'llama_index.prompts'&quot; First I faced the problem in a sagemaker notebook so I thought the issue was with the sagemaker notebook so I spun up a clean new notebook and I got the same error. So, I tried the code in my local Jypiter notebook, google collab notebook, sagemaker studiolab notebook and I got the same error.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The llama index library was recently updated so I was able to solve the issue by using updating the import according to the documentation from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate from llama_index.llms.huggingface import HuggingFaceLLM https://docs.llamaindex.ai/en/stable/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Llamaindex constantly changes the modules directories. The module you are searching for is: from llama_index.core import VectorStoreIndex And for an especific vectorstore using chromadb as example, you need to install: pip install llama-index-vector-stores-chroma and would be imported as follows from llama_index.vector_stores.chroma import ChromaVectorStore Source: https://docs.llamaindex.ai/en/stable/examples/vector_stores/chroma_metadata_filter.html",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "jupyter-notebook",
        "amazon-sagemaker",
        "huggingface",
        "llama-index"
      ],
      "question_score": 17,
      "answer_score": 29,
      "created": "2024-02-12T22:56:20",
      "question_id": 77984729,
      "answer_id": 77985512
    }
  },
  {
    "question": "ImportError: cannot import name &#39;url_decode&#39; from &#39;werkzeug.urls&#39;",
    "expected_answer": "I can only assume you got the Werkzeug 3.0 update (as flask-login didn't up-bound their werkzeug dependency). In their ongoing quest to remove all the non-core public APIs of werkzeug, the developers deprecated most of werkzeug.urls in Werkzeug 2.3 (released April 25th 2023), and removed it in Werkzeug 3.0 (released September 30th 2023). Your options are: force werkzeug to a pre-3.0 version wait for flask-login to release a version compatible with werkzeug 3, a fix of that and a bunch of other stuff was merged a few minutes ago edit: flask-login 0.6.3 with the compatibility fix was released October 30th: https://github.com/maxcountryman/flask-login/releases/tag/0.6.3",
    "context_chunks": [
      {
        "text": "I am building a webapp using Flask. I imported the flask-login library to handle user login. But it shows an ImportError. Below is my folder structure: &gt;flask_blog1 &gt;flaskblog &gt;static &gt;templates &gt;__init__.py &gt;forms.py &gt;models.py &gt;routes.py &gt;instance &gt;site.db &gt;venv &gt;requirements.txt &gt;run.py My run.py: from flaskblog import app if __name__ == &quot;__main__&quot;: app.run(debug=True) My __init__.py: from flask import Flask from flask_sqlalchemy import SQLAlchemy from flask_bcrypt import Bcrypt from flask_login import LoginManager app = Flask(__name__) app.config[&quot;SECRET_KEY&quot;] = &quot;5791628bb0b13ce0c676dfde280ba245&quot; app.config[&quot;SQLALCHEMY_DATABASE_URI&quot;] = &quot;sqlite:///site.db&quot; db = SQLAlchemy(app) bcrypt = Bcrypt(app) login_manager = LoginManager(app) from flaskblog import routes My models.py: from datetime import datetime # from .extensions import db from flaskblog import db, login_manager from flask_login import UserMixin @login_manager.user_loader def load_user(user_id): return User.query.get(int(user_id)) class User(db.Model, UserMixin): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(20), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) image_file = db.Column(db.String(20), nullable=False, default=&quot;default.jpg&quot;) password = db.Column(db.String(60), nullable=False) posts = db.relationship(&quot;Post&quot;, backref=&quot;author&quot;, lazy=True) def __repr__(self): return f&quot;User('{self.username}', '{self.email}', '{self.image_file}')&quot; class Post(db.Model): id = db.Column(db.Integer, primary_key=True) title = db.Column(db.String(100), nullable=False) date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow) content = db.Column(db.Text, nullable=False) user_id = db.Column(db.Integer, db.ForeignKey(&quot;user.id&quot;), nullable=False) def __repr__(self): return f&quot;Post('{self.title}', '{self.date_posted}')&quot; My routes.py: from flask import render_template, flash, redirect, url_for from flaskblog import app, db, bcrypt from flaskblog.forms import RegistrationForm, LoginForm from flaskblog.models import User, Post from flask_login import login_user posts = [ { &quot;author&quot;: &quot;Ashutosh Chapagain&quot;, &quot;title&quot;: &quot;Blog Post 1&quot;, &quot;content&quot;: &quot;First Post Content&quot;, &quot;date_posted&quot;: &quot;October 1, 2023&quot;, }, { &quot;author&quot;: &quot;Ash Dhakal&quot;, &quot;title&quot;: &quot;Blog Post 2&quot;, &quot;content&quot;: &quot;Second Post Content&quot;, &quot;date_posted&quot;: &quot;October 2, 2023&quot;, }, ] @app.route(&quot;/&quot;) @app.route(&quot;/home&quot;) def home(): return render_template(&quot;home.html&quot;, posts=posts) @app.route(&quot;/about&quot;) def about(): return render_template(&quot;about.html&quot;, title=&quot;About&quot;) @app.route(&quot;/register&quot;, methods=[&quot;GET&quot;, &quot;POST&quot;]) def register(): form = RegistrationForm() if form.validate_on_submit(): hashed_password = bcrypt.generate_password_hash(form.password.data).decode( &quot;utf-8&quot; ) user = User( username=form.username.data, email=form.email.data, password=hashed_password ) db.session.add(user) db.session.commit() flash(f&quot;Your account has been created! You are now able to log in!&quot;, &quot;success&quot;) return redirect(url_for(&quot;login&quot;)) return render_template(&quot;register.html&quot;, title=&quot;Register&quot;, form=form) @app.route(&quot;/login&quot;, methods=[&quot;GET&quot;, &quot;POST&quot;]) def login(): form = LoginForm() if form.validate_on_submit(): user = User.query.filter_by(email=form.email.data).first() if user and bcrypt.check_password_hash(user.password, form.password.data): login_user(user, remember=form.remember.data) return redirect(url_for(&quot;home&quot;)) else: flash(&quot;Login Unsuccessful. Please check email and password&quot;, &quot;danger&quot;) return render_template(&quot;login.html&quot;, title=&quot;Login&quot;, form=form) My forms.py: from flask_wtf import FlaskForm from wtforms import StringField, PasswordField, SubmitField, BooleanField from wtforms.validators import DataRequired, Length, Email, EqualTo, ValidationError from flaskblog.models import User class RegistrationForm(FlaskForm): username = StringField( &quot;Username&quot;, validators=[DataRequired(), Length(min=2, max=20)] ) email = StringField(&quot;Email&quot;, validators=[DataRequired(), Email()]) password = PasswordField(&quot;Password&quot;, validators=[DataRequired()]) confirm_password = PasswordField( &quot;Confirm Password&quot;, validators=[DataRequired(), EqualTo(&quot;password&quot;)] ) submit = SubmitField(&quot;Sign Up&quot;) def validate_username(self, username): user = User.query.filter_by(username=username.data).first() if user: raise ValidationError( &quot;That username is taken. Please choose a different one.&quot; ) def validate_email(self, email): user = User.query.filter_by(email=email.data).first() if user: raise ValidationError(&quot;That email is taken. Please choose a different one.&quot;) class LoginForm(FlaskForm): email = StringField(&quot;Email&quot;, validators=[DataRequired(), Email()]) password = PasswordField(&quot;Password&quot;, validators=[DataRequired()]) remember = BooleanField(&quot;Remember Me&quot;) submit = SubmitField(&quot;Login&quot;) The exact error is: (venv) asu@asu-Lenovo-Legion-5-15ARH05:/media/asu/Data/Projects/flask_blog1$ python3 run.py Traceback (most recent call last): File &quot;/media/asu/Data/Projects/flask_blog1/run.py&quot;, line 1, in &lt;module&gt; from flaskblog import app File &quot;/media/asu/Data/Projects/flask_blog1/flaskblog/__init__.py&quot;, line 4, in &lt;module&gt; from flask_login import LoginManager File &quot;/media/asu/Data/Projects/flask_blog1/venv/lib/python3.10/site-packages/flask_login/__init__.py&quot;, line 12, in &lt;module&gt; from .login_manager import LoginManager File &quot;/media/asu/Data/Projects/flask_blog1/venv/lib/python3.10/site-packages/flask_login/login_manager.py&quot;, line 33, in &lt;module&gt; from .utils import _create_identifier File &quot;/media/asu/Data/Projects/flask_blog1/venv/lib/python3.10/site-packages/flask_login/utils.py&quot;, line 14, in &lt;module&gt; from werkzeug.urls import url_decode ImportError: cannot import name 'url_decode' from 'werkzeug.urls' (/media/asu/Data/Projects/flask_blog1/venv/lib/python3.10/site-packages/werkzeug/urls.py)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I can only assume you got the Werkzeug 3.0 update (as flask-login didn't up-bound their werkzeug dependency). In their ongoing quest to remove all the non-core public APIs of werkzeug, the developers deprecated most of werkzeug.urls in Werkzeug 2.3 (released April 25th 2023), and removed it in Werkzeug 3.0 (released September 30th 2023). Your options are: force werkzeug to a pre-3.0 version wait for flask-login to release a version compatible with werkzeug 3, a fix of that and a bunch of other stuff was merged a few minutes ago edit: flask-login 0.6.3 with the compatibility fix was released October 30th: https://github.com/maxcountryman/flask-login/releases/tag/0.6.3",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The problem was as described by @Masklinn. I fixed the problem by changing the versions of werkzeug and flask in my requirements.py and reinstalling them. My original requirements.txt: bcrypt==4.0.1 blinker==1.6.2 click==8.1.7 dnspython==2.4.2 email-validator==2.0.0.post2 Flask==3.0.0 Flask-Bcrypt==1.0.1 Flask-Login==0.6.2 Flask-SQLAlchemy==3.1.1 Flask-WTF==1.2.0 greenlet==2.0.2 idna==3.4 itsdangerous==2.1.2 Jinja2==3.1.2 MarkupSafe==2.1.3 SQLAlchemy==2.0.21 typing_extensions==4.8.0 Werkzeug==3.0.0 WTForms==3.0.1 My modified requirements.txt: bcrypt==4.0.1 blinker==1.6.2 click==8.1.7 dnspython==2.4.2 email-validator==2.0.0.post2 Flask==2.3.0 Flask-Bcrypt==1.0.1 Flask-Login==0.6.2 Flask-SQLAlchemy==3.1.1 Flask-WTF==1.2.1 greenlet==2.0.2 idna==3.4 itsdangerous==2.1.2 Jinja2==3.1.2 MarkupSafe==2.1.3 SQLAlchemy==2.0.21 typing_extensions==4.8.0 Werkzeug==2.3.0 WTForms==3.0.1",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "flask",
        "importerror",
        "flask-login",
        "werkzeug"
      ],
      "question_score": 17,
      "answer_score": 29,
      "created": "2023-10-02T11:07:45",
      "question_id": 77215107,
      "answer_id": 77215455
    }
  },
  {
    "question": "Efficiently using Hugging Face transformers pipelines on GPU with large datasets",
    "expected_answer": "I think you can ignore this message. I found it being reported on different websites this year, but if I get it correctly, this Github issue on the Huggingface transformers (https://github.com/huggingface/transformers/issues/22387) shows that the warning can be safely ignored. In addition, batching or using datasets might not remove the warning or automatically use the resources in the best way. You can do call_count = 0 in here (https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1100) to ignore the warning, as explained by Martin Weyssow above. How can I modify my code to batch my data and use parallel computing to make better use of my GPU resources: You can add batching like this: py_sentimiento = pipeline(&quot;sentiment-analysis&quot;, model=&quot;finiteautomata/beto-sentiment-analysis&quot;, tokenizer=&quot;finiteautomata/beto-sentiment-analysis&quot;, batch_size=8, device=device, truncation=True) and most importantly, you can experiment with the batch size that will result to the highest GPU usage possible on your device and particular task. Huggingface provides here some rules to help users figure out how to batch: https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching. Making the best resource/GPU usage possible might take some experimentation and it depends on the use case you work on every time. What does this warning mean, and why should I use a dataset for efficiency? This means the GPU utilization is not optimal, because the data is not grouped together and it is thus not processed efficiently. Using a dataset from the Huggingface library datasets will utilize your resources more efficiently. However, it is not so easy to tell what exactly is going on, especially considering that we don’t know exactly how the data looks like, what the device is and how the model deals with the data internally. The warning might go away by using the datasets library, but that does not necessarily mean that the resources are optimally used. What code or function or library should be used with hugging face transformers? Here is a code example with pipelines and the datasets library: https://huggingface.co/docs/transformers/v4.27.1/pipeline_tutorial#using-pipelines-on-a-dataset. It mentions that using iterables will fill your GPU as fast as possible and batching might also help with computational time improvements. In your case it seems you are doing a relatively small POC (doing inference for under 10,000 documents with a medium size model), so I don’t think you need to use pipelines. I assume the sentiment analysis model is a classifier and you want to keep using Pandas as shown in the post, so here is how you can combine both. This is usually fast enough for my experiments and prints no warnings about the resources. from transformers import AutoTokenizer, AutoModelForSequenceClassification import torch as t import pandas as pd model = AutoModelForSequenceClassification.from_pretrained(&quot;finiteautomata/beto-sentiment-analysis&quot;) tokenizer = AutoTokenizer.from_pretrained(&quot;finiteautomata/beto-sentiment-analysis&quot;) def classify_dataframe_row( example: pd.Series, ): output = model(**tokenizer(example[&quot;text&quot;], return_tensors=&quot;pt&quot;)) prediction = t.argmax(output[0]).detach().numpy() return prediction dataset = pd.read_csv(&quot;file&quot;) dataset = dataset.assign( prediction=dataset.progress_apply(classify_dataframe_row, axis=1) ) As soon as your inference starts, either with this snippet or with the datasets library code, you can run nvidia-smi in a terminal and check what the GPU usage is and play around with the parameters to optimize it. Beware that running the code on your local machine with a GPU vs running it on a larger machine, e.g., a Linux server with perhaps a more powerful GPU might lead to different performance and might need different tuning. If you wish to run the code for larger document collections, you can split the data in order to avoid GPU memory errors locally, or in order to speed up the inference with concurrent runs in a server.",
    "context_chunks": [
      {
        "text": "I'm relatively new to Python and facing some performance issues while using Hugging Face Transformers for sentiment analysis on a relatively large dataset. I've created a DataFrame with 6000 rows of text data in Spanish, and I'm applying a sentiment analysis pipeline to each row of text. Here's a simplified version of my code: import pandas as pd import torch from tqdm import tqdm from transformers import pipeline data = { 'TD': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'text': [ # ... (your text data here) ] } df_model = pd.DataFrame(data) device = 0 if torch.cuda.is_available() else -1 py_sentimiento = pipeline(&quot;sentiment-analysis&quot;, model=&quot;finiteautomata/beto-sentiment-analysis&quot;, tokenizer=&quot;finiteautomata/beto-sentiment-analysis&quot;, device=device, truncation=True) tqdm.pandas() df_model['py_sentimiento'] = df_model['text'].progress_apply(py_sentimiento) df_model['py_sentimiento'] = df_model['py_sentimiento'].apply(lambda x: x[0]['label']) However, I've encountered a warning message that suggests I should use a dataset for more efficient processing. The warning message is as follows: &quot;You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset.&quot; I have a two questions: What does this warning mean, and why should I use a dataset for efficiency? How can I modify my code to batch my data and use parallel computing to make better use of my GPU resources, what code or function or library should be used with hugging face transformers? I'm eager to learn and optimize my code.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I think you can ignore this message. I found it being reported on different websites this year, but if I get it correctly, this Github issue on the Huggingface transformers (https://github.com/huggingface/transformers/issues/22387) shows that the warning can be safely ignored. In addition, batching or using datasets might not remove the warning or automatically use the resources in the best way. You can do call_count = 0 in here (https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1100) to ignore the warning, as explained by Martin Weyssow above. How can I modify my code to batch my data and use parallel computing to make better use of my GPU resources: You can add batching like this: py_sentimiento = pipeline(&quot;sentiment-analysis&quot;, model=&quot;finiteautomata/beto-sentiment-analysis&quot;, tokenizer=&quot;finiteautomata/beto-sentiment-analysis&quot;, batch_size=8, device=device, truncation=True) and most importantly, you can experiment with the batch size that will result to the highest GPU usage possible on your device and particular task. Huggingface provides here some rules to help users figure out how to batch: https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching. Making the best resource/GPU usage possible might take some experimentation and it depends on the use case you work on every time. What does this warning mean, and why should I use a dataset for efficiency? This means the GPU utilization is not optimal, because the data is not grouped together and it is thus not processed efficiently. Using a dataset from the Huggingface library datasets will utilize your resources more efficiently. However, it is not so easy to tell what exactly is going on, especially considering that we don’t know exactly how the data looks like, what the device is and how the model deals with the data internally. The warning might go away by using the datasets library, but that does not necessarily mean that the resources are optimally used. What code or function or library should be used with hugging face transformers? Here is a code example with pipelines and the datasets library: https://huggingface.co/docs/transformers/v4.27.1/pipeline_tutorial#using-pipelines-on-a-dataset. It mentions that using iterables will fill your GPU as fast as possible and batching might also help with computational time improvements. In your case it seems you are doing a relatively small POC (doing inference for under 10,000 documents with a medium size model), so I don’t think you need to use pipelines. I assume the sentiment analysis model is a classifier and you want to keep using Pandas as shown in the post, so here is how you can combine both. This is usually fast enough for my experiments and prints no warnings about the resources. from transformers import AutoTokenizer, AutoModelForSequenceClassification import torch as t import pandas as pd model = AutoModelForSequenceClassification.from_pretrained(&quot;finiteautomata/beto-sentiment-analysis&quot;) tokenizer = AutoTokenizer.from_pretrained(&quot;finiteautomata/beto-sentiment-analysis&quot;) def classify_dataframe_row( example: pd.Series, ): output = model(**tokenizer(example[&quot;text&quot;], return_tensors=&quot;pt&quot;)) prediction = t.argmax(output[0]).detach().numpy() return prediction dataset = pd.read_csv(&quot;file&quot;) dataset = dataset.assign( prediction=dataset.progress_apply(classify_dataframe_row, axis=1) ) As soon as your inference starts, either with this snippet or with the datasets library code, you can run nvidia-smi in a terminal and check what the GPU usage is and play around with the parameters to optimize it. Beware that running the code on your local machine with a GPU vs running it on a larger machine, e.g., a Linux server with perhaps a more powerful GPU might lead to different performance and might need different tuning. If you wish to run the code for larger document collections, you can split the data in order to avoid GPU memory errors locally, or in order to speed up the inference with concurrent runs in a server.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "What does this warning mean, and why should I use a dataset for efficiency? It means you will gain in efficiency by wrapping up your data in a Dataset object: from datasets import Dataset data = { 'TD': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'text': [ # ... (your text data here) ] } dataset = Dataset.from_dict(data) If you want to understand the reason why you get this warning, you can have a look at this file: https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1100. The idea is that it detects that you call the pipeline more than 10 times. It results in inefficient computations compared to using a Dataset object as input because the __call__ function of the base class of the pipeline is evaluated for each single input example. If you use an iterator, e.g., a Dataset object, then all the examples are processed at once, and you can batch the preprocessing and inference processes: from transformers.pipelines.pt_utils import KeyDataset classifier = pipeline(&quot;sentiment-analysis&quot;, model=&quot;finiteautomata/beto-sentiment-analysis&quot;, device=device, truncation=True, batch_size=4) for out in classifier(KeyDataset(dataset, &quot;text&quot;)): print(out) How can I modify my code to batch my data and use parallel computing to make better use of my GPU resources, what code or function or library should be used with hugging face transformers? In the above solution, you can tune the batch_size to fit your available GPU memory and fasten the inference. Another option is to leverage Accelerate for distributed inference: https://huggingface.co/docs/accelerate/usage_guides/distributed_inference",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "gpu",
        "huggingface-transformers",
        "huggingface-datasets"
      ],
      "question_score": 18,
      "answer_score": 18,
      "created": "2023-09-22T15:57:36",
      "question_id": 77159136,
      "answer_id": 77452808
    }
  },
  {
    "question": "Why do f-strings require parentheses around assignment expressions?",
    "expected_answer": "This behavior was explicitly specified in the original PEP for the assignment expressions (aka the walrus operator). The reason for this was to preserve backward compatibility with formatted string literals. Before assignment expressions were added, you could already write f-strings like f&quot;{x:=y}&quot;, which meant &quot;format x using the format specification =y&quot;. Quoting PEP 572 – Assignment Expressions: Assignment expressions inside of f-strings require parentheses. Example: &gt;&gt;&gt; f'{(x:=10)}' # Valid, uses assignment expression '10' &gt;&gt;&gt; x = 10 &gt;&gt;&gt; f'{x:=10}' # Valid, passes '=10' to formatter ' 10' This shows that what looks like an assignment operator in an f-string is not always an assignment operator. The f-string parser uses : to indicate formatting options. To preserve backward compatibility, assignment operator usage inside of f-strings must be parenthesized. As noted above, this usage of the assignment operator is not recommended.",
    "context_chunks": [
      {
        "text": "In Python (3.11) why does the use of an assignment expression (the &quot;walrus operator&quot;) require wrapping in parentheses when used inside an f-string? For example: #!/usr/bin/env python from pathlib import Path import torch DEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) ckpt_dir = Path(&quot;/home/path/to/checkpoints&quot;) _ckpt = next(ckpt_dir.iterdir()) print(_ckpt) sdict = torch.load(_ckpt, map_location=DEVICE) model_dict = sdict[&quot;state_dict&quot;] for k, v in model_dict.items(): print(k) print(type(v)) print(_shape := v.size()) print(f&quot;{(_numel := v.numel())}&quot;) print(_numel == torch.prod(torch.tensor(_shape))) The code block above with print(f&quot;{_numel := v.numel()}&quot;) instead does not parse. What about the parsing / AST creation mandates this?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This behavior was explicitly specified in the original PEP for the assignment expressions (aka the walrus operator). The reason for this was to preserve backward compatibility with formatted string literals. Before assignment expressions were added, you could already write f-strings like f&quot;{x:=y}&quot;, which meant &quot;format x using the format specification =y&quot;. Quoting PEP 572 – Assignment Expressions: Assignment expressions inside of f-strings require parentheses. Example: &gt;&gt;&gt; f'{(x:=10)}' # Valid, uses assignment expression '10' &gt;&gt;&gt; x = 10 &gt;&gt;&gt; f'{x:=10}' # Valid, passes '=10' to formatter ' 10' This shows that what looks like an assignment operator in an f-string is not always an assignment operator. The f-string parser uses : to indicate formatting options. To preserve backward compatibility, assignment operator usage inside of f-strings must be parenthesized. As noted above, this usage of the assignment operator is not recommended.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Both the colon : and the = have meanings inside f-strings, and the walrus := does the same as just a colon :. x = 4 print(f&quot;{x=}&quot;) # prints &quot;x = 4&quot; print(f&quot;{x:10}&quot;) # prints output in 10 spaces print(f&quot;{x:=10}&quot;) # does same as : The disassembly of {x:=10} just loads =10 and passes it to the colon format operation. 5 4 LOAD_GLOBAL 0 (print) 6 LOAD_FAST 0 (x) 8 LOAD_CONST 2 ('=10') 10 FORMAT_VALUE 4 (with format) 12 CALL_FUNCTION 1 14 POP_TOP 16 LOAD_CONST 0 (None) 18 RETURN_VALUE",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "f-string",
        "python-assignment-expression"
      ],
      "question_score": 17,
      "answer_score": 25,
      "created": "2023-08-14T18:24:18",
      "question_id": 76901337,
      "answer_id": 76901474
    }
  },
  {
    "question": "How to maximize the cache hit rate of the 2-element combinations?",
    "expected_answer": "Here is a simple approach that depends on the cache and gets 230 on your benchmark. def diagonal_block (lower, upper): for i in range(lower, upper + 1): for j in range(i, upper + 1): yield (i, j) def strip (i_lower, i_upper, j_lower, j_upper): for i in range(i_lower, i_upper+1): for j in range (j_lower, j_upper + 1): yield (i, j) # def your_solution_here(n: int, cache_size: int) -&gt; Iterable[Tuple[int, int]]: def your_solution_here(n: int, cache_size: int): i_lower = 0 i_upper = n-1 k = cache_size - 2 is_asc = True while i_lower &lt;= i_upper: # Handle a k*k block first. At the end that is likely loaded. if is_asc: upper = min(i_lower + k - 1, i_upper) yield from diagonal_block(i_lower, upper) j_lower = i_lower j_upper = upper i_lower = upper + 1 else: lower = max(i_lower, i_upper - k + 1) yield from diagonal_block(lower, i_upper) j_lower = lower j_upper = i_upper i_upper = lower - 1 yield from strip(i_lower, i_upper, j_lower, j_upper) is_asc = not is_asc A comment about how I thought this one up. We want to compare a group of objects with every other uncompared object. The group should be everything that fits in the cache except one. So we start with the first k objects, compare them with each other, then just proceed along in a strip to the end. And now we need our second group. Well, we already have the last object, and we don't need the rest. So we take k objects from the end, make that a group. Compare the group with itself, then proceed along a strip to the first object outside of our original group. Now reverse direction, and so on. At all points, i_lower represents the first object still needing comparing, and i_upper represents the last. If we're going forward, we take k objects starting at i_lower. If we're going backwards we take k objects starting at i_upper and go backwards. When I was implementing it, there were two complications. The first is that we have to worry about the edge condition when we meet in the middle. The second is that we might have to do the strip in 2 directions. I chose to only do the strip ascending. This is actually a bug. On most of the ascending loads, I did not get the first element in my cache. Oops. But it is still pretty good.",
    "context_chunks": [
      {
        "text": "My question is simple, but I find it difficult to get the point straight, so please allow me to explain step by step. Suppose I have N items and N corresponding indices. Each item can be loaded using the corresponding index. def load_item(index: int) -&gt; ItemType: # Mostly just reading, but very slow. return item Also I have a function that takes two (loaded) items and calculates a score. def calc_score(item_a: ItemType, item_b: ItemType) -&gt; ScoreType: # Much faster than load function. return score Note that calc_score(a, b) == calc_score(b, a). What I want to do is calculate the score for all 2-item combinations and find (at least) one combination that gives the maximum score. This can be implemented as follows: def dumb_solution(n: int) -&gt; Tuple[int, int]: best_score = 0 best_combination = None for index_a, index_b in itertools.combinations(range(n), 2): item_a = load_item(index_a) item_b = load_item(index_b) score = calc_score(item_a, item_b) if score &gt; best_score: best_score = score best_combination = (index_a, index_b) return best_combination However, this solution calls the load_item function 2*C(N,2) = N*(N-1) times, which is the bottleneck for this function. This can be resolved by using a cache. Unfortunately, however, the items are so large that it is impossible to keep all items in memory. Therefore, we need to use a size-limited cache. from functools import lru_cache @lru_cache(maxsize=M) def load(index: int) -&gt; ItemType: # Very slow process. return item Note that M (cache size) is much smaller than N (approx. N // 10 to N // 2). The problem is that the typical sequence of combinations is not ideal for the LRU cache. For instance, when N=6, M=3, itertools.combinations generates the following sequence, and the number of calls of the load_item function is 17. [ (0, 1), # 1, 2 (0, 2), # -, 3 (0, 3), # -, 4 (0, 4), # -, 5 (0, 5), # -, 6 (1, 2), # 7, 8 (1, 3), # -, 9 (1, 4), # -, 10 (1, 5), # -, 11 (2, 3), # 12, 13 (2, 4), # -, 14 (2, 5), # -, 15 (3, 4), # 16, 17 (3, 5), # -, - (4, 5), # -, - ] However, if I rearrange the above sequence as follows, the number of calls will be 10. [ (0, 1), # 1, 2 (0, 2), # -, 3 (1, 2), # -, - (0, 3), # -, 4 (2, 3), # -, - (0, 4), # -, 5 (3, 4), # -, - (0, 5), # -, 6 (4, 5), # -, - (1, 4), # 7, - (1, 5), # -, - (1, 3), # -, 8 (3, 5), # -, - (2, 5), # 9, - (2, 4), # -, 10 ] Question: How can I generate a sequence of 2-item combinations that maximizes the cache hit rate? What I tried: The solution I came up with is to prioritize items that are already in the cache. from collections import OrderedDict def prioritizes_item_already_in_cache(n, cache_size): items = list(itertools.combinations(range(n), 2)) cache = OrderedDict() reordered = [] def update_cache(x, y): cache[x] = cache[y] = None cache.move_to_end(x) cache.move_to_end(y) while len(cache) &gt; cache_size: cache.popitem(last=False) while items: # Find a pair where both are cached. for i, (a, b) in enumerate(items): if a in cache and b in cache: reordered.append((a, b)) update_cache(a, b) del items[i] break else: # Find a pair where one of them is cached. for i, (a, b) in enumerate(items): if a in cache or b in cache: reordered.append((a, b)) update_cache(a, b) del items[i] break else: # Cannot find item in cache. a, b = items.pop(0) reordered.append((a, b)) update_cache(a, b) return reordered For N=100, M=10, this sequence resulted in 1660 calls, which is about 1/3 of the typical sequence. For N=100, M=50 there are only 155 calls. So I think I can say that this is a promising approach. Unfortunately, this function is too slow and useless for large N. I was not able to finish for N=1000, but the actual data is in the tens of thousands. Also, it does not take into account how to select an item when no cached item is found. Therefore, even if it is fast, it is doubtful that it is theoretically the best solution (so please note my question is not how to make the above function faster). (Edited) Here is the complete code including everyone's answers and the test and benchmark code. import functools import itertools import math import time from collections import Counter, OrderedDict from itertools import chain, combinations, product from pathlib import Path from typing import Callable, Iterable, Tuple import joblib import matplotlib.pyplot as plt import numpy as np import pandas as pd from PIL import Image, ImageDraw ItemType = int ScoreType = int def load_item(index: int) -&gt; ItemType: return int(index) def calc_score(item_a: ItemType, item_b: ItemType) -&gt; ScoreType: return abs(item_a - item_b) class LRUCacheWithCounter: def __init__(self, maxsize: int): def wrapped_func(key): self.load_count += 1 return load_item(key) self.__cache = functools.lru_cache(maxsize=maxsize)(wrapped_func) self.load_count = 0 def __call__(self, key: int) -&gt; int: return self.__cache(key) def basic_loop(iterator: Iterable[Tuple[int, int]], cached_load: Callable[[int], int]): best_score = 0 best_combination = None for i, j in iterator: a = cached_load(i) b = cached_load(j) score = calc_score(a, b) if score &gt; best_score: best_score = score best_combination = (i, j) return best_score, best_combination def baseline(n, _): return itertools.combinations(range(n), 2) def prioritizes(n, cache_size): items = list(itertools.combinations(range(n), 2)) cache = OrderedDict() reordered = [] def update_cache(x, y): cache[x] = cache[y] = None cache.move_to_end(x) cache.move_to_end(y) while len(cache) &gt; cache_size: cache.popitem(last=False) while items: # Find a pair where both are cached. for i, (a, b) in enumerate(items): if a in cache and b in cache: reordered.append((a, b)) update_cache(a, b) del items[i] break else: # Find a pair where one of them is cached. for i, (a, b) in enumerate(items): if a in cache or b in cache: reordered.append((a, b)) update_cache(a, b) del items[i] break else: # Cannot find item in cache. a, b = items.pop(0) reordered.append((a, b)) update_cache(a, b) return reordered def Matt_solution(n: int, cache_size: int) -&gt; Iterable[Tuple[int, int]]: dest = [] def findPairs(lo1: int, n1: int, lo2: int, n2: int): if n1 &lt; 1 or n2 &lt; 1: return if n1 == 1: for i in range(max(lo1 + 1, lo2), lo2 + n2): dest.append((lo1, i)) elif n2 == 1: for i in range(lo1, min(lo1 + n1, lo2)): dest.append((i, lo2)) elif n1 &gt;= n2: half = n1 // 2 findPairs(lo1, half, lo2, n2) findPairs(lo1 + half, n1 - half, lo2, n2) else: half = n2 // 2 findPairs(lo1, n1, lo2, half) findPairs(lo1, n1, lo2 + half, n2 - half) findPairs(0, n, 0, n) return dest def Kelly_solution(n: int, cache_size: int) -&gt; Iterable[Tuple[int, int]]: k = cache_size // 2 r = range(n) return chain.from_iterable(combinations(r[i : i + k], 2) if i == j else product(r[i : i + k], r[j : j + k]) for i in r[::k] for j in r[i::k]) def Kelly_solution2(n: int, cache_size: int) -&gt; Iterable[Tuple[int, int]]: k = cache_size - 2 r = range(n) return chain.from_iterable(combinations(r[i : i + k], 2) if i == j else product(r[i : i + k], r[j : j + k]) for i in r[::k] for j in r[i::k]) def diagonal_block(lower, upper): for i in range(lower, upper + 1): for j in range(i + 1, upper + 1): yield i, j def strip(i_lower, i_upper, j_lower, j_upper): for i in range(i_lower, i_upper + 1): for j in range(j_lower, j_upper + 1): yield i, j def btilly_solution(n: int, cache_size: int): i_lower = 0 i_upper = n - 1 k = cache_size - 2 is_asc = True while i_lower &lt;= i_upper: # Handle a k*k block first. At the end that is likely loaded. if is_asc: upper = min(i_lower + k - 1, i_upper) yield from diagonal_block(i_lower, upper) j_lower = i_lower j_upper = upper i_lower = upper + 1 else: lower = max(i_lower, i_upper - k + 1) yield from diagonal_block(lower, i_upper) j_lower = lower j_upper = i_upper i_upper = lower - 1 yield from strip(i_lower, i_upper, j_lower, j_upper) is_asc = not is_asc def btilly_solution2(n: int, cache_size: int): k = cache_size - 2 for top in range(0, n, k): bottom = top + k # Diagonal part. for y in range(top, min(bottom, n)): # Y-axis Top to Bottom for x in range(y + 1, min(bottom, n)): # X-axis Left to Right yield y, x # Strip part. # Stripping right to left works well when cache_size is very small, but makes little difference when it is not. for x in range(n - 1, bottom - 1, -1): # X-axis Right to Left for y in range(top, min(bottom, n)): # Y-axis Top to Bottom yield y, x def btilly_solution3(n: int, cache_size: int): k = cache_size - 2 r = range(n) for i in r[::k]: yield from combinations(r[i : i + k], 2) yield from product(r[i + k :], r[i : i + k]) def btilly_solution4(n: int, cache_size: int): def parts(): k = cache_size - 2 r = range(n) for i in r[::k]: yield combinations(r[i : i + k], 2) yield product(r[i + k :], r[i : i + k]) return chain.from_iterable(parts()) def plot(df, series, ignore, y, label, title): df = df[df[&quot;name&quot;].isin(series)] # plt.figure(figsize=(10, 10)) for name, group in df.groupby(&quot;name&quot;): plt.plot(group[&quot;n&quot;], group[y], label=name) y_max = df[~df[&quot;name&quot;].isin(ignore)][y].max() plt.ylim(0, y_max * 1.1) plt.xlabel(&quot;n&quot;) plt.ylabel(label) plt.title(title) plt.legend(loc=&quot;upper left&quot;) plt.tight_layout() plt.grid() plt.show() def run(func, n, cache_ratio, output_dir: Path): cache_size = int(n * cache_ratio / 100) output_path = output_dir / f&quot;{n}_{cache_ratio}_{func.__name__}.csv&quot; if output_path.exists(): return started = time.perf_counter() for a, b in func(n, cache_size): pass elapsed_iterate = time.perf_counter() - started # test_combinations(func(n, cache_size), n) started = time.perf_counter() cache = LRUCacheWithCounter(cache_size) basic_loop(iterator=func(n, cache_size), cached_load=cache) elapsed_cache = time.perf_counter() - started output_path.write_text(f&quot;{func.__name__},{n},{cache_ratio},{cache_size},{cache.load_count},{elapsed_iterate},{elapsed_cache}&quot;) def add_lower_bound(df): def calc_lower_bound(ni, mi): n = ni m = n * mi // 100 return m + math.ceil((math.comb(n, 2) - math.comb(m, 2)) / (m - 1)) return pd.concat( [ df, pd.DataFrame( [ {&quot;name&quot;: &quot;lower_bound&quot;, &quot;n&quot;: ni, &quot;m&quot;: mi, &quot;count&quot;: calc_lower_bound(ni, mi)} for ni, mi in itertools.product(df[&quot;n&quot;].unique(), df[&quot;m&quot;].unique()) ] ), ] ) def benchmark(output_dir: Path): log_dir = output_dir / &quot;log&quot; log_dir.mkdir(parents=True, exist_ok=True) candidates = [ baseline, prioritizes, Matt_solution, Kelly_solution, Kelly_solution2, btilly_solution, btilly_solution2, btilly_solution3, btilly_solution4, ] nc = np.linspace(100, 500, num=9).astype(int) # nc = np.linspace(500, 10000, num=9).astype(int)[1:] # nc = np.linspace(10000, 100000, num=9).astype(int).tolist()[1:] print(nc) mc = np.linspace(10, 50, num=2).astype(int) print(mc) joblib.Parallel(n_jobs=1, verbose=5, batch_size=1)([joblib.delayed(run)(func, ni, mi, log_dir) for ni in nc for mi in mc for func in candidates]) def plot_graphs(output_dir: Path): log_dir = output_dir / &quot;log&quot; results = [] for path in log_dir.glob(&quot;*.csv&quot;): results.append(path.read_text().strip()) (output_dir / &quot;stat.csv&quot;).write_text(&quot;\\n&quot;.join(results)) df = pd.read_csv(output_dir / &quot;stat.csv&quot;, header=None, names=[&quot;name&quot;, &quot;n&quot;, &quot;m&quot;, &quot;size&quot;, &quot;count&quot;, &quot;time&quot;, &quot;time_full&quot;]) df = add_lower_bound(df) df = df.sort_values([&quot;name&quot;, &quot;n&quot;, &quot;m&quot;]) for m in [10, 50]: plot( df[df[&quot;m&quot;] == m], series=[ baseline.__name__, prioritizes.__name__, Matt_solution.__name__, Kelly_solution.__name__, Kelly_solution2.__name__, btilly_solution.__name__, &quot;lower_bound&quot;, ], ignore=[ baseline.__name__, prioritizes.__name__, ], y=&quot;count&quot;, label=&quot;load count&quot;, title=f&quot;cache_size = {m}% of N&quot;, ) plot( df[df[&quot;m&quot;] == 10], series=[ baseline.__name__, prioritizes.__name__, Matt_solution.__name__, Kelly_solution.__name__, Kelly_solution2.__name__, btilly_solution.__name__, btilly_solution2.__name__, btilly_solution3.__name__, btilly_solution4.__name__, ], ignore=[ prioritizes.__name__, Matt_solution.__name__, ], y=&quot;time&quot;, label=&quot;time (sec)&quot;, title=f&quot;cache_size = {10}% of N&quot;, ) class LRUCacheForTest: def __init__(self, maxsize: int): self.cache = OrderedDict() self.maxsize = maxsize self.load_count = 0 def __call__(self, key: int) -&gt; int: if key in self.cache: value = self.cache[key] self.cache.move_to_end(key) else: if len(self.cache) == self.maxsize: self.cache.popitem(last=False) value = load_item(key) self.cache[key] = value self.load_count += 1 return value def hit(self, i, j): count = int(i in self.cache) self(i) count += int(j in self.cache) self(j) return count def visualize(): # Taken from https://stackoverflow.com/a/77024514/18125313 and modified. n, m = 100, 30 func = btilly_solution2 pairs = func(n, m) cache = LRUCacheForTest(m) # Create the images, save as animated png. images = [] s = 5 img = Image.new(&quot;RGB&quot;, (s * n, s * n), (255, 255, 255)) draw = ImageDraw.Draw(img) colors = [(255, 0, 0), (255, 255, 0), (0, 255, 0)] for step, (i, j) in enumerate(pairs): draw.rectangle((s * j, s * i, s * j + s - 2, s * i + s - 2), colors[cache.hit(i, j)]) if not step % 17: images.append(img.copy()) images += [img] * 40 images[0].save(f&quot;{func.__name__}_{m}.gif&quot;, save_all=True, append_images=images[1:], optimize=False, duration=30, loop=0) def test_combinations(iterator: Iterable[Tuple[int, int]], n: int): # Note that this function is not suitable for large N. expected = set(frozenset(pair) for pair in itertools.combinations(range(n), 2)) items = list(iterator) actual = set(frozenset(pair) for pair in items) assert len(actual) == len(items), f&quot;{[item for item, count in Counter(items).items() if count &gt; 1]}&quot; assert actual == expected, f&quot;dup={actual - expected}, missing={expected - actual}&quot; def test(): n = 100 # N cache_size = 30 # M def run(func): func(n, cache_size) # Measure generation performance. started = time.perf_counter() for a, b in func(n, cache_size): pass elapsed = time.perf_counter() - started # Test generated combinations. test_combinations(func(n, cache_size), n) # Measure cache hit (load count) performance. cache = LRUCacheWithCounter(cache_size) _ = basic_loop(iterator=func(n, cache_size), cached_load=cache) print(f&quot;{func.__name__}: {cache.load_count=}, {elapsed=}&quot;) candidates = [ baseline, prioritizes, Matt_solution, Kelly_solution, Kelly_solution2, btilly_solution, btilly_solution2, btilly_solution3, btilly_solution4, ] for f in candidates: run(f) def main(): test() visualize() output_dir = Path(&quot;./temp2&quot;) benchmark(output_dir) plot_graphs(output_dir) if __name__ == &quot;__main__&quot;: main() I have no problem with you not using the above test code or changing the behavior of basic_loop or LRUCacheWithCounter. Additional Note: The score calculation cannot be pruned using neighbor scores. The score calculation cannot be pruned using only a portion of the item. It is impossible to guess where the best combination will be. Using faster media is one option, but I'm already at my limit, so I'm looking for a software solution. Thank you for reading this long post to the end. Edit: Thanks to btilly's answer and help with Kelly's visualization, I have come to the conclusion that btilly's solution is the best and (possibly) optimal one. Here is a theoretical explanation (although I am not very good at math, so it could be wrong). Let N represent the number of indexes, M the cache size, and C the number of combinations (same as math.comb). Consider a situation where the cache is full and no further combinations can be generated without loading. If we add a new index at this point, the only combinations that can be generated are combinations of the newly added index and the remaining indexes in the cache. This pattern holds for each subsequent iteration. Hence, while the cache is full, the maximum number of combinations can be generated per load is M - 1. This logic holds if the cache isn't full as well. If M' indexes are currently in the cache, then the next index can generate at most M' combinations. The subsequent index can generate at most M' + 1 combinations, and so forth. In total, at most C(M,2) combinations can be generated before the cache is full. Thus, to generate C(N,2) combinations, at least M loads are required to fill the cache, at least (C(N,2) - C(M,2)) / (M - 1) loads are required after the cache is filled. From above, the load counts complexity of this problem is Ω(N^2 / M). I have plotted this formula as a lower_bound in the graphs below. Note that it is only a lower bound and no guarantee that it can actually be achieved. As an aside, Kelly's solution needs to configure k to maximize its performance. For M = 50% of N, it's about M * 2/3. For M = 30% of N, it's about M * 5/6. Although I couldn't figure out how to calculate it. As a general configuration, I use k = M - 2 (which is not best, but relatively good) in the Kelly_solution2 in the graphs below. For M = 10% of N: For M = 50% of N: Note that, in these graphs, it looks like O(N), but this is because I determined M based on N. When M does not change, it is O(N^2) as described above. Here is an animation visualizing the cache hit rate of btilly_solution2, composed by a modified version of Kelly's code. Each pixel represents a combination, with red representing combinations where both indexes are loaded, yellow where one index is loaded, and green where neither index is loaded. In addition, since I'm looking for the optimal sequence, execution time doesn't matter much. But just in case anyone is curious, here is a comparison of execution times (iteration only). btilly_solution4 (btilly's solution modified by Kelly) is almost as fast as itertools.combinations, which should be optimal in this case. Note, however, that even without the modification, it took only 112 nanoseconds per combination. That's it. Thanks to everyone involved.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Here is a simple approach that depends on the cache and gets 230 on your benchmark. def diagonal_block (lower, upper): for i in range(lower, upper + 1): for j in range(i, upper + 1): yield (i, j) def strip (i_lower, i_upper, j_lower, j_upper): for i in range(i_lower, i_upper+1): for j in range (j_lower, j_upper + 1): yield (i, j) # def your_solution_here(n: int, cache_size: int) -&gt; Iterable[Tuple[int, int]]: def your_solution_here(n: int, cache_size: int): i_lower = 0 i_upper = n-1 k = cache_size - 2 is_asc = True while i_lower &lt;= i_upper: # Handle a k*k block first. At the end that is likely loaded. if is_asc: upper = min(i_lower + k - 1, i_upper) yield from diagonal_block(i_lower, upper) j_lower = i_lower j_upper = upper i_lower = upper + 1 else: lower = max(i_lower, i_upper - k + 1) yield from diagonal_block(lower, i_upper) j_lower = lower j_upper = i_upper i_upper = lower - 1 yield from strip(i_lower, i_upper, j_lower, j_upper) is_asc = not is_asc A comment about how I thought this one up. We want to compare a group of objects with every other uncompared object. The group should be everything that fits in the cache except one. So we start with the first k objects, compare them with each other, then just proceed along in a strip to the end. And now we need our second group. Well, we already have the last object, and we don't need the rest. So we take k objects from the end, make that a group. Compare the group with itself, then proceed along a strip to the first object outside of our original group. Now reverse direction, and so on. At all points, i_lower represents the first object still needing comparing, and i_upper represents the last. If we're going forward, we take k objects starting at i_lower. If we're going backwards we take k objects starting at i_upper and go backwards. When I was implementing it, there were two complications. The first is that we have to worry about the edge condition when we meet in the middle. The second is that we might have to do the strip in 2 directions. I chose to only do the strip ascending. This is actually a bug. On most of the ascending loads, I did not get the first element in my cache. Oops. But it is still pretty good.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Going in k×k blocks. With k=25, it gets 372 loads on your benchmark. With k=cache_size/2, it gets 570 loads. from itertools import combinations, product, chain def your_solution_here(n: int, cache_size: int) -&gt; Iterable[Tuple[int, int]]: k = cache_size // 2 r = range(n) return list(chain.from_iterable( combinations(r[i:i+k], 2) if i == j else product(r[i:i+k], r[j:j+k]) for i in r[::k] for j in r[i::k] ))",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "caching",
        "combinations"
      ],
      "question_score": 17,
      "answer_score": 12,
      "created": "2023-08-30T15:58:43",
      "question_id": 77009691,
      "answer_id": 77010771
    }
  },
  {
    "question": "Getting TypeError: WebDriver.__init__() got an unexpected keyword argument ‘desired_capabilities’ when using Appium with Selenium 4.10",
    "expected_answer": "This is due to changes in selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that desired_capabilities has been removed from the __init__, but there is now another way of passing it in. See https://www.selenium.dev/documentation/webdriver/getting_started/upgrade_to_selenium_4/#capabilities for the documentation on how to pass in desired capabilities when using selenium 4.10.0 (or newer). Here's a code snippet on using capabilities in the new version: from selenium.webdriver.firefox.options import Options as FirefoxOptions options = FirefoxOptions() cloud_options = {} cloud_options['build'] = &quot;build_1&quot; cloud_options['name'] = &quot;test_abc&quot; options.set_capability('cloud:options', cloud_options) driver = webdriver.Remote(&quot;http://0.0.0.0:4723/wd/hub&quot;, options=options)",
    "context_chunks": [
      {
        "text": "Error: HOOK-ERROR in before_scenario: TypeError: WebDriver.__init__() got an unexpected keyword argument 'desired_capabilities' Hello, we currently cannot run our script together with the latest Selenium 4.10. Is this Appium error or Python error? Here is the capabilities that we used. We're currently trying to get the capabilities for platformName by targetOS = self.driver.capabilities['platformName'] but we're hit with this error capabilities = { &quot;platformName&quot;: &quot;Android&quot;, &quot;appium:platformVersion&quot;: &quot;11.0&quot;, &quot;appium:deviceName&quot;: &quot;emulator-5554&quot;, &quot;appium:app&quot;: &quot;/Users/faithberroya/Downloads/test.apk&quot;, &quot;appium:automationName&quot;: &quot;UiAutomator2&quot;, &quot;appium:appPackage&quot;: &quot;com.test.school.assignment.rc&quot;, &quot;appium:appActivity&quot;: &quot;com.test.school.assignment.ui.SplashActivity&quot; } # launch app context.driver = webdriver.Remote(&quot;http://0.0.0.0:4723/wd/hub&quot;, capabilities) # add wait time context.driver.implicitly_wait(20) # app context.app = Application(context.driver) Current pip list Appium-Python-Client 2.10.1 behave 1.2.6 certifi 2023.5.7 pip 23.1.1 requests 2.31.0 selenium 4.9.0",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is due to changes in selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that desired_capabilities has been removed from the __init__, but there is now another way of passing it in. See https://www.selenium.dev/documentation/webdriver/getting_started/upgrade_to_selenium_4/#capabilities for the documentation on how to pass in desired capabilities when using selenium 4.10.0 (or newer). Here's a code snippet on using capabilities in the new version: from selenium.webdriver.firefox.options import Options as FirefoxOptions options = FirefoxOptions() cloud_options = {} cloud_options['build'] = &quot;build_1&quot; cloud_options['name'] = &quot;test_abc&quot; options.set_capability('cloud:options', cloud_options) driver = webdriver.Remote(&quot;http://0.0.0.0:4723/wd/hub&quot;, options=options)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I face similar issue. By default Appium python client uses latest selenium release(4.10.0). So i downgraded the version to 4.9.0 with Appium-python-client as 2.9.0 and it worked fine.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver",
        "appium",
        "python-appium"
      ],
      "question_score": 17,
      "answer_score": 26,
      "created": "2023-06-08T08:56:44",
      "question_id": 76430192,
      "answer_id": 76432856
    }
  },
  {
    "question": "Error while installing python package: llama-cpp-python",
    "expected_answer": "You need to install the desktop c++ block with visual studio to get cmake properly installed.Open the Visual Studio Installer and click Modify, then check Desktop development with C++ and click Modify to start the install. I also recommend the Windows 10 SDK. https://learn.microsoft.com/en-us/cpp/build/cmake-projects-in-visual-studio?view=msvc-170 https://developer.microsoft.com/en-us/windows/downloads/windows-sdk/ After that, !pip install llama-cpp-python should build just fine.",
    "context_chunks": [
      {
        "text": "I am using Llama to create an application. Previously I used openai but am looking for a free alternative. Based on my limited research, this library provides openai-like api access making it quite easy to add into my prexisting code. However this library has errors while downloading. I tried installing cmake which did not help. Building wheels for collected packages: llama-cpp-python Building wheel for llama-cpp-python (pyproject.toml) ... error error: subprocess-exited-with-error × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully. │ exit code: 1 ╰─&gt; [20 lines of output] *** scikit-build-core 0.5.1 using CMake 3.27.7 (wheel) *** Configuring CMake... 2023-10-10 21:23:02,749 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None loading initial cache file C:\\Users\\ARUSHM~1\\AppData\\Local\\Temp\\tmpf1bzj6ul\\build\\CMakeInit.txt -- Building for: NMake Makefiles CMake Error at CMakeLists.txt:3 (project): Running 'nmake' '-?' failed with: The system cannot find the file specified CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage -- Configuring incomplete, errors occurred! *** CMake configuration failed [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for llama-cpp-python Failed to build llama-cpp-python ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects Although not directly related to this question, these are other questions I am unable to get answers for: Does this library use Llama or Llama 2? Will this be secure on a Python Flask Application?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You need to install the desktop c++ block with visual studio to get cmake properly installed.Open the Visual Studio Installer and click Modify, then check Desktop development with C++ and click Modify to start the install. I also recommend the Windows 10 SDK. https://learn.microsoft.com/en-us/cpp/build/cmake-projects-in-visual-studio?view=msvc-170 https://developer.microsoft.com/en-us/windows/downloads/windows-sdk/ After that, !pip install llama-cpp-python should build just fine.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For this error message CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage The gcc and g++ are NOT installed, and the version of GCC should be over gcc 11 per this issue Install gcc and g++ under ubuntu sudo apt update sudo apt upgrade sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update sudo apt install gcc-11 g++-11 Install gcc and g++ under centos yum install scl-utils yum install centos-release-scl # find devtoolset-11 yum list all --enablerepo='centos-sclo-rh' | grep &quot;devtoolset&quot; yum install -y devtoolset-11-toolchain Install gcc and g++ under Amamzon Linux 2023 sudo dnf install gcc sudo dnf install g++",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "llama",
        "llama-cpp-python"
      ],
      "question_score": 16,
      "answer_score": 17,
      "created": "2023-10-10T16:02:20",
      "question_id": 77267346,
      "answer_id": 77307148
    }
  },
  {
    "question": "How to run ollama in google colab?",
    "expected_answer": "1. Run ollama but don't stop it !curl https://ollama.ai/install.sh | sh # should produce, among other thigns: # The Ollama API is now available at 0.0.0.0:11434 This means Ollama is running (but do check to see if there are errors, especially around graphics capability/Cuda as these may interfere. However, Don't run !command -v systemctl &gt;/dev/null &amp;&amp; sudo systemctl stop ollama (unless you want to stop Ollama). The next step is to start the Ollama service, but since you are using ngrok I'm assuming you want to be able to run the LLM from other environments outside the Colab? If this isn't the case, then you don't really need ngrok, but since Colabs are tricky to get working nicely with async code and threads it's useful to use the Colab to e.g. run a powerful enough VM to play with larger models than (say) anthing you could run on your dev environment (if this is an issue). 2. Set up ngrok and forward the local ollama service to a public URI Ollama isn't yet running as a service but we can set up ngrok in advance of this: import threading import time import os import asyncio from pyngrok import ngrok import threading import queue import time from threading import Thread # Get your ngrok token from your ngrok account: # https://dashboard.ngrok.com/get-started/your-authtoken token=&quot;your token goes here - don't forget to replace this with it!&quot; ngrok.set_auth_token(token) # set up a stoppable thread (not mandatory, but cleaner if you want to stop this later class StoppableThread(threading.Thread): def __init__(self, *args, **kwargs): super(StoppableThread, self).__init__(*args, **kwargs) self._stop_event = threading.Event() def stop(self): self._stop_event.set() def is_stopped(self): return self._stop_event.is_set() def start_ngrok(q, stop_event): try: # Start an HTTP tunnel on the specified port public_url = ngrok.connect(11434) # Put the public URL in the queue q.put(public_url) # Keep the thread alive until stop event is set while not stop_event.is_set(): time.sleep(1) # Adjust sleep time as needed except Exception as e: print(f&quot;Error in start_ngrok: {e}&quot;) Run that code so the functions exist, then in the next cell, start ngrok in a separate thread so it doesn't hang your colab - we'll use a queue so we can still share data between threads because we want to know what the ngrok public URL will be when it runs: # Create a queue to share data between threads url_queue = queue.Queue() # Start ngrok in a separate thread ngrok_thread = StoppableThread(target=start_ngrok, args=(url_queue, StoppableThread.is_stopped)) ngrok_thread.start() That will be running, but you need to get the results from the queue to see what ngrok returned, so then do: # Wait for the ngrok tunnel to be established while True: try: public_url = url_queue.get() if public_url: break print(&quot;Waiting for ngrok URL...&quot;) time.sleep(1) except Exception as e: print(f&quot;Error in retrieving ngrok URL: {e}&quot;) print(&quot;Ngrok tunnel established at:&quot;, public_url) This should output something like: Ngrok tunnel established at: NgrokTunnel: &quot;https://{somelongsubdomain}.ngrok-free.app&quot; -&gt; &quot;http://localhost:11434&quot; 3. Run ollama as an async process import os import asyncio # NB: You may need to set these depending and get cuda working depending which backend you are running. # Set environment variable for NVIDIA library # Set environment variables for CUDA os.environ['PATH'] += ':/usr/local/cuda/bin' # Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64' async def run_process(cmd): print('&gt;&gt;&gt; starting', *cmd) process = await asyncio.create_subprocess_exec( *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) # define an async pipe function async def pipe(lines): async for line in lines: print(line.decode().strip()) await asyncio.gather( pipe(process.stdout), pipe(process.stderr), ) # call it await asyncio.gather(pipe(process.stdout), pipe(process.stderr)) That creates the function to run an async command but doesn't run it yet. This will start ollama in a separate thread so your Colab isn't blocked: import asyncio import threading async def start_ollama_serve(): await run_process(['ollama', 'serve']) def run_async_in_thread(loop, coro): asyncio.set_event_loop(loop) loop.run_until_complete(coro) loop.close() # Create a new event loop that will run in a new thread new_loop = asyncio.new_event_loop() # Start ollama serve in a separate thread so the cell won't block execution thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve())) thread.start() It should produce something like: &gt;&gt;&gt; starting ollama serve Couldn't find '/root/.ollama/id_ed25519'. Generating new private key. Your new public key is: ssh-ed25519 {some key} 2024/01/16 20:19:11 images.go:808: total blobs: 0 2024/01/16 20:19:11 images.go:815: total unused blobs removed: 0 2024/01/16 20:19:11 routes.go:930: Listening on 127.0.0.1:11434 (version 0.1.20) Now you're all set up. You can either do the next steps in the Colab, but it might be easier to run on your local machine if you normally dev there. 4. Run an ollama model remotely from your local dev environment Assuming you have installed ollama on your local dev environment (say WSL2), I'm assuming it's linux anyway... but i.e. your laptop or desktop machine in front of you (as opposed to Colab). Replace the actual URI below with whatever public URI ngrok reported above: export OLLAMA_HOST=https://{longcode}.ngrok-free.app/ You can now run ollama and it will run on the remote in your Colab (so long as that's stays up and running). e.g. run this on your local machine and it will look as if it's running locally but it's really running in your Colab and the results are being served to wherever you call this from (so long as the OLLAMA_HOST is set correctly and is a valid tunnel to your ollama service: ollama run mistral You can now interact with the model on the command line locally but the model runs on the Colab. If you want to run larger models, like mixtral, then you need to be sure to connect your Colab to a Back end compute that's powerful enough (e.g. 48GB+ of RAM, so V100 GPU is minimum spec for this at the time of writing). Note: If you have any issues with cuda or nvidia showing in the ouputs of any steps above, don't proceed until you fix them.",
    "context_chunks": [
      {
        "text": "I have a code like this. And I'm launching it. I get an ngrok link. !pip install aiohttp pyngrok import os import asyncio from aiohttp import ClientSession # Set LD_LIBRARY_PATH so the system NVIDIA library becomes preferred # over the built-in library. This is particularly important for # Google Colab which installs older drivers os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'}) async def run(cmd): ''' run is a helper function to run subcommands asynchronously. ''' print('&gt;&gt;&gt; starting', *cmd) p = await asyncio.subprocess.create_subprocess_exec( *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE, ) async def pipe(lines): async for line in lines: print(line.strip().decode('utf-8')) await asyncio.gather( pipe(p.stdout), pipe(p.stderr), ) await asyncio.gather( run(['ollama', 'serve']), run(['ngrok', 'http', '--log', 'stderr', '11434']), ) Which I'm following, but the following is on the page How can I fix this? Before that, I did the following !choco install ngrok !ngrok config add-authtoken ----- !curl https://ollama.ai/install.sh | sh !command -v systemctl &gt;/dev/null &amp;&amp; sudo systemctl stop ollama",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "1. Run ollama but don't stop it !curl https://ollama.ai/install.sh | sh # should produce, among other thigns: # The Ollama API is now available at 0.0.0.0:11434 This means Ollama is running (but do check to see if there are errors, especially around graphics capability/Cuda as these may interfere. However, Don't run !command -v systemctl &gt;/dev/null &amp;&amp; sudo systemctl stop ollama (unless you want to stop Ollama). The next step is to start the Ollama service, but since you are using ngrok I'm assuming you want to be able to run the LLM from other environments outside the Colab? If this isn't the case, then you don't really need ngrok, but since Colabs are tricky to get working nicely with async code and threads it's useful to use the Colab to e.g. run a powerful enough VM to play with larger models than (say) anthing you could run on your dev environment (if this is an issue). 2. Set up ngrok and forward the local ollama service to a public URI Ollama isn't yet running as a service but we can set up ngrok in advance of this: import threading import time import os import asyncio from pyngrok import ngrok import threading import queue import time from threading import Thread # Get your ngrok token from your ngrok account: # https://dashboard.ngrok.com/get-started/your-authtoken token=&quot;your token goes here - don't forget to replace this with it!&quot; ngrok.set_auth_token(token) # set up a stoppable thread (not mandatory, but cleaner if you want to stop this later class StoppableThread(threading.Thread): def __init__(self, *args, **kwargs): super(StoppableThread, self).__init__(*args, **kwargs) self._stop_event = threading.Event() def stop(self): self._stop_event.set() def is_stopped(self): return self._stop_event.is_set() def start_ngrok(q, stop_event): try: # Start an HTTP tunnel on the specified port public_url = ngrok.connect(11434) # Put the public URL in the queue q.put(public_url) # Keep the thread alive until stop event is set while not stop_event.is_set(): time.sleep(1) # Adjust sleep time as needed except Exception as e: print(f&quot;Error in start_ngrok: {e}&quot;) Run that code so the functions exist, then in the next cell, start ngrok in a separate thread so it doesn't hang your colab - we'll use a queue so we can still share data between threads because we want to know what the ngrok public URL will be when it runs: # Create a queue to share data between threads url_queue = queue.Queue() # Start ngrok in a separate thread ngrok_thread = StoppableThread(target=start_ngrok, args=(url_queue, StoppableThread.is_stopped)) ngrok_thread.start() That will be running, but you need to get the results from the queue to see what ngrok returned, so then do: # Wait for the ngrok tunnel to be established while True: try: public_url = url_queue.get() if public_url: break print(&quot;Waiting for ngrok URL...&quot;) time.sleep(1) except Exception as e: print(f&quot;Error in retrieving ngrok URL: {e}&quot;) print(&quot;Ngrok tunnel established at:&quot;, public_url) This should output something like: Ngrok tunnel established at: NgrokTunnel: &quot;https://{somelongsubdomain}.ngrok-free.app&quot; -&gt; &quot;http://localhost:11434&quot; 3. Run ollama as an async process import os import asyncio # NB: You may need to set these depending and get cuda working depending which backend you are running. # Set environment variable for NVIDIA library # Set environment variables for CUDA os.environ['PATH'] += ':/usr/local/cuda/bin' # Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64' async def run_process(cmd): print('&gt;&gt;&gt; starting', *cmd) process = await asyncio.create_subprocess_exec( *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) # define an async pipe function async def pipe(lines): async for line in lines: print(line.decode().strip()) await asyncio.gather( pipe(process.stdout), pipe(process.stderr), ) # call it await asyncio.gather(pipe(process.stdout), pipe(process.stderr)) That creates the function to run an async command but doesn't run it yet. This will start ollama in a separate thread so your Colab isn't blocked: import asyncio import threading async def start_ollama_serve(): await run_process(['ollama', 'serve']) def run_async_in_thread(loop, coro): asyncio.set_event_loop(loop) loop.run_until_complete(coro) loop.close() # Create a new event loop that will run in a new thread new_loop = asyncio.new_event_loop() # Start ollama serve in a separate thread so the cell won't block execution thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve())) thread.start() It should produce something like: &gt;&gt;&gt; starting ollama serve Couldn't find '/root/.ollama/id_ed25519'. Generating new private key. Your new public key is: ssh-ed25519 {some key} 2024/01/16 20:19:11 images.go:808: total blobs: 0 2024/01/16 20:19:11 images.go:815: total unused blobs removed: 0 2024/01/16 20:19:11 routes.go:930: Listening on 127.0.0.1:11434 (version 0.1.20) Now you're all set up. You can either do the next steps in the Colab, but it might be easier to run on your local machine if you normally dev there. 4. Run an ollama model remotely from your local dev environment Assuming you have installed ollama on your local dev environment (say WSL2), I'm assuming it's linux anyway... but i.e. your laptop or desktop machine in front of you (as opposed to Colab). Replace the actual URI below with whatever public URI ngrok reported above: export OLLAMA_HOST=https://{longcode}.ngrok-free.app/ You can now run ollama and it will run on the remote in your Colab (so long as that's stays up and running). e.g. run this on your local machine and it will look as if it's running locally but it's really running in your Colab and the results are being served to wherever you call this from (so long as the OLLAMA_HOST is set correctly and is a valid tunnel to your ollama service: ollama run mistral You can now interact with the model on the command line locally but the model runs on the Colab. If you want to run larger models, like mixtral, then you need to be sure to connect your Colab to a Back end compute that's powerful enough (e.g. 48GB+ of RAM, so V100 GPU is minimum spec for this at the time of writing). Note: If you have any issues with cuda or nvidia showing in the ouputs of any steps above, don't proceed until you fix them.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Just want to add a few personal takeaways when I try the above methods. (Sorry that I cannot comment) Both Gruff and Alan Turing's approaches work for me after I add the following line: os.environ.update({'OLLAMA_HOST': '0.0.0.0'}) Before I add this, the current version of ollama installation script seems to restrict the listening host to localhost, making the access outside not possible. Before adding this line, if I access the service using ngrok, I would get a 403 access denied message. Modifying the host helps to deal with this issue, but it also means that anyone can access it now if s/he knows your ngrok tunnel link.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pyngrok",
        "ollama"
      ],
      "question_score": 17,
      "answer_score": 13,
      "created": "2023-12-21T10:28:07",
      "question_id": 77697302,
      "answer_id": 77828874
    }
  },
  {
    "question": "Python 3.12 SyntaxWarning: invalid escape sequence on triple-quoted string, `\\d` must be `\\\\d`",
    "expected_answer": "Back in Python 3.6, using invalid escape sequences in string literals was deprecated (bpo-27364). Since then, attempting to use an invalid escape sequence has emitted a DeprecationWarning. This can often go unnoticed if you don't run Python with warnings enabled. DeprecationWarnings are silenced by default. Python 3.12 upgraded the DeprecationWarning to a SyntaxWarning. SyntaxWarnings are emitted by the compiler when the code is parsed, not when it's being run, so they cannot be ignored using a runtime warning filter. Unlike DeprecationWarnings, SyntaxWarnings are displayed by default, which is why you're seeing it now. This increase in visibility was intentional. In a future version of Python, using invalid escape sequences in string literals is planned to eventually become a hard SyntaxError. The simplest solution would be to use # comments for comments instead of string literals. Unlike string literals, comments aren't required to follow any special syntax rules. See also the discussion in Python comments: # vs. strings for more on the drawbacks of using string literals as comments. To address this warning in general, you can make the string literal a raw string literal r&quot;...&quot;. Raw string literals do not process escape sequences. For example, the string &quot;\\n&quot; contains a single newline character, whereas the string r&quot;\\n&quot; contains the two characters \\ and n.",
    "context_chunks": [
      {
        "text": "After updating to Python 3.12, I get warnings about invalid escape sequence on some triple-quotes comments. Is this a new restriction? I have the habit of documenting code using triple-quoted string, but this has never been a problem prior to Python 3.12. python3 --version Python 3.12.0 $ ./some_script.py /some_script.py:123: SyntaxWarning: invalid escape sequence '\\d' &quot;&quot;&quot; I tried replacing all lines with \\d: 20230808122708.445|INFO|C:\\dist\\work\\trk-fullstack-test\\namespaces.py with \\\\d: 20230808122708.445|INFO|C:\\\\dist\\work\\trk-fullstack-test\\namespaces.py The warning disappears. Suppressing the warning do not seem to work: import warnings warnings.filterwarnings('ignore', category=SyntaxWarning) Any pointers on how to do this correctly? I hope I do not have to escape all Windows paths documented in triplequotes in our code.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Back in Python 3.6, using invalid escape sequences in string literals was deprecated (bpo-27364). Since then, attempting to use an invalid escape sequence has emitted a DeprecationWarning. This can often go unnoticed if you don't run Python with warnings enabled. DeprecationWarnings are silenced by default. Python 3.12 upgraded the DeprecationWarning to a SyntaxWarning. SyntaxWarnings are emitted by the compiler when the code is parsed, not when it's being run, so they cannot be ignored using a runtime warning filter. Unlike DeprecationWarnings, SyntaxWarnings are displayed by default, which is why you're seeing it now. This increase in visibility was intentional. In a future version of Python, using invalid escape sequences in string literals is planned to eventually become a hard SyntaxError. The simplest solution would be to use # comments for comments instead of string literals. Unlike string literals, comments aren't required to follow any special syntax rules. See also the discussion in Python comments: # vs. strings for more on the drawbacks of using string literals as comments. To address this warning in general, you can make the string literal a raw string literal r&quot;...&quot;. Raw string literals do not process escape sequences. For example, the string &quot;\\n&quot; contains a single newline character, whereas the string r&quot;\\n&quot; contains the two characters \\ and n.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can encode your regex string under r'' in Python 3.12.3. It works for me. In 3.11.4 before : '(?:\\d{4}(-)?\\d{3}(-)?[8]\\d{2}(-)?\\d{2})' In 3.12.3 after : r'(?:\\d{4}(-)?\\d{3}(-)?[8]\\d{2}(-)?\\d{2})'",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "warnings",
        "string-literals",
        "quoting",
        "python-3.12"
      ],
      "question_score": 16,
      "answer_score": 32,
      "created": "2023-11-22T15:25:57",
      "question_id": 77531208,
      "answer_id": 77531416
    }
  },
  {
    "question": "ModuleNotFoundError: No module named &#39;kafka.vendor.six.moves&#39; in Dockerized Django Application",
    "expected_answer": "This is issue with the current version of kafka-python Instead use kafka-python-ng $ pip install kafka-python-ng",
    "context_chunks": [
      {
        "text": "I am facing an issue with my Dockerized Django application. I am using the following Dockerfile to build my application: FROM python:alpine ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 ENV DJANGO_SUPERUSER_PASSWORD datahub RUN mkdir app WORKDIR /app COPY ./app . RUN mkdir -p volumes RUN apk update RUN apk add --no-cache gcc python3-dev musl-dev mariadb-dev RUN pip3 install --upgrade pip RUN pip3 install -r requirements.txt RUN apk del gcc python3-dev musl-dev CMD python3 manage.py makemigrations --noinput &amp;&amp;\\ while ! python3 manage.py migrate --noinput; do sleep 1; done &amp;&amp; \\ python3 manage.py collectstatic --noinput &amp;&amp;\\ python3 manage.py createsuperuser --user datahub --email admin@localhost --noinput;\\ python3 manage.py runserver 0.0.0.0:8000 In my requirements.txt file: kafka-python==2.0.2 When I run my application inside the Docker container, I encounter the following error: ModuleNotFoundError: No module named 'kafka.vendor.six.moves' Compelete Error: Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/usr/local/lib/python3.12/site-packages/kafka/__init__.py&quot;, line 23, in &lt;module&gt; from kafka.consumer import KafkaConsumer File &quot;/usr/local/lib/python3.12/site-packages/kafka/consumer/__init__.py&quot;, line 3, in &lt;module&gt; from kafka.consumer.group import KafkaConsumer File &quot;/usr/local/lib/python3.12/site-packages/kafka/consumer/group.py&quot;, line 13, in &lt;module&gt; from kafka.consumer.fetcher import Fetcher File &quot;/usr/local/lib/python3.12/site-packages/kafka/consumer/fetcher.py&quot;, line 19, in &lt;module&gt; from kafka.record import MemoryRecords File &quot;/usr/local/lib/python3.12/site-packages/kafka/record/__init__.py&quot;, line 1, in &lt;module&gt; from kafka.record.memory_records import MemoryRecords, MemoryRecordsBuilder File &quot;/usr/local/lib/python3.12/site-packages/kafka/record/memory_records.py&quot;, line 27, in &lt;module&gt; from kafka.record.legacy_records import LegacyRecordBatch, LegacyRecordBatchBuilder File &quot;/usr/local/lib/python3.12/site-packages/kafka/record/legacy_records.py&quot;, line 50, in &lt;module&gt; from kafka.codec import ( File &quot;/usr/local/lib/python3.12/site-packages/kafka/codec.py&quot;, line 9, in &lt;module&gt; from kafka.vendor.six.moves import range ModuleNotFoundError: No module named 'kafka.vendor.six.moves' I have already tried updating the Kafka package, checking dependencies, and installing the six package manually. However, the issue still persists. Can anyone provide insights on how to resolve this error? Thank you in advance for your help!",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is issue with the current version of kafka-python Instead use kafka-python-ng $ pip install kafka-python-ng",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This appears to be a Python 3.12 issue, I have the same error but in an entirely different context. Instead of FROM python:alpine I suggest you use FROM python:3.11 3.12 is still very new are there are many projects still trying to work out issues.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "django",
        "apache-kafka"
      ],
      "question_score": 16,
      "answer_score": 33,
      "created": "2023-10-13T12:11:04",
      "question_id": 77287622,
      "answer_id": 78147386
    }
  },
  {
    "question": "OpenAI API error: &quot;You tried to access openai.Completion, but this is no longer supported in openai&gt;=1.0.0&quot;",
    "expected_answer": "Problem The method you're trying to use doesn't work with the OpenAI Python SDK &gt;=v1.0.0 (if you're using Python) or OpenAI Node.js SDK &gt;=v4.0.0 (if you're using Node.js). See the Python SDK migration guide or the Node.js SDK migration guide. Python The old SDK (i.e., v0.28.1) works with the following method: client.Completion.create() The new SDK (i.e., &gt;=v1.0.0) works with the following method: client.completions.create() Note: Be careful because the API is case-sensitive (i.e., client.Completions.create() will not work with the new SDK version). Node.js The old SDK (i.e., v3.3.0) works with the following method: client.createCompletion() The new SDK (i.e., &gt;=v4.0.0) works with the following method: client.completions.create() Note: Be careful because the API is case-sensitive (i.e., client.Completions.create() will not work with the new SDK version). Solution Python SDK v1.0.0 working example If you run test.py, the OpenAI API will return the following completion: This is a test. test.py import os from openai import OpenAI client = OpenAI( api_key = os.getenv(&quot;OPENAI_API_KEY&quot;), ) completion = client.completions.create( model = &quot;gpt-3.5-turbo-instruct&quot;, prompt = &quot;Say this is a test&quot;, max_tokens = 7, temperature = 0, ) print(completion.choices[0].text.strip()) Node.js SDK v4.0.0 working example If you run test.js, the OpenAI API will return the following completion: This is a test. test.js const OpenAI = require(&quot;openai&quot;); const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY, }); async function main() { const completion = await client.completions.create({ model: &quot;gpt-3.5-turbo-instruct&quot;, prompt: &quot;Say this is a test&quot;, max_tokens: 7, temperature: 0, }); console.log(completion.choices[0].text.trim()); } main();",
    "context_chunks": [
      {
        "text": "I want to translate the texts in a csv file into English using the GPT 4 model, but I constantly get the following error. Even though I updated the version, I continue to get the same error. import openai import pandas as pd import os from tqdm import tqdm openai.api_key = os.getenv(&quot;API&quot;) def translate_text(text): response = openai.Completion.create( model=&quot;text-davinci-003&quot;, # GPT-4 modeli prompt=f&quot;Translate the following Turkish text to English: '{text}'&quot;, max_tokens=60 ) # Yeni API yapısına göre yanıtın alınması return response.choices[0].text.strip() df = pd.read_excel('/content/3500-turkish-dataset-column-name.xlsx') column_to_translate = 'review' df[column_to_translate + '_en'] = '' for index, row in tqdm(df.iterrows(), total=df.shape[0]): translated_text = translate_text(row[column_to_translate]) df.at[index, column_to_translate + '_en'] = translated_text df.to_csv('path/to/your/translated_csvfile.csv', index=False) 0%| | 0/3500 [00:00&lt;?, ?it/s] --------------------------------------------------------------------------- APIRemovedInV1 Traceback (most recent call last) &lt;ipython-input-27-337b5b6f4d32&gt; in &lt;cell line: 29&gt;() 28 # Her satırdaki metni çevir ve yeni sütuna kaydet 29 for index, row in tqdm(df.iterrows(), total=df.shape[0]): ---&gt; 30 translated_text = translate_text(row[column_to_translate]) 31 df.at[index, column_to_translate + '_en'] = translated_text 32 3 frames /usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py in __load__(self) APIRemovedInV1: You tried to access openai.Completion, but this is no longer supported in openai&gt;=1.0.0 - see the README at https://github.com/openai/openai-python for the API. You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28` A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742 Even though I updated the OpenAI package version, I get the same error.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Problem The method you're trying to use doesn't work with the OpenAI Python SDK &gt;=v1.0.0 (if you're using Python) or OpenAI Node.js SDK &gt;=v4.0.0 (if you're using Node.js). See the Python SDK migration guide or the Node.js SDK migration guide. Python The old SDK (i.e., v0.28.1) works with the following method: client.Completion.create() The new SDK (i.e., &gt;=v1.0.0) works with the following method: client.completions.create() Note: Be careful because the API is case-sensitive (i.e., client.Completions.create() will not work with the new SDK version). Node.js The old SDK (i.e., v3.3.0) works with the following method: client.createCompletion() The new SDK (i.e., &gt;=v4.0.0) works with the following method: client.completions.create() Note: Be careful because the API is case-sensitive (i.e., client.Completions.create() will not work with the new SDK version). Solution Python SDK v1.0.0 working example If you run test.py, the OpenAI API will return the following completion: This is a test. test.py import os from openai import OpenAI client = OpenAI( api_key = os.getenv(&quot;OPENAI_API_KEY&quot;), ) completion = client.completions.create( model = &quot;gpt-3.5-turbo-instruct&quot;, prompt = &quot;Say this is a test&quot;, max_tokens = 7, temperature = 0, ) print(completion.choices[0].text.strip()) Node.js SDK v4.0.0 working example If you run test.js, the OpenAI API will return the following completion: This is a test. test.js const OpenAI = require(&quot;openai&quot;); const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY, }); async function main() { const completion = await client.completions.create({ model: &quot;gpt-3.5-turbo-instruct&quot;, prompt: &quot;Say this is a test&quot;, max_tokens: 7, temperature: 0, }); console.log(completion.choices[0].text.trim()); } main();",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "1st of all, I'd recommend you to check the official doc for the current version. That said, the Completion method is no longer available. 2ndly, the da-vinci model you are using is deprecated, check the available models here So after fixing the code with those review: import openai import pandas as pd import os from tqdm import tqdm # lets initialize the API client 1st client = openai.OpenAI( # This is the default and can be omitted api_key=os.getenv(&quot;API&quot;) ) def translate_text(text): prompt=f&quot;Translate the following Turkish text to English: '{text}'&quot; response = client.chat.completions.create( model=&quot;gpt-3.5-turbo&quot;, # or gpt-4-turbo or gpt-4 # yes, OpenAI recommends using roleplay like message system messages=[ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt, } ], max_tokens=60 ) # Yeni API yapısına göre yanıtın alınması return response.choices[0].message.content.strip() df = pd.read_excel('/content/3500-turkish-dataset-column-name.xlsx') column_to_translate = 'review' df[column_to_translate + '_en'] = '' for index, row in tqdm(df.iterrows(), total=df.shape[0]): translated_text = translate_text(row[column_to_translate]) df.at[index, column_to_translate + '_en'] = translated_text df.to_csv('path/to/your/translated_csvfile.csv', index=False)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "openai-api",
        "gpt-3"
      ],
      "question_score": 16,
      "answer_score": 17,
      "created": "2023-11-12T17:39:58",
      "question_id": 77469966,
      "answer_id": 77470008
    }
  },
  {
    "question": "What is the new way to declare Mongo ObjectId with PyDantic v2.0^?",
    "expected_answer": "Generally best to ask questions like this on pydantic's GitHub discussions. Your solution is pretty close, I think you just have the wrong core schema. I think our documentation on using custom types via Annotated cover this fairly well, but just to help you, here is a working implementation: from typing import Annotated, Any from bson import ObjectId from pydantic_core import core_schema from pydantic import BaseModel from pydantic.json_schema import JsonSchemaValue class ObjectIdPydanticAnnotation: @classmethod def validate_object_id(cls, v: Any, handler) -&gt; ObjectId: if isinstance(v, ObjectId): return v s = handler(v) if ObjectId.is_valid(s): return ObjectId(s) else: raise ValueError(&quot;Invalid ObjectId&quot;) @classmethod def __get_pydantic_core_schema__(cls, source_type, _handler) -&gt; core_schema.CoreSchema: assert source_type is ObjectId return core_schema.no_info_wrap_validator_function( cls.validate_object_id, core_schema.str_schema(), serialization=core_schema.to_string_ser_schema(), ) @classmethod def __get_pydantic_json_schema__(cls, _core_schema, handler) -&gt; JsonSchemaValue: return handler(core_schema.str_schema()) class Model(BaseModel): id: Annotated[ObjectId, ObjectIdPydanticAnnotation] print(Model(id='64b7abdecf2160b649ab6085')) print(Model(id='64b7abdecf2160b649ab6085').model_dump_json()) print(Model(id=ObjectId())) print(Model.model_json_schema()) print(Model(id='foobar')) # will error",
    "context_chunks": [
      {
        "text": "This week, I started working with MongoDB and Flask, so I found a helpful article on how to use them together by using PyDantic library to define MongoDB's models. However, the article is somewhat outdated, mostly could be updated to new PyDantic's version, but the problem is that the ObjectId is a third party field and that changed drastically between versions. The article defines the ObjectId using the following code: from bson import ObjectId from pydantic.json import ENCODERS_BY_TYPE class PydanticObjectId(ObjectId): &quot;&quot;&quot; Object Id field. Compatible with Pydantic. &quot;&quot;&quot; @classmethod def __get_validators__(cls): yield cls.validate #The validator is doing nothing @classmethod def validate(cls, v): return PydanticObjectId(v) #Here you modify the schema to tell it that it will work as an string @classmethod def __modify_schema__(cls, field_schema: dict): field_schema.update( type=&quot;string&quot;, examples=[&quot;5eb7cf5a86d9755df3a6c593&quot;, &quot;5eb7cfb05e32e07750a1756a&quot;], ) #Here you encode the ObjectId as a string ENCODERS_BY_TYPE[PydanticObjectId] = str In the past, this code worked well. However, I recently discovered that the latest version of PyDantic has a more complex way of defining custom data types. I've tried following the Pydantic documentation, but I'm still confused and haven't been able to implement it successfully. I've tried the implementation to do the implementation for third party types, but it's not working. It's almost the same code of the documentation, but changing ints for strings, and the third party callabels for ObjectId. Again, I'm not sure why it's not working. from bson import ObjectId from pydantic_core import core_schema from typing import Annotated, Any from pydantic import BaseModel, GetJsonSchemaHandler, ValidationError from pydantic.json_schema import JsonSchemaValue class PydanticObjectId(ObjectId): &quot;&quot;&quot; Object Id field. Compatible with Pydantic. &quot;&quot;&quot; x: str def __init__(self): self.x = '' class _ObjectIdPydanticAnnotation: @classmethod def __get_pydantic_core_schema__( cls, _source_type: Any, _handler: ObjectId[[Any], core_schema.CoreSchema], ) -&gt; core_schema.CoreSchema: @classmethod def validate_object_id(cls, v: ObjectId) -&gt; PydanticObjectId: if not ObjectId.is_valid(v): raise ValueError(&quot;Invalid objectid&quot;) return PydanticObjectId(v) from_str_schema = core_schema.chain_schema( [ core_schema.str_schema(), core_schema.no_info_plain_validator_function(validate_object_id), ] ) return core_schema.json_or_python_schema( json_schema=from_str_schema, python_schema=core_schema.union_schema( [ # check if it's an instance first before doing any further work core_schema.is_instance_schema(PydanticObjectId), from_str_schema, ] ), serialization=core_schema.plain_serializer_function_ser_schema( lambda instance: instance.x ), ) @classmethod def __get_pydantic_json_schema__( cls, _core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler ) -&gt; JsonSchemaValue: # Use the same schema that would be used for `int` return handler(core_schema.int_schema()) I've searched for answers on StackOverflow, but all the answers I've found refer to older versions of Pydantic and use code that's similar to what I pasted above. If anyone knows of an alternative solution or can provide clear guidance on how to define a custom data type in the latest version of PyDantic, I would greatly appreciate it. Update A constant error that I'm getting because I'm not creating right the ObjectId type is this Unable to generate pydantic-core schema for &lt;class 'bson.objectid.ObjectId'&gt;. Set arbitrary_types_allowed=True in the model_config to ignore this error or implement __get_pydantic_core_schema__ on your type to fully support it. If you got this error by calling handler() within __get_pydantic_core_schema__ then you likely need to call handler.generate_schema(&lt;some type&gt;) since we do not call __get_pydantic_core_schema__ on &lt;some type&gt; otherwise to avoid infinite recursion. For further information visit https://errors.pydantic.dev/2.0.2/u/schema-for-unknown-type And the answer is to declare it as an unknown type, but I don't want it, I want to declare it as an ObjectId.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Generally best to ask questions like this on pydantic's GitHub discussions. Your solution is pretty close, I think you just have the wrong core schema. I think our documentation on using custom types via Annotated cover this fairly well, but just to help you, here is a working implementation: from typing import Annotated, Any from bson import ObjectId from pydantic_core import core_schema from pydantic import BaseModel from pydantic.json_schema import JsonSchemaValue class ObjectIdPydanticAnnotation: @classmethod def validate_object_id(cls, v: Any, handler) -&gt; ObjectId: if isinstance(v, ObjectId): return v s = handler(v) if ObjectId.is_valid(s): return ObjectId(s) else: raise ValueError(&quot;Invalid ObjectId&quot;) @classmethod def __get_pydantic_core_schema__(cls, source_type, _handler) -&gt; core_schema.CoreSchema: assert source_type is ObjectId return core_schema.no_info_wrap_validator_function( cls.validate_object_id, core_schema.str_schema(), serialization=core_schema.to_string_ser_schema(), ) @classmethod def __get_pydantic_json_schema__(cls, _core_schema, handler) -&gt; JsonSchemaValue: return handler(core_schema.str_schema()) class Model(BaseModel): id: Annotated[ObjectId, ObjectIdPydanticAnnotation] print(Model(id='64b7abdecf2160b649ab6085')) print(Model(id='64b7abdecf2160b649ab6085').model_dump_json()) print(Model(id=ObjectId())) print(Model.model_json_schema()) print(Model(id='foobar')) # will error",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I have built an asynchronous ODM for MongoDB on-top of Motor, called &quot;motormongo&quot;. It includes safeguards for using ObjectIDs. Additionally, it has a similar API to mongoengine. Making it easy for user's of mongoengine users to transition to use motormongo asynchronous capabilities. It is designed specifically for use in FastAPI projects. You can find the source code here: https://github.com/pprunty/motormongo and documentation: https://motormongo.readthedocs.io/en/latest/. Hope this helps!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "mongodb",
        "pydantic"
      ],
      "question_score": 17,
      "answer_score": 13,
      "created": "2023-07-14T09:07:30",
      "question_id": 76686267,
      "answer_id": 76719893
    }
  },
  {
    "question": "Why is Visual Studio Code saying my code in unreachable after using the Pandas concat function?",
    "expected_answer": "This is a bug currently existing in pandas-stubs. The matching overload of concat in pandas-stubs currently returns Never. According to this suggestion in Pylance github, you could work around the pandas-stubs issue by commenting out the Never overload in ...\\.vscode\\extensions\\ms-python.vscode-pylance-2024.3.1\\dist\\bundled\\stubs\\pandas\\core\\reshape\\concat.pyi. @overload def concat( objs: Iterable[None] | Mapping[HashableT1, None], *, axis: Axis = ..., join: Literal[&quot;inner&quot;, &quot;outer&quot;] = ..., ignore_index: bool = ..., keys: Iterable[HashableT2] = ..., levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ..., names: list[HashableT4] = ..., verify_integrity: bool = ..., sort: bool = ..., copy: bool = ..., ) -&gt; Never: ...",
    "context_chunks": [
      {
        "text": "Koda Ulaşılamıyor -&gt; Code is unreachable Visual Studio code is graying out my code and saying it is unreachable after I used pd.concat(). The IDE seems to run smoothly but it's disturbing and I want my colorful editor back. How do I disable the editor graying out my code without changing the current language?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is a bug currently existing in pandas-stubs. The matching overload of concat in pandas-stubs currently returns Never. According to this suggestion in Pylance github, you could work around the pandas-stubs issue by commenting out the Never overload in ...\\.vscode\\extensions\\ms-python.vscode-pylance-2024.3.1\\dist\\bundled\\stubs\\pandas\\core\\reshape\\concat.pyi. @overload def concat( objs: Iterable[None] | Mapping[HashableT1, None], *, axis: Axis = ..., join: Literal[&quot;inner&quot;, &quot;outer&quot;] = ..., ignore_index: bool = ..., keys: Iterable[HashableT2] = ..., levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ..., names: list[HashableT4] = ..., verify_integrity: bool = ..., sort: bool = ..., copy: bool = ..., ) -&gt; Never: ...",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The reason is because there is an overloaded method signature in pandas-stubs which states that giving an Iterable[None] will always throw an exception. Instead it only accepts Iterable[Series] or Iterable[DataFrame]. VSCode's static analysis doesn't check whether this is true, of course, so it falsely claims there is unreachable code. There is some debate about who's responsibility it is to make this work as expected (see github issue), but I'd like to offer an easy solution that doesn't involve monkey-patching the stubs, and should always work. If you add type hints to reassure VSCode that the input will be one of the accepted types, then it won't think that the program will crash. In your case, the problem is that sort_values says it will return Any. So, you can fix that by just add type hints to the preceding variables: sorted_excel_file_1: pd.DataFrame = df.iloc[:82].sort_values(by='H-arm', ascending=True) sorted_excel_file_2: pd.DataFrame = df.iloc[82:].sort_values(by='H-arm', ascending=True)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "visual-studio-code"
      ],
      "question_score": 17,
      "answer_score": 17,
      "created": "2024-03-13T20:25:52",
      "question_id": 78156640,
      "answer_id": 78157665
    }
  },
  {
    "question": "Installing mamba on a machine with conda",
    "expected_answer": "Mamba can theoretically be added to any install, but is no longer recommended.1 Since Mamba is only distributed through Conda Forge, one must convert base to having the conda-forge channel prioritized. Be aware that this is generally incompatible with Anaconda base1, but can work fine with Miniconda base. Essentially, ## prioritize 'conda-forge' channel conda config --add channels conda-forge ## update existing packages to use 'conda-forge' channel conda update -n base --all ## install 'mamba' conda install -n base mamba Now you can use mamba instead of conda. [1]: Mamba developers recommend installing Miniforge as a base, since it prioritizes conda-forge channel by default. [2]: The need to prioritize conda-forge channel is effectively incompatible with the Anaconda base because the combination of a large environment (Anaconda) and plus a larger package search space (Conda Forge) make the solution space enormous. Basically every question on SO about things taking infinitely long to solve is really about this. If you are starting from Anaconda base you'll want to first gut out the base environment to be minimal (remove the anaconda package).",
    "context_chunks": [
      {
        "text": "I dont know if I have missed it, but this is not clear to me. I already have miniconda on my machine and I want now to install mamba. How this should be done please, I am supposed to download/run the correct mambaforge installer? Does this co-exist happily side by side with conda or you are supposed to have just one of them on your system. Doing something like conda install mamba is not recommended",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Mamba can theoretically be added to any install, but is no longer recommended.1 Since Mamba is only distributed through Conda Forge, one must convert base to having the conda-forge channel prioritized. Be aware that this is generally incompatible with Anaconda base1, but can work fine with Miniconda base. Essentially, ## prioritize 'conda-forge' channel conda config --add channels conda-forge ## update existing packages to use 'conda-forge' channel conda update -n base --all ## install 'mamba' conda install -n base mamba Now you can use mamba instead of conda. [1]: Mamba developers recommend installing Miniforge as a base, since it prioritizes conda-forge channel by default. [2]: The need to prioritize conda-forge channel is effectively incompatible with the Anaconda base because the combination of a large environment (Anaconda) and plus a larger package search space (Conda Forge) make the solution space enormous. Basically every question on SO about things taking infinitely long to solve is really about this. If you are starting from Anaconda base you'll want to first gut out the base environment to be minimal (remove the anaconda package).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Mamba is a replacement conda package manager. They do not co-exist happily as they both rely on a base environment for their dependencies. The recommended way is to install mambaforge or micromamba separately as a replacement. documentation for Mamba: https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html documentation for Micromamba: https://mamba.readthedocs.io/en/latest/installation/micromamba-installation.html",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "conda",
        "mamba"
      ],
      "question_score": 16,
      "answer_score": 25,
      "created": "2023-07-25T08:40:15",
      "question_id": 76760906,
      "answer_id": 76765625
    }
  },
  {
    "question": "Python pillow/PIL doesn&#39;t recognize the attribute &quot;textsize&quot; of the object &quot;imagedraw&quot;",
    "expected_answer": "textsize was deprecated, the correct attribute is textlength which gives you the width of the text. for the height use the fontsize * how many rows of text you wrote. Example code: w = draw.textlength(text, font=font) h = fontSize * rows",
    "context_chunks": [
      {
        "text": "I already checked python version on my environment (sublime text) and it is 3.11.0, the latest, I checked pillow version which is 10.0.0, the latest, and my code looks similar to other examples online. the code has a part in Italian, but its pretty understandable. the problem is at &quot;disegno.textsize(testo, font=font) after I run the code: line 14, in metti_testo_su_sfondo text_width, text_height = disegno.textsize(testo, font=font) ^^^^^^^^^^^^^^^^ AttributeError: 'ImageDraw' object has no attribute 'textsize' its strange because imagedraw should have the textsize attribute. I'm a novice, I hope I didn't miss anything blatant from PIL import Image, ImageDraw, ImageFont def metti_testo_su_sfondo(testo, sfondo, posizione=(10, 10), colore_testo=(0, 0, 0), dimensione_font=25): # Apri l'immagine dello sfondo immagine_sfondo = Image.open(sfondo) disegno = ImageDraw.Draw(immagine_sfondo) font = ImageFont.truetype(&quot;ARIAL.TTF&quot;, dimensione_font) text_width, text_height = disegno.textsize(testo, font=font) # Calcola le coordinate del testo centrato x = (immagine_sfondo.width - text_width) // 2 y = (immagine_sfondo.height - text_height) // 2 disegno.text((x, y), testo, fill=colore_testo, font=font) immagine_sfondo.save(&quot;spotted.png&quot;) testo_da_inserire = &quot;Ciao, mondo!&quot; sfondo_da_utilizzare = &quot;spotted_bianco.jpg&quot; metti_testo_su_sfondo(testo_da_inserire, sfondo_da_utilizzare) The objective is a code that makes me images automatically without needing to edit them manually. I checked build system, python version and pillow version. when I run the code through cmd though it gives me this error: from PIL import Image, ImageDraw, ImageFont ModuleNotFoundError: No module named 'PIL'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "textsize was deprecated, the correct attribute is textlength which gives you the width of the text. for the height use the fontsize * how many rows of text you wrote. Example code: w = draw.textlength(text, font=font) h = fontSize * rows",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As other answers have mentioned, textsize is deprecated. There is no textheight, but you can use textlength. If you just want a function that can test a given text and font pair, you can do something like this with textbbox: def textsize(text, font): im = Image.new(mode=&quot;P&quot;, size=(0, 0)) draw = ImageDraw.Draw(im) _, _, width, height = draw.textbbox((0, 0), text=text, font=font) return width, height Which should work the same way.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "python-imaging-library"
      ],
      "question_score": 15,
      "answer_score": 25,
      "created": "2023-09-04T13:27:28",
      "question_id": 77038132,
      "answer_id": 77074371
    }
  },
  {
    "question": "How to run any quantized GGUF model on CPU for local inference?",
    "expected_answer": "llama-cpp-python is my personal choice, because it is easy to use and it is usually one of the first to support quantized versions of new models. To install it for CPU, just run pip install llama-cpp-python. Compiling for GPU is a little more involved, so I'll refrain from posting those instructions here since you asked specifically about CPU inference. I also recommend installing huggingface_hub (pip install huggingface_hub) to easily download models. Once you have both llama-cpp-python and huggingface_hub installed, you can download and use a model (e.g. mixtral-8x7b-instruct-v0.1-gguf) like so: ## Imports from huggingface_hub import hf_hub_download from llama_cpp import Llama ## Download the GGUF model model_name = &quot;TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF&quot; model_file = &quot;mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&quot; # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred model_path = hf_hub_download(model_name, filename=model_file) ## Instantiate model from downloaded file llm = Llama( model_path=model_path, n_ctx=16000, # Context length to use n_threads=32, # Number of CPU threads to use n_gpu_layers=0 # Number of model layers to offload to GPU ) ## Generation kwargs generation_kwargs = { &quot;max_tokens&quot;:20000, &quot;stop&quot;:[&quot;&lt;/s&gt;&quot;], &quot;echo&quot;:False, # Echo the prompt in the output &quot;top_k&quot;:1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value &gt; 1 for sampling decoding } ## Run inference prompt = &quot;The meaning of life is &quot; res = llm(prompt, **generation_kwargs) # Res is a dictionary ## Unpack and the generated text from the LLM response dictionary and print it print(res[&quot;choices&quot;][0][&quot;text&quot;]) # res is short for result Keep in mind that mixtral is a fairly large model for most laptops and requires ~25+ GB RAM, so if you need a smaller model, try using one like llama-13b-chat-gguf (model_name=&quot;TheBloke/Llama-2-13B-chat-GGUF&quot;; model_file=&quot;llama-2-13b-chat.Q4_K_M.gguf&quot;) or mistral-7b-openorca-gguf (model_name=&quot;TheBloke/Mistral-7B-OpenOrca-GGUF&quot;; model_file=&quot;mistral-7b-openorca.Q4_K_M.gguf&quot;).",
    "context_chunks": [
      {
        "text": "In ctransformers library, I can only load around a dozen supported models. How can I run local inference on CPU (not just on GPU) from any open-source LLM quantized in the GGUF format (e.g. Llama 3, Mistral, Zephyr, i.e. ones unsupported in ctransformers)?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "llama-cpp-python is my personal choice, because it is easy to use and it is usually one of the first to support quantized versions of new models. To install it for CPU, just run pip install llama-cpp-python. Compiling for GPU is a little more involved, so I'll refrain from posting those instructions here since you asked specifically about CPU inference. I also recommend installing huggingface_hub (pip install huggingface_hub) to easily download models. Once you have both llama-cpp-python and huggingface_hub installed, you can download and use a model (e.g. mixtral-8x7b-instruct-v0.1-gguf) like so: ## Imports from huggingface_hub import hf_hub_download from llama_cpp import Llama ## Download the GGUF model model_name = &quot;TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF&quot; model_file = &quot;mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&quot; # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred model_path = hf_hub_download(model_name, filename=model_file) ## Instantiate model from downloaded file llm = Llama( model_path=model_path, n_ctx=16000, # Context length to use n_threads=32, # Number of CPU threads to use n_gpu_layers=0 # Number of model layers to offload to GPU ) ## Generation kwargs generation_kwargs = { &quot;max_tokens&quot;:20000, &quot;stop&quot;:[&quot;&lt;/s&gt;&quot;], &quot;echo&quot;:False, # Echo the prompt in the output &quot;top_k&quot;:1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value &gt; 1 for sampling decoding } ## Run inference prompt = &quot;The meaning of life is &quot; res = llm(prompt, **generation_kwargs) # Res is a dictionary ## Unpack and the generated text from the LLM response dictionary and print it print(res[&quot;choices&quot;][0][&quot;text&quot;]) # res is short for result Keep in mind that mixtral is a fairly large model for most laptops and requires ~25+ GB RAM, so if you need a smaller model, try using one like llama-13b-chat-gguf (model_name=&quot;TheBloke/Llama-2-13B-chat-GGUF&quot;; model_file=&quot;llama-2-13b-chat.Q4_K_M.gguf&quot;) or mistral-7b-openorca-gguf (model_name=&quot;TheBloke/Mistral-7B-OpenOrca-GGUF&quot;; model_file=&quot;mistral-7b-openorca.Q4_K_M.gguf&quot;).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Transformers now supports loading quantized models in GGUF format as unquantized versions, allowing them to be run like standard models. Please note that this feature is still experimental and subject to change: https://huggingface.co/docs/transformers/main/en/gguf from transformers import AutoTokenizer, AutoModelForCausalLM model_id = &quot;TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF&quot; filename = &quot;tinyllama-1.1b-chat-v1.0.Q6_K.gguf&quot; tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename) model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "artificial-intelligence",
        "cpu",
        "ctransformers"
      ],
      "question_score": 15,
      "answer_score": 23,
      "created": "2023-12-09T03:18:51",
      "question_id": 77630013,
      "answer_id": 77734862
    }
  },
  {
    "question": "Install jupyter despite pin-1 not installable because it requires python 3.12",
    "expected_answer": "You can do this: conda install python=3.10 then, rerun this command: conda install jupyter",
    "context_chunks": [
      {
        "text": "Attempting to install jupyter with conda install jupyter, from an Anaconda prompt, raised pin-1 in not installable because it requires python 3.12 I have python 3.11 installed, and the version I installed of miniconda uses python 3.11. Any other way to install jupyter? Full error traceback: Collecting package metadata (repodata.json): done Solving environment: / warning libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE failed LibMambaUnsatisfiableError: Encountered problems while solving: - package jupyter-1.0.0-py27_4 requires python &gt;=2.7,&lt;2.8.0a0, but none of the providers can be installed Could not solve for environment specs The following packages are incompatible ├─ jupyter is installable with the potential options │ ├─ jupyter 1.0.0 would require │ │ └─ python &gt;=2.7,&lt;2.8.0a0 , which can be installed; │ ├─ jupyter 1.0.0 would require │ │ └─ python &gt;=3.10,&lt;3.11.0a0 , which can be installed; │ ├─ jupyter 1.0.0 would require │ │ └─ python &gt;=3.11,&lt;3.12.0a0 , which can be installed; │ ├─ jupyter 1.0.0 would require │ │ └─ python &gt;=3.5,&lt;3.6.0a0 , which can be installed; │ ├─ jupyter 1.0.0 would require │ │ └─ python &gt;=3.6,&lt;3.7.0a0 , which can be installed; │ ├─ jupyter 1.0.0 would require │ │ └─ python &gt;=3.7,&lt;3.8.0a0 , which can be installed; │ ├─ jupyter 1.0.0 would require │ │ └─ python &gt;=3.8,&lt;3.9.0a0 , which can be installed; │ └─ jupyter 1.0.0 would require │ └─ python &gt;=3.9,&lt;3.10.0a0 , which can be installed; └─ pin-1 is not installable because it requires └─ python 3.12.* , which conflicts with any installable versions previously reported. Pins seem to be involved in the conflict. Currently pinned specs: - python 3.12.* (labeled as 'pin-1')",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can do this: conda install python=3.10 then, rerun this command: conda install jupyter",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "just install python with version between 3.10 - 3.11 in your environment i've just fix this issue with this command first: conda install python=3.10 then: conda install jupyter",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "jupyter-notebook",
        "jupyter",
        "miniconda",
        "python-3.11"
      ],
      "question_score": 15,
      "answer_score": 13,
      "created": "2023-11-18T19:16:12",
      "question_id": 77508384,
      "answer_id": 77560929
    }
  },
  {
    "question": "FBGEMM load error trying to use PyTorch on Windows",
    "expected_answer": "Uninstalled all: pip uninstall torch torchvision torchaudio Then installed the one-step older version: pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 Now, it's working fine. Initially, I got this problem after the installation of torchvision, torch was working fine before that.",
    "context_chunks": [
      {
        "text": "I'm working on a code that uses Whisper, and I need PyTorch with CUDA to improve the speed of the model execution, I have CUDA installed (verified using nvidia-smi command where it shows that I have CUDA 12.6) and I installed PyTorch using the command pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121, but when I try to import torch in Python (import torch) I get the error: OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\\Users\\Windows10\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\fbgemm.dll&quot; or one of its dependencies. I tried uninstalling PyTorch completely and reinstalling several times but it didn't work, apparently the installation is successful but for some reason Python cannot access it, i have Python 3.11.3. Any help is appreciated",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Uninstalled all: pip uninstall torch torchvision torchaudio Then installed the one-step older version: pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 Now, it's working fine. Initially, I got this problem after the installation of torchvision, torch was working fine before that.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "as you have found out yourself, let's summarize it briefly because it's difficult to read the solution between comments. To fix this problem we need to add the missing dependency libomp140.x86_64.dll to the C:\\Windows\\System32 directory. To achieve this, we need to install the C++ build tools, as pointed out in: https://github.com/pytorch/pytorch/issues/131662#issuecomment-2252589253. Follow the issue in the official PyTorch repository on GitHub to stay up to date on this issue: https://github.com/pytorch/pytorch/issues/131662 EDIT: will be fixed in version 2.4.1",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pytorch"
      ],
      "question_score": 15,
      "answer_score": 18,
      "created": "2024-07-24T03:44:58",
      "question_id": 78786306,
      "answer_id": 78799152
    }
  },
  {
    "question": "Problems installing libraries via pip after installing Python 3.12",
    "expected_answer": "python3.12 -m ensurepip --upgrade fixed my problem! solution",
    "context_chunks": [
      {
        "text": "Today I installed the new Python 3.12 on my Ubuntu 22.04 from the ppa repository ppa:deadsnakes/ppa. Everything works, but when I try to install some library with the command python3.12 -m pip install somelibrary, I get the following error ERROR: Exception: Traceback (most recent call last): File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/base_command.py&quot;, line 165, in exc_logging_wrapper status = run_func(*args) ^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/req_command.py&quot;, line 205, in wrapper return func(self, options, args) ^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/pip/_internal/commands/install.py&quot;, line 285, in run session = self.get_default_session(options) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/req_command.py&quot;, line 75, in get_default_session self._session = self.enter_context(self._build_session(options)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/req_command.py&quot;, line 89, in _build_session session = PipSession( ^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/pip/_internal/network/session.py&quot;, line 282, in __init__ self.headers[&quot;User-Agent&quot;] = user_agent() ^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/pip/_internal/network/session.py&quot;, line 157, in user_agent setuptools_dist = get_default_environment().get_distribution(&quot;setuptools&quot;) ^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/pip/_internal/metadata/__init__.py&quot;, line 24, in get_default_environment from .pkg_resources import Environment File &quot;/usr/lib/python3/dist-packages/pip/_internal/metadata/pkg_resources.py&quot;, line 9, in &lt;module&gt; from pip._vendor import pkg_resources File &quot;/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py&quot;, line 2164, in &lt;module&gt; register_finder(pkgutil.ImpImporter, find_on_path) ^^^^^^^^^^^^^^^^^^^ AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'? Any suggestions why this is happening? EDIT: This problem doesn't exist when I use venv, it seems to me that the problem is that pip uses /usr/lib/python3 instead of /usr/lib/python3.12",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "python3.12 -m ensurepip --upgrade fixed my problem! solution",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Here is what I did: curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pip"
      ],
      "question_score": 15,
      "answer_score": 20,
      "created": "2023-10-05T16:10:58",
      "question_id": 77238856,
      "answer_id": 77848441
    }
  },
  {
    "question": "How can I suppress ruff linting on a block of code",
    "expected_answer": "You did not specify this in your question, but if this is about ruff format, and you want to generally disable it for a block of code, you can actually do this using the fmt comment: # fmt: off def my_unformatted_function(): do_something() # fmt: on",
    "context_chunks": [
      {
        "text": "I would like to disable/suppress the ruff linter (or certain linting rules) on a block of code. I know that I can do this for single lines (by using # noqa: &lt;rule_code&gt; at the end of the line) or for entire files/folder (#ruff: noqa &lt;rule_code&gt; at the top of the file). However, I would like to disable the linter ruff on one function or on a multi-line block of code, but not an entire file. Is there a way to do this, without adding noqa to the end of each line?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You did not specify this in your question, but if this is about ruff format, and you want to generally disable it for a block of code, you can actually do this using the fmt comment: # fmt: off def my_unformatted_function(): do_something() # fmt: on",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Not currently possible (as of 29 Jan 2024). See here for updates on the issue",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "lint",
        "ruff"
      ],
      "question_score": 15,
      "answer_score": 16,
      "created": "2024-01-26T09:08:08",
      "question_id": 77885145,
      "answer_id": 78188042
    }
  },
  {
    "question": "session not created: This version of ChromeDriver only supports Chrome version 114",
    "expected_answer": "The versions of chrome (116) and chromedriver (114) do not match. This is because the latest version of chromedriver (as described by chromedriver.storage.googleapis.com/LATEST_RELEASE) is not necessarily always going to match the latest version of Chrome from the debian repo. Although these major versions will often match (which is why this has worked for you in the past), you cannot rely on this to be the case all the time, as you're now seeing. Instead, you should inspect the version of chrome and then install an appropriate version of chromedriver. As described on the chromedriver downloads page, you can use their API endpoint to find download links for various versions of chromedriver or find the links on the dashboard, both of which will include links to download versions of chromedriver that are compatible with chrome 116 -- for example at the time of writing: https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chromedriver-linux64.zip (but be aware this zip structure may be different than the download links you're already using). In my own dockerfile, I specify the chromedriver download URL manually and just run a script to test that the major versions match. Though, you could use the API endpoint mentioned above to automate getting the correct chromedriver URL. As for why chromedriver.storage.googleapis.com/LATEST_RELEASE points to major version 114 instead of 116, despite a download being available for 116 and the stable debian version being 116, I'm not really sure, to be honest.",
    "context_chunks": [
      {
        "text": "I am running a Docker image from a Docker container in AWS Batch environment. It was all working nicely for a while now, but since today I am getting the following error. E selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 114 E Current browser version is 116.0.5845.96 with binary path /opt/google/chrome/google-chrome The Dockerfile that has the chrome installation is as below FROM python:3.10 WORKDIR /usr/src/app COPY . . RUN pip install --trusted-host pypi.org --upgrade pip RUN pip install --no-cache-dir \\ --extra-index-url https://artifactory.int.csgdev01.citcosvc.com/artifactory/api/pypi/citco- pypi/simple \\ -r requirements.txt RUN pip install awscli RUN apt-get install -yqq unzip curl RUN apt-get -y update RUN apt-get install zip -y RUN apt-get install unzip -y RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - RUN curl -sS -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - RUN echo &quot;deb http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt;&gt; /etc/apt/sources.list.d/google-chrome.list RUN apt-get -y update RUN apt-get -y install -y google-chrome-stable # Install chrome driver RUN wget -N https://chromedriver.storage.googleapis.com/`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`/chromedriver_linux64.zip -P ~/ RUN unzip ~/chromedriver_linux64.zip -d ~/ RUN rm ~/chromedriver_linux64.zip RUN mv -f ~/chromedriver /usr/local/bin/chromedriver RUN chmod 0755 /usr/local/bin/chromedriver RUN ls -lt RUN ls -lt /usr/local/bin RUN chmod +x ./run.sh CMD [&quot;bash&quot;, &quot;./run.sh&quot;] My selenium python test class is below from selenium import webdriver import unittest class Test_SecTransferWorkflow(unittest.TestCase): options = webdriver.ChromeOptions() options.add_argument('--no-sandbox') options.add_argument(&quot;--enable-javascript&quot;) options.add_argument(&quot;--start-maximized&quot;) options.add_argument(&quot;--incognito&quot;) options.add_argument('--headless') options.add_argument('--ignore-certificate-errors') options.add_argument('--enable-features=NetworkService') options.add_argument('--shm-size=1g') options.add_argument('--disable-gpu') options.add_experimental_option(&quot;excludeSwitches&quot;, [&quot;enable-automation&quot;]) options.add_argument(&quot;--window-size=1920,1080&quot;) options.add_argument(&quot;--disable-extensions&quot;) options.add_argument('--disable-dev-shm-usage') options.add_experimental_option('useAutomationExtension', False) options.add_experimental_option(&quot;detach&quot;, True) options.add_argument('--allow-running-insecure-content') options.add_argument('--allow-insecure-localhost') options.add_argument('--ignore-ssl-errors=yes') options.add_argument('--user-agent=Chrome/77') driver = webdriver.Chrome(options=options) @classmethod def setUpClass(cls): try: cls.driver.delete_all_cookies() cls.driver.get(TestData_common.BASE_URL) time.sleep(2) except WebDriverException as e: print('Site down...&gt; ', e) cls.driver.delete_all_cookies() time.sleep(3) def test_001_login(self): if not TestData_common.URL_FOUND: pytest.skip('Site seems to be down...') self.loginPage = LoginPage(self.driver) self.loginPage.do_click_agree_button() self.driver.maximize_window() print('Successfully clicked on AGREE button...') time.sleep(2) I didn't have any issues running this image so far, until I encountered this error today. Any help is much appreciated.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The versions of chrome (116) and chromedriver (114) do not match. This is because the latest version of chromedriver (as described by chromedriver.storage.googleapis.com/LATEST_RELEASE) is not necessarily always going to match the latest version of Chrome from the debian repo. Although these major versions will often match (which is why this has worked for you in the past), you cannot rely on this to be the case all the time, as you're now seeing. Instead, you should inspect the version of chrome and then install an appropriate version of chromedriver. As described on the chromedriver downloads page, you can use their API endpoint to find download links for various versions of chromedriver or find the links on the dashboard, both of which will include links to download versions of chromedriver that are compatible with chrome 116 -- for example at the time of writing: https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chromedriver-linux64.zip (but be aware this zip structure may be different than the download links you're already using). In my own dockerfile, I specify the chromedriver download URL manually and just run a script to test that the major versions match. Though, you could use the API endpoint mentioned above to automate getting the correct chromedriver URL. As for why chromedriver.storage.googleapis.com/LATEST_RELEASE points to major version 114 instead of 116, despite a download being available for 116 and the stable debian version being 116, I'm not really sure, to be honest.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As above, https://chromedriver.chromium.org/downloads now suggests getting the download of chromedriver from https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json Try this out to get the latest v116: export CHROMEDRIVER_VERSION=116 export CHROMEDRIVER_URL=$(curl -s https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json | \\ jq -r --arg version &quot;$CHROMEDRIVER_VERSION&quot; '[.versions[] | select(.version | startswith($version + &quot;.&quot;))] | last | .downloads.chromedriver[] | select(.platform == &quot;linux64&quot;).url') (This assumes that the JSON file is sorted with older to newest, so we want the last one with version=&quot;160.x.y.z&quot;.) Note, the downloaded .zip is now a folder chromedriver-linux64 containing chromedriver executable. In a Dockerfile, try: ARG CHROMEDRIVER_VERSION='116' # Install Chrome WebDriver RUN CHROMEDRIVER_URL=$(curl -s https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json | \\ jq -r --arg version &quot;$CHROMEDRIVER_VERSION&quot; '[.versions[] | select(.version | startswith($version + &quot;.&quot;))] | last | .downloads.chromedriver[] | select(.platform == &quot;linux64&quot;).url') &amp;&amp; \\ mkdir -p /opt/chromedriver-$CHROMEDRIVER_VERSION &amp;&amp; \\ curl -sS -o /tmp/chromedriver_linux64.zip &quot;$CHROMEDRIVER_URL&quot; &amp;&amp; \\ unzip -qq /tmp/chromedriver_linux64.zip -d /opt/chromedriver-$CHROMEDRIVER_VERSION &amp;&amp; \\ rm /tmp/chromedriver_linux64.zip &amp;&amp; \\ chmod +x /opt/chromedriver-$CHROMEDRIVER_VERSION/chromedriver-linux64/chromedriver &amp;&amp; \\ ln -fs /opt/chromedriver-$CHROMEDRIVER_VERSION/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver FWIW, gpt4 helped me write the jq clause to parse the JSON https://chat.openai.com/share/ebe38666-9ea7-4bd4-9935-5430fec339f5 Also, here are all the other platforms available if you replace select(.platform == &quot;linux64&quot;) with one of the platforms below: curl -s https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json | \\ jq '[.versions[] | select(.version | startswith(&quot;116.&quot;))] | last | .downloads.chromedriver' [ { &quot;platform&quot;: &quot;linux64&quot;, &quot;url&quot;: &quot;...&quot; }, { &quot;platform&quot;: &quot;mac-arm64&quot;, &quot;url&quot;: &quot;...&quot; }, { &quot;platform&quot;: &quot;mac-x64&quot;, &quot;url&quot;: &quot;...&quot; }, { &quot;platform&quot;: &quot;win32&quot;, &quot;url&quot;: &quot;...&quot; }, { &quot;platform&quot;: &quot;win64&quot;, &quot;url&quot;: &quot;...&quot; } ]",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "amazon-web-services",
        "docker",
        "google-chrome",
        "selenium-webdriver"
      ],
      "question_score": 16,
      "answer_score": 5,
      "created": "2023-08-15T22:21:03",
      "question_id": 76909437,
      "answer_id": 76909779
    }
  },
  {
    "question": "import numpy failed after upgrading MacOS to 15.4",
    "expected_answer": "You can run the following from within the offending environment. 'Never run commands from the internet without checking out the script!' - source The command: curl -fsSL 'https://gist.githubusercontent.com/basnijholt/811307f125619c53ef876f8ea6ab6685/raw/c678b893c223c3ec3dec3bdb67937c5adc2fab7f/fix.sh' | bash The script: #!/bin/bash # filepath: fix_lib_paths.sh # https://github.com/conda-forge/numpy-feedstock/issues/347#issuecomment-2746317575 # Activate the conda environment and run this script. set -e LIB_PATH=&quot;$CONDA_PREFIX/lib&quot; # Find all dylib files find &quot;$LIB_PATH&quot; -name &quot;*.dylib&quot; -o -name &quot;*.so&quot; | while read -r library; do echo &quot;Processing $library...&quot; # Extract all LC_RPATH entries rpaths=$(otool -l &quot;$library&quot; | grep -A2 LC_RPATH | grep &quot;path &quot; | awk '{print $2}') # Create a temporary file to track seen rpaths temp_file=$(mktemp) # Check for duplicates and remove them echo &quot;$rpaths&quot; | while read -r rpath; do if [[ -z &quot;$rpath&quot; ]]; then continue fi if grep -q &quot;^$rpath$&quot; &quot;$temp_file&quot;; then echo &quot; Removing duplicate RPATH: $rpath&quot; install_name_tool -delete_rpath &quot;$rpath&quot; &quot;$library&quot; || true else echo &quot;$rpath&quot; &gt;&gt; &quot;$temp_file&quot; echo &quot; Keeping RPATH: $rpath&quot; fi done # Re-sign the library echo &quot; Re-signing $library&quot; codesign --force --sign - &quot;$library&quot; || echo &quot; Warning: Could not sign $library&quot; # Clean up the temporary file rm -f &quot;$temp_file&quot; echo &quot;Done with $library&quot; echo &quot;-----------------------&quot; done echo &quot;All libraries processed!&quot; This worked for conda and mamba and should also work for pixi. It took less than a minute for a small environment.",
    "context_chunks": [
      {
        "text": "After upgrading MacOS to 15.4. the import failed: Exception has occurred: ImportError Error importing numpy: you should not try to import numpy from its source directory; please exit the numpy source tree, and relaunch your python interpreter from there. Error importing numpy: you should not try to import numpy from its source directory; please exit the numpy source tree, and relaunch your python interpreter from there. ImportError: dlopen(/opt/miniconda3/envs/ho/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libgfortran.5.dylib Referenced from: &lt;0B9C315B-A1DD-3527-88DB-4B90531D343F&gt; /opt/miniconda3/envs/ho/lib/libopenblas.0.dylib Reason: tried: '/opt/miniconda3/envs/ho/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/lib/python3.12/site-packages/numpy/_core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/lib/python3.12/site-packages/numpy/_core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/usr/local/lib/libgfortran.5.dylib' (no such file), '/usr/lib/libgfortran.5.dylib' (no such file, not in dyld cache) During handling of the above exception, another exception occurred: ImportError: IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for many reasons, often due to issues with your setup or how NumPy was installed. We have compiled some common reasons and troubleshooting tips at: https://numpy.org/devdocs/user/troubleshooting-importerror.html Please note and check the following: * The Python version is: Python3.12 from &quot;/opt/miniconda3/envs/ho/bin/python&quot; * The NumPy version is: &quot;2.0.0&quot; and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: dlopen(/opt/miniconda3/envs/ho/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libgfortran.5.dylib Referenced from: &lt;0B9C315B-A1DD-3527-88DB-4B90531D343F&gt; /opt/miniconda3/envs/ho/lib/libopenblas.0.dylib Reason: tried: '/opt/miniconda3/envs/ho/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/lib/python3.12/site-packages/numpy/_core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/lib/python3.12/site-packages/numpy/_core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/opt/miniconda3/envs/ho/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/usr/local/lib/libgfortran.5.dylib' (no such file), '/usr/lib/libgfortran.5.dylib' (no such file, not in dyld cache) The above exception was the direct cause of the following exception: File &quot;/Users/yf/Library/CloudStorage/OneDrive-Personal/Documents/Projects/hohoho/playground.py&quot;, line 1, in &lt;module&gt; import numpy as np ImportError: Error importing numpy: you should not try to import numpy from its source directory; please exit the numpy source tree, and relaunch your python interpreter from there. Everything was OK yesterday. The environment is created by miniconda. Python is 3.12.4, numpy is 2.0.0 I tried all ways below, but cannot solve the exception: Delete and recreate the environment, install the same package backup in yaml file before. Clean all conda cache, reinstall miniconda, recreate the environment, reinstall the packages. search and delete the numpy path in the sys.path before import: sys.path = [p for p in sys.path if not os.path.basename(p).lower() == &quot;numpy&quot;] Clone a new environment, upgrade all packages the latest version. I have a packed .app file created by the pyinstaller before, which worked well before upgrading the OS, but cannot run now and report the same error. Another Mac also installed the .app before, but cannot run after upgrading the OS. Any other way I can solve the problem please.. More details: previously OS version is 15.3 Macbook Pro M1 upgrade to NumPy 2.2.4 but not solve the problem:numpy 2.0.0-py312hb544834_0 --&gt; 2.2.4-py312h7c1f314_0 no gfortran in conda list another Macbook Air M1, which works fine with the pyinstaller build .app, and not any other python environment outside the .app, raise the same error after upgrade OS to 15.4",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can run the following from within the offending environment. 'Never run commands from the internet without checking out the script!' - source The command: curl -fsSL 'https://gist.githubusercontent.com/basnijholt/811307f125619c53ef876f8ea6ab6685/raw/c678b893c223c3ec3dec3bdb67937c5adc2fab7f/fix.sh' | bash The script: #!/bin/bash # filepath: fix_lib_paths.sh # https://github.com/conda-forge/numpy-feedstock/issues/347#issuecomment-2746317575 # Activate the conda environment and run this script. set -e LIB_PATH=&quot;$CONDA_PREFIX/lib&quot; # Find all dylib files find &quot;$LIB_PATH&quot; -name &quot;*.dylib&quot; -o -name &quot;*.so&quot; | while read -r library; do echo &quot;Processing $library...&quot; # Extract all LC_RPATH entries rpaths=$(otool -l &quot;$library&quot; | grep -A2 LC_RPATH | grep &quot;path &quot; | awk '{print $2}') # Create a temporary file to track seen rpaths temp_file=$(mktemp) # Check for duplicates and remove them echo &quot;$rpaths&quot; | while read -r rpath; do if [[ -z &quot;$rpath&quot; ]]; then continue fi if grep -q &quot;^$rpath$&quot; &quot;$temp_file&quot;; then echo &quot; Removing duplicate RPATH: $rpath&quot; install_name_tool -delete_rpath &quot;$rpath&quot; &quot;$library&quot; || true else echo &quot;$rpath&quot; &gt;&gt; &quot;$temp_file&quot; echo &quot; Keeping RPATH: $rpath&quot; fi done # Re-sign the library echo &quot; Re-signing $library&quot; codesign --force --sign - &quot;$library&quot; || echo &quot; Warning: Could not sign $library&quot; # Clean up the temporary file rm -f &quot;$temp_file&quot; echo &quot;Done with $library&quot; echo &quot;-----------------------&quot; done echo &quot;All libraries processed!&quot; This worked for conda and mamba and should also work for pixi. It took less than a minute for a small environment.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Activate the offending environment then: conda update --all worked for me after updating to Mac OS 15.4.1",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "macos",
        "numpy"
      ],
      "question_score": 15,
      "answer_score": 10,
      "created": "2025-04-02T09:41:54",
      "question_id": 79550279,
      "answer_id": 79566425
    }
  },
  {
    "question": "Why do I keep getting this Tensorflow related message in Selenium errors?",
    "expected_answer": "As browsermator already mentioned &quot;Created TensorFlow Lite XNNPACK delegate for CPU.&quot; is not an error message but simply an info (cf. https://github.com/google-ai-edge/mediapipe/issues/3017). It can therefore be suppressed by adding options.add_argument(&quot;--log-level=1&quot;) to your webdriver's options.",
    "context_chunks": [
      {
        "text": "I'm new to Selenium, and I've been trying to write some scripts for web scraping but sometimes when I make a mistake as part of the error message I see this line: &quot;Created TensorFlow Lite XNNPACK delegate for CPU.&quot; for example in the latest error it was this: &quot;[5900:21516:0425/160524.581:ERROR:ssl_client_socket_impl.cc(879)] handshake failed; returned -1, SSL error code 1, net_error -201 Created TensorFlow Lite XNNPACK delegate for CPU.&quot; why is TensorFlow mentioned here?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "As browsermator already mentioned &quot;Created TensorFlow Lite XNNPACK delegate for CPU.&quot; is not an error message but simply an info (cf. https://github.com/google-ai-edge/mediapipe/issues/3017). It can therefore be suppressed by adding options.add_argument(&quot;--log-level=1&quot;) to your webdriver's options.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For anyone just looking for an answer to the question of why/whether Selenium uses TensorFlow, some further research shows this is most likely not a Selenium info message at all but directly from Google Chrome/Chromium/ChromeDriver. TensorFlow is a Google project created at Google Brain, and it seems recent versions of Chrome/Chromium have either started using TensorFlow or started emitting the info message - I first noticed it from around version 129. Here's an example of the message being emitted by the Google Chrome command-line interface, and here's one from MediaPipe, another Google product. Both examples were posted around the time version 129 was released. What exactly Chrome/Chromium uses TensorFlow for would have to be answered by combing through its source code and/or documentation, but based on the above it seems definitive that this message is not related to Selenium in any way, and that Selenium itself doesn't use TensorFlow.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "tensorflow",
        "selenium-webdriver",
        "selenium-chromedriver"
      ],
      "question_score": 15,
      "answer_score": 9,
      "created": "2024-04-25T15:25:48",
      "question_id": 78385667,
      "answer_id": 78563111
    }
  },
  {
    "question": "Using bson.ObjectId in Pydantic v2",
    "expected_answer": "None of the above worked for me. I've followed Pydantic documentation to come up with this solution: from typing import Annotated, Any, Callable from bson import ObjectId from fastapi import FastAPI from pydantic import BaseModel, ConfigDict, Field, GetJsonSchemaHandler from pydantic.json_schema import JsonSchemaValue from pydantic_core import core_schema class _ObjectIdPydanticAnnotation: # Based on https://docs.pydantic.dev/latest/usage/types/custom/#handling-third-party-types. @classmethod def __get_pydantic_core_schema__( cls, _source_type: Any, _handler: Callable[[Any], core_schema.CoreSchema], ) -&gt; core_schema.CoreSchema: def validate_from_str(input_value: str) -&gt; ObjectId: return ObjectId(input_value) return core_schema.union_schema( [ # check if it's an instance first before doing any further work core_schema.is_instance_schema(ObjectId), core_schema.no_info_plain_validator_function(validate_from_str), ], serialization=core_schema.to_string_ser_schema(), ) PydanticObjectId = Annotated[ ObjectId, _ObjectIdPydanticAnnotation ] class User(BaseModel): model_config = ConfigDict(populate_by_name=True) id: PydanticObjectId = Field(alias='_id') name: str app = FastAPI() @app.get(&quot;/user/{id}&quot;) def get_usr(id: str) -&gt; User: # Here we would connect to MongoDB and return the user. # Method is here just to test that FastAPI will not complain about the &quot;User&quot; return type pass # Some usage examples user1 = User(_id=ObjectId('64cca8a68efc81fc425aa864'), name='John Doe') user2 = User(_id='64cca8a68efc81fc425aa864', name='John Doe') assert user1 == user2 # Can use str and ObjectId interchangeably # Serialization assert repr(user1) == &quot;User(id=ObjectId('64cca8a68efc81fc425aa864'), name='John Doe')&quot; assert user1.model_dump() == {'id': '64cca8a68efc81fc425aa864', 'name': 'John Doe'} assert user1.model_dump_json() == '{&quot;id&quot;:&quot;64cca8a68efc81fc425aa864&quot;,&quot;name&quot;:&quot;John Doe&quot;}' # Deserialization user2 = User.model_validate_json('{&quot;id&quot;:&quot;64cca8a68efc81fc425aa864&quot;,&quot;name&quot;:&quot;John Doe&quot;}') user3 = User.model_validate_json('{&quot;_id&quot;:&quot;64cca8a68efc81fc425aa864&quot;,&quot;name&quot;:&quot;John Doe&quot;}') assert user1 == user2 == user3 user4 = User(_id=ObjectId(), name='Jane Doe') # Default ObjectId constructor # Validation user5 = User(_id=ObjectId('qwe'), name='Jack Failure') # Will throw bson.errors.InvalidId",
    "context_chunks": [
      {
        "text": "I found some examples on how to use ObjectId within BaseModel classes. Basically, this can be achieved by creating a Pydantic-friendly class as follows: class PyObjectId(ObjectId): @classmethod def __get_validators__(cls): yield cls.validate @classmethod def validate(cls, v): if not ObjectId.is_valid(v): raise ValueError(&quot;Invalid objectid&quot;) return ObjectId(v) @classmethod def __modify_schema__(cls, field_schema): field_schema.update(type=&quot;string&quot;) However, this seems to be for Pydantic v1, as this mechanism has been superseeded by the __get_pydantic_core_schema__ classmethod. However, I have been unable to achieve an equivalent solution with Pydantic v2. Is it possible? What validators do I need? I tried to refactor things but was unable to get anything usable.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "None of the above worked for me. I've followed Pydantic documentation to come up with this solution: from typing import Annotated, Any, Callable from bson import ObjectId from fastapi import FastAPI from pydantic import BaseModel, ConfigDict, Field, GetJsonSchemaHandler from pydantic.json_schema import JsonSchemaValue from pydantic_core import core_schema class _ObjectIdPydanticAnnotation: # Based on https://docs.pydantic.dev/latest/usage/types/custom/#handling-third-party-types. @classmethod def __get_pydantic_core_schema__( cls, _source_type: Any, _handler: Callable[[Any], core_schema.CoreSchema], ) -&gt; core_schema.CoreSchema: def validate_from_str(input_value: str) -&gt; ObjectId: return ObjectId(input_value) return core_schema.union_schema( [ # check if it's an instance first before doing any further work core_schema.is_instance_schema(ObjectId), core_schema.no_info_plain_validator_function(validate_from_str), ], serialization=core_schema.to_string_ser_schema(), ) PydanticObjectId = Annotated[ ObjectId, _ObjectIdPydanticAnnotation ] class User(BaseModel): model_config = ConfigDict(populate_by_name=True) id: PydanticObjectId = Field(alias='_id') name: str app = FastAPI() @app.get(&quot;/user/{id}&quot;) def get_usr(id: str) -&gt; User: # Here we would connect to MongoDB and return the user. # Method is here just to test that FastAPI will not complain about the &quot;User&quot; return type pass # Some usage examples user1 = User(_id=ObjectId('64cca8a68efc81fc425aa864'), name='John Doe') user2 = User(_id='64cca8a68efc81fc425aa864', name='John Doe') assert user1 == user2 # Can use str and ObjectId interchangeably # Serialization assert repr(user1) == &quot;User(id=ObjectId('64cca8a68efc81fc425aa864'), name='John Doe')&quot; assert user1.model_dump() == {'id': '64cca8a68efc81fc425aa864', 'name': 'John Doe'} assert user1.model_dump_json() == '{&quot;id&quot;:&quot;64cca8a68efc81fc425aa864&quot;,&quot;name&quot;:&quot;John Doe&quot;}' # Deserialization user2 = User.model_validate_json('{&quot;id&quot;:&quot;64cca8a68efc81fc425aa864&quot;,&quot;name&quot;:&quot;John Doe&quot;}') user3 = User.model_validate_json('{&quot;_id&quot;:&quot;64cca8a68efc81fc425aa864&quot;,&quot;name&quot;:&quot;John Doe&quot;}') assert user1 == user2 == user3 user4 = User(_id=ObjectId(), name='Jane Doe') # Default ObjectId constructor # Validation user5 = User(_id=ObjectId('qwe'), name='Jack Failure') # Will throw bson.errors.InvalidId",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "None of the previous solutions worked for me when I want to use PyObjectId not only as type of a Pydantic model field but also for input parameters (path or query). But the ObjectIdField class in the most recent release of the pydantic-mongo package seems to work fine with Pydantic v2 and exactly reproduces the behaviour which I was used to from the Pydantic v1 implementation. If you do not want to install an additional dependency, you could also directly use the ObjectIdField implementation from the package sources to find at: https://github.com/jefersondaniel/pydantic-mongo/blob/f517e7161a8fb10002ef64881f092f6f84b40971/pydantic_mongo/fields.py UPDATE for Pydantic v2.4 This approach went well on Pydantic 2.3.0, but on Pydantic 2.4.2 the OpenAPI schema generation fails with the following exception when trying to access /docs: pydantic.errors.PydanticInvalidForJsonSchema: Cannot generate a JsonSchema for core_schema.PlainValidatorFunctionSchema ({'type': 'no-info', 'function': &lt;bound method ObjectIdField.validate of &lt;class 'ObjectIdField'&gt;&gt;}) I could solve this by excluding the validator function from the json_schema only. This is the full implementation, which works for me with Pydantic 2.4.2: from typing import Any from bson import ObjectId from pydantic_core import core_schema class PyObjectId(str): @classmethod def __get_pydantic_core_schema__( cls, _source_type: Any, _handler: Any ) -&gt; core_schema.CoreSchema: return core_schema.json_or_python_schema( json_schema=core_schema.str_schema(), python_schema=core_schema.union_schema([ core_schema.is_instance_schema(ObjectId), core_schema.chain_schema([ core_schema.str_schema(), core_schema.no_info_plain_validator_function(cls.validate), ]) ]), serialization=core_schema.plain_serializer_function_ser_schema( lambda x: str(x) ), ) @classmethod def validate(cls, value) -&gt; ObjectId: if not ObjectId.is_valid(value): raise ValueError(&quot;Invalid ObjectId&quot;) return ObjectId(value) UPDATE to limit serialization to JSON As pointed out in the comments, the solution above serializes ObjectId to str in both 'python' and 'json' mode. To limit this to 'json' mode, one could change the serialization parameter to: serialization=core_schema.plain_serializer_function_ser_schema( lambda x: str(x), when_used='json' )",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pydantic",
        "bson"
      ],
      "question_score": 14,
      "answer_score": 18,
      "created": "2023-07-14T10:35:39",
      "question_id": 76686888,
      "answer_id": 76837550
    }
  },
  {
    "question": "How to Resolve AttributeError: module &#39;fiona&#39; has no attribute &#39;path&#39;?",
    "expected_answer": "TL;DR update to geopandas==0.14.4 OR pin fiona to version 1.9.6 -- It seems fiona recently upgraded to 1.10.0 (as of 2024-09-04 01:14 UTC) and that may have broken some older versions of geopandas, which only depend on fiona being higher than some version, not lower than. Upon closer look, geopandas up to version 0.14.3 still calls fiona.path, but in version 0.14.4 it no longer does. So upgrading geopandas to 0.14.4 should fix it. Alternatively, forcing fiona to stay on version 1.9.6 should also work. NOTE: upgrading geopandas to &gt;=1.0 seems to remove fiona as a dependency altogether, so it will also solve this issue. But it opens up a whole new can of worms by removing geopandas.dataset. For details on that one, see How to get maps to geopandas after datasets are removed?",
    "context_chunks": [
      {
        "text": "I have a piece of code that was working fine until last week, but now it's failing with the following error: AttributeError: module 'fiona' has no attribute 'path' I’ve ensured that all the necessary libraries are installed and imported. Does anyone have any ideas on what might be going wrong or how I can resolve this issue? Thanks! pip install geopandas pip install fiona import geopandas as gpd import fiona countries = gpd.read_file(gpd.datasets.get_path(&quot;naturalearth_lowres&quot;))",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "TL;DR update to geopandas==0.14.4 OR pin fiona to version 1.9.6 -- It seems fiona recently upgraded to 1.10.0 (as of 2024-09-04 01:14 UTC) and that may have broken some older versions of geopandas, which only depend on fiona being higher than some version, not lower than. Upon closer look, geopandas up to version 0.14.3 still calls fiona.path, but in version 0.14.4 it no longer does. So upgrading geopandas to 0.14.4 should fix it. Alternatively, forcing fiona to stay on version 1.9.6 should also work. NOTE: upgrading geopandas to &gt;=1.0 seems to remove fiona as a dependency altogether, so it will also solve this issue. But it opens up a whole new can of worms by removing geopandas.dataset. For details on that one, see How to get maps to geopandas after datasets are removed?",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "(Feb 20, 2025) We just ran into this with Anaconda; in particular, its most recent geopandas is 0.14.2, which when selected for installation, automatically installs fiona 1.10 as one of its dependencies. Unfortunately as noted above, this triggers the error. Forcing geopandas to 0.14.4 is possible, but other things broke (including spyder). Things worked ok if you install the most recent geopandas and then separately downgrade the fiona to 1.9.5. (python v is 3.12).",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "dataframe",
        "databricks",
        "geopandas",
        "fiona"
      ],
      "question_score": 14,
      "answer_score": 24,
      "created": "2024-09-04T14:06:26",
      "question_id": 78949093,
      "answer_id": 78949565
    }
  },
  {
    "question": "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas",
    "expected_answer": "I think it is bug - BUG: incompatible dtype when creating string column with loc #55025 . In next version of pandas should be solved.",
    "context_chunks": [
      {
        "text": "I have the below code which for instance work as excepted but won't work in the future: total.name = 'New_Row' total_df = total.to_frame().T total_df.at['New_Row', 'CURRENCY'] = '' total_df.at['New_Row', 'MANDATE'] = Portfolio total_df.at['New_Row', 'COMPOSITE'] = 'GRAND TOTAL' total_df.set_index('COMPOSITE',inplace=True) since an error is thrown in FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'GRAND TOTAL' has dtype incompatible with float64, please explicitly cast to a compatible dtype first. total_df.at['New_Row', 'COMPOSITE'] = 'GRAND TOTAL' How to fix this? variable total is: CURRENCY MANDATE Mandate_Test USD AMOUNT 123 LOCAL AMOUNT 12 Beg. Mkt 123 End. Mkt 456 Name: New_Row, dtype: object",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I think it is bug - BUG: incompatible dtype when creating string column with loc #55025 . In next version of pandas should be solved.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I had the same problem. I solved it by setting the whole column to None and then making it string type. This should probably also work with rows (as in your case) or cells etc... df['CURRENCY'] = None df['CURRENCY'] = df['CURRENCY'].astype(str)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas"
      ],
      "question_score": 15,
      "answer_score": 10,
      "created": "2023-09-27T08:03:42",
      "question_id": 77185621,
      "answer_id": 77185669
    }
  },
  {
    "question": "Why do I need another pair of curly braces when using a variable in a format specifier in Python f-strings?",
    "expected_answer": "The braces are needed because the part starting with : is the format specification mini-language. That is parsed as its own f-string, where all is literal, except if put in braces. That format specification language would bump into ambiguities if those braces were not required: that language gives meaning to certain letters, and it would not be clear whether such letter(s) would need to be taken literally or as an expression to be evaluated dynamically. For instance, in Python 3.11+ we have the z option that can occur at the position where you specified the width: z = 5 f&quot;The result is {(a + b):&lt;z.2f}end&quot; So what would this mean if braces were not needed? Would it be the z option or what the z variable represents? There are many more examples that could be constructed to bring the same conclusion home: the braces are needed for anything that needs to be evaluated inside the format specification part.",
    "context_chunks": [
      {
        "text": "I'm learning how Python f-strings handle formatting and came across this syntax: a = 5.123 b = 2.456 width = 10 result = f&quot;The result is {(a + b):&lt;{width}.2f}end&quot; print(result) This works as expected, however I don't understand why {width} needs its own curly braces within the format specification. Why can't I just use width directly just as &quot;a&quot; and &quot;b&quot;? Isn't width already inside the outer curly braces?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The braces are needed because the part starting with : is the format specification mini-language. That is parsed as its own f-string, where all is literal, except if put in braces. That format specification language would bump into ambiguities if those braces were not required: that language gives meaning to certain letters, and it would not be clear whether such letter(s) would need to be taken literally or as an expression to be evaluated dynamically. For instance, in Python 3.11+ we have the z option that can occur at the position where you specified the width: z = 5 f&quot;The result is {(a + b):&lt;z.2f}end&quot; So what would this mean if braces were not needed? Would it be the z option or what the z variable represents? There are many more examples that could be constructed to bring the same conclusion home: the braces are needed for anything that needs to be evaluated inside the format specification part.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The grammar for f-string is (emphasis mine) f_string ::= (literal_char | \"{{\" | \"}}\" | replacement_field)* replacement_field ::= \"{\" f_expression [\"=\"] [\"!\" conversion] [\":\" format_spec] \"}\" f_expression ::= (conditional_expression | \"*\" or_expr) (\",\" conditional_expression | \",\" \"*\" or_expr)* [\",\"] | yield_expression conversion ::= \"s\" | \"r\" | \"a\" format_spec ::= (literal_char | replacement_field)* literal_char ::= &lt;any code point except \"{\", \"}\" or NULL&gt; It says a replacement_field should have f_expression along with optional [&quot;:&quot; format_spec]. format_spec says it can have a replacement_field. According to the grammar it allows having replacement_field that has format_spec with a replacement_field. so, f&quot;{0.777:{'.02f'}}&quot; is a valid string. replacement_field -&gt; {0.777:{'.02f'}} | | -------- format_spec with replacement_field Another example, f&quot;{10.12345466768789:{0.100:{'.2f' if True else ''}}}&quot; # '10.12345467' # replacement_field -&gt; {10.12345466768789:{0.100:{'.2f' if True else ''}}} # format_spec with replacement_field -&gt; {0.100:{'.2f' if True else ''}} # the above format_spec has replacement_field with format_spec # Breakdown - {'.2f' if True else ''} -&gt; .2f - {0.100:{'.2f' if True else ''}} {0.100:.2f} -&gt; 0.10 - f&quot;{10.12345466768789:{0.100:{'.2f' if True else ''}}}&quot; f&quot;{10.12345466768789:0.10}&quot; -&gt; '10.12345467' Let's say you want to left-pad with char x stored in a variable a. a = 'x' f&quot;{10.022:a&gt;10.2f}&quot; # 'aaaaa10.02' # with {} f&quot;{10.022:{a}&gt;10.2f}&quot; # 'xxxxx10.02' # If the padding character is either { or } f&quot;{10.022:{'{'}&lt;10}&quot; # '10.022{{{{' # Without {} # f&quot;{10.022:{&lt;10}&quot; # Error f-strings allows to write conditional_expressions in f_expressions. Without {} it would be very hard to parse the format_spec. pos = False approx = True f&quot;{-0.00000 :{'z' if pos else ''}{'.2f' if approx else '.7f'}}&quot; # '-0.00' f&quot;{-0.00000 :{'z' if not pos else ''}{'.2f' if not approx else '.7f'}}&quot; # '0.0000000' # Without {} it would be impossible to parse through this. # f&quot;{0.777:z if pos else ''.2f if approx else .7f}&quot; # error",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "f-string"
      ],
      "question_score": 14,
      "answer_score": 22,
      "created": "2024-11-07T19:47:44",
      "question_id": 79167901,
      "answer_id": 79167979
    }
  },
  {
    "question": "Pydantic model fields with typing.Optional[] vs. typing.Optional[] = None",
    "expected_answer": "Whilst the previous answer is correct for pydantic v1, note that pydantic v2, released 2023-06-30, changed this behavior. As specified in the migration guide: Pydantic V2 changes some of the logic for specifying whether a field annotated as Optional is required (i.e., has no default value) or not (i.e., has a default value of None or any other value of the corresponding type), and now more closely matches the behavior of dataclasses. Similarly, fields annotated as Any no longer have a default value of None.",
    "context_chunks": [
      {
        "text": "What is the distinction between implicitly setting an optional attribute to None with typing.Optional[] versus explicitly assigning typing.Optional[] = None when creating Pydantic models? In both cases, the attribute will eventually have a value of None when the class object is instantiated. import typing import pydantic class Bar(pydantic.BaseModel): a: typing.Optional[int] @pydantic.validator('a', always=True, pre=True) def check_a(cls, v, values, field): print(&quot;WITHOUT NONE&quot;) print(values) print(field) return v class Baz(pydantic.BaseModel): a: typing.Optional[int] = None @pydantic.validator('a', always=True, pre=True) def check_a(cls, v, values, field): print(&quot;WITH NONE&quot;) print(values) print(field) return v print(Bar()) print(Baz()) Output: WITHOUT NONE {} name='a' type=Optional[int] required=False default=None a=None WITH NONE {} name='a' type=Optional[int] required=False default=None a=None",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Whilst the previous answer is correct for pydantic v1, note that pydantic v2, released 2023-06-30, changed this behavior. As specified in the migration guide: Pydantic V2 changes some of the logic for specifying whether a field annotated as Optional is required (i.e., has no default value) or not (i.e., has a default value of None or any other value of the corresponding type), and now more closely matches the behavior of dataclasses. Similarly, fields annotated as Any no longer have a default value of None.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Attention, this only holds for v1. For v2, check the answer by @elachere. TL;DR There is no difference. At least in terms of functionality. One implicitly sets the default value as None (unless another default is specified). The other explicitly sets the default as None. Details A deliberate (?) inconsistency You may already know that Optional[T] is equivalent to Union[T, None] or T | None (in the newer notation). In all other cases with types T and U simply annotating a field as a: T | U and not providing an explicit default value would make that a required field. In the Pydantic lingo that means you cannot instantiate the model without (somehow) providing a value for that field and that value will have to be either of type T or of type U to pass validation. The union T | None is an exception to this rule that is (arguably) not obvious, so you might call it an inconsistency. But it seems to be intentional. Annotating a field with such a union (so for example a: Optional[T]) will always create a field that is not required and has the default value None (unless of course you specify some other default). Implementation details Under the hood, this happens during model creation after a ModelField instance has been all but created and its prepare method is called. It calls the _type_analysis method to determine more details about the field type. And after recognizing the type origin to be a union, its type arguments are looked at in turn and once one of them is determined to be the NoneType, the field's required attribute is set to False and its allow_none attribute to True. Then, a bit later because the field still has an undefined default attribute, that attribute is set to None. That last step is obviously skipped, if you explicitly define a default value (None or some instance of T) or a default factory. Runtime implications In practice this means that for the following model all fields behave identically in the sense that neither is required during initialization because all of them will receive None as their value, when no other value is provided, and the type of each of them is set to be Optional[str]. from typing import Optional, Union from pydantic import BaseModel, Field class Model(BaseModel): a: str | None b: Union[str, None] c: Optional[str] d: Optional[str] = None e: Optional[str] = Field(default=None) f: Optional[str] = Field(default_factory=lambda: None) g: str = None obj = Model() print(obj.json(indent=4)) Output: { &quot;a&quot;: null, &quot;b&quot;: null, &quot;c&quot;: null, &quot;d&quot;: null, &quot;e&quot;: null, &quot;f&quot;: null, &quot;g&quot;: null } Note that even g is implicitly handled in a way that sets its type to be Optional[str], even though we annotate it with just str. You can verify this by doing the following: for field in Model.__fields__.values(): print(repr(field)) Output: ModelField(name='a', type=Optional[str], required=False, default=None) ModelField(name='b', type=Optional[str], required=False, default=None) ModelField(name='c', type=Optional[str], required=False, default=None) ModelField(name='d', type=Optional[str], required=False, default=None) ModelField(name='e', type=Optional[str], required=False, default=None) ModelField(name='f', type=Optional[str], required=False, default_factory='&lt;function &lt;lambda&gt;&gt;') ModelField(name='g', type=Optional[str], required=False, default=None) By the way, you can also make a field of the type Optional[T] and required. To do that you simply have to set the default to the ellipsis ..., so field: Optional[str] = ... for example would make that field accept None as a value, but still require you to provide a value during initialization. (see docs) Clean code considerations This is more subjective of course, but I would suggest adhering to the Zen of Python: Explicit is better than implicit. Especially since this behavior is so inconsistent and not obvious unless you are already familiar with Pydantic, you should not omit the default value even though you can. It just takes you a few more characters to add = None to the field definition, but it is much clearer for you later on or some other person reading the code. I would also not recommend omitting the NoneType from the annotation. The reason is the same. Field g from the previous example might work just fine, but it undergoes an implicit change of its type. My recommendation is to do field: Optional[str] = None, if you are dealing with just one additional type and field: str | int | None, if there are more types (or the older Union notation, if you are on Python &lt;=3.9).",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-typing",
        "pydantic"
      ],
      "question_score": 14,
      "answer_score": 15,
      "created": "2023-06-13T15:27:44",
      "question_id": 76466468,
      "answer_id": 76857618
    }
  },
  {
    "question": "FutureWarning: Passing literal html to &#39;read_html&#39; is deprecated and will be removed in a future version",
    "expected_answer": "Instead: line_score = pd.read_html(str(soup), attrs = {'id': 'line_score'})[0] Use io.StringIO: from io import StringIO def parse_html(box_scores): with open(box_scores) as f: html = f.read() soup = BeautifulSoup(html, features=&quot;lxml&quot;) [s.decompose() for s in soup.select(&quot;tr.over_header&quot;)] [s.decompose() for s in soup.select(&quot;tr.theader&quot;)] return soup def read_line_score(soup): line_score = pd.read_html(StringIO(str(soup)), attrs = {'id': 'line_score'})[0] cols = list(line_score.columns) cols[0] = &quot;team&quot; cols[-1] = &quot;total&quot; line_score.columns = cols line_score = line_score[[&quot;team&quot;, &quot;total&quot;]] return line_score def read_stats(soup, team, stat): df = pd.read_html(StringIO(str(soup)), attrs={&quot;id&quot;: f&quot;box-{team}-game-{stat}&quot;}, index_col=0)[0] df = df.apply(pd.to_numeric, errors=&quot;coerce&quot;) return df",
    "context_chunks": [
      {
        "text": "I'm getting this error when wrapping a soup element in a str. I'm trying to parse a table with pandas. I'm getting the correct output but also this warning: &quot;FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object def parse_html(box_scores): with open(box_scores) as f: html = f.read() soup = BeautifulSoup(html, features=&quot;lxml&quot;) [s.decompose() for s in soup.select(&quot;tr.over_header&quot;)] [s.decompose() for s in soup.select(&quot;tr.theader&quot;)] return soup def read_line_score(soup): line_score = pd.read_html(str(soup), attrs = {'id': 'line_score'})[0] cols = list(line_score.columns) cols[0] = &quot;team&quot; cols[-1] = &quot;total&quot; line_score.columns = cols line_score = line_score[[&quot;team&quot;, &quot;total&quot;]] return line_score def read_stats(soup, team, stat): df = pd.read_html(str(soup), attrs={&quot;id&quot;: f&quot;box-{team}-game-{stat}&quot;}, index_col=0)[0] df = df.apply(pd.to_numeric, errors=&quot;coerce&quot;) return df",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Instead: line_score = pd.read_html(str(soup), attrs = {'id': 'line_score'})[0] Use io.StringIO: from io import StringIO def parse_html(box_scores): with open(box_scores) as f: html = f.read() soup = BeautifulSoup(html, features=&quot;lxml&quot;) [s.decompose() for s in soup.select(&quot;tr.over_header&quot;)] [s.decompose() for s in soup.select(&quot;tr.theader&quot;)] return soup def read_line_score(soup): line_score = pd.read_html(StringIO(str(soup)), attrs = {'id': 'line_score'})[0] cols = list(line_score.columns) cols[0] = &quot;team&quot; cols[-1] = &quot;total&quot; line_score.columns = cols line_score = line_score[[&quot;team&quot;, &quot;total&quot;]] return line_score def read_stats(soup, team, stat): df = pd.read_html(StringIO(str(soup)), attrs={&quot;id&quot;: f&quot;box-{team}-game-{stat}&quot;}, index_col=0)[0] df = df.apply(pd.to_numeric, errors=&quot;coerce&quot;) return df",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "you can use io.StringIO to create a text buffer and then read the HTML content from it. from io import StringIO def parse_html(box_scores): with open(box_scores) as f: html = f.read() soup = BeautifulSoup(html, features=&quot;lxml&quot;) [s.decompose() for s in soup.select(&quot;tr.over_header&quot;)] [s.decompose() for s in soup.select(&quot;tr.theader&quot;)] # Use StringIO to wrap the HTML content html_buffer = StringIO(html) return BeautifulSoup(html_buffer, features=&quot;lxml&quot;)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "beautifulsoup"
      ],
      "question_score": 15,
      "answer_score": 13,
      "created": "2024-01-02T19:24:02",
      "question_id": 77748143,
      "answer_id": 77748177
    }
  },
  {
    "question": "Cannot properly create a virtual enviroment in Python 3.12",
    "expected_answer": "This worked for me: sudo apt install python3.12-venv and then python3.12 -m venv new_venv",
    "context_chunks": [
      {
        "text": "I use the command line python3 -m venv .venv and a new .venv module is created but an error appears: Error: Command '['/home/achu/Documentos/portfolio2023/.venv/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1 the bin directory doesn't contain the activate file. I've tried $ apt install python3.12-dev python3.12-venv",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This worked for me: sudo apt install python3.12-venv and then python3.12 -m venv new_venv",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Check if pip installed. Check if you using python3.12, and not other versions of python3. curl -sSL https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3.12 get-pip.py",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "virtualenv",
        "python-venv"
      ],
      "question_score": 14,
      "answer_score": 35,
      "created": "2023-10-04T13:58:28",
      "question_id": 77230254,
      "answer_id": 77352397
    }
  },
  {
    "question": "Solving incompatible dtype warning for pandas DataFrame when setting new column iteratively",
    "expected_answer": "I had the same problem. My intuition of this is that when you are setting value for the first time to the column source_data_url, the column does not yet exists, so pandas creates a column source_data_url and assigns value NaN to all of its elements. This makes Pandas think that the column's dtype is float64. Then it raises this warning. My solution was to create the column with some default value, e.g. empty string, before adding values to it: df[&quot;source_data_url&quot;] = &quot;&quot; or None seems also to work: df[&quot;source_data_url&quot;] = None",
    "context_chunks": [
      {
        "text": "Setting the value of a new dataframe column: df.loc[df[&quot;Measure] == metric.label, &quot;source_data_url&quot;] = metric.source_data_url now (as of Pandas version 2.1.0) gives a warning, FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value ' metric_3' has dtype incompatible with float64, please explicitly cast to a compatible dtype first. The Pandas documentation discusses how the problem can be solved for a Series but it is not clear how to do this iteratively (the line above is called in a loop over metrics and it's the final metric that gives the warning) when assigning a new DataFrame column. How can this be done?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I had the same problem. My intuition of this is that when you are setting value for the first time to the column source_data_url, the column does not yet exists, so pandas creates a column source_data_url and assigns value NaN to all of its elements. This makes Pandas think that the column's dtype is float64. Then it raises this warning. My solution was to create the column with some default value, e.g. empty string, before adding values to it: df[&quot;source_data_url&quot;] = &quot;&quot; or None seems also to work: df[&quot;source_data_url&quot;] = None",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Since Pandas 2.1.0 setitem-like operations on Series (or DataFrame columns) which silently upcast the dtype are deprecated and show a warning. In a future version, these will raise an error and you should cast to a common dtype first. Previous behavior: In [1]: ser = pd.Series([1, 2, 3]) In [2]: ser Out[2]: 0 1 1 2 2 3 dtype: int64 In [3]: ser[0] = 'not an int64' In [4]: ser Out[4]: 0 not an int64 1 2 2 3 dtype: object New behavior: In [1]: ser = pd.Series([1, 2, 3]) In [2]: ser Out[2]: 0 1 1 2 2 3 dtype: int64 In [3]: ser[0] = 'not an int64' FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'not an int64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first. In [4]: ser Out[4]: 0 not an int64 1 2 2 3 dtype: object To retain the current behaviour, you could cast ser to object dtype first: In [21]: ser = pd.Series([1, 2, 3]) In [22]: ser = ser.astype('object') In [23]: ser[0] = 'not an int64' In [24]: ser Out[24]: 0 not an int64 1 2 2 3 dtype: object Source: https://pandas.pydata.org/docs/dev/whatsnew/v2.1.0.html#deprecated-silent-upcasting-in-setitem-like-series-operations",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ],
      "question_score": 14,
      "answer_score": 13,
      "created": "2023-09-13T14:38:38",
      "question_id": 77098113,
      "answer_id": 77134156
    }
  },
  {
    "question": "Python3 pickle: Expected type &#39;SupportsWrite[bytes]&#39;, got &#39;BinaryIO&#39; instead",
    "expected_answer": "The warning is erroneous. You can suppress it as follows: import pickle fruits = ['apples', 'oranges', 'banana'] with open('myData.pkl', 'wb') as f: # noinspection PyTypeChecker pickle.dump(fruits, f) Note: This is specific to PyCharm",
    "context_chunks": [
      {
        "text": "A simple program in PyCharm 2024.2.3.(Community Edition) import pickle fruits = ['apples', 'oranges', 'banana'] with open('myData.pkl', 'wb') as f: pickle.dump(fruits, f) PyCharm gives me a warning: Expected type 'SupportsWrite[bytes]', got 'BinaryIO' instead The program runs correctly, but how do I get rid of this warning? Python tutorials say that this simple usage of pickle should work.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The warning is erroneous. You can suppress it as follows: import pickle fruits = ['apples', 'oranges', 'banana'] with open('myData.pkl', 'wb') as f: # noinspection PyTypeChecker pickle.dump(fruits, f) Note: This is specific to PyCharm",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The issue is a type hint issue, but it's not an error. To solve the warning, you can use BinaryIO. I provide code for it. It will solve the warning. import pickle from typing import cast, BinaryIO fruits = ['apples', 'oranges', 'banana'] with open('myData.pkl', 'wb') as f: f = cast(BinaryIO, f) pickle.dump(fruits, f) Hope it helps you.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pycharm",
        "pickle",
        "python-typing"
      ],
      "question_score": 14,
      "answer_score": 12,
      "created": "2024-10-03T06:50:49",
      "question_id": 79049420,
      "answer_id": 79049514
    }
  },
  {
    "question": "Finding solutions to linear system of equations with integer constraint in scipy",
    "expected_answer": "I'm not sure I grasp all the details of your project, but if your goal is to determine which variables are necessarily 0 or 1 (or constants), wouldn't symbolic mathematics help here rather than trying to find many numeric solutions? For instance, using sympy: import sympy x1, x2, x3, x4, x5 = sympy.symbols('x1 x2 x3 x4 x5') eq1 = sympy.Eq(x1 + x2 + x3, 2) eq2 = sympy.Eq(x1 + x4, 1) eq3 = sympy.Eq(x2 + x1, 1) eq4 = sympy.Eq(x3 + x5, 1) result = sympy.solve([eq1, eq2, eq3, eq4], (x1, x2, x3, x4, x5)) result is: {x1: 1 - x4, x2: x4, x3: 1, x5: 0} Which makes it easy to determine the values that are necessarily 0/1 (or any other constant): if result: for var, expr in result.items(): if expr.is_constant(): # or: if expr in {0, 1}: print(f'{var} is guaranteed to be {expr}') else: print('No possible solution') Output: x3 is guaranteed to be 1 x5 is guaranteed to be 0 Alternatively, if you want to input your equations from matrices: import sympy A = sympy.Matrix([[1, 1, 1, 0], # x1 + x2 + x3 = 2 [1, 0, 0, 1], # x1 + x4 = 1 [1, 1, 0, 0] # x1 + x2 = 1 ]) b = sympy.Matrix([2, 1, 1]) result = sympy.linsolve((A, b)) # {(1−𝜏0, 𝜏0, 1, 𝜏0)} constants = {f'x{i}': x for r in result for i, x in enumerate(r, start=1) if x.is_constant()} Output {'x3': 1}",
    "context_chunks": [
      {
        "text": "I have a system of equations where each equation is a linear equation with boolean constraints. For example: x1 + x2 + x3 = 2 x1 + x4 = 1 x2 + x1 = 1 And each x_i is either 0 or 1. Sometimes there might be a small positive (&lt;5) coefficient (for example x1 + 2 * x3 + x4 = 3. Basically a standard linear programming task. What I need to do is to find all x_i which are guaranteed to be 0 and all x_j which are guaranteed to be 1. Sorry if my terminology is not correct here but by guaranteed I mean that if you generate all possible solutions you in all of them all x_i will be 0 and in all of them x_j will be 1. For example my equation has only 2 solutions: 1, 0, 1, 0 0, 1, 1, 1 So you do not have guaranteed 0 and have x_3 as a guaranteed 1. I know how to solve this problem with or-tools by generating all solutions and it works for my usecases (equations are pretty constrained so usually there are &lt; 500 solutions although the number of variables is big enough to make the whole combinatorial search impossible). The big problem is that I can't use that library (system restrictions above my control) and only libraries available in my case are numpy and scipy. I found that scipy has scipy.optimize.linprog. It seems like I have found a way to generate one solution import numpy as np from scipy.optimize import linprog A_eq = np.array([ [1, 1, 1, 0], # x1 + x2 + x3 = 2 [1, 0, 0, 1], # x1 + x4 = 1 [1, 1, 0, 0] # x1 + x2 = 1 ]) b_eq = np.array([2, 1, 1]) c = np.zeros(4) bounds = [(0, 1)] * 4 res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs-ipm') if res.success: print(res.x) But I can't find a way to generate all solutions. Also I am not sure whether there is a better way to do it as all I need to know is to find guaranteed values.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I'm not sure I grasp all the details of your project, but if your goal is to determine which variables are necessarily 0 or 1 (or constants), wouldn't symbolic mathematics help here rather than trying to find many numeric solutions? For instance, using sympy: import sympy x1, x2, x3, x4, x5 = sympy.symbols('x1 x2 x3 x4 x5') eq1 = sympy.Eq(x1 + x2 + x3, 2) eq2 = sympy.Eq(x1 + x4, 1) eq3 = sympy.Eq(x2 + x1, 1) eq4 = sympy.Eq(x3 + x5, 1) result = sympy.solve([eq1, eq2, eq3, eq4], (x1, x2, x3, x4, x5)) result is: {x1: 1 - x4, x2: x4, x3: 1, x5: 0} Which makes it easy to determine the values that are necessarily 0/1 (or any other constant): if result: for var, expr in result.items(): if expr.is_constant(): # or: if expr in {0, 1}: print(f'{var} is guaranteed to be {expr}') else: print('No possible solution') Output: x3 is guaranteed to be 1 x5 is guaranteed to be 0 Alternatively, if you want to input your equations from matrices: import sympy A = sympy.Matrix([[1, 1, 1, 0], # x1 + x2 + x3 = 2 [1, 0, 0, 1], # x1 + x4 = 1 [1, 1, 0, 0] # x1 + x2 = 1 ]) b = sympy.Matrix([2, 1, 1]) result = sympy.linsolve((A, b)) # {(1−𝜏0, 𝜏0, 1, 𝜏0)} constants = {f'x{i}': x for r in result for i, x in enumerate(r, start=1) if x.is_constant()} Output {'x3': 1}",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You don't need to (fully) brute-force, and you don't need to find all of your solutions. You just need to find solutions for which each of your variables meets each of their extrema. The following is a fairly brain-off LP approach with 2n² columns and 2mn rows. It's sparse, and for your inputs does not need to be integral. That said, I somewhat doubt it will be the most efficient method possible. import numpy as np from scipy.optimize import milp, Bounds, LinearConstraint import scipy.sparse as sp lhs = np.array(( (1, 1, 1, 0), (1, 0, 0, 1), (1, 1, 0, 0), )) rhs = np.array((2, 1, 1)) m, n = lhs.shape # Variables: n * 2 (minimize, maximize) * n c = sp.kron( sp.eye_array(n), np.array(( (+1,), (-1,), )), ) b = np.tile(rhs, 2*n) system_constraint = LinearConstraint( A=sp.kron(sp.eye_array(2*n), lhs, format='csc'), lb=b, ub=b, ) result = milp( c=c.toarray().ravel(), # must be dense integrality=0, bounds=Bounds(lb=0, ub=1), constraints=system_constraint, ) assert result.success extrema = result.x.reshape((n, 2, n)) mins = extrema[:, 0] maxs = extrema[:, 1] vmins = np.diag(mins) vmaxs = np.diag(maxs) print('Solutions for minima on the diagonal:') print(mins) print('Solutions for maxima on the diagonal:') print(maxs) print('Variable minima:', vmins) print('Variable maxima:', vmaxs) print('Guaranteed 0:', vmaxs &lt; 0.5) print('Guaranteed 1:', vmins &gt; 0.5) Solutions for minima on the diagonal: [[-0. 1. 1. 1.] [ 1. 0. 1. -0.] [ 1. 0. 1. -0.] [ 1. 0. 1. -0.]] Solutions for maxima on the diagonal: [[ 1. 0. 1. -0.] [-0. 1. 1. 1.] [ 1. 0. 1. -0.] [-0. 1. 1. 1.]] Variable minima: [-0. 0. 1. -0.] Variable maxima: [1. 1. 1. 1.] Guaranteed 0: [False False False False] Guaranteed 1: [False False True False] There is a variant on this idea where rather than using sparse modelling, you just loop don't use LP at all fix each variable at each of its extrema, and iteratively column-eliminate from the left-hand side attempt a least-squares solution of the linear system, and infer a high residual to mean that there is no solution This somewhat naively assumes that all solutions will see integer values, and (unlike milp) does not have the option to set integrality=1. For demonstration I was forced to add a row to get a residual. import numpy as np lhs = np.array(( (1, 1, 1, 0), (1, 0, 0, 1), (1, 1, 0, 0), (0, 0, 1, 1), )) rhs = np.array((2, 1, 1, 1)) m, n = lhs.shape epsilon = 1e-12 lhs_select = np.ones(n, dtype=bool) for i in range(n): lhs_select[i] = False x0, (residual,), rank, singular = np.linalg.lstsq(lhs[:, lhs_select], rhs) zero_solves = residual &lt; epsilon x1, (residual,), rank, singular = np.linalg.lstsq(lhs[:, lhs_select], rhs - lhs[:, i]) one_solves = residual &lt; epsilon lhs_select[i] = True if zero_solves and not one_solves: print(f'x{i}=0, solution {x0.round(12)}') elif one_solves and not zero_solves: print(f'x{i}=1, solution {x1.round(12)}') x0=1, solution [-0. 1. 0.] x1=0, solution [ 1. 1. -0.] x2=1, solution [1. 0. 0.] x3=0, solution [ 1. -0. 1.]",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "scipy",
        "linear-programming",
        "scipy-optimize"
      ],
      "question_score": 13,
      "answer_score": 10,
      "created": "2025-01-09T09:59:05",
      "question_id": 79342159,
      "answer_id": 79342276
    }
  },
  {
    "question": "Using Ruff Linter with Python in VS Code",
    "expected_answer": "Take another look at the ruff documentation. You must enable or disable your desired linter rules and/or your formatting rules. For example, if you create a ruff.toml configuration in the root of your project with [lint] select = [&quot;ALL&quot;] the output looks more like what you expect.",
    "context_chunks": [
      {
        "text": "I have installed the Ruff extension and enabled it in VS Code, but it doesn't seem to be underlining my code at all and providing suggestions like my previous linters did. I did a clean install of VS Code, so most of the Python/Ruff extension settings are default. Is there an additional step I need to take to get it to start underlining my code and providing recommendations? Here is a screenshot: It's highlighting the imports for not being used, but I would expect other things to be highlighted like the line length, the additional spaces at the end of the file, not having 2 spaces before function declaration, etc. Here is the sample code as requested: import pandas as pd import numpy as np print('kkkkkkkkjlskdjflksdjflsdjflkdsjflksdjflkjdslkfjsdlkjflsdjflsdjfldsjflsdkjflsdjflksdjflksdjflksdjflskdjflsdkjfklsdjkl') def test_func(x): y=x+1 return y",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Take another look at the ruff documentation. You must enable or disable your desired linter rules and/or your formatting rules. For example, if you create a ruff.toml configuration in the root of your project with [lint] select = [&quot;ALL&quot;] the output looks more like what you expect.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Based on the answer by @anit3res, I had to use something like this in my pyproject.toml to get ruff going in vsc: [tool.ruff.lint] select = [&quot;ALL&quot;] [tool.ruff] line-length = 120 I also needed to restart the ruff server for this to take effect.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code",
        "ruff"
      ],
      "question_score": 14,
      "answer_score": 9,
      "created": "2024-01-20T17:54:48",
      "question_id": 77852109,
      "answer_id": 77853898
    }
  },
  {
    "question": "Processing requests in FastAPI sequentially while staying responsive",
    "expected_answer": "As you may have figured out already, the main issue in your example is that you run a synchronous blocking operation within an async def endpoint, which blocks the event loop (of the main thread), and hence, the entire server. As explained in this answer, if one has to use an async def endpoint, they could run such CPU-bound tasks in an external ProcessPool and then await it (using asyncio's loop.run_in_executor()), which would return control back to the event loop, thus allowing other tasks in the event loop to run, until that task is completed—please have a look at the linked answer above, as well as this answer for more details. As explained in the linked answers, when using ProcessPoolExecutor on Windows, it is important to protect the entry point of the program to avoid recursive spawning of subprocesses, etc. Basically, your code must be under if __name__ == '__main__' (as shown in the example below). I would also suggest using a lifespan handler, as demonstrated in this answer and this answer, instead of the deprecated startup and shutdown event handlers, to start the process_requests function, as well as instantiate the asyncio.Queue() and the ProcessPoolExecutor, and then add them to the request.state, so that they can be shared and re-used by every request/endpoint (especially, in the case of ProcessPoolExecutor, to avoid creating a new ProcessPool every time, as the computational costs for setting up processes can become expensive, when creating and destroying a lot of processes over and over). Further, I would suggest creating a unique ID for every request arrived, and return that ID to the client, so that they can use it to check on the status of their request, i.e., whether is completed or still pending processing. You could save that ID to your database storage (or a Key-Value store, such as Redis), as explained in this answer; however, for simplicity and demo purposes, the example belows uses a dict object for that purpose. It should also be noted that in the example below the ID is expected as a query parameter to the /status endpoint, but in real-world scenarios, you should never pass sensitive information to the query string, as this would pose security/privacy risks (see Solution 1 of this answer, where some of the risks are outlined). You should instead pass sensitive information to the request body, and always use the HTTPS protocol. Working Example from fastapi import FastAPI, Request from fastapi.responses import JSONResponse from contextlib import asynccontextmanager from dataclasses import dataclass from concurrent.futures import ProcessPoolExecutor import time import asyncio import uuid @dataclass class Item: id: str name: str # Simulating a Computationally Intensive Task def cpu_bound_task(item: Item): print(f&quot;Processing: {item.name}&quot;) time.sleep(15) return 'ok' async def process_requests(q: asyncio.Queue, pool: ProcessPoolExecutor): while True: item = await q.get() # Get a request from the queue loop = asyncio.get_running_loop() fake_db[item.id] = 'Processing...' r = await loop.run_in_executor(pool, cpu_bound_task, item) q.task_done() # tell the queue that the processing on the task is completed fake_db[item.id] = 'Done.' @asynccontextmanager async def lifespan(app: FastAPI): q = asyncio.Queue() # note that asyncio.Queue() is not thread safe pool = ProcessPoolExecutor() asyncio.create_task(process_requests(q, pool)) # Start the requests processing task yield {'q': q, 'pool': pool} pool.shutdown() # free any resources that the pool is using when the currently pending futures are done executing fake_db = {} app = FastAPI(lifespan=lifespan) @app.get(&quot;/add&quot;) async def add_task(request: Request, name: str): item_id = str(uuid.uuid4()) item = Item(item_id, name) request.state.q.put_nowait(item) # Add request to the queue fake_db[item_id] = 'Pending...' return item_id @app.get(&quot;/status&quot;) async def check_status(item_id: str): if item_id in fake_db: return {'status': fake_db[item_id]} else: return JSONResponse(&quot;Item ID Not Found&quot;, status_code=404) if __name__ == '__main__': import uvicorn uvicorn.run(app) Note In case you encountered a memory leak (i.e., memory that is no longer needed, but is not released), when re-using a ProcessPoolExecutor—for any reason, e.g., likely due to issues with some third-party library that you might be using—you could instead create a new instance of the ProcessPoolExecutor class for every request that needs to be processed and have it terminated (using the with statement) right after the processing is completed. Note, however, that creating and destroying many processes over and over could become computationally expensive. Example: async def process_requests(q: asyncio.Queue): while True: # ... with ProcessPoolExecutor() as pool: r = await loop.run_in_executor(pool, cpu_bound_task, item) # ...",
    "context_chunks": [
      {
        "text": "My server exposes an API for a resource-intensive rendering work. The job it does involves a GPU and as such the server can handle only a single request at a time. Clients should submit a job and receive 201 - ACCEPTED - as a response immediately after. The processing can take up to a minute and there can be a few dozens of requests scheduled. Here's what I came up with, boiled to a minimal reproducible example: import time import asyncio from fastapi import FastAPI, status app = FastAPI() fifo_queue = asyncio.Queue() async def process_requests(): while True: name = await fifo_queue.get() # Wait for a request from the queue print(name) time.sleep(10) # A RESOURCE INTENSIVE JOB THAT BLOCKS THE THREAD fifo_queue.task_done() # Indicate that the request has been processed @app.on_event(&quot;startup&quot;) async def startup_event(): asyncio.create_task(process_requests()) # Start the request processing task @app.get(&quot;/render&quot;) async def render(name): fifo_queue.put_nowait(name) # Add the request parameter to the queue return status.HTTP_201_CREATED # Return a 201 status code The problem with this approach is that the server does not stay responsive. After sending the first request it gets busy full time with it and does not respond as I have hoped. curl http://127.0.0.1:8000/render\\?name\\=001 In this example simply replacing time.sleep(10) with await asyncio.sleep(10) solves the problem, but not in the real use case (though possibly offers a clue as for what I am doing incorrectly). Any ideas?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "As you may have figured out already, the main issue in your example is that you run a synchronous blocking operation within an async def endpoint, which blocks the event loop (of the main thread), and hence, the entire server. As explained in this answer, if one has to use an async def endpoint, they could run such CPU-bound tasks in an external ProcessPool and then await it (using asyncio's loop.run_in_executor()), which would return control back to the event loop, thus allowing other tasks in the event loop to run, until that task is completed—please have a look at the linked answer above, as well as this answer for more details. As explained in the linked answers, when using ProcessPoolExecutor on Windows, it is important to protect the entry point of the program to avoid recursive spawning of subprocesses, etc. Basically, your code must be under if __name__ == '__main__' (as shown in the example below). I would also suggest using a lifespan handler, as demonstrated in this answer and this answer, instead of the deprecated startup and shutdown event handlers, to start the process_requests function, as well as instantiate the asyncio.Queue() and the ProcessPoolExecutor, and then add them to the request.state, so that they can be shared and re-used by every request/endpoint (especially, in the case of ProcessPoolExecutor, to avoid creating a new ProcessPool every time, as the computational costs for setting up processes can become expensive, when creating and destroying a lot of processes over and over). Further, I would suggest creating a unique ID for every request arrived, and return that ID to the client, so that they can use it to check on the status of their request, i.e., whether is completed or still pending processing. You could save that ID to your database storage (or a Key-Value store, such as Redis), as explained in this answer; however, for simplicity and demo purposes, the example belows uses a dict object for that purpose. It should also be noted that in the example below the ID is expected as a query parameter to the /status endpoint, but in real-world scenarios, you should never pass sensitive information to the query string, as this would pose security/privacy risks (see Solution 1 of this answer, where some of the risks are outlined). You should instead pass sensitive information to the request body, and always use the HTTPS protocol. Working Example from fastapi import FastAPI, Request from fastapi.responses import JSONResponse from contextlib import asynccontextmanager from dataclasses import dataclass from concurrent.futures import ProcessPoolExecutor import time import asyncio import uuid @dataclass class Item: id: str name: str # Simulating a Computationally Intensive Task def cpu_bound_task(item: Item): print(f&quot;Processing: {item.name}&quot;) time.sleep(15) return 'ok' async def process_requests(q: asyncio.Queue, pool: ProcessPoolExecutor): while True: item = await q.get() # Get a request from the queue loop = asyncio.get_running_loop() fake_db[item.id] = 'Processing...' r = await loop.run_in_executor(pool, cpu_bound_task, item) q.task_done() # tell the queue that the processing on the task is completed fake_db[item.id] = 'Done.' @asynccontextmanager async def lifespan(app: FastAPI): q = asyncio.Queue() # note that asyncio.Queue() is not thread safe pool = ProcessPoolExecutor() asyncio.create_task(process_requests(q, pool)) # Start the requests processing task yield {'q': q, 'pool': pool} pool.shutdown() # free any resources that the pool is using when the currently pending futures are done executing fake_db = {} app = FastAPI(lifespan=lifespan) @app.get(&quot;/add&quot;) async def add_task(request: Request, name: str): item_id = str(uuid.uuid4()) item = Item(item_id, name) request.state.q.put_nowait(item) # Add request to the queue fake_db[item_id] = 'Pending...' return item_id @app.get(&quot;/status&quot;) async def check_status(item_id: str): if item_id in fake_db: return {'status': fake_db[item_id]} else: return JSONResponse(&quot;Item ID Not Found&quot;, status_code=404) if __name__ == '__main__': import uvicorn uvicorn.run(app) Note In case you encountered a memory leak (i.e., memory that is no longer needed, but is not released), when re-using a ProcessPoolExecutor—for any reason, e.g., likely due to issues with some third-party library that you might be using—you could instead create a new instance of the ProcessPoolExecutor class for every request that needs to be processed and have it terminated (using the with statement) right after the processing is completed. Note, however, that creating and destroying many processes over and over could become computationally expensive. Example: async def process_requests(q: asyncio.Queue): while True: # ... with ProcessPoolExecutor() as pool: r = await loop.run_in_executor(pool, cpu_bound_task, item) # ...",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The basic thing that I missed is that in asyncio one can't do anything blocking in a function marked async or it will freeze the event loop. Solution Run the process with the asyncio event loop run_in_executor. The method allows to run the code in a process pool and returns an awaitable. Since it runs in a separate process, the main loop stays responsive. import time import asyncio from fastapi import FastAPI, status from functools import partial from concurrent.futures import ProcessPoolExecutor app = FastAPI() fifo_queue = asyncio.Queue() def compute_intensive_func(name): print(name) time.sleep(10) return 43 async def process_requests(): while True: name = await fifo_queue.get() # Wait for a request from the queue r = await asyncio.get_running_loop().run_in_executor(pool, partial(compute_intensive_func, name)) fifo_queue.task_done() # Indicate that the request has been processed @app.on_event(&quot;startup&quot;) async def startup_event(): asyncio.create_task(process_requests()) # Start the request processing task @app.get(&quot;/render&quot;) async def render(name): fifo_queue.put_nowait(name) # Add the request parameter to the queue return status.HTTP_201_CREATED # Return a 201 status code",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "concurrency",
        "python-asyncio",
        "fastapi"
      ],
      "question_score": 14,
      "answer_score": 16,
      "created": "2024-01-19T18:00:12",
      "question_id": 77847983,
      "answer_id": 77862153
    }
  },
  {
    "question": "trying to setup docker for home server getting &#39;chunked&#39;",
    "expected_answer": "This is due to Docker moving to compose-v2 See https://docs.docker.com/compose/migrate/ for details. On Ubuntu 24.04, if you installed the docker-compose package, uninstall it and install docker-compose-v2. Now, instead of running docker-compose up -d rather run docker compose up -d.",
    "context_chunks": [
      {
        "text": "On Ubuntu 24.04 LTS running docker-compose command caused below error. sudo docker-compose up -d Traceback (most recent call last): File &quot;/usr/lib/python3/dist-packages/docker/api/client.py&quot;, line 214, in _retrieve_server_version return self.version(api_version=False)[&quot;ApiVersion&quot;] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/docker/api/daemon.py&quot;, line 181, in version return self._result(self._get(url), json=True) ^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/docker/utils/decorators.py&quot;, line 46, in inner return f(self, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/docker/api/client.py&quot;, line 237, in _get return self.get(url, **self._set_request_timeout(kwargs)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/requests/sessions.py&quot;, line 602, in get return self.request(&quot;GET&quot;, url, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/requests/sessions.py&quot;, line 589, in request resp = self.send(prep, **send_kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/requests/sessions.py&quot;, line 703, in send r = adapter.send(request, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/requests/adapters.py&quot;, line 486, in send resp = conn.urlopen( ^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 791, in urlopen response = self._make_request( ^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/urllib3/connectionpool.py&quot;, line 497, in _make_request conn.request( TypeError: HTTPConnection.request() got an unexpected keyword argument 'chunked' During handling of the above exception, another exception occurred: Traceback (most recent call last): File &quot;/usr/bin/docker-compose&quot;, line 33, in &lt;module&gt; sys.exit(load_entry_point('docker-compose==1.29.2', 'console_scripts', 'docker-compose')()) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/compose/cli/main.py&quot;, line 81, in main command_func() File &quot;/usr/lib/python3/dist-packages/compose/cli/main.py&quot;, line 200, in perform_command project = project_from_options('.', options) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/compose/cli/command.py&quot;, line 60, in project_from_options return get_project( ^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/compose/cli/command.py&quot;, line 152, in get_project client = get_client( ^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/compose/cli/docker_client.py&quot;, line 41, in get_client client = docker_client( ^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/compose/cli/docker_client.py&quot;, line 170, in docker_client client = APIClient(use_ssh_client=not use_paramiko_ssh, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/docker/api/client.py&quot;, line 197, in __init__ self._version = self._retrieve_server_version() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3/dist-packages/docker/api/client.py&quot;, line 221, in _retrieve_server_version raise DockerException( docker.errors.DockerException: Error while fetching server API version: HTTPConnection.request() got an unexpected keyword argument 'chunked' version 3.12.3 python unbuntu 24.04",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is due to Docker moving to compose-v2 See https://docs.docker.com/compose/migrate/ for details. On Ubuntu 24.04, if you installed the docker-compose package, uninstall it and install docker-compose-v2. Now, instead of running docker-compose up -d rather run docker compose up -d.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "By using Jean Visser answer here is the complete workaround to getting docker up and running on Ubuntu 24.04 LTS. In newer version of docker (like Ubuntu 24.04) docker-compose packages is removed and we must use docker compose instead. So having docker command is enough. Before install new package use docker official documentation to remove all of old docker related packages on your distribution. for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done Install We add our regular user i.e. user to docker group with usermod command to use docker without need super user privileges. sudo apt update; sudo apt upgrade; sudo apt install docker.io docker-compose-v2 sudo usermod -aG docker ${USER} Logout and Login to change take effect user privileges. Test is it OK? docker run -it hello-world If its work well below messages show on terminal Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Run with compose If there is a compose file like docker-compose.yml or compose.yml we can run it by docker compose up -d docker compose ps Here is a simple compose file compose.yml version: '3.8' services: app: image: 'hello-world' EDIT As version top-level property is obsolete in docker compose 25.05 so above file turns to. compose.yml services: app: image: 'hello-world' And someone may need newer docker builder. So run sudo apt install docker-buildx After all we have a complete docker installation on Ubuntu 24.04. docker ps $ docker version Client: Version: 24.0.7 API version: 1.43 Go version: go1.22.2 Git commit: 24.0.7-0ubuntu4.1 Built: Fri Aug 9 02:33:20 2024 OS/Arch: linux/amd64 Context: default Server: Engine: Version: 24.0.7 API version: 1.43 (minimum version 1.12) Go version: go1.22.2 Git commit: 24.0.7-0ubuntu4.1 Built: Fri Aug 9 02:33:20 2024 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.7.12 GitCommit: runc: Version: 1.1.12-0ubuntu3.1 GitCommit: docker-init: Version: 0.19.0 GitCommit: Ansible rule to install docker in Ubuntu 24.04 and Debian. docker.yml --- - hosts: ' {{ host }} ' become: true tasks: - name: Install docker, docker-compose packages become: yes package: name: - docker.io - docker-compose - docker-compose-v2 # if ubuntu 24.04 - docker-buildx state: present - name: add user to docker group become: yes user: name=user append=yes groups=&quot;docker&quot;",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "docker",
        "docker-compose",
        "ubuntu-24.04"
      ],
      "question_score": 13,
      "answer_score": 37,
      "created": "2024-05-06T11:20:50",
      "question_id": 78436274,
      "answer_id": 78463411
    }
  },
  {
    "question": "UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR",
    "expected_answer": "In version 2.3.0 of pytorch, it prints this unwanted warning even if no exception is thrown: see https://github.com/pytorch/pytorch/pull/125790 As you mentioned, though, the training is processing correctly. If you want to get rid of this warning, you should revert to torch 2.2.2 (you then also have to revert torchvision to 0.17.2): pip3 install torchvision==0.17.2 pip3 install torch==2.2.2",
    "context_chunks": [
      {
        "text": "I'm trying to train a model with Yolov8. Everything was good but today I suddenly notice getting this warning apparently related to PyTorch and cuDNN. In spite the warning, the training seems to be progressing though. I'm not sure if it has any negative effects on the training progress. site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.) return Variable._execution_engine.run_backward( # Calls into the C++ engine to run the backward pass What is the problem and how to address this? Here is the output of collect_env: Collecting environment information... PyTorch version: 2.3.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.29.3 Libc version: glibc-2.31 Python version: 3.9.7 | packaged by conda-forge | (default, Sep 2 2021, 17:58:34) [GCC 9.4.0] (64-bit runtime) Python platform: Linux-5.15.0-69-generic-x86_64-with-glibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe Nvidia driver version: 515.105.01 cuDNN version: Probably one of the following: /usr/lib/x86_64-linux-gnu/libcudnn.so.8.8.0 /usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.8.0 /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.8.0 /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.8.0 /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.8.0 /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.8.0 /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.8.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] onnx==1.16.0 [pip3] onnxruntime==1.17.3 [pip3] onnxruntime-gpu==1.17.1 [pip3] onnxsim==0.4.36 [pip3] optree==0.11.0 [pip3] torch==2.3.0+cu118 [pip3] torchaudio==2.3.0+cu118 [pip3] torchvision==0.18.0+cu118 [pip3] triton==2.3.0 [conda] numpy 1.24.4 pypi_0 pypi [conda] pytorch-quantization 2.2.1 pypi_0 pypi [conda] torch 2.1.1+cu118 pypi_0 pypi [conda] torchaudio 2.1.1+cu118 pypi_0 pypi [conda] torchmetrics 0.8.0 pypi_0 pypi [conda] torchvision 0.16.1+cu118 pypi_0 pypi [conda] triton 2.1.0 pypi_0 pypi",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In version 2.3.0 of pytorch, it prints this unwanted warning even if no exception is thrown: see https://github.com/pytorch/pytorch/pull/125790 As you mentioned, though, the training is processing correctly. If you want to get rid of this warning, you should revert to torch 2.2.2 (you then also have to revert torchvision to 0.17.2): pip3 install torchvision==0.17.2 pip3 install torch==2.2.2",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "June 2024 Solution: Upgrade torch version to 2.3.1 to fix it: pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pytorch",
        "nvidia",
        "torchvision",
        "cudnn"
      ],
      "question_score": 14,
      "answer_score": 10,
      "created": "2024-05-18T02:13:18",
      "question_id": 78498481,
      "answer_id": 78512125
    }
  },
  {
    "question": "TypeError when chaining Runnables in LangChain: Expected a Runnable, callable or dict",
    "expected_answer": "The issue is context is of type str. Pass in a lambda function instead: rag_chain = ( {&quot;context&quot;: lambda x: context, &quot;question&quot;: RunnablePassthrough()} | rag_custom_prompt | llm ) This is what fixed it for me",
    "context_chunks": [
      {
        "text": "I'm working with LangChain to create a retrieval-based QA system. However, when I attempt to chain Runnables, I encounter a TypeError that I'm unable to resolve. The error occurs when I try to use the | (pipe) operator to chain a RunnablePassthrough with a custom prompt and a ChatOpenAI instance. Here is the error message I'm receiving: TypeError: Expected a Runnable, callable or dict. Instead got an unsupported type: &lt;class 'str'&gt; I've pinpointed the error to this part of the code: rag_chain = ( {&quot;context&quot;: context, &quot;question&quot;: RunnablePassthrough()} | rag_custom_prompt | llm ) I expect the RunnablePassthrough() to pass the context and question to the next step in the chain, but it seems to fail during the coercion to a Runnable. The following is pretty much my entire code: ## Convert the pdf into txt def pdf_to_txt(inst_manuals): txt = &quot;&quot; for manual in inst_manuals: reader = PdfReader(inst_manuals) for page in reader.pages: txt += page.extract_text() return txt ## Convert txt into chunks def chunkify_txt(txt): txt_splitter = CharacterTextSplitter( separator= &quot;\\n&quot;, chunk_size= 1000, chunk_overlap= 200, length_function= len ) chunks = txt_splitter.split_text(txt) return chunks ## Obtain the vector store def get_vector(chunks): embeddings = OpenAIEmbeddings() vectorstore = FAISS.from_texts(texts= chunks, embedding = embeddings) return vectorstore ## Retrieve useful info similar to user query def retrieve(vectorstore, question): logging.basicConfig() logging.getLogger(&quot;langchain.retrievers.multi_query&quot;).setLevel(logging.INFO) retriever_from_llm = MultiQueryRetriever.from_llm( retriever=vectorstore.as_retriever(), llm=ChatOpenAI(temperature=0) ) unique_docs = retriever_from_llm.get_relevant_documents(query=question) print(f&quot;Number of unique documents retrieved: {len(unique_docs)}&quot;) return unique_docs ## Generate response for user query def gen_resp(retriever, question): llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;, temperature=0) template = &quot;&quot;&quot;... [custom prompt template] ...&quot;&quot;&quot; rag_custom_prompt = PromptTemplate.from_template(template) context = &quot;\\n&quot;.join(doc.page_content for doc in retriever) rag_chain = ( {&quot;context&quot;: context, &quot;question&quot;: RunnablePassthrough()} | rag_custom_prompt | llm ) answer = rag_chain.invoke(question) return answer Has anyone encountered this before? Any suggestions on how to properly chain these Runnables or what I might be doing wrong? I have attempted the following: Using different retrievers (details of which could be provided upon request). I've checked the LangChain documentation for proper usage of Runnables and chaining operations. I've tried interchanging the context and question keys in the rag_chain dictionary to see if the order was the issue.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The issue is context is of type str. Pass in a lambda function instead: rag_chain = ( {&quot;context&quot;: lambda x: context, &quot;question&quot;: RunnablePassthrough()} | rag_custom_prompt | llm ) This is what fixed it for me",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "If you're working with langchain and trying to implement RAG (Retrieval-Augmented Generation), here's how I solved an issue with creating a retriever within the get_vector function. The key is to initialize a retriever that uses the FAISS vector store from the provided documents. Here’s the function that correctly sets up the retriever: from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import FAISS from langchain.llms import ChatOpenAI from langchain.prompts import PromptTemplate, RunnablePassthrough def get_vector(chunks): embeddings = OpenAIEmbeddings() vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings) retriever = vectorstore.as_retriever() return retriever Next, when generating a response, you don't need to retrieve the documents explicitly. Instead, pass the retriever to the gen_resp function. The RAG logic within this function will handle the similarity search against your documents: def gen_resp(retriever, question): llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;, temperature=0) template = &quot;&quot;&quot; Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you do not have the relevant information needed to provide a verified answer, don't try to make up an answer. When providing an answer, aim for clarity and precision. Position yourself as a knowledgeable authority on the topic, but also be mindful to explain the information in a manner that is accessible and comprehensible to those without a technical background. Always say &quot;Do you have any more questions pertaining to this instrument?&quot; at the end of the answer. {context} Question: {question} Helpful Answer:&quot;&quot;&quot; rag_custom_prompt = PromptTemplate.from_template(template) # Set up the RAG chain rag_chain = ( {&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()} | rag_custom_prompt | llm ) # Invoke the RAG chain with the question answer = rag_chain.invoke(question) return answer This approach uses the power of the langchain RAG to find the most relevant context and generate a response based on that context. I hope this helps others who are learning and experimenting with langchain RAG functionality.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "nlp",
        "artificial-intelligence",
        "langchain"
      ],
      "question_score": 14,
      "answer_score": 12,
      "created": "2023-11-09T10:29:40",
      "question_id": 77452363,
      "answer_id": 77794205
    }
  },
  {
    "question": "Inheriting str and enum, why is the output different?",
    "expected_answer": "This is a quirk of multiple inheritance (one of the reasons why a lot of people choose to shun it). print(&quot;&quot; + Status.NEW) Here you're using the + operator on your Status.NEW object. Since Status inherits from str, it inherits the __add__ method from there. str.__add__ does string concatenation and uses its raw string value. print(Status.NEW) Here there's no string concatenation, but print calls the __str__ method on any object you pass to it. Your Status.NEW object inherits its __str__ method from the Enum side of the family, so in that context it gets printed as an Enum value instead of a string. If you're in Python 3.11 or greater, you can do this: import enum class Status(enum.StrEnum): &quot;&quot;&quot;Status options.&quot;&quot;&quot; NEW = &quot;NEW&quot; EXCLUDED = &quot;EXCLUDED&quot; print(&quot;&quot; + Status.NEW) print(Status.NEW) That avoids ambiguity about whether your objects are treated as Enums or strings. Otherwise, you just have to be careful.",
    "context_chunks": [
      {
        "text": "I have this python code. But why does it print &quot;NEW&quot; in the first case, and &quot;Status.NEW&quot; in the second case? import enum class Status(str, enum.Enum): &quot;&quot;&quot;Status options.&quot;&quot;&quot; NEW = &quot;NEW&quot; EXCLUDED = &quot;EXCLUDED&quot; print(&quot;&quot; + Status.NEW) print(Status.NEW)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is a quirk of multiple inheritance (one of the reasons why a lot of people choose to shun it). print(&quot;&quot; + Status.NEW) Here you're using the + operator on your Status.NEW object. Since Status inherits from str, it inherits the __add__ method from there. str.__add__ does string concatenation and uses its raw string value. print(Status.NEW) Here there's no string concatenation, but print calls the __str__ method on any object you pass to it. Your Status.NEW object inherits its __str__ method from the Enum side of the family, so in that context it gets printed as an Enum value instead of a string. If you're in Python 3.11 or greater, you can do this: import enum class Status(enum.StrEnum): &quot;&quot;&quot;Status options.&quot;&quot;&quot; NEW = &quot;NEW&quot; EXCLUDED = &quot;EXCLUDED&quot; print(&quot;&quot; + Status.NEW) print(Status.NEW) That avoids ambiguity about whether your objects are treated as Enums or strings. Otherwise, you just have to be careful.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Printing Status.NEW gives you the enum's string presentation, str(Status.NEW), which is 'Status.NEW'. Concatenating it with a string gives you its value as a string, which is 'NEW', because your Status enum inherits from str. For normal strings, str(value) and value would be the same string, but because of the multiple inheritance here, the string you get from str(Status.NEW) is different from the string value of Status.NEW, which is the string 'NEW'.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "enums"
      ],
      "question_score": 13,
      "answer_score": 19,
      "created": "2025-01-14T14:59:26",
      "question_id": 79355428,
      "answer_id": 79355526
    }
  },
  {
    "question": "ValueError: Invalid pattern: &#39;**&#39; can only be an entire path component",
    "expected_answer": "The error is likely due to a change in datasets package (somewhere between 2.1 to 2.14) is breaking fsspec. It has been fixed (see discussion in issues) in the latest datasets release (2.15.0). Update your installation with pip install -U datasets to fix the fsspec ValueError. The solution works for datasets version 2.10.1 on Python 3.10, as it should update the package with a hotfix that was added for version &gt; 2.15.0.",
    "context_chunks": [
      {
        "text": "I am trying to fine tune a LLM My code so far: from datasets import load_dataset, DatasetDict, Dataset from transformers import ( AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer) from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig import evaluate import torch import numpy as np # load dataset dataset = load_dataset('TokenBender/code_instructions_122k_alpaca_style') dataset Error: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [12], line 2 1 # load dataset ----&gt; 2 dataset = load_dataset('TokenBender/code_instructions_122k_alpaca_style') 3 dataset File /usr/local/lib/python3.9/dist-packages/datasets/load.py:1664, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs) 1661 ignore_verifications = ignore_verifications or save_infos 1663 # Create a dataset builder -&gt; 1664 builder_instance = load_dataset_builder( 1665 path=path, 1666 name=name, 1667 data_dir=data_dir, 1668 data_files=data_files, 1669 cache_dir=cache_dir, 1670 features=features, 1671 download_config=download_config, 1672 download_mode=download_mode, 1673 revision=revision, 1674 use_auth_token=use_auth_token, 1675 **config_kwargs, 1676 ) 1678 # Return iterable dataset in case of streaming 1679 if streaming: File /usr/local/lib/python3.9/dist-packages/datasets/load.py:1490, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs) 1488 download_config = download_config.copy() if download_config else DownloadConfig() 1489 download_config.use_auth_token = use_auth_token -&gt; 1490 dataset_module = dataset_module_factory( 1491 path, 1492 revision=revision, 1493 download_config=download_config, 1494 download_mode=download_mode, 1495 data_dir=data_dir, 1496 data_files=data_files, 1497 ) 1499 # Get dataset builder class from the processing script 1500 builder_cls = import_main_class(dataset_module.module_path) File /usr/local/lib/python3.9/dist-packages/datasets/load.py:1242, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs) 1237 if isinstance(e1, FileNotFoundError): 1238 raise FileNotFoundError( 1239 f&quot;Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. &quot; 1240 f&quot;Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}&quot; 1241 ) from None -&gt; 1242 raise e1 from None 1243 else: 1244 raise FileNotFoundError( 1245 f&quot;Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory.&quot; 1246 ) File /usr/local/lib/python3.9/dist-packages/datasets/load.py:1223, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs) 1215 return HubDatasetModuleFactoryWithScript( 1216 path, 1217 revision=revision, (...) 1220 dynamic_modules_path=dynamic_modules_path, 1221 ).get_module() 1222 else: -&gt; 1223 return HubDatasetModuleFactoryWithoutScript( 1224 path, 1225 revision=revision, 1226 data_dir=data_dir, 1227 data_files=data_files, 1228 download_config=download_config, 1229 download_mode=download_mode, 1230 ).get_module() 1231 except Exception as e1: # noqa: all the attempts failed, before raising the error we should check if the module is already cached. 1232 try: File /usr/local/lib/python3.9/dist-packages/datasets/load.py:846, in HubDatasetModuleFactoryWithoutScript.get_module(self) 836 token = self.download_config.use_auth_token 837 hfh_dataset_info = HfApi(config.HF_ENDPOINT).dataset_info( 838 self.name, 839 revision=self.revision, 840 token=token, 841 timeout=100.0, 842 ) 843 patterns = ( 844 sanitize_patterns(self.data_files) 845 if self.data_files is not None --&gt; 846 else get_patterns_in_dataset_repository(hfh_dataset_info) 847 ) 848 data_files = DataFilesDict.from_hf_repo( 849 patterns, 850 dataset_info=hfh_dataset_info, 851 allowed_extensions=ALL_ALLOWED_EXTENSIONS, 852 ) 853 infered_module_names = { 854 key: infer_module_for_data_files(data_files_list, use_auth_token=self.download_config.use_auth_token) 855 for key, data_files_list in data_files.items() 856 } File /usr/local/lib/python3.9/dist-packages/datasets/data_files.py:471, in get_patterns_in_dataset_repository(dataset_info) 469 resolver = partial(_resolve_single_pattern_in_dataset_repository, dataset_info) 470 try: --&gt; 471 return _get_data_files_patterns(resolver) 472 except FileNotFoundError: 473 raise FileNotFoundError( 474 f&quot;The dataset repository at '{dataset_info.id}' doesn't contain any data file.&quot; 475 ) from None File /usr/local/lib/python3.9/dist-packages/datasets/data_files.py:99, in _get_data_files_patterns(pattern_resolver) 97 try: 98 for pattern in patterns: ---&gt; 99 data_files = pattern_resolver(pattern) 100 if len(data_files) &gt; 0: 101 non_empty_splits.append(split) File /usr/local/lib/python3.9/dist-packages/datasets/data_files.py:303, in _resolve_single_pattern_in_dataset_repository(dataset_info, pattern, allowed_extensions) 301 data_files_ignore = FILES_TO_IGNORE 302 fs = HfFileSystem(repo_info=dataset_info) --&gt; 303 glob_iter = [PurePath(filepath) for filepath in fs.glob(PurePath(pattern).as_posix()) if fs.isfile(filepath)] 304 matched_paths = [ 305 filepath 306 for filepath in glob_iter 307 if filepath.name not in data_files_ignore and not filepath.name.startswith(&quot;.&quot;) 308 ] 309 if allowed_extensions is not None: File /usr/local/lib/python3.9/dist-packages/fsspec/spec.py:606, in AbstractFileSystem.glob(self, path, maxdepth, **kwargs) 602 depth = None 604 allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs) --&gt; 606 pattern = glob_translate(path + (&quot;/&quot; if ends_with_sep else &quot;&quot;)) 607 pattern = re.compile(pattern) 609 out = { 610 p: info 611 for p, info in sorted(allpaths.items()) (...) 618 ) 619 } File /usr/local/lib/python3.9/dist-packages/fsspec/utils.py:734, in glob_translate(pat) 732 continue 733 elif &quot;**&quot; in part: --&gt; 734 raise ValueError( 735 &quot;Invalid pattern: '**' can only be an entire path component&quot; 736 ) 737 if part: 738 results.extend(_translate(part, f&quot;{not_sep}*&quot;, not_sep)) ValueError: Invalid pattern: '**' can only be an entire path component I tried to find something online the closet I found is this article https://github.com/coala/coala/issues/401 but I could not understand their solution. Can anyone help me in understanding the solution for the error I am facing. Thanks. My library versions: peft : '0.6.0' torch : '2.1.2+cu121' datasets : '2.1.0' transformers : '4.21.3'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The error is likely due to a change in datasets package (somewhere between 2.1 to 2.14) is breaking fsspec. It has been fixed (see discussion in issues) in the latest datasets release (2.15.0). Update your installation with pip install -U datasets to fix the fsspec ValueError. The solution works for datasets version 2.10.1 on Python 3.10, as it should update the package with a hotfix that was added for version &gt; 2.15.0.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I fixed this error by updating the datasets package with: pip install -U datasets (more details). Then, I commented it out before restarting the kernel and executing everything again. This is important as you don't want to re-install the updates after restarting the kernel. Hope this helps!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "large-language-model",
        "huggingface-datasets"
      ],
      "question_score": 13,
      "answer_score": 26,
      "created": "2023-12-16T14:30:38",
      "question_id": 77671277,
      "answer_id": 77692294
    }
  },
  {
    "question": "*win32ctypes.pywin32.pywintypes.error when using pyinstaller in VS Code - Possible Virus/Trojan?",
    "expected_answer": "I had a similar issue today when using the latest pyinstaller (6.X.X) on Windows 11. For me, changing to an older release (5.13.2) fixed the issue. To install 5.13.2 you can use pip module with following: pip install pyinstaller==5.13.2",
    "context_chunks": [
      {
        "text": "I am using pyinstaller to generate an executable code for my python.py file. However, I am getting this error: File &quot;C:\\Users\\xxxxx\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PyInstaller\\utils\\win32\\icon.py&quot;, line 143, in CopyIcons_FromIco hdst = win32api.BeginUpdateResource(dstpath, 0) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\xxxxx\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\win32ctypes\\pywin32\\win32api.py&quot;, line 208, in BeginUpdateResource with _pywin32error(): File &quot;C:\\Users\\xxxxx\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py&quot;, line 155, in __exit__ self.gen.throw(typ, value, traceback) File &quot;C:\\Users\\xxxxx\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\win32ctypes\\pywin32\\pywintypes.py&quot;, line 37, in pywin32error raise error(exception.winerror, exception.function, exception.strerror) **win32ctypes.pywin32.pywintypes.error: (225, 'BeginUpdateResourceW', 'Operation did not complete successfully because the file contains a virus or potentially unwanted software.')** My antivirus is allegating there is a virus when running pyinstaller: Trojan:Win64/Malgent!MSR I tried to uninstall python, reinstall, and nothing",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I had a similar issue today when using the latest pyinstaller (6.X.X) on Windows 11. For me, changing to an older release (5.13.2) fixed the issue. To install 5.13.2 you can use pip module with following: pip install pyinstaller==5.13.2",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Try not using -w in pyinstaller. Previously, I used… pyinstaller --onefile -w image.py …and got the same problem. But when I do… pyinstaller --onefile image.py …it is good.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pyinstaller",
        "pywin32",
        "virus"
      ],
      "question_score": 13,
      "answer_score": 26,
      "created": "2023-10-05T17:50:03",
      "question_id": 77239487,
      "answer_id": 77250826
    }
  },
  {
    "question": "How to check the validity of the OpenAI key from python?",
    "expected_answer": "The Python codes shown, accesses openai.Model, but this is no longer supported in openai&gt;=1.0.0, see the v1.0.0 Migration Guide or README at https://github.com/openai/openai-python for the API. Here is the adapted python code: import openai def check_openai_api_key(api_key): client = openai.OpenAI(api_key=api_key) try: client.models.list() except openai.AuthenticationError: return False else: return True OPENAI_API_KEY = &quot;sk-7.....&quot; if check_openai_api_key(OPENAI_API_KEY): print(&quot;Valid OpenAI API key.&quot;) else: print(&quot;Invalid OpenAI API key.&quot;)",
    "context_chunks": [
      {
        "text": "https://pypi.org/project/openai/ &quot;The library needs to be configured with your account's secret key which is available on the website. [...] Set it as the OPENAI_API_KEY environment variable&quot; When I ask Chat GPT to complete a message import openai response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What are the trade-offs around deadwood in forests?&quot;}] ) print(response) I get a RateLimitError: You exceeded your current quota, please check your plan and billing details. Is there a python method to check that the key is valid? In [35]: openai.api_key Out[35]: 'sk-...'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The Python codes shown, accesses openai.Model, but this is no longer supported in openai&gt;=1.0.0, see the v1.0.0 Migration Guide or README at https://github.com/openai/openai-python for the API. Here is the adapted python code: import openai def check_openai_api_key(api_key): client = openai.OpenAI(api_key=api_key) try: client.models.list() except openai.AuthenticationError: return False else: return True OPENAI_API_KEY = &quot;sk-7.....&quot; if check_openai_api_key(OPENAI_API_KEY): print(&quot;Valid OpenAI API key.&quot;) else: print(&quot;Invalid OpenAI API key.&quot;)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "To check if your OpenAI API key is valid, you can try making a test API call and see if it works without any errors. Here's a simple code example: import openai openai.api_key = 'YOUR_API_KEY' def is_api_key_valid(): try: response = openai.Completion.create( engine=&quot;davinci&quot;, prompt=&quot;This is a test.&quot;, max_tokens=5 ) except: return False else: return True # Check the validity of the API key api_key_valid = is_api_key_valid() print(&quot;API key is valid:&quot;, api_key_valid) Replace 'YOUR_API_KEY' with your actual OpenAI API key. The is_api_key_valid function attempts a simple test API call. If the call succeeds without any errors, it means your API key is valid, and the function returns True. However, if any error occurs during the API call, it means the key is likely invalid, and the function returns False. By running the code and checking the value of api_key_valid, you can determine if your OpenAI API key is valid.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "openai-api",
        "chatgpt-api"
      ],
      "question_score": 13,
      "answer_score": 9,
      "created": "2023-06-21T11:16:50",
      "question_id": 76522693,
      "answer_id": 77814220
    }
  },
  {
    "question": "Migrate PostgresDsn.build from pydentic v1 to pydantic v2",
    "expected_answer": "You can use unicode_string() to stringify your URI as follow: from sqlalchemy.ext.asyncio import create_async_engine create_async_engine(settings.POSTGRES_URI.unicode_string()) Check documentation page here for additional explanation.",
    "context_chunks": [
      {
        "text": "I have simple Config class from FastAPI tutorial. But it seems like it uses old pydantic version. I run my code with pydantic v2 version and get a several errors. I fix almost all of them, but the last one I cannot fix yet. This is part of code which does not work: from pydantic import AnyHttpUrl, HttpUrl, PostgresDsn, field_validator from pydantic_settings import BaseSettings from pydantic_core.core_schema import FieldValidationInfo load_dotenv() class Settings(BaseSettings): ... POSTGRES_SERVER: str = 'localhost:5432' POSTGRES_USER: str = os.getenv('POSTGRES_USER') POSTGRES_PASSWORD: str = os.getenv('POSTGRES_PASSWORD') POSTGRES_DB: str = os.getenv('POSTGRES_DB') SQLALCHEMY_DATABASE_URI: Optional[PostgresDsn] = None @field_validator(&quot;SQLALCHEMY_DATABASE_URI&quot;, mode='before') @classmethod def assemble_db_connection(cls, v: Optional[str], info: FieldValidationInfo) -&gt; Any: if isinstance(v, str): return v postgres_dsn = PostgresDsn.build( scheme=&quot;postgresql&quot;, username=info.data.get(&quot;POSTGRES_USER&quot;), password=info.data.get(&quot;POSTGRES_PASSWORD&quot;), host=info.data.get(&quot;POSTGRES_SERVER&quot;), path=f&quot;{info.data.get('POSTGRES_DB') or ''}&quot;, ) return str(postgres_dsn) That is the error which I get: sqlalchemy.exc.ArgumentError: Expected string or URL object, got MultiHostUrl('postgresql://user:password@localhost:5432/database') I check a lot of places, but cannot find how I can fix that, it looks like build method pass data to the sqlalchemy create_engine method as a MultiHostUrl instance instead of string. How should I properly migrate this code to use pydantic v2? UPDATE I have fixed that issue by changing typing for SQLALCHEMY_DATABASE_URI: Optional[PostgresDsn] = None to SQLALCHEMY_DATABASE_URI: Optional[str] = None. Because pydantic makes auto conversion of result for some reason. But I am not sure if that approach is the right one, maybe there are better way to do that?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can use unicode_string() to stringify your URI as follow: from sqlalchemy.ext.asyncio import create_async_engine create_async_engine(settings.POSTGRES_URI.unicode_string()) Check documentation page here for additional explanation.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I had the same problem and eventually came up with the following migration: Pydantic V1: from pydantic import BaseSettings, PostgresDsn, validator class Settings(BaseSettings): POSTGRES_SERVER: Optional[str] POSTGRES_USER: Optional[str] POSTGRES_PASSWORD: Optional[str] POSTGRES_DB: Optional[str] SQLALCHEMY_DATABASE_URI: Union[Optional[PostgresDsn], Optional[str]] = None @validator(&quot;SQLALCHEMY_DATABASE_URI&quot;, pre=True) def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -&gt; Any: if isinstance(v, str): print(&quot;Loading SQLALCHEMY_DATABASE_URI from docker.env file ...&quot;) return v print(&quot;Creating SQLALCHEMY_DATABASE_URI from .env file ...&quot;) return PostgresDsn.build( scheme=&quot;postgresql&quot;, user=values.get(&quot;POSTGRES_USER&quot;), password=values.get(&quot;POSTGRES_PASSWORD&quot;), host=values.get(&quot;POSTGRES_SERVER&quot;), path=f&quot;/{values.get('POSTGRES_DB') or ''}&quot;, ) class Config: env_file = &quot;.env&quot; case_sensitive = True Pydantic V2: from pydantic import PostgresDsn, field_validator, ValidationInfo from pydantic_settings import BaseSettings, SettingsConfigDict class Settings(BaseSettings): model_config = SettingsConfigDict(env_file=&quot;.env&quot;, case_sensitive=True) POSTGRES_SERVER: Optional[str] = None POSTGRES_USER: Optional[str] = None POSTGRES_PASSWORD: Optional[str] = None POSTGRES_DB: Optional[str] = None SQLALCHEMY_DATABASE_URI: Union[Optional[PostgresDsn], Optional[str]] = None @field_validator(&quot;SQLALCHEMY_DATABASE_URI&quot;, mode=&quot;before&quot;) @classmethod def assemble_db_connection(cls, v: Optional[str], values: ValidationInfo) -&gt; Any: if isinstance(v, str): print(&quot;Loading SQLALCHEMY_DATABASE_URI from .docker.env file ...&quot;) return v print(&quot;Creating SQLALCHEMY_DATABASE_URI from .env file ...&quot;) return PostgresDsn.build( scheme=&quot;postgresql&quot;, username=values.data.get(&quot;POSTGRES_USER&quot;), password=values.data.get(&quot;POSTGRES_PASSWORD&quot;), host=values.data.get(&quot;POSTGRES_SERVER&quot;), path=f&quot;{values.data.get('POSTGRES_DB') or ''}&quot;, ) Also, for calling the settings instance objects you need casting to string in V2: settings.SQLALCHEMY_DATABASE_URI.unicode_string() # or f&quot;{settings.SQLALCHEMY_DATABASE_URI}",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "sqlalchemy",
        "fastapi",
        "pydantic"
      ],
      "question_score": 13,
      "answer_score": 8,
      "created": "2023-09-19T12:24:36",
      "question_id": 77134535,
      "answer_id": 77173134
    }
  },
  {
    "question": "pendulum.tz.timezone(&quot;UTC&quot;) TypeError: &#39;module&#39; object is not callable",
    "expected_answer": "In pendulum-3.0.0 : pendulum.tz.timezone is removed. You can use: pendulum.timezone(&quot;UTC&quot;) Or you downngrade your pendulum to 2.0.0 In pendulum 3 (beta) there is no access to pendulum.tz.timezone anymore otherwise we should call pendulum.timezone for convert string/integer to pendulum timezones https://www.mail-archive.com/commits@airflow.apache.org/msg299265.html",
    "context_chunks": [
      {
        "text": "I am facing a issue wherein it is not able to import the airflow modules properly giving the below error. Could someone please help me understand it 10:39:36 ERROR: FileIngestion.test_download_file_mercury (unittest.loader._FailedTest.FileIngestion.test_download_file_mercury) 10:39:36 ---------------------------------------------------------------------- 10:39:36 ImportError: Failed to import test module: FileIngestion.test_download_file_mercury 10:39:36 Traceback (most recent call last): 10:39:36 File &quot;/opt/managed-artifacts/python/3.11.6/lib/python3.11/unittest/loader.py&quot;, line 419, in _find_test_path 10:39:36 module = self._get_module_from_name(name) 10:39:36 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 10:39:36 File &quot;/opt/managed-artifacts/python/3.11.6/lib/python3.11/unittest/loader.py&quot;, line 362, in _get_module_from_name 10:39:36 __import__(name) 10:39:36 File &quot;/jenkins/workspace/dataaws-1417_web_proxy_migration/PythonUtils/test/FileIngestion/test_download_file_mercury.py&quot;, line 6, in &lt;module&gt; 10:39:36 from PythonUtils.src.FileIngestion import download_file_mercury 10:39:36 File &quot;/jenkins/workspace/dataaws-1417_web_proxy_migration/PythonUtils/src/FileIngestion/download_file_mercury.py&quot;, line 2, in &lt;module&gt; 10:39:36 from PythonUtils.src.common.hooks.audit_service import AuditServiceHook 10:39:36 File &quot;/jenkins/workspace/dataaws-1417_web_proxy_migration/PythonUtils/src/common/hooks/audit_service.py&quot;, line 6, in &lt;module&gt; 10:39:36 from airflow.utils.log.logging_mixin import LoggingMixin 10:39:36 File &quot;/jenkins/python/lib/python3.11/site-packages/airflow/__init__.py&quot;, line 52, in &lt;module&gt; 10:39:36 from airflow import configuration 10:39:36 File &quot;/jenkins/python/lib/python3.11/site-packages/airflow/configuration.py&quot;, line 1815, in &lt;module&gt; 10:39:36 secrets_backend_list = initialize_secrets_backends() 10:39:36 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 10:39:36 File &quot;/jenkins/python/lib/python3.11/site-packages/airflow/configuration.py&quot;, line 1743, in initialize_secrets_backends 10:39:36 secrets_backend_cls = import_string(class_name) 10:39:36 ^^^^^^^^^^^^^^^^^^^^^^^^^ 10:39:36 File &quot;/jenkins/python/lib/python3.11/site-packages/airflow/utils/module_loading.py&quot;, line 36, in import_string 10:39:36 module = import_module(module_path) 10:39:36 ^^^^^^^^^^^^^^^^^^^^^^^^^^ 10:39:36 File &quot;/opt/managed-artifacts/python/3.11.6/lib/python3.11/importlib/__init__.py&quot;, line 126, in import_module 10:39:36 return _bootstrap._gcd_import(name[level:], package, level) 10:39:36 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 10:39:36 File &quot;/jenkins/python/lib/python3.11/site-packages/airflow/secrets/metastore.py&quot;, line 28, in &lt;module&gt; 10:39:36 from airflow.utils.session import NEW_SESSION, provide_session 10:39:36 File &quot;/jenkins/python/lib/python3.11/site-packages/airflow/utils/session.py&quot;, line 24, in &lt;module&gt; 10:39:36 from airflow import settings 10:39:36 File &quot;/jenkins/python/lib/python3.11/site-packages/airflow/settings.py&quot;, line 51, in &lt;module&gt; 10:39:36 TIMEZONE = pendulum.tz.timezone(&quot;UTC&quot;) 10:39:36 ^^^^^^^^^^^^^^^^^^^^^^^^^^^ 10:39:36 TypeError: 'module' object is not callable I tried updating the airflow version but same error is coming. Currently i am using airflow version 2.5.3",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In pendulum-3.0.0 : pendulum.tz.timezone is removed. You can use: pendulum.timezone(&quot;UTC&quot;) Or you downngrade your pendulum to 2.0.0 In pendulum 3 (beta) there is no access to pendulum.tz.timezone anymore otherwise we should call pendulum.timezone for convert string/integer to pendulum timezones https://www.mail-archive.com/commits@airflow.apache.org/msg299265.html",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The main goal for everyone who use Apache Airflow is follow Installation from PyPI for avoid surprises when upstream package is updated and breaks some critical part of the Airflow, for more detail you could check mail [Reminder] How to reproducibly install Airflow from the users mailing list. Presumably pendulum 3 support will be added in Airflow 2.8.1, but not sooner. There is no guarantee that 2.8.1 would have this support, so better check Airflow's Release Notes when new version of Airflow would be released. Update: 2024-01-19 Airflow 2.8.1 just released, pendulum v3 now is a target version, support of pendulum 2.1.2 keep for a while presumably until Airflow 2.9.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "airflow",
        "typeerror"
      ],
      "question_score": 13,
      "answer_score": 16,
      "created": "2023-12-22T12:22:28",
      "question_id": 77703615,
      "answer_id": 77705021
    }
  },
  {
    "question": "Langchain: ModuleNotFoundError: No module named &#39;langchain&#39;",
    "expected_answer": "I had installed packages with python 3.9.7 but this version was causing issues so I switched to Python 3.10. When I installed the langhcain it was in python 3.9.7 directory. If yo run pip show langchain, you get this Name: langchain Version: 0.0.220 Summary: Building applications with LLMs through composability Home-page: https://www.github.com/hwchase17/langchain Author: Author-email: License: MIT Location: /home/anaconda3/lib/python3.9/site-packages Requires: aiohttp, async-timeout, dataclasses-json, langchainplus-sdk, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity Required-by: jupyter_ai, jupyter_ai_magics If you look at the Location property, you see this /home/anaconda3/lib/python3.9/site-packages. But since I am using Pyhton3.10 I had to make sure langchain is in the directory of Python 3.10. so installed the langhchain with python3.10 -m pip install langchain now when I run, python3.10 -m pip show langchain I get this Name: langchain Version: 0.0.264 Summary: Building applications with LLMs through composability Home-page: https://www.github.com/hwchase17/langchain Author: Author-email: License: MIT Location: /home/.local/lib/python3.10/site-packages Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity Required-by: Now new Location is referring to Python3.10 directory",
    "context_chunks": [
      {
        "text": "When I write code in VS Code, beginning with: import os from langchain.chains import RetrievalQA from langchain.llms import OpenAI from langchain.document_loaders import TextLoader I am met with the error: ModuleNotFoundError: No module named 'langchain' I have updated my Python to version 3.11.4, have updated pip, and reinstalled langchain. I have also checked sys.path and the folder C:\\\\Python311\\\\Lib\\\\site-packages in which the Langchain folder is, is appended. EDIT: Langchain import works when I run it in the Python console (functionality works too), but when I run the code from the VSCode run button it still provides the ModuleNotFoundError. Has anyone else run into this issue and found a solution?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I had installed packages with python 3.9.7 but this version was causing issues so I switched to Python 3.10. When I installed the langhcain it was in python 3.9.7 directory. If yo run pip show langchain, you get this Name: langchain Version: 0.0.220 Summary: Building applications with LLMs through composability Home-page: https://www.github.com/hwchase17/langchain Author: Author-email: License: MIT Location: /home/anaconda3/lib/python3.9/site-packages Requires: aiohttp, async-timeout, dataclasses-json, langchainplus-sdk, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity Required-by: jupyter_ai, jupyter_ai_magics If you look at the Location property, you see this /home/anaconda3/lib/python3.9/site-packages. But since I am using Pyhton3.10 I had to make sure langchain is in the directory of Python 3.10. so installed the langhchain with python3.10 -m pip install langchain now when I run, python3.10 -m pip show langchain I get this Name: langchain Version: 0.0.264 Summary: Building applications with LLMs through composability Home-page: https://www.github.com/hwchase17/langchain Author: Author-email: License: MIT Location: /home/.local/lib/python3.10/site-packages Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity Required-by: Now new Location is referring to Python3.10 directory",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "you probably didn't put your python file location into the enviroment variables. copy your python file location go to search&gt;edit enviroment enviroment variables&gt;user variables&gt;path&gt;new put the copied python location in the user variables. then try again!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "import",
        "module",
        "python-import",
        "langchain"
      ],
      "question_score": 13,
      "answer_score": 13,
      "created": "2023-07-20T03:05:14",
      "question_id": 76726419,
      "answer_id": 76902984
    }
  },
  {
    "question": "llama-cpp-python not using NVIDIA GPU CUDA",
    "expected_answer": "After searching around and suffering quite for 3 weeks I found out this issue on its repository. The llama-cpp-python needs to known where is the libllama.so shared library. So exporting it before running my python interpreter, jupyter notebook etc. did the trick. For using the miniconda3 installation used by oobabooga text-generation-webui I exported it like bellow: export LLAMA_CPP_LIB=/yourminicondapath/miniconda3/lib/python3.10/site-packages/llama_cpp_cuda/libllama.so Voilà!!!! On importing from llama_cpp import Llama I get ggml_init_cublas: found 1 CUDA devices: Device 0: NVIDIA GeForce GTX 1060, compute capability 6.1 And on llm = Llama(model_path=&quot;/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin&quot;, n_gpu_layers=28, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), ... llama_model_load_internal: using CUDA for GPU acceleration llama_model_load_internal: mem required = 2381.32 MB (+ 1026.00 MB per state) llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 480 MB VRAM for the scratch buffer llama_model_load_internal: offloading 28 repeating layers to GPU llama_model_load_internal: offloaded 28/35 layers to GPU llama_model_load_internal: total VRAM used: 3521 MB ...",
    "context_chunks": [
      {
        "text": "I have been playing around with oobabooga text-generation-webui on my Ubuntu 20.04 with my NVIDIA GTX 1060 6GB for some weeks without problems. I have been using llama2-chat models sharing memory between my RAM and NVIDIA VRAM. I installed without much problems following the intructions on its repository. So what I want now is to use the model loader llama-cpp with its package llama-cpp-python bindings to play around with it by myself. So using the same miniconda3 environment that oobabooga text-generation-webui uses I started a jupyter notebook and I could make inferences and everything is working well BUT ONLY for CPU. A working example bellow, from llama_cpp import Llama llm = Llama(model_path=&quot;/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin&quot;, n_gpu_layers=32, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), prompt = &quot;&quot;&quot;[INST] &lt;&lt;SYS&gt;&gt; Name the planets in the solar system? &lt;&lt;/SYS&gt;&gt; [/INST] &quot;&quot;&quot; output = llm(prompt, max_tokens=350, echo=True) print(output['choices'][0]['text'].split('[/INST]')[-1]) Of course! Here are the eight planets in our solar system, listed in order from closest to farthest from the Sun: Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune Note that Pluto was previously considered a planet but is now classified as a dwarf planet due to its small size and unique orbit. I want to make inference using GPU as well. What is wrong? Why can't I offload to gpu like the parameter n_gpu_layers=32 specifies and also like oobabooga text-generation-webui already does on the same miniconda environment whithout any problems?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "After searching around and suffering quite for 3 weeks I found out this issue on its repository. The llama-cpp-python needs to known where is the libllama.so shared library. So exporting it before running my python interpreter, jupyter notebook etc. did the trick. For using the miniconda3 installation used by oobabooga text-generation-webui I exported it like bellow: export LLAMA_CPP_LIB=/yourminicondapath/miniconda3/lib/python3.10/site-packages/llama_cpp_cuda/libllama.so Voilà!!!! On importing from llama_cpp import Llama I get ggml_init_cublas: found 1 CUDA devices: Device 0: NVIDIA GeForce GTX 1060, compute capability 6.1 And on llm = Llama(model_path=&quot;/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin&quot;, n_gpu_layers=28, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), ... llama_model_load_internal: using CUDA for GPU acceleration llama_model_load_internal: mem required = 2381.32 MB (+ 1026.00 MB per state) llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 480 MB VRAM for the scratch buffer llama_model_load_internal: offloading 28 repeating layers to GPU llama_model_load_internal: offloaded 28/35 layers to GPU llama_model_load_internal: total VRAM used: 3521 MB ...",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I fixed the problem by making sure cuda toolkit was installed : nvcc --version If not, I installed cuda toolkit. You should have access to CUDA_HOME after installing cuda toolkit: echo $CUDA_HOME Then reinstall llama-cpp : CMAKE_ARGS=&quot;-DLLAMA_CUBLAS=on&quot; pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "nlp",
        "llama",
        "llama-cpp-python"
      ],
      "question_score": 12,
      "answer_score": 13,
      "created": "2023-08-23T16:35:57",
      "question_id": 76963311,
      "answer_id": 76963364
    }
  },
  {
    "question": "How to update requirements.txt file using uv",
    "expected_answer": "Use uv pip freeze &gt; requirements.txt",
    "context_chunks": [
      {
        "text": "I'm using uv to manage my Python environment locally, but my production site still uses pip. So when I update packages locally (from pyproject.toml, updating the uv.lock file) I also need to generate a new requirements.txt file. But I can't get that to contain the latest versions. For example, I recently upgraded packages to the latest versions: uv lock --upgrade That command's output included the line: Updated dj-database-url v2.2.0 -&gt; v2.3.0 And the uv.lock file now contains this, as expected: [[package]] name = &quot;dj-database-url&quot; version = &quot;2.3.0&quot; ... I thought that this command would then update my requirements.txt file: uv pip compile pyproject.toml --quiet --output-file requirements.txt But when I run that, requirements.txt still specifies the previous version: dj-database-url==2.2.0 \\ --hash=... What am I missing?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Use uv pip freeze &gt; requirements.txt",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Explicit export functionality uv export --no-hashes --format requirements-txt &gt; requirements.txt Remove --no-hashes if need to export hashes From uv export docs",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "uv"
      ],
      "question_score": 12,
      "answer_score": 13,
      "created": "2024-11-14T11:32:47",
      "question_id": 79188565,
      "answer_id": 79406264
    }
  },
  {
    "question": "unrecognized configuration parameter &quot;standard_conforming_strings&quot; while querying from Redshift",
    "expected_answer": "Use of pyscopg2 is now outdated for connecting to redshift. For direct connection use redshift_connector - see https://docs.aws.amazon.com/redshift/latest/mgmt/python-connect-examples.html import redshift_connector conn = redshift_connector.connect( host='examplecluster.abc123xyz789.us-west-1.redshift.amazonaws.com', database='dev', port=5439, user='awsuser', password='my_password' ) For sqlAlchemy use redshift_connect and sqlalchemy-redshift - see https://aws.amazon.com/blogs/big-data/use-the-amazon-redshift-sqlalchemy-dialect-to-interact-with-amazon-redshift/ import sqlalchemy as sa from sqlalchemy.engine.url import URL # build the sqlalchemy URL url = URL.create( drivername='redshift+redshift_connector', # indicate redshift_connector driver and dialect will be used host='&lt;clusterid&gt;.xxxxxx.&lt;aws-region&gt;.redshift.amazonaws.com', # Amazon Redshift host port=5439, # Amazon Redshift port database='dev', # Amazon Redshift database username='awsuser', # Amazon Redshift username password='&lt;pwd&gt;' # Amazon Redshift password ) engine = sa.create_engine(url)",
    "context_chunks": [
      {
        "text": "I am trying to connect to my redshift cluster using Sqlalchemy in a linux environment, but i am facing the following issue. from sqlalchemy import create_engine import pandas as pd conn = create_engine('postgresql://connection string') data_frame = pd.read_sql_query(&quot;SELECT * FROM schema.table&quot;, conn) sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedObject) unrecognized configuration parameter &quot;standard_conforming_strings&quot; I really dont understand what the issue is. It works perfectly fine in Windows. PS: Not sure if this makes any difference but i have installed psycopg2-binary on the linux machine in contrast to psycopg2 on windows. EDIT 1 .The version of pyscopg2 is windows is 2.9.3 while the version of pyscopg2-binary in linux is 2.9.6 The version of Sqlalchemy in windows is 1.4.39 while in linux is 2.0.16",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Use of pyscopg2 is now outdated for connecting to redshift. For direct connection use redshift_connector - see https://docs.aws.amazon.com/redshift/latest/mgmt/python-connect-examples.html import redshift_connector conn = redshift_connector.connect( host='examplecluster.abc123xyz789.us-west-1.redshift.amazonaws.com', database='dev', port=5439, user='awsuser', password='my_password' ) For sqlAlchemy use redshift_connect and sqlalchemy-redshift - see https://aws.amazon.com/blogs/big-data/use-the-amazon-redshift-sqlalchemy-dialect-to-interact-with-amazon-redshift/ import sqlalchemy as sa from sqlalchemy.engine.url import URL # build the sqlalchemy URL url = URL.create( drivername='redshift+redshift_connector', # indicate redshift_connector driver and dialect will be used host='&lt;clusterid&gt;.xxxxxx.&lt;aws-region&gt;.redshift.amazonaws.com', # Amazon Redshift host port=5439, # Amazon Redshift port database='dev', # Amazon Redshift database username='awsuser', # Amazon Redshift username password='&lt;pwd&gt;' # Amazon Redshift password ) engine = sa.create_engine(url)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I had the same issue File &quot;/home/ec2-user/miniconda3/envs/test_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py&quot;, line 2339, in _handle_dbapi_exception raise sqlalchemy_exception.with_traceback(exc_info[2]) from e File &quot;/home/ec2-user/miniconda3/envs/test_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py&quot;, line 1965, in _exec_single_context self.dialect.do_execute( File &quot;/home/ec2-user/miniconda3/envs/test_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py&quot;, line 921, in do_execute cursor.execute(statement, parameters) sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedObject) unrecognized configuration parameter &quot;standard_conforming_strings&quot; resolved it by installing sqlalchemy-redshift https://pypi.org/project/sqlalchemy-redshift/ what it did pip install sqlalchemy-redshift Collecting sqlalchemy-redshift Downloading sqlalchemy_redshift-0.8.14-py2.py3-none-any.whl (38 kB) Requirement already satisfied: packaging in /home/ec2-user/miniconda3/envs/test_env/lib/python3.10/site-packages (from sqlalchemy-redshift) (23.1) Collecting SQLAlchemy&lt;2.0.0,&gt;=0.9.2 Downloading SQLAlchemy-1.4.49-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 73.1 MB/s eta 0:00:00 Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/miniconda3/envs/test_env/lib/python3.10/site-packages (from SQLAlchemy&lt;2.0.0,&gt;=0.9.2-&gt;sqlalchemy-redshift) (2.0.2) Installing collected packages: SQLAlchemy, sqlalchemy-redshift Attempting uninstall: SQLAlchemy Found existing installation: SQLAlchemy 2.0.19 Uninstalling SQLAlchemy-2.0.19: Successfully uninstalled SQLAlchemy-2.0.19 Successfully installed SQLAlchemy-1.4.49 sqlalchemy-redshift-0.8.14",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "linux",
        "sqlalchemy",
        "amazon-redshift",
        "psycopg2"
      ],
      "question_score": 12,
      "answer_score": 10,
      "created": "2023-06-22T14:22:12",
      "question_id": 76532906,
      "answer_id": 76629781
    }
  },
  {
    "question": "Django Allauth - ModuleNotFoundError: No module named &#39;allauth.account.middleware&#39; even when django-allauth is properly installed",
    "expected_answer": "The culprit is the Middleware entry. MIDDLEWARE = [ ... # Add the account middleware: &quot;allauth.account.middleware.AccountMiddleware&quot;, # remove this, which only used in v0.56+ ... For the installation documentation for v0.55: https://django-allauth.readthedocs.io/en/0.55.0/installation.html",
    "context_chunks": [
      {
        "text": "&quot;ModuleNotFoundError: No module named 'allauth.account.middleware'&quot; I keep getting this error in my django project even when django-allauth is all installed and setup??? I tried even reinstalling and changing my python to python3 but didn't change anything, can't figure out why all other imports are working but the MIDDLEWARE one... Help pls? settings.py: &quot;&quot;&quot; Django settings for youtube2blog2 project. Generated by 'django-admin startproject' using Django 4.2.4. For more information on this file, see For the full list of settings and their values, see &quot;&quot;&quot; from pathlib import Path import django import os import logging import pyfiglet import allauth # Build paths inside the project like this: BASE_DIR / 'subdir'. BASE_DIR = Path(__file__).resolve().parent.parent # Quick-start development settings - unsuitable for production # SECURITY WARNING: keep the secret key used in production secret! SECRET_KEY = 'omegalul' # SECURITY WARNING: don't run with debug turned on in production! DEBUG = True ALLOWED_HOSTS = [] # CUSTOM CODE # os.environ['FFMPEG_PATH'] = '/third-party/ffmpeg.exe' # os.environ['FFPROBE_PATH'] = '/third-party/ffplay.exe' OFFLINE_VERSION = False def offline_version_setup(databases): if (OFFLINE_VERSION): # WRITE CODE TO REPLACE DATABASES DICT DATA FOR OFFLINE SETUP HERE return True return banner_ascii_art = pyfiglet.figlet_format(&quot;CHRIST IS KING ENTERPRISES&quot;) logger = logging.getLogger() logger.setLevel(logging.DEBUG) print(&quot;\\n - CURRENT DJANGO VERSION: &quot; + str(django.get_version())) print(&quot;\\n - settings.py: Current logger level is &quot; + str(logger.getEffectiveLevel())) logger.debug('settings.py: Logger is working.\\n\\n') MEDIA_ROOT = os.path.join(BASE_DIR, 'media') MEDIA_URL = '/media/' AUTHENTICATION_BACKENDS = [ # Needed to login by username in Django admin, regardless of `allauth` 'django.contrib.auth.backends.ModelBackend', # `allauth` specific authentication methods, such as login by email 'allauth.account.auth_backends.AuthenticationBackend', ] ''' NEEDED SETUP FOR SOCIAL AUTH REQUIRES DEVELOPER CREDENTIALS ON PAUSE UNTIL MVP IS DONE # Provider specific settings SOCIALACCOUNT_PROVIDERS = { 'google': { # For each OAuth based provider, either add a ``SocialApp`` # (``socialaccount`` app) containing the required client # credentials, or list them here: 'APP': { 'client_id': '123', 'secret': '456', 'key': '' } } 'apple': { } 'discord' { } } ''' LOGIN_REDIRECT_URL = 'dashboard' # # Application definition INSTALLED_APPS = [ # My Apps 'yt2b2', 'home', 'dashboard', # Django Apps 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', # Downloaded Apps 'rest_framework', 'embed_video', 'allauth', 'allauth.account', 'allauth.socialaccount', #'allauth.socialaccount.providers.google', #'allauth.socialaccount.providers.apple', #'allauth.socialaccount.providers.discord', ] MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', # Downloaded Middleware 'allauth.account.middleware.AccountMiddleware', ] ROOT_URLCONF = 'youtube2blog2.urls' TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] WSGI_APPLICATION = 'youtube2blog2.wsgi.application' # Database DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': BASE_DIR / 'db.sqlite3', # &lt;--------- OFFLINE VERSION # Consider masking these secret variables using a .env file to beef up your Django app's security. Besides, Vercel allows you to list your environment variables during deployment. #'URL' : 'postgresql://postgres:oibkk5LL9sI5dzY5PAnj@containers-us-west-128.railway.app:5968/railway', #'NAME' : 'railway', #'USER' : 'postgres', #'PASSWORD' : 'oibkk5LL9sI5dzY5PAnj', #'HOST' : 'containers-us-west-128.railway.app', #'PORT' : '5968' } } # Password validation AUTH_PASSWORD_VALIDATORS = [ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', }, ] # Internationalization LANGUAGE_CODE = 'en-us' TIME_ZONE = 'UTC' USE_I18N = True USE_TZ = True # Static files (CSS, JavaScript, Images) STATIC_URL = '/static/' # the path in url STATICFILES_DIRS = [ os.path.join(BASE_DIR, &quot;static&quot;), ] # Default primary key field type DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField' Tried changing to python3, reinstalling django-allauth through pip, other stackoverflow solutions, shifting through allauth docs... Nothing worked until now Update: removed https links because of spam filter Error location: MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', # Downloaded Middleware 'allauth.account.middleware.AccountMiddleware', ]",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The culprit is the Middleware entry. MIDDLEWARE = [ ... # Add the account middleware: &quot;allauth.account.middleware.AccountMiddleware&quot;, # remove this, which only used in v0.56+ ... For the installation documentation for v0.55: https://django-allauth.readthedocs.io/en/0.55.0/installation.html",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The middleware is only present in the unreleased 0.56.0-dev, likely you are using 0.55.2 and following 0.56 documentation.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "django",
        "django-allauth",
        "django-settings"
      ],
      "question_score": 12,
      "answer_score": 13,
      "created": "2023-08-30T23:39:49",
      "question_id": 77012106,
      "answer_id": 77036615
    }
  },
  {
    "question": "Error while loading shared libraries: libmpv.so.1: cannot open shared object file: No such file or directory",
    "expected_answer": "In Ubuntu 24.04 LTS, I got the same error and solved it with this: sudo apt update sudo apt install libmpv-dev libmpv2 sudo ln -s /usr/lib/x86_64-linux-gnu/libmpv.so /usr/lib/libmpv.so.1 Credit to this YouTube tutorial.",
    "context_chunks": [
      {
        "text": "I've installed flet through &quot;pip install flet&quot; from the terminal on Ubuntu. Upon running the basic flet code, just to check whether it worked this pops up: error while loading shared libraries: libmpv.so.1: cannot open shared object file: No such file or directory The code I ran is: import flet as ft def main(page: ft.Page): # add/update controls on Page pass ft.app(target=main) I also followed the Linux system instructions from terminal to install flet: sudo apt-get update sudo apt-get install libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libgstreamer-plugins-bad1.0-dev gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio` What can I try next? I'm relatively new to Ubuntu. I expected a basic flet window to pop up, and not an error from the terminal. Something like: Instead I got the error: error while loading shared libraries: libmpv.so.1: cannot open shared object file: No such file or directory",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In Ubuntu 24.04 LTS, I got the same error and solved it with this: sudo apt update sudo apt install libmpv-dev libmpv2 sudo ln -s /usr/lib/x86_64-linux-gnu/libmpv.so /usr/lib/libmpv.so.1 Credit to this YouTube tutorial.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Actually ran into this issue today and came across your post whilst troubleshooting it. What worked for me was installing libmpv by running: sudo apt install libmpv1 After, you should be able to follow the flet introduction as intended.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "linux",
        "ubuntu",
        "flet"
      ],
      "question_score": 12,
      "answer_score": 12,
      "created": "2024-02-16T12:00:57",
      "question_id": 78007193,
      "answer_id": 78651969
    }
  },
  {
    "question": "Pydantic BaseSettings cant find .env when running commands from different places",
    "expected_answer": "For me it was the fact that .env was not in the same directory as the running script. I had to manually anchor the .env file to an absolute path. Here is my settings.py file with .env residing alongside in the same directory: import os from pydantic_settings import BaseSettings, SettingsConfigDict DOTENV = os.path.join(os.path.dirname(__file__), &quot;.env&quot;) class Settings(BaseSettings): pg_dsn: str pg_pool_min_size: int pg_pool_max_size: int pg_pool_max_queries: int pg_pool_max_intactive_conn_lifetime: int model_config = SettingsConfigDict(env_file=DOTENV) So with the os.path.join(os.path.dirname(__file__), &quot;.env&quot;) line I basically instruct to search for .env file inside the same directory as the settings.py file. This way you can run any of your scripts, and the Settings object will always point to your .dotenv file.",
    "context_chunks": [
      {
        "text": "So, Im trying to setup Alembic with FastAPI and Im having a problem with Pydantic's BaseSettings, I get a validation error (variables not found) because it doesnt find the .env file (?) It can be solved by changing env_file = &quot;.env&quot; to env_file = &quot;../.env&quot; in the BaseSettings class Config but that makes the error happen when running main.py, I tried setting it as an absolute path with env_file = os.path.abspath(&quot;../../.env&quot;) but that didnt work. What should I do? config.py: import os from functools import lru_cache from pydantic_settings import BaseSettings abs_path_env = os.path.abspath(&quot;../../.env&quot;) class Settings(BaseSettings): APP_NAME: str = &quot;AppName&quot; SQLALCHEMY_URL: str ENVIRONMENT: str class Config: env_file = &quot;.env&quot; # Works with uvicorn run command from my-app/project/ # env_file = &quot;../.env&quot; Works with alembic command from my-app/alembic # env_file = abs_path_env @lru_cache() def get_settings(): return Settings() Project folders: my-app ├── alembic │ ├── versions │ ├── alembic.ini │ ├── env.py │ ├── README │ └── script.py.mako ├── project │ ├── core │ │ ├── __init__.py │ │ └── config.py │ └── __init__.py ├── __init__.py ├── .env └── main.py",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "For me it was the fact that .env was not in the same directory as the running script. I had to manually anchor the .env file to an absolute path. Here is my settings.py file with .env residing alongside in the same directory: import os from pydantic_settings import BaseSettings, SettingsConfigDict DOTENV = os.path.join(os.path.dirname(__file__), &quot;.env&quot;) class Settings(BaseSettings): pg_dsn: str pg_pool_min_size: int pg_pool_max_size: int pg_pool_max_queries: int pg_pool_max_intactive_conn_lifetime: int model_config = SettingsConfigDict(env_file=DOTENV) So with the os.path.join(os.path.dirname(__file__), &quot;.env&quot;) line I basically instruct to search for .env file inside the same directory as the settings.py file. This way you can run any of your scripts, and the Settings object will always point to your .dotenv file.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You should say the version of libraries that you are currently using. In the case you are using pydantic2, this way of specifying the config file (and other config) has been deprecated, and then, lately, completely removed. https://docs.pydantic.dev/2.1/usage/model_config/ https://docs.pydantic.dev/2.7/usage/model_config/ Unluckly there is no warnings and the &quot;class Config&quot; is ignored silently. As per other answer, you have to use simply model_config = SettingsConfigDict(env_file='customfile.env') instead of the nested class &quot;Config&quot;.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "fastapi",
        "pydantic",
        "alembic"
      ],
      "question_score": 12,
      "answer_score": 15,
      "created": "2023-07-12T20:34:28",
      "question_id": 76674272,
      "answer_id": 77778412
    }
  },
  {
    "question": "Is there a way to install pytorch on python 3.12.0?",
    "expected_answer": "Its possible now if you use the nightly version. Instructions here https://pytorch.org/, but pasted here for completeness pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118",
    "context_chunks": [
      {
        "text": "I'm making an app using gpt-neo and I'm trying to install torch, but it won't install. The error message is as follows: C:\\Users\\Ben&gt;pip install torch ERROR: Could not find a version that satisfies the requirement torch (from versions: none) ERROR: No matching distribution found for torch Is there any way to install torch without pip?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Its possible now if you use the nightly version. Instructions here https://pytorch.org/, but pasted here for completeness pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I also tried to find out how to install PyTorch without using pip like you but the answer is: impossible. You can find wheel files for all environments at https://download.pytorch.org/whl/torch/ but they haven't uploaded them yet. The nightly build will probably come out first even if they do upload them. Unfortunately, as of now, PyTorch only supports Python up to 3.11.x. Therefore, you may find it helpful to install the latest version of 3.11 (3.11.6).",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pip",
        "pytorch"
      ],
      "question_score": 12,
      "answer_score": 13,
      "created": "2023-10-03T22:26:31",
      "question_id": 77225812,
      "answer_id": 77597368
    }
  },
  {
    "question": "How to generate rolling subsequences into a dataframe in Python",
    "expected_answer": "Use numpy.lib.stride_tricks.sliding_window_view and transpose (T): from numpy.lib.stride_tricks import sliding_window_view as swv out = pd.DataFrame(swv(df['col0'], 5).T).add_prefix('col') Output: col0 col1 col2 col3 col4 col5 col6 col7 col8 col9 ... col986 \\ 0 1 2 3 4 5 6 7 8 9 10 ... 987 1 2 3 4 5 6 7 8 9 10 11 ... 988 2 3 4 5 6 7 8 9 10 11 12 ... 989 3 4 5 6 7 8 9 10 11 12 13 ... 990 4 5 6 7 8 9 10 11 12 13 14 ... 991 col987 col988 col989 col990 col991 col992 col993 col994 col995 0 988 989 990 991 992 993 994 995 996 1 989 990 991 992 993 994 995 996 997 2 990 991 992 993 994 995 996 997 998 3 991 992 993 994 995 996 997 998 999 4 992 993 994 995 996 997 998 999 1000 [5 rows x 996 columns] Reproducible input: N = 1000 df = pd.DataFrame({'col0': range(1, N+1)}) Timing for 100k rows: 25 ms ± 402 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)",
    "context_chunks": [
      {
        "text": "I have the following data frame: col0 1 2 3 4 5 ... 1000 I'd like roll col0 into a data frame with a window size of 5, so the outcome would be like this: col0 col1 col2 ... col995 1 2 3 ... 996 2 3 4 ... 997 3 4 5 ... 998 4 5 6 ... 999 5 6 7 ... 1000 I've tried using loops and &quot;iloc&quot; which would produce correct results, but as the original data frame gets much longer, it would take too long to finish. To complete 10,000, it'd take almost 2 minutes, 20,000 almost 10 minutes, and so on... Is there any way to do it faster, more efficiently in Python?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Use numpy.lib.stride_tricks.sliding_window_view and transpose (T): from numpy.lib.stride_tricks import sliding_window_view as swv out = pd.DataFrame(swv(df['col0'], 5).T).add_prefix('col') Output: col0 col1 col2 col3 col4 col5 col6 col7 col8 col9 ... col986 \\ 0 1 2 3 4 5 6 7 8 9 10 ... 987 1 2 3 4 5 6 7 8 9 10 11 ... 988 2 3 4 5 6 7 8 9 10 11 12 ... 989 3 4 5 6 7 8 9 10 11 12 13 ... 990 4 5 6 7 8 9 10 11 12 13 14 ... 991 col987 col988 col989 col990 col991 col992 col993 col994 col995 0 988 989 990 991 992 993 994 995 996 1 989 990 991 992 993 994 995 996 997 2 990 991 992 993 994 995 996 997 998 3 991 992 993 994 995 996 997 998 999 4 992 993 994 995 996 997 998 999 1000 [5 rows x 996 columns] Reproducible input: N = 1000 df = pd.DataFrame({'col0': range(1, N+1)}) Timing for 100k rows: 25 ms ± 402 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can use sliding_window_view for this: import pandas as pd from numpy.lib.stride_tricks import sliding_window_view df = pd.DataFrame({'Col0': range(1,1001)}) data = sliding_window_view(df['Col0'], 5).T df_new = pd.DataFrame(data, columns=[f'Col{i}' for i in range(data.shape[1])]) print(df_new) Col0 Col1 Col2 Col3 Col4 ... Col991 Col992 Col993 Col994 Col995 0 1 2 3 4 5 ... 992 993 994 995 996 1 2 3 4 5 6 ... 993 994 995 996 997 2 3 4 5 6 7 ... 994 995 996 997 998 3 4 5 6 7 8 ... 995 996 997 998 999 4 5 6 7 8 9 ... 996 997 998 999 1000 [5 rows x 996 columns]",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ],
      "question_score": 12,
      "answer_score": 17,
      "created": "2024-02-17T12:57:52",
      "question_id": 78012172,
      "answer_id": 78012380
    }
  },
  {
    "question": "PEP 622 - can match statement be used as an expression?",
    "expected_answer": "No, the match statement cannot be used as an expresssion. This is addressed in the Rejected Ideas section of PEP 622: Make it an expression In most other languages pattern matching is represented by an expression, not statement. But making it an expression would be inconsistent with other syntactic choices in Python. All decision making logic is expressed almost exclusively in statements, so we decided to not deviate from this.",
    "context_chunks": [
      {
        "text": "PEP 622 introduced match statement as an alternative to if-elif-else. However, one thing I can't find in the proposal or in any of the material online is whether the match statement can be used as an expression and not just as a statement. A couple of examples to make it clear: Example 1: def make_point_2d(pt): match pt: case (x, y): return Point2d(x, y) case _: raise TypeError(&quot;not a point we support&quot;) Example 2: match response.status: case 200: do_something(response.data) case 301 | 302: retry(response.location) In the first example, the function returns from inside a case clause, and in the second example, nothing is returned. But I want to be able to do something like the following hypothetical example: spouse = match name: case &quot;John&quot;: &quot;Jane&quot; case &quot;David&quot;: &quot;Alice&quot; print(spouse) But it doesn't compile.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "No, the match statement cannot be used as an expresssion. This is addressed in the Rejected Ideas section of PEP 622: Make it an expression In most other languages pattern matching is represented by an expression, not statement. But making it an expression would be inconsistent with other syntactic choices in Python. All decision making logic is expressed almost exclusively in statements, so we decided to not deviate from this.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Not in Python. In Rust and Haskell, matches are expressions composed of expressions: let spouse = match name { // expr =&gt; expr, &quot;John&quot; =&gt; &quot;Jane&quot;, // expr =&gt; {stmt; stmt; expr}, &quot;David&quot; =&gt; {let s = &quot;Alice&quot;; println!(&quot;Matched David&quot;); s}, _ =&gt; panic!(&quot;Unknown name&quot;), }; do spouse &lt;- case name of &quot;John&quot; -&gt; return &quot;Jane&quot; &quot;David&quot; -&gt; do let s = &quot;Alice&quot; putStrLn &quot;Matched David&quot; return s ...so it's not technically impossible that the Python match statement couldn't have been designed to work as an expression. Presumably, the reason it was avoided for Python are syntactical reasons: Python does not have syntax for expression statements (e.g. {stmt; stmt; expr}). spouse = match name: case &quot;John&quot;: &quot;Jane&quot; case &quot;David&quot;: # stmt; stmt; expr s = &quot;Alice&quot; print(&quot;Matched David&quot;) s Indentation to indicate a block being treated as an expression may look unnatural to some people. (See also: match grammar.) That said, it is possible your proposed syntax could be accepted in the future.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.10",
        "structural-pattern-matching"
      ],
      "question_score": 12,
      "answer_score": 9,
      "created": "2023-08-20T22:24:10",
      "question_id": 76941553,
      "answer_id": 76941829
    }
  },
  {
    "question": "How exactly the forward and backward hooks work in PyTorch",
    "expected_answer": "How does a hook work? A hook allows you to execute a specific function - referred to as a &quot;callback&quot; - when a particular action has been performed. In this case, you are expecting self.get_attention to be called once the forward function of module has been accessed. To give a minimal example of how a hook would look like. I define a simple class on which you can register new callbacks through register_hook, then when the instance is called (via __call__), all hooks will be called with the provided arguments: class Obj: def __init__(self): self.hooks = [] def register_hook(self, hook): self.hooks.append(hook) def __call__(self, x, y): print('instance called') for hook in self.hooks: hook(x, y) First, implement two hooks for demonstration purposes: def foo(x, y): print(f'foo called with {x} and {y}') def bar(x, _): print(f'bar called with {x}') And initialize an instance of Obj: obj = Obj() You can register a hook and call the instance: &gt;&gt;&gt; obj.register_hook(foo) &gt;&gt;&gt; obj('yes', 'no') instance called foo called with yes and no You can add hooks on top and call again to compare, here both hooks are triggered: &gt;&gt;&gt; obj.register_hook(bar) &gt;&gt;&gt; obj('yes', 'no') instance called foo called with yes and no bar called with yes Using hooks in PyTorch There are two primary hooks in PyTorch: forward and backward. You also have pre- and post-hooks. Additionally there exists hooks on other actions such as load_state_dict... To attach a hook on the forward process of a nn.Module, you should use register_forward_hook, the argument is a callback function that expects module, args, and output. This callback will be triggered on every forward execution. For backward hooks, you should use register_full_backward_hook, the registered hook expects three arguments: module, grad_input, and grad_output. As of recent PyTorch versions, register_backward_hook has been deprecated and should not be used. One side effect here is that you are registering the hook with self.get_attention and self.get_attention_gradient. The function passed to the register handler is not unbound to the class instance! In other words, on execution, these will be called without the self argument like: self.get_attention(module, input, output) self.get_attention_gradient(module, grad_input, grad_output) This will fail. A simple way to fix this is to wrap the hook with a lambda when you register it: module.register_forward_hook( lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs)) All in all, your class could look like this: class Routine: def __init__(self, model, attention_layer_name): self.model = model for name, module in self.model.named_modules(): if attention_layer_name in name: module.register_forward_hook( lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs)) module.register_full_backward_hook( lambda *args, **kwargs: Routine.get_attention_gradient(self, *args, **kwargs)) self.attentions = [] self.attention_gradients = [] def get_attention(self, module, input, output): self.attentions.append(output.cpu()) def get_attention_gradient(self, module, grad_input, grad_output): self.attention_gradients.append(grad_input[0].cpu()) def __call__(self, input_tensor): self.model.zero_grad() output = self.model(input_tensor) loss = output.mean() loss.backward() When initialized with a single linear layer model: routine = Routine(nn.Sequential(nn.Linear(10,10)), attention_layer_name='0') You can call the instance, this will first trigger the forward hook with (because of self.model(input_tensor), and then the backward hook (because of loss.backward()). &gt;&gt;&gt; routine(torch.rand(1,10, requires_grad=True)) Following your implementation, your forward hook is caching the output of the &quot;attention_layer_name&quot; layer in self.attentions. &gt;&gt;&gt; routine.attentions [tensor([[-0.3137, -0.2265, -0.2197, 0.2211, -0.6700, -0.5034, -0.1878, -1.1334, 0.2025, 0.8679]], grad_fn=&lt;...&gt;)] Similarly for the self.attention_gradients: &gt;&gt;&gt; routine.attentions_gradients [tensor([[-0.0501, 0.0393, 0.0353, -0.0257, 0.0083, 0.0426, -0.0004, -0.0095, -0.0759, -0.0213]])] It is important to note that the cached outputs and gradients will remain in self.attentions and self.attentions_gradients and get appended on every execution of Routine.__call__.",
    "context_chunks": [
      {
        "text": "I am trying to understand how exactly code-wise the hooks operate in PyTorch. I have a model and I would like to set a forward and backward hook in my code. I would like to set a hook in my model after a specific layer and I guess the easiest way is to set a hook to this specific module. This introductory video warns that the backward module contains a bug, but I am not sure if that is still the case. My code looks as follows: def __init__(self, model, attention_layer_name='desired_name_module',discard_ratio=0.9): self.model = model self.discard_ratio = discard_ratio for name, module in self.model.named_modules(): if attention_layer_name in name: module.register_forward_hook(self.get_attention) module.register_backward_hook(self.get_attention_gradient) self.attentions = [] self.attention_gradients = [] def get_attention(self, module, input, output): self.attentions.append(output.cpu()) def get_attention_gradient(self, module, grad_input, grad_output): self.attention_gradients.append(grad_input[0].cpu()) def __call__(self, input_tensor, category_index): self.model.zero_grad() output = self.model(input_tensor) loss = ... loss.backward() I am puzzled to understand how code-wise the following lines work: module.register_forward_hook(self.get_attention) module.register_backward_hook(self.get_attention_gradient) I am registering a hook to my desired module, however, then, I am calling a function in each case without any input. My question is Python-wise, how does this call work exactly? How the arguments of the register_forward_hook and register_backward_hook operate when the function it's called?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "How does a hook work? A hook allows you to execute a specific function - referred to as a &quot;callback&quot; - when a particular action has been performed. In this case, you are expecting self.get_attention to be called once the forward function of module has been accessed. To give a minimal example of how a hook would look like. I define a simple class on which you can register new callbacks through register_hook, then when the instance is called (via __call__), all hooks will be called with the provided arguments: class Obj: def __init__(self): self.hooks = [] def register_hook(self, hook): self.hooks.append(hook) def __call__(self, x, y): print('instance called') for hook in self.hooks: hook(x, y) First, implement two hooks for demonstration purposes: def foo(x, y): print(f'foo called with {x} and {y}') def bar(x, _): print(f'bar called with {x}') And initialize an instance of Obj: obj = Obj() You can register a hook and call the instance: &gt;&gt;&gt; obj.register_hook(foo) &gt;&gt;&gt; obj('yes', 'no') instance called foo called with yes and no You can add hooks on top and call again to compare, here both hooks are triggered: &gt;&gt;&gt; obj.register_hook(bar) &gt;&gt;&gt; obj('yes', 'no') instance called foo called with yes and no bar called with yes Using hooks in PyTorch There are two primary hooks in PyTorch: forward and backward. You also have pre- and post-hooks. Additionally there exists hooks on other actions such as load_state_dict... To attach a hook on the forward process of a nn.Module, you should use register_forward_hook, the argument is a callback function that expects module, args, and output. This callback will be triggered on every forward execution. For backward hooks, you should use register_full_backward_hook, the registered hook expects three arguments: module, grad_input, and grad_output. As of recent PyTorch versions, register_backward_hook has been deprecated and should not be used. One side effect here is that you are registering the hook with self.get_attention and self.get_attention_gradient. The function passed to the register handler is not unbound to the class instance! In other words, on execution, these will be called without the self argument like: self.get_attention(module, input, output) self.get_attention_gradient(module, grad_input, grad_output) This will fail. A simple way to fix this is to wrap the hook with a lambda when you register it: module.register_forward_hook( lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs)) All in all, your class could look like this: class Routine: def __init__(self, model, attention_layer_name): self.model = model for name, module in self.model.named_modules(): if attention_layer_name in name: module.register_forward_hook( lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs)) module.register_full_backward_hook( lambda *args, **kwargs: Routine.get_attention_gradient(self, *args, **kwargs)) self.attentions = [] self.attention_gradients = [] def get_attention(self, module, input, output): self.attentions.append(output.cpu()) def get_attention_gradient(self, module, grad_input, grad_output): self.attention_gradients.append(grad_input[0].cpu()) def __call__(self, input_tensor): self.model.zero_grad() output = self.model(input_tensor) loss = output.mean() loss.backward() When initialized with a single linear layer model: routine = Routine(nn.Sequential(nn.Linear(10,10)), attention_layer_name='0') You can call the instance, this will first trigger the forward hook with (because of self.model(input_tensor), and then the backward hook (because of loss.backward()). &gt;&gt;&gt; routine(torch.rand(1,10, requires_grad=True)) Following your implementation, your forward hook is caching the output of the &quot;attention_layer_name&quot; layer in self.attentions. &gt;&gt;&gt; routine.attentions [tensor([[-0.3137, -0.2265, -0.2197, 0.2211, -0.6700, -0.5034, -0.1878, -1.1334, 0.2025, 0.8679]], grad_fn=&lt;...&gt;)] Similarly for the self.attention_gradients: &gt;&gt;&gt; routine.attentions_gradients [tensor([[-0.0501, 0.0393, 0.0353, -0.0257, 0.0083, 0.0426, -0.0004, -0.0095, -0.0759, -0.0213]])] It is important to note that the cached outputs and gradients will remain in self.attentions and self.attentions_gradients and get appended on every execution of Routine.__call__.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Nice sharing ! One suggestion - extract register calls from __init__() to ensure PyTorch can serialize model for saving it; it meight fail to pickle these hooks. Create hook and unhook functions to take care both of registering hooks and removing their handlers. For your reference: class FooNet(nn.Module): def hook_backward(self, module, grad_input, grad_output): self.logg_grad_output = torch.stack(grad_output) def hook_forward(self, module, args, output): self.logg_forward_output = torch.stack(output) def __init__(self, num_class=10): super().__init__() self.net = nn.Sequential( nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.LazyLinear(128), nn.Sigmoid(), nn.LazyLinear(64), nn.Sigmoid(), nn.LazyLinear(num_class)) def hook(self, idx_layer_hook=-1, enable_hook_fp=False, enable_hook_bp=True): if enable_hook_fp: self.logg_forward_args = [] self.logg_forward_output = [] self.handler_fp = None if enable_hook_bp: self.handler_bp = None self.logg_grad_input = [] self.logg_grad_output = [] for idxx, layer in enumerate(self.net): if isinstance(layer,nn.Conv2d) and (idx_layer_hook != -1 and idx_layer_hook == idxx): print(f&quot;Hook layer[{idxx}]:&quot;, layer) if enable_hook_fp: self.handler_fp = layer.register_forward_hook(lambda *args,**kwargs:LeNet.hook_forward(self,*args,**kwargs)) if enable_hook_bp: self.handler_bp = layer.register_full_backward_hook(lambda *args,**kwargs:LeNet.hook_backward(self,*args,**kwargs)) def unhook(self): if hasattr(self, 'handler_fp'): self.handler_fp.remove() if hasattr(self, 'handler_bp'): self.handler_bp.remove() def forward(self, x): assert hasattr(self, 'net') x = self.net(x) return x As long as grad got logged, we can review it gg = nw.logg_grad_output[0,0,0,:,:].numpy() str_title = &quot;grad_layer%d&quot;%(IDX_OF_LAYER_HOOK); plt.style.use('grayscale') fig, ax = plt.subplots(figsize=(4,2)); fig.canvas.manager.set_window_title(str_title); ax.imshow(gg); ax.set_title(str_title); plt.show();",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pytorch",
        "hook"
      ],
      "question_score": 12,
      "answer_score": 9,
      "created": "2024-04-05T12:15:14",
      "question_id": 78279823,
      "answer_id": 78287938
    }
  },
  {
    "question": "How to get exact message sent to LLM using LangChain&#39;s LLMChain (python)?",
    "expected_answer": "pass verbose=True to LLMChain constructor chain = LLMChain(prompt=..., llm=..., verbose=True) but the problems is that it just prints thru stdout. i'm also finding how to get exact used prompt string",
    "context_chunks": [
      {
        "text": "Currently, when using an LLMChain in LangChain, I can get the template prompt used and the response from the model, but is it possible to get the exact text message sent as query to the model, without having to manually do the prompt template filling? An example: from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains import LLMChain llm = OpenAI(model_name=&quot;gpt-3.5-turbo-0613&quot;) prompt = PromptTemplate(input_variables=[&quot;a&quot;, &quot;b&quot;], template=&quot;Hello {a} and {b}&quot;) chain = LLMChain(llm=llm, prompt=prompt) result = chain.call({&quot;a&quot;: &quot;some text&quot;, &quot;b&quot;: &quot;some other text&quot;}) I cannot find something like I am looking for in the chain or result objects. I tried some options such as return_final_only=True and include_run_info=True but they don't include what I am looking for.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "pass verbose=True to LLMChain constructor chain = LLMChain(prompt=..., llm=..., verbose=True) but the problems is that it just prints thru stdout. i'm also finding how to get exact used prompt string",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Here is the way to see that: LLMChain(prompt=prompt,llm=llm).prompt.format_prompt(your_prompt_variables_here).to_string()",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "langchain",
        "py-langchain"
      ],
      "question_score": 12,
      "answer_score": 4,
      "created": "2023-07-07T19:01:54",
      "question_id": 76639580,
      "answer_id": 76840092
    }
  },
  {
    "question": "Specify dependencies in pyproject.toml with install URL or with index-url",
    "expected_answer": "I recommend reading the article &quot;install_requires vs. requirements files&quot; in the &quot;Python packaging user guide&quot;. This article is partly outdated but the principle of abstract dependencies vs. concrete dependencies is unchanged. Concrete dependencies do not belong in packaging metadata, i.e. concrete dependencies can not be added to the dependencies list in the [project] table of the pyproject.toml file. As you have noted, it is possible to add &quot;direct references&quot; (for example psychopy @ git+https://github.com/psychopy/psychopy), but not all packaging tools support this and notably PyPI rejects the upload of packages containing such dependencies. What I often recommend, is that if there are dependencies that are not on PyPI then the documentation should reflect that very clearly and prominently. Where to get the dependencies and some examples of how to install them should be in the documentation, for example the URLs of the alternative package repositories. And another thing that I recommend on top of the documentation is to provide one or more requirements.txt files that contain a list of concrete dependencies that have been tested and are known to work well. For example: MyPackage psychopy @ git+https://github.com/psychopy/psychopy wxPython @ https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04/wxPython-4.2.1-cp310-cp310-linux_x86_64.whl; python_version == &quot;3.10&quot;; sys_platform == &quot;linux&quot; And if I am not mistaken pip can install directly from the URL of such a requirements file: python -m pip install --requirement https://host.example/my-package/requirements.txt so such a command could be added to the documentation as well. Related: https://stackoverflow.com/a/76548420",
    "context_chunks": [
      {
        "text": "I like to have my package installable with pip install ... and to use the pyproject.toml standard. I can specify dependencies to install from git, with: dependencies = [ 'numpy&gt;=1.21', 'psychopy @ git+https://github.com/psychopy/psychopy', ] But how can I specify a dependency to install from a different indexer, equivalent to: python -m pip install --pre --only-binary :all: -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple numpy scipy With or without the pre-release flag? And how can I specify a dependency to install from a URL, e.g. https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04/wxPython-4.2.1-cp310-cp310-linux_x86_64.whl I tried with no luck: dependencies = [ 'wxPython @ https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04/wxPython-4.2.1-cp310-cp310-linux_x86_64.whl; python_version == &quot;3.10&quot;; sys_platform == &quot;linux&quot;' ]",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I recommend reading the article &quot;install_requires vs. requirements files&quot; in the &quot;Python packaging user guide&quot;. This article is partly outdated but the principle of abstract dependencies vs. concrete dependencies is unchanged. Concrete dependencies do not belong in packaging metadata, i.e. concrete dependencies can not be added to the dependencies list in the [project] table of the pyproject.toml file. As you have noted, it is possible to add &quot;direct references&quot; (for example psychopy @ git+https://github.com/psychopy/psychopy), but not all packaging tools support this and notably PyPI rejects the upload of packages containing such dependencies. What I often recommend, is that if there are dependencies that are not on PyPI then the documentation should reflect that very clearly and prominently. Where to get the dependencies and some examples of how to install them should be in the documentation, for example the URLs of the alternative package repositories. And another thing that I recommend on top of the documentation is to provide one or more requirements.txt files that contain a list of concrete dependencies that have been tested and are known to work well. For example: MyPackage psychopy @ git+https://github.com/psychopy/psychopy wxPython @ https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04/wxPython-4.2.1-cp310-cp310-linux_x86_64.whl; python_version == &quot;3.10&quot;; sys_platform == &quot;linux&quot; And if I am not mistaken pip can install directly from the URL of such a requirements file: python -m pip install --requirement https://host.example/my-package/requirements.txt so such a command could be added to the documentation as well. Related: https://stackoverflow.com/a/76548420",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "using python 3.8.10 pip 23.3.1 im using a venv. first install or update your dependencies. $ python -m pip install --pre --only-binary :all: -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple numpy scipy &gt;&gt; Looking in indexes: https://pypi.anaconda.org/scientific-python-nightly-wheels/simple ... Successfully installed numpy-2.0.0.dev0 scipy-1.12.0.dev0 git example: $ pip install git+https://github.com/psychopy/psychopy &gt;&gt; Collecting git+https://github.com/psychopy/psychopy ... Successfully built psychopy esprima future Installing collected packages: pywin32, pytz, ... freeze all the dependencies: $ pip freeze &gt; requirements.txt now all the libraries an dependencies for your project are listed in the requirements.txt file. Assuming you're using SetupTools, I'll do the pyproject.toml: [build-system] requires = [&quot;setuptools&quot;] build-backend = &quot;setuptools.build_meta&quot; [project] name = &quot;project&quot; version = &quot;0.0.1&quot; dynamic = [&quot;dependencies&quot;] [tool.setuptools.dynamic] dependencies = {file = &quot;requirements.txt&quot;} That's all you need, the tests I did worked perfectly with what you ordered. I hope I was able to help you.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "dependencies",
        "packaging",
        "pyproject.toml"
      ],
      "question_score": 12,
      "answer_score": 5,
      "created": "2023-09-19T11:42:38",
      "question_id": 77134254,
      "answer_id": 77464614
    }
  },
  {
    "question": "Use LLama 2 7B with python",
    "expected_answer": "I know you mentioned huggingface is unnecessary in your case but to download and use the model, it's much easier to use their transformers. After you download the weights - you need to re-structure the folder as follows:(notice I moved 3 of the files under 7B) ├── 7B │ ├── checklist.chk │ ├── consolidated.00.pth │ └── params.json ├── config.json ├── generation_config.json ├── LICENSE ├── tokenizer_checklist.chk ├── tokenizer.model └── USE_POLICY.md Next download the conversion script from here: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py And finally run this script: python convert_llama_weights_to_hf.py --input_dir llama-2-7b/ --model_size 7B --output_dir model Once it's finished - you can import the model as follows: from transformers import LlamaForCausalLM, LlamaTokenizer tokenizer = LlamaTokenizer.from_pretrained(&quot;./model&quot;) model = LlamaForCausalLM.from_pretrained(&quot;./model&quot;) You can then learn more on how to prompt the model here: https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/llama2#transformers.LlamaForCausalLM.forward.example",
    "context_chunks": [
      {
        "text": "I would like to use llama 2 7B locally on my win 11 machine with python. I have a conda venv installed with cuda and pytorch with cuda support and python 3.10. So I am ready to go. The files a here locally downloaded from meta: folder llama-2-7b-chat with: checklist.chk consolidated.00.pth params.json Now I would like to interact with the model. But I only find code snippets downloading the model from huggingface, which is not needed in my case. Can someone provide me with a few lines of code to interact with the model via Python?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I know you mentioned huggingface is unnecessary in your case but to download and use the model, it's much easier to use their transformers. After you download the weights - you need to re-structure the folder as follows:(notice I moved 3 of the files under 7B) ├── 7B │ ├── checklist.chk │ ├── consolidated.00.pth │ └── params.json ├── config.json ├── generation_config.json ├── LICENSE ├── tokenizer_checklist.chk ├── tokenizer.model └── USE_POLICY.md Next download the conversion script from here: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py And finally run this script: python convert_llama_weights_to_hf.py --input_dir llama-2-7b/ --model_size 7B --output_dir model Once it's finished - you can import the model as follows: from transformers import LlamaForCausalLM, LlamaTokenizer tokenizer = LlamaTokenizer.from_pretrained(&quot;./model&quot;) model = LlamaForCausalLM.from_pretrained(&quot;./model&quot;) You can then learn more on how to prompt the model here: https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/llama2#transformers.LlamaForCausalLM.forward.example",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The downloaded files are not all needed. I got it to work using cuda gpu on win 11, but with a slightly other way: First of all, I used this repo and not the code provided by Meta itsself (but I had to download the files via huggingface): https://github.com/oobabooga/text-generation-webui The cuda installation via conda did have some errors, even when everything looked fine at first. I could solve this by installing the stack like it was provided here: https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/manual_setup2.ipynb I hope that helps others as well ...",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pytorch",
        "llama"
      ],
      "question_score": 12,
      "answer_score": 4,
      "created": "2023-08-05T13:51:54",
      "question_id": 76841958,
      "answer_id": 76967510
    }
  },
  {
    "question": "pkgutil.ImpImporter not found installing pyaudio on Python 3.12",
    "expected_answer": "I just updated the virtualenv: pip install --upgrade virtualenv After that create a new virtual environment using: virtualenv venv --python=python3.12 Activate the virtual environment &quot;venv&quot; and it should be working and able to install your packages.",
    "context_chunks": [
      {
        "text": "I tried to build an AI program which I followed a video on youtube but I got error. When I try to &quot;pip install pyaudio&quot; and then I got that error. I have got that &quot;pip install numpy&quot; and &quot; pip install wheel&quot; but that things weren't fix my troubles. pip version: 23.2.1 python version: 3.12.0 C:\\Users\\admin!&gt;pip install pyaudio Collecting pyaudio Using cached PyAudio-0.2.13.tar.gz (46 kB) Installing build dependencies ... done Getting requirements to build wheel ... error error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; [33 lines of output] Traceback (most recent call last): File &quot;C:\\Users\\admin!\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 353, in &lt;module&gt; main() File &quot;C:\\Users\\admin!\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\admin!\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 112, in get_requires_for_build_wheel backend = _build_backend() ^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\admin!\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 77, in _build_backend obj = import_module(mod_path) ^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\admin!\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py&quot;, line 90, in import_module return _bootstrap._gcd_import(name[level:], package, level) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1381, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1354, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1304, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 488, in _call_with_frames_removed File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1381, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1354, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1325, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 929, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 994, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 488, in _call_with_frames_removed File &quot;C:\\Users\\admin!\\AppData\\Local\\Temp\\pip-build-env-5_67tfiy\\overlay\\Lib\\site-packages\\setuptools\\__init__.py&quot;, line 16, in &lt;module&gt; import setuptools.version File &quot;C:\\Users\\admin!\\AppData\\Local\\Temp\\pip-build-env-5_67tfiy\\overlay\\Lib\\site-packages\\setuptools\\version.py&quot;, line 1, in &lt;module&gt; import pkg_resources File &quot;C:\\Users\\admin!\\AppData\\Local\\Temp\\pip-build-env-5_67tfiy\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py&quot;, line 2191, in &lt;module&gt; register_finder(pkgutil.ImpImporter, find_on_path) ^^^^^^^^^^^^^^^^^^^ AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'? [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. How can I fix that troubles. Please help me. Thanks you all.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I just updated the virtualenv: pip install --upgrade virtualenv After that create a new virtual environment using: virtualenv venv --python=python3.12 Activate the virtual environment &quot;venv&quot; and it should be working and able to install your packages.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Try creating a virtualenv with --reset-app-data param. This seems to have helped me, suggestion found at https://pythontest.com/posts/2023/2023-10-02-py312-impimporter/ The author also mentions virtualenv --upgrade-embed-wheels, I did that as well, not sure if that made a difference.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 12,
      "answer_score": 5,
      "created": "2023-10-08T12:48:39",
      "question_id": 77253684,
      "answer_id": 77291015
    }
  },
  {
    "question": "Generate teams of similar strengths with contiguous areas",
    "expected_answer": "First let's talk about contiguity. I think that you should value compactness over contiguity; after all, the Americans have proven that anything can be contiguous if you believe it hard enough. Contiguity is a &quot;hard problem&quot; to represent and compactness is an &quot;easy problem&quot; to represent. Scalability becomes a challenge because you aren't performing a simple similarity clustering (e.g. k-means); you're performing clustering with much more complex criteria. This is not my specialty, and so in my demonstration I show a simplification that is stochastically correct: assuming player skill and position are all uniformly distributed, a random selection of team centroid based on population density that is also of uniform distribution will represent reality (especially if you run multiple outer iterations with random perturbation). Specifically, if you have time to burn and want to refine the solution, you can wrap this approach in a differential evolution loop with the team centroid as the evolved parameter. Speaking of complex criteria - this system is only useful if you construct correct teams, and correct teams require position criteria. I present a matrix that you can adjust to set lower and upper bounds on the required number of positions per team. It is possible - but potentially very slow - to tell an optimizer to narrow the range of allowed skill levels. You may be best off to perform this as a pre-solve step, eliminating potential players that are some selected number of standard deviations away from your target skill level. Either that, or represent fairness not as an LP objective but as an LP constraint, with bounds on the minimum and maximum skill sum per team. I demonstrate the latter; but note that this is most feasible when we constrain the mean team skill toward the mean skill overall (in this case 50). Defining a range to be too narrow (less than ~ 20%) or defining the target to be far away from the mean make solution difficult. All together, import geopandas import matplotlib.pyplot as plt import numpy as np import pandas as pd import pulp import shapely from numpy.random import default_rng, Generator n_teams = 64 players_per_team = 11 total_players = n_teams * players_per_team max_skill = 100 def synthesize_world_data(rand: Generator) -&gt; tuple[ geopandas.GeoDataFrame, # player and country data pd.DataFrame, # field position data ]: # Load the default world dataset from geopandas, # dropping a couple of countries with degenerate bounds. # There are better ways to fix this (not shown here). world = ( geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres')) .rename(columns={'name': 'country'}) ) world.index.name = 'country' world[['left', 'bottom', 'right', 'top']] = shapely.bounds(world['geometry']) is_degenerate = np.isclose(world['right'] - world['left'], 360) world = world[~is_degenerate] # Assume that talent is uniformly distributed throughout the world, # so players are spatially uniformly distributed. Draw from a pool # of players ~2 times what is needed for final team formation. pop_ratio = 2 * total_players / world['pop_est'].sum() world['players_est'] = world['pop_est'] * pop_ratio # Hack: perform an inefficient but statistically correct spatial random distribution of players # within their countries by allocating random coordinates within rectilinear bounds and then doing # geometric elimination of those players that have decided to live in the ocean world['geodensity'] = shapely.area(world['geometry'])/(world['right'] - world['left'])/(world['top'] - world['bottom']) n_soggy_players = (world['players_est'] / world['geodensity']).round().astype(int) soggy_allocated = pd.DataFrame( np.arange(n_soggy_players.max()) &lt; n_soggy_players.values[:, np.newaxis], index=n_soggy_players.index, columns=pd.RangeIndex(name='country_player', start=0, stop=n_soggy_players.max()), ).replace(False, np.nan).stack() world, _ = world.align(soggy_allocated, axis=0) world['player_lon'] = rand.uniform(low=world['left'], high=world['right']) world['player_lat'] = rand.uniform(low=world['bottom'], high=world['top']) not_soggy = shapely.contains_xy(world['geometry'], world[['player_lon', 'player_lat']]) world = world[not_soggy] # Assume that skill and position are uniformly distributed # https://en.wikipedia.org/wiki/Association_football_positions world['player_skill'] = rand.uniform(low=0, high=max_skill, size=len(world)) positions = pd.DataFrame( { 'min_players': [1, 2, 2, 2], 'max_players': [1, 5, 5, 5], }, index=pd.Index(name='name', data=['goalkeep', 'defender', 'midfield', 'forward']), ) pos_mean = (positions['min_players'] + positions['max_players'])/2 world['position'] = rand.choice( a=positions.index, p=pos_mean/pos_mean.sum(), size=len(world), ) world.set_index( pd.RangeIndex(name='player', start=0, stop=len(world)), append=False, inplace=True, ) return world, positions def guess_team_centroids(rand: Generator, world: geopandas.GeoDataFrame) -&gt; tuple[ pd.DataFrame, # centroids per team pd.Series, # player-centroid distances ]: coord_fields = ['player_lat', 'player_lon'] centroid_idx = rand.choice(a=world.index, size=n_teams) centroids = ( world.loc[ centroid_idx, coord_fields, ] .rename(columns={'player_lat': 'team_lat', 'player_lon': 'team_lon'}) .set_index(pd.RangeIndex(name='team', start=0, stop=n_teams)) ) coords = np.deg2rad( pd.merge( how='cross', left=centroids.reset_index(), right=world[coord_fields].reset_index(), ).set_index(['team', 'player']) ) # Haversine norm (how is this not in geopandas?) a = ( np.sin((coords['player_lat'] - coords['team_lat'])/2)**2 + np.sin((coords['player_lon'] - coords['team_lon'])/2)**2 * np.cos(coords['team_lat']) * np.cos(coords['player_lat']) ) c = np.arctan2(np.sqrt(a), np.sqrt(1 - a)) r = (world.crs.ellipsoid.semi_major_metre + world.crs.ellipsoid.semi_minor_metre) * 1e-3 distances = r*c return centroids, distances def make_vars(world: geopandas.GeoDataFrame) -&gt; pd.Series: team_idx = pd.RangeIndex(name='team', start=0, stop=n_teams) assign_idx = pd.MultiIndex.from_product((team_idx, world.index)) names = assign_idx.to_frame().astype(str) assigns = ( 'asn_t' + names['team'] + '_p' + names['player'] ).apply(pulp.LpVariable, cat=pulp.LpBinary) return assigns def make_objective( assigns: pd.Series, distances: pd.Series, ) -&gt; pulp.LpAffineExpression: # For compactness, minimize the distance from each player to their team's centroid return pulp.lpDot(assigns, distances)/total_players/100 def add_constraints( prob: pulp.LpProblem, world: geopandas.GeoDataFrame, positions: pd.DataFrame, assigns: pd.Series, ) -&gt; None: for team, group in assigns.groupby('team'): # There must be 11 players per team prob.addConstraint( name=f'teamsize_t{team}', constraint=pulp.lpSum(group) == players_per_team, ) # Enforce competitive team skill sum skill_deviation = pulp.lpDot( group, world.loc[group.index.get_level_values('player'), 'player_skill'], )/players_per_team - max_skill/2 prob.addConstraint( name=f'skill_lo_t{team}', constraint=skill_deviation &gt;= -10, ) prob.addConstraint( name=f'skill_hi_t{team}', constraint=skill_deviation &lt;= 10, ) # Each player may only be assigned up to one team for player, group in assigns.groupby('player'): prob.addConstraint( name=f'playerexcl_p{player}', constraint=pulp.lpSum(group) &lt;= 1, ) # Enforce the team position bounds for position, (tmin, tmax) in positions.iterrows(): pos_players = world.index[world['position'] == position] pos_assigns = assigns.loc[(slice(None), pos_players)] for team, group in pos_assigns.groupby('team'): total = pulp.lpSum(group) prob.addConstraint( name=f'poslo_t{team}_{position}', constraint=total &gt;= tmin, ) prob.addConstraint( name=f'poshi_t{team}_{position}', constraint=total &lt;= tmax, ) def solve(prob: pulp.LpProblem, world: geopandas.GeoDataFrame, assigns: pd.Series) -&gt; geopandas.GeoDataFrame: prob.solve() if prob.status != pulp.LpStatusOptimal: raise ValueError('Solution status', prob.status) assigns = ( assigns.apply(pulp.LpVariable.value) .astype(int) .unstack(level='team') ) team_player_idx, team_idx = assigns.values.nonzero() world.loc[team_player_idx, 'team'] = team_idx return world def dump_solution( world: geopandas.GeoDataFrame, ) -&gt; None: pd.set_option('display.max_rows', 1000) pd.set_option('display.max_columns', 1000) pd.set_option('display.width', 1000) world = world.loc[ world['team'].notna(), ['continent', 'country', 'team', 'position', 'player_skill'], ] print('Players by country:') print(world.sort_values(['continent', 'country', 'team', 'position']), end='\\n\\n') print('Players by team and position:') print(world.sort_values(['team', 'position']), end='\\n\\n') print('Team summary:') grouped = world.groupby('team') teams = world.groupby(['team', 'position']).country.count().unstack('position') teams.insert(loc=0, column='continent', value=grouped['continent'].agg(pd.Series.mode).astype(str)) teams.insert(loc=1, column='country', value=grouped['country'].agg(pd.Series.mode).astype(str)) teams.insert(loc=2, column='skill', value=grouped['player_skill'].sum() / players_per_team) teams.sort_values(['continent', 'country', 'skill'], inplace=True) print(teams, end='\\n\\n') def plot_solution(world: geopandas.GeoDataFrame) -&gt; plt.Figure: fig, ax = plt.subplots() world['geometry'].boundary.plot(ax=ax) for team, group in world.groupby('team'): ax.scatter(group['player_lon'], group['player_lat']) return fig def main() -&gt; None: rand = default_rng(seed=0) print('Synthesizing world data...') world, positions = synthesize_world_data(rand) centroids, distances = guess_team_centroids(rand, world) print('Making assignment variables...') assigns = make_vars(world) prob = pulp.LpProblem(name='football_clustering', sense=pulp.LpMinimize) print('Defining objective...') prob.objective = make_objective(assigns, distances) print('Adding constraints...') add_constraints(prob, world, positions, assigns) # print(prob) print('Solving...') world = solve(prob, world, assigns) dump_solution(world) plot_solution(world) plt.show() if __name__ == '__main__': main() ... Team summary: position continent country skill defender forward goalkeep midfield team 6.0 Africa Algeria 44.963361 3 2 1 5 2.0 Africa Burkina Faso 45.802411 5 3 1 2 56.0 Africa Dem. Rep. Congo 47.018698 3 2 1 5 10.0 Africa Ethiopia 40.294830 3 4 1 3 11.0 Africa Ethiopia 43.277617 2 5 1 3 38.0 Africa Madagascar 59.876627 3 2 1 5 54.0 Africa Morocco 55.331606 3 2 1 5 63.0 Africa Nigeria 40.088676 4 3 1 3 55.0 Africa South Africa 55.269027 3 4 1 3 62.0 Africa Uganda 47.396431 3 2 1 5 36.0 Africa Uganda 48.577035 2 5 1 3 35.0 Africa ['Burundi' 'Dem. Rep. Congo' 'Tanzania'] 53.517437 3 4 1 3 20.0 Asia Bangladesh 57.757983 4 2 1 4 22.0 Asia Bangladesh 58.492236 5 3 1 2 3.0 Asia China 40.099363 2 4 1 4 9.0 Asia China 40.716526 3 2 1 5 34.0 Asia China 41.488606 4 2 1 4 53.0 Asia China 42.986541 4 4 1 2 7.0 Asia China 43.353136 2 5 1 3 49.0 Asia China 44.045816 3 2 1 5 1.0 Asia China 45.908339 3 3 1 4 52.0 Asia China 49.910424 5 3 1 2 40.0 Asia China 50.852176 3 2 1 5 14.0 Asia China 53.105643 3 2 1 5 15.0 Asia China 53.245060 4 2 1 4 23.0 Asia China 57.018255 4 2 1 4 45.0 Asia China 58.395190 3 2 1 5 57.0 Asia China 59.197112 2 3 1 5 26.0 Asia China 59.516640 3 4 1 3 21.0 Asia China 59.535480 4 3 1 3 44.0 Asia India 41.307787 2 5 1 3 48.0 Asia India 41.448672 2 5 1 3 37.0 Asia India 41.713956 2 3 1 5 5.0 Asia India 44.313757 2 5 1 3 19.0 Asia India 47.688853 2 5 1 3 39.0 Asia India 53.535032 4 4 1 2 43.0 Asia India 54.314630 2 3 1 5 50.0 Asia India 56.103479 2 5 1 3 0.0 Asia India 56.155755 2 3 1 5 59.0 Asia India 56.180263 4 4 1 2 17.0 Asia India 58.247667 2 5 1 3 16.0 Asia India 58.393727 2 4 1 4 31.0 Asia India 58.884043 3 3 1 4 41.0 Asia India 59.004900 2 3 1 5 12.0 Asia India 59.838218 2 4 1 4 28.0 Asia Indonesia 44.747853 4 4 1 2 27.0 Asia Indonesia 53.665286 4 2 1 4 60.0 Asia Japan 44.237347 4 2 1 4 4.0 Asia Pakistan 41.385745 2 5 1 3 58.0 Asia Philippines 42.990101 2 3 1 5 42.0 Asia Saudi Arabia 42.746346 4 2 1 4 32.0 Asia Turkey 49.816575 3 4 1 3 51.0 Asia ['China' 'Taiwan'] 42.435033 2 4 1 4 47.0 Asia ['Pakistan' 'Tajikistan'] 45.447561 4 2 1 4 25.0 Europe France 53.116500 5 2 1 3 30.0 Europe Germany 40.177276 3 5 1 2 13.0 Europe Italy 46.469427 3 2 1 5 61.0 Europe Poland 46.816901 3 5 1 2 33.0 Europe Romania 58.854495 3 3 1 4 29.0 North America Mexico 49.474287 2 5 1 3 46.0 North America Mexico 55.503530 3 2 1 5 24.0 North America United States of America 40.777742 2 4 1 4 8.0 South America Brazil 55.562492 5 2 1 3 18.0 South America ['Brazil' 'Venezuela'] 53.190197 3 5 1 2",
    "context_chunks": [
      {
        "text": "Idea It's quite sad that so many great countries (e.g. India) and players (e.g. Mo Salah) may never play at the FIFA (football/soccer) World Cup (the same argument could apply as well to other sports events dominated by a handful of dominant teams, such international cricket and basketball tournaments). It would be neat to try and create a more balanced event, while still keeping the location-based element, where each team is of a (roughly) similar strength, with all of the players from a contiguous area (preferably of the same landmass, but obviously not possible in all circumstances, and beyond the scope of this question). For example, in the case of a weaker region, maybe their team would span many countries (e.g. maybe a South Asian team), whereas for a strong country, maybe there would be multiple teams (e.g. a team of just the players from the suburbs of Paris). I could then afterwards plot the areas, maybe using Voronois. Background I managed to gather the relevant information, through scraping (of Football Manager and Transfermarkt), but I'm stuck on how to design the algorithm to select the teams. Problem There is a large number of coordinates, which correspond to places of birth of players. These places all have players, and the players all have ratings (from 1 - 100) and positions. The task is, given a certain team size (11), and a certain number of teams (in the example below, 10), and a certain number of required players in each position (1, though substitutes would be handy), divide the area up into contiguous areas where the best team you can form from the players of the given area has roughly equal skill to the teams of other areas. Question I've been reading a bit about graph theory things, but I'm unsure where to begin creating a program which solves this problem. I was looking for some guidance on this, and (if it's possible), if you could create something with the toy example, that would be amazing!! Also, if tackling the overall problem is too difficult, if you can find a way to narrow the problem, and can create something which addresses that smaller problem, which can then be generalised to the larger problem, that would be great too. Sample code (in R, but I've included Python and Julia equivalent code further below- an answer using one of these (or similar) would be great) set.seed(0) library(tidyverse) df &lt;- tibble( # generate random latitudes and longitudes, and the number of players for that location lat = runif(100, 0, 90), lon = runif(100, 0, 180), players = rnorm(100, 0, 5) |&gt; abs() |&gt; ceiling() |&gt; as.integer() ) num_positions &lt;- 11 position &lt;- letters[1:num_positions] df &lt;- df |&gt; # generate position and skill data, and unnest mutate(position = map(players, ~sample(position, .x, replace = TRUE)), skill = map(players, ~sample(1:100, .x, replace = TRUE)))|&gt; unnest_longer(c(position, skill)) # plot the data df |&gt; summarise(skill = mean(skill), players = first(players), .by = c(lat, lon)) |&gt; ggplot(aes(x = lon, y = lat)) + geom_point(aes(size = players, color = skill)) + scale_size_continuous(range = c(1, 10)) + scale_color_viridis_c(option = &quot;magma&quot;) + # similar to the Colombian shirt colours theme_minimal() + theme(legend.position = &quot;none&quot;, panel.grid.major = element_blank(), panel.grid.minor = element_blank()) n_teams &lt;- 10 Scatter plot of the sample data (circle size is number of players, colour is average skill) Notes In the thing I want to do, it would involve 64 teams, so at least 704 players, and probably around 3x the size of the sample dataset. The real dataset has a lot of rows, but by filtering it, I should be able to get it down to a few thousand. In real life, some players can play well in more than one position, but in the example code I gave, each player only has one position. Adding multiple positions per player would likely make this a lot more difficult to solve, so it's outside the scope of this question. If you can do it across a sphere (like the globe), that would be amazing, but a rectangle would be okay too. Reinderien has helpfully pointed out that using contiguousness without compactness could lead to some Amigara Fault-like jerrymandered monstrosities, similar to what we see in US electoral maps. So although I'm still trying to figure out a better title, I would say prioritise compactness over continguity if it works better for you (though I'm guessing this would throw up the problem of exclaves?) Update: Python code: import random, math, pandas random.seed(0) df = pandas.DataFrame({'lat': [random.uniform(0, 90) for i in range(100)], 'lon': [random.uniform(0, 180) for i in range(100)], 'players':[math.ceil(abs(random.normalvariate(0, 5))) for i in range(100)]}) num_players = 11 positions = list(map(chr, range(97, 97+num_players))) df['position'] = df['players'].apply(lambda x: random.choices(positions, k=x)) df['skill'] = df['players'].apply(lambda x: random.choices(range(1, 101), k=x)) Julia code: using Random, DataFrames Random.seed!(0) df = DataFrame(lat = rand(100) * 90, lon = rand(100) * 180, players = Int64.(ceil.(abs.(5randn(100))))) num_positions = 11 position = ['a'+ i for i in 0:num_positions-1] df[!, :position] = [rand(position, players) for players in df.players] df[!, :skill] = [rand(1:100, players) for players in df.players] df = flatten(df, [:position, :skill])",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "First let's talk about contiguity. I think that you should value compactness over contiguity; after all, the Americans have proven that anything can be contiguous if you believe it hard enough. Contiguity is a &quot;hard problem&quot; to represent and compactness is an &quot;easy problem&quot; to represent. Scalability becomes a challenge because you aren't performing a simple similarity clustering (e.g. k-means); you're performing clustering with much more complex criteria. This is not my specialty, and so in my demonstration I show a simplification that is stochastically correct: assuming player skill and position are all uniformly distributed, a random selection of team centroid based on population density that is also of uniform distribution will represent reality (especially if you run multiple outer iterations with random perturbation). Specifically, if you have time to burn and want to refine the solution, you can wrap this approach in a differential evolution loop with the team centroid as the evolved parameter. Speaking of complex criteria - this system is only useful if you construct correct teams, and correct teams require position criteria. I present a matrix that you can adjust to set lower and upper bounds on the required number of positions per team. It is possible - but potentially very slow - to tell an optimizer to narrow the range of allowed skill levels. You may be best off to perform this as a pre-solve step, eliminating potential players that are some selected number of standard deviations away from your target skill level. Either that, or represent fairness not as an LP objective but as an LP constraint, with bounds on the minimum and maximum skill sum per team. I demonstrate the latter; but note that this is most feasible when we constrain the mean team skill toward the mean skill overall (in this case 50). Defining a range to be too narrow (less than ~ 20%) or defining the target to be far away from the mean make solution difficult. All together, import geopandas import matplotlib.pyplot as plt import numpy as np import pandas as pd import pulp import shapely from numpy.random import default_rng, Generator n_teams = 64 players_per_team = 11 total_players = n_teams * players_per_team max_skill = 100 def synthesize_world_data(rand: Generator) -&gt; tuple[ geopandas.GeoDataFrame, # player and country data pd.DataFrame, # field position data ]: # Load the default world dataset from geopandas, # dropping a couple of countries with degenerate bounds. # There are better ways to fix this (not shown here). world = ( geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres')) .rename(columns={'name': 'country'}) ) world.index.name = 'country' world[['left', 'bottom', 'right', 'top']] = shapely.bounds(world['geometry']) is_degenerate = np.isclose(world['right'] - world['left'], 360) world = world[~is_degenerate] # Assume that talent is uniformly distributed throughout the world, # so players are spatially uniformly distributed. Draw from a pool # of players ~2 times what is needed for final team formation. pop_ratio = 2 * total_players / world['pop_est'].sum() world['players_est'] = world['pop_est'] * pop_ratio # Hack: perform an inefficient but statistically correct spatial random distribution of players # within their countries by allocating random coordinates within rectilinear bounds and then doing # geometric elimination of those players that have decided to live in the ocean world['geodensity'] = shapely.area(world['geometry'])/(world['right'] - world['left'])/(world['top'] - world['bottom']) n_soggy_players = (world['players_est'] / world['geodensity']).round().astype(int) soggy_allocated = pd.DataFrame( np.arange(n_soggy_players.max()) &lt; n_soggy_players.values[:, np.newaxis], index=n_soggy_players.index, columns=pd.RangeIndex(name='country_player', start=0, stop=n_soggy_players.max()), ).replace(False, np.nan).stack() world, _ = world.align(soggy_allocated, axis=0) world['player_lon'] = rand.uniform(low=world['left'], high=world['right']) world['player_lat'] = rand.uniform(low=world['bottom'], high=world['top']) not_soggy = shapely.contains_xy(world['geometry'], world[['player_lon', 'player_lat']]) world = world[not_soggy] # Assume that skill and position are uniformly distributed # https://en.wikipedia.org/wiki/Association_football_positions world['player_skill'] = rand.uniform(low=0, high=max_skill, size=len(world)) positions = pd.DataFrame( { 'min_players': [1, 2, 2, 2], 'max_players': [1, 5, 5, 5], }, index=pd.Index(name='name', data=['goalkeep', 'defender', 'midfield', 'forward']), ) pos_mean = (positions['min_players'] + positions['max_players'])/2 world['position'] = rand.choice( a=positions.index, p=pos_mean/pos_mean.sum(), size=len(world), ) world.set_index( pd.RangeIndex(name='player', start=0, stop=len(world)), append=False, inplace=True, ) return world, positions def guess_team_centroids(rand: Generator, world: geopandas.GeoDataFrame) -&gt; tuple[ pd.DataFrame, # centroids per team pd.Series, # player-centroid distances ]: coord_fields = ['player_lat', 'player_lon'] centroid_idx = rand.choice(a=world.index, size=n_teams) centroids = ( world.loc[ centroid_idx, coord_fields, ] .rename(columns={'player_lat': 'team_lat', 'player_lon': 'team_lon'}) .set_index(pd.RangeIndex(name='team', start=0, stop=n_teams)) ) coords = np.deg2rad( pd.merge( how='cross', left=centroids.reset_index(), right=world[coord_fields].reset_index(), ).set_index(['team', 'player']) ) # Haversine norm (how is this not in geopandas?) a = ( np.sin((coords['player_lat'] - coords['team_lat'])/2)**2 + np.sin((coords['player_lon'] - coords['team_lon'])/2)**2 * np.cos(coords['team_lat']) * np.cos(coords['player_lat']) ) c = np.arctan2(np.sqrt(a), np.sqrt(1 - a)) r = (world.crs.ellipsoid.semi_major_metre + world.crs.ellipsoid.semi_minor_metre) * 1e-3 distances = r*c return centroids, distances def make_vars(world: geopandas.GeoDataFrame) -&gt; pd.Series: team_idx = pd.RangeIndex(name='team', start=0, stop=n_teams) assign_idx = pd.MultiIndex.from_product((team_idx, world.index)) names = assign_idx.to_frame().astype(str) assigns = ( 'asn_t' + names['team'] + '_p' + names['player'] ).apply(pulp.LpVariable, cat=pulp.LpBinary) return assigns def make_objective( assigns: pd.Series, distances: pd.Series, ) -&gt; pulp.LpAffineExpression: # For compactness, minimize the distance from each player to their team's centroid return pulp.lpDot(assigns, distances)/total_players/100 def add_constraints( prob: pulp.LpProblem, world: geopandas.GeoDataFrame, positions: pd.DataFrame, assigns: pd.Series, ) -&gt; None: for team, group in assigns.groupby('team'): # There must be 11 players per team prob.addConstraint( name=f'teamsize_t{team}', constraint=pulp.lpSum(group) == players_per_team, ) # Enforce competitive team skill sum skill_deviation = pulp.lpDot( group, world.loc[group.index.get_level_values('player'), 'player_skill'], )/players_per_team - max_skill/2 prob.addConstraint( name=f'skill_lo_t{team}', constraint=skill_deviation &gt;= -10, ) prob.addConstraint( name=f'skill_hi_t{team}', constraint=skill_deviation &lt;= 10, ) # Each player may only be assigned up to one team for player, group in assigns.groupby('player'): prob.addConstraint( name=f'playerexcl_p{player}', constraint=pulp.lpSum(group) &lt;= 1, ) # Enforce the team position bounds for position, (tmin, tmax) in positions.iterrows(): pos_players = world.index[world['position'] == position] pos_assigns = assigns.loc[(slice(None), pos_players)] for team, group in pos_assigns.groupby('team'): total = pulp.lpSum(group) prob.addConstraint( name=f'poslo_t{team}_{position}', constraint=total &gt;= tmin, ) prob.addConstraint( name=f'poshi_t{team}_{position}', constraint=total &lt;= tmax, ) def solve(prob: pulp.LpProblem, world: geopandas.GeoDataFrame, assigns: pd.Series) -&gt; geopandas.GeoDataFrame: prob.solve() if prob.status != pulp.LpStatusOptimal: raise ValueError('Solution status', prob.status) assigns = ( assigns.apply(pulp.LpVariable.value) .astype(int) .unstack(level='team') ) team_player_idx, team_idx = assigns.values.nonzero() world.loc[team_player_idx, 'team'] = team_idx return world def dump_solution( world: geopandas.GeoDataFrame, ) -&gt; None: pd.set_option('display.max_rows', 1000) pd.set_option('display.max_columns', 1000) pd.set_option('display.width', 1000) world = world.loc[ world['team'].notna(), ['continent', 'country', 'team', 'position', 'player_skill'], ] print('Players by country:') print(world.sort_values(['continent', 'country', 'team', 'position']), end='\\n\\n') print('Players by team and position:') print(world.sort_values(['team', 'position']), end='\\n\\n') print('Team summary:') grouped = world.groupby('team') teams = world.groupby(['team', 'position']).country.count().unstack('position') teams.insert(loc=0, column='continent', value=grouped['continent'].agg(pd.Series.mode).astype(str)) teams.insert(loc=1, column='country', value=grouped['country'].agg(pd.Series.mode).astype(str)) teams.insert(loc=2, column='skill', value=grouped['player_skill'].sum() / players_per_team) teams.sort_values(['continent', 'country', 'skill'], inplace=True) print(teams, end='\\n\\n') def plot_solution(world: geopandas.GeoDataFrame) -&gt; plt.Figure: fig, ax = plt.subplots() world['geometry'].boundary.plot(ax=ax) for team, group in world.groupby('team'): ax.scatter(group['player_lon'], group['player_lat']) return fig def main() -&gt; None: rand = default_rng(seed=0) print('Synthesizing world data...') world, positions = synthesize_world_data(rand) centroids, distances = guess_team_centroids(rand, world) print('Making assignment variables...') assigns = make_vars(world) prob = pulp.LpProblem(name='football_clustering', sense=pulp.LpMinimize) print('Defining objective...') prob.objective = make_objective(assigns, distances) print('Adding constraints...') add_constraints(prob, world, positions, assigns) # print(prob) print('Solving...') world = solve(prob, world, assigns) dump_solution(world) plot_solution(world) plt.show() if __name__ == '__main__': main() ... Team summary: position continent country skill defender forward goalkeep midfield team 6.0 Africa Algeria 44.963361 3 2 1 5 2.0 Africa Burkina Faso 45.802411 5 3 1 2 56.0 Africa Dem. Rep. Congo 47.018698 3 2 1 5 10.0 Africa Ethiopia 40.294830 3 4 1 3 11.0 Africa Ethiopia 43.277617 2 5 1 3 38.0 Africa Madagascar 59.876627 3 2 1 5 54.0 Africa Morocco 55.331606 3 2 1 5 63.0 Africa Nigeria 40.088676 4 3 1 3 55.0 Africa South Africa 55.269027 3 4 1 3 62.0 Africa Uganda 47.396431 3 2 1 5 36.0 Africa Uganda 48.577035 2 5 1 3 35.0 Africa ['Burundi' 'Dem. Rep. Congo' 'Tanzania'] 53.517437 3 4 1 3 20.0 Asia Bangladesh 57.757983 4 2 1 4 22.0 Asia Bangladesh 58.492236 5 3 1 2 3.0 Asia China 40.099363 2 4 1 4 9.0 Asia China 40.716526 3 2 1 5 34.0 Asia China 41.488606 4 2 1 4 53.0 Asia China 42.986541 4 4 1 2 7.0 Asia China 43.353136 2 5 1 3 49.0 Asia China 44.045816 3 2 1 5 1.0 Asia China 45.908339 3 3 1 4 52.0 Asia China 49.910424 5 3 1 2 40.0 Asia China 50.852176 3 2 1 5 14.0 Asia China 53.105643 3 2 1 5 15.0 Asia China 53.245060 4 2 1 4 23.0 Asia China 57.018255 4 2 1 4 45.0 Asia China 58.395190 3 2 1 5 57.0 Asia China 59.197112 2 3 1 5 26.0 Asia China 59.516640 3 4 1 3 21.0 Asia China 59.535480 4 3 1 3 44.0 Asia India 41.307787 2 5 1 3 48.0 Asia India 41.448672 2 5 1 3 37.0 Asia India 41.713956 2 3 1 5 5.0 Asia India 44.313757 2 5 1 3 19.0 Asia India 47.688853 2 5 1 3 39.0 Asia India 53.535032 4 4 1 2 43.0 Asia India 54.314630 2 3 1 5 50.0 Asia India 56.103479 2 5 1 3 0.0 Asia India 56.155755 2 3 1 5 59.0 Asia India 56.180263 4 4 1 2 17.0 Asia India 58.247667 2 5 1 3 16.0 Asia India 58.393727 2 4 1 4 31.0 Asia India 58.884043 3 3 1 4 41.0 Asia India 59.004900 2 3 1 5 12.0 Asia India 59.838218 2 4 1 4 28.0 Asia Indonesia 44.747853 4 4 1 2 27.0 Asia Indonesia 53.665286 4 2 1 4 60.0 Asia Japan 44.237347 4 2 1 4 4.0 Asia Pakistan 41.385745 2 5 1 3 58.0 Asia Philippines 42.990101 2 3 1 5 42.0 Asia Saudi Arabia 42.746346 4 2 1 4 32.0 Asia Turkey 49.816575 3 4 1 3 51.0 Asia ['China' 'Taiwan'] 42.435033 2 4 1 4 47.0 Asia ['Pakistan' 'Tajikistan'] 45.447561 4 2 1 4 25.0 Europe France 53.116500 5 2 1 3 30.0 Europe Germany 40.177276 3 5 1 2 13.0 Europe Italy 46.469427 3 2 1 5 61.0 Europe Poland 46.816901 3 5 1 2 33.0 Europe Romania 58.854495 3 3 1 4 29.0 North America Mexico 49.474287 2 5 1 3 46.0 North America Mexico 55.503530 3 2 1 5 24.0 North America United States of America 40.777742 2 4 1 4 8.0 South America Brazil 55.562492 5 2 1 3 18.0 South America ['Brazil' 'Venezuela'] 53.190197 3 5 1 2",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Disclaimer Sorry that I don't have a real and exact solution for your full question so far (as we discussed in the comments, the question involves with many factors/constraints and seems not easy to tackle it within just one shot), but I guess probably we can depart from a minimal example to see if we can find some clues. A Minimal Sub-Problem (with one of constraints only) If I understand your objective correctly, you want to group the players such that the formed teams has close strength, along with the following constraints: Each team should be of equal size, i.e., 11. Each team has all its needed roles (or skills); The areas of teams should be contiguous. From the optimization perspective, it is true that adding more constraints can somewhat accelerate the problem solving procedure, since the size of feasible set of variables is narrowed down, but it dramatically increases the complexity of formulating the optimization problem in the meanwhile. (as corrected by @Reinderien in the comment: &quot;this is only really true if the constraints are in a pre-solve step prior to optimization. Otherwise, the opposite is generally true. The optimizer needs to work harder to find feasible solutions because they occupy a smaller portion of the parameter space.&quot;) I am not aiming to completely solve your problem within this single answer, but would like to take a tiny bite from a minimal sub-problem of yours and see the possibilities for adding more constraints. If we focus on the the objective plus the constraint 1) only, the problem can be translated as a classical integer programming problem: Given a numeric vector vec and number of groups grpNr, how to evenly allocate the entries of vec into grpNr groups (grpNr is an integer that always divides length(vec)), such that the sums of groups gives the minimal difference. Problem Formulation and Solving There are lots of options for solving optimization problems in R, and here is just one implementation with CVXR package for example library(CVXR) # Input numeric vector where entries are to be grouped set.seed(0) vec &lt;- runif(20, 0, 10) # Given number of groups grpNr &lt;- 5 # Define optimization variables: Binary variables indicate x &lt;- Variable(length(vec), grpNr, boolean = TRUE) # Define objective: minimize the difference in group sums obj &lt;- Minimize(norm(t(vec) %*% x - sum(vec) / grpNr)) # Constraint: each item must be in exactly one group constrs &lt;- list( sum_entries(x, 1) == 1, sum_entries(x, 2) == length(vec) / grpNr ) # Formulate and solve the problem prb &lt;- Problem(obj, constrs) res &lt;- solve(prb) # Info of resulting groups grpInfo &lt;- apply(round(res$getValue(x)) &gt; 0, 2, which, simplify = FALSE) and we can obtain a the grouping information &gt; grpInfo [[1]] [1] 1 3 8 11 [[2]] [1] 7 12 13 19 [[3]] [1] 6 10 14 18 [[4]] [1] 4 16 17 20 [[5]] [1] 2 5 9 15 and the distribution of group sum &gt; vec %*% round(res$getValue(x)) [,1] [,2] [,3] [,4] [,5] [1,] 22.75283 22.72827 22.35437 22.20429 22.18618 Way Forward To be honest, my capability is limited by my knowledge, so I have no idea if I am on the right track or where shall we go from this point ... It should be noted that, the above is just for my first try on a minimal sub-problem, where the scalability might be unfortunately terrible (since the size of x explodes if we have a longer vec or a larger grpNr). I will keep thinking about the potential improvement, and also would like to see contributions from other genius answers, particularly targeting on the entire problem. :)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "r",
        "algorithm",
        "julia",
        "mathematical-optimization"
      ],
      "question_score": 12,
      "answer_score": 6,
      "created": "2023-08-14T04:08:20",
      "question_id": 76896221,
      "answer_id": 76949264
    }
  },
  {
    "question": "Cannot install mysqlclient on MacOS",
    "expected_answer": "This doc helped me: https://github.com/PyMySQL/mysqlclient I followed the section below. macOS (Homebrew) Install MySQL and mysqlclient: # Assume you are activating Python 3 venv $ brew install mysql pkg-config $ pip install mysqlclient If you don't want to install MySQL server, you can use mysql-client instead: # Assume you are activating Python 3 venv $ brew install mysql-client pkg-config $ export PKG_CONFIG_PATH=&quot;/opt/homebrew/opt/mysql-client/lib/pkgconfig&quot; $ pip install mysqlclient I'm using MariaDB docker image, so I didn't want to install mysql in my Mac so I followed the second instruction.",
    "context_chunks": [
      {
        "text": "I have been trying to integrate mysql into my Flask project. I recently moved to a MacOS so having a hard time installing the dependencies. I tried to download mysql client using pip but gives the following error Using cached mysqlclient-2.2.0.tar.gz (89 kB) Installing build dependencies ... done Getting requirements to build wheel ... error error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; [25 lines of output] Trying pkg-config --exists mysqlclient Command 'pkg-config --exists mysqlclient' returned non-zero exit status 1. Trying pkg-config --exists mariadb Command 'pkg-config --exists mariadb' returned non-zero exit status 1. Traceback (most recent call last): File &quot;/Users/L053984/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 353, in &lt;module&gt; main() File &quot;/Users/L053984/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/Users/L053984/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 118, in get_requires_for_build_wheel return hook(config_settings) ^^^^^^^^^^^^^^^^^^^^^ File &quot;/private/var/folders/m2/4ys_q90d1530pr6h8c70xj340000gp/T/pip-build-env-0g_2cgj8/overlay/lib/python3.11/site-packages/setuptools/build_meta.py&quot;, line 341, in get_requires_for_build_wheel return self._get_build_requires(config_settings, requirements=['wheel']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/private/var/folders/m2/4ys_q90d1530pr6h8c70xj340000gp/T/pip-build-env-0g_2cgj8/overlay/lib/python3.11/site-packages/setuptools/build_meta.py&quot;, line 323, in _get_build_requires self.run_setup() File &quot;/private/var/folders/m2/4ys_q90d1530pr6h8c70xj340000gp/T/pip-build-env-0g_2cgj8/overlay/lib/python3.11/site-packages/setuptools/build_meta.py&quot;, line 338, in run_setup exec(code, locals()) File &quot;&lt;string&gt;&quot;, line 154, in &lt;module&gt; File &quot;&lt;string&gt;&quot;, line 48, in get_config_posix File &quot;&lt;string&gt;&quot;, line 27, in find_package_name Exception: Can not find valid pkg-config name. Specify MYSQLCLIENT_CFLAGS and MYSQLCLIENT_LDFLAGS env vars manually [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip I installed brew as well as MacPorts but not able to solve the issue. Help would be appreciated",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This doc helped me: https://github.com/PyMySQL/mysqlclient I followed the section below. macOS (Homebrew) Install MySQL and mysqlclient: # Assume you are activating Python 3 venv $ brew install mysql pkg-config $ pip install mysqlclient If you don't want to install MySQL server, you can use mysql-client instead: # Assume you are activating Python 3 venv $ brew install mysql-client pkg-config $ export PKG_CONFIG_PATH=&quot;/opt/homebrew/opt/mysql-client/lib/pkgconfig&quot; $ pip install mysqlclient I'm using MariaDB docker image, so I didn't want to install mysql in my Mac so I followed the second instruction.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "MacOS - M3 Pro (Using Official MySQL Community Server ) Encountering issues with MySQL on your Mac M3 Pro or Apple Silicon ? As of now, MySQL isn't fully compatible with Apple Silicon when installed via Homebrew. You can verify the compatibility status for MySQL on Apple Silicon here. To overcome this, follow these steps: 1. Download MySQL from the Official Website: Visit MySQL's official download page and fetch the appropriate version for your system. 2. Specify Environment Variables: Manually set the MYSQLCLIENT_CFLAGS and MYSQLCLIENT_LDFLAGS environment variables. In your ~/.zshrc file, append the following lines (adjust the version number accordingly): you can find path using following command sudo find / -name mysql.h export MYSQL_HOME=/usr/local/mysql-8.3.0-macos14-arm64/include export MYSQLCLIENT_CFLAGS=&quot;-I/usr/local/mysql-8.3.0-macos14-arm64/include&quot; export MYSQLCLIENT_LDFLAGS=&quot;-L/usr/local/mysql-8.3.0-macos14-arm64/lib -lmysqlclient&quot; Run source ~/.zshrc in your terminal to reload configuration files 3. Install the MySQL Client Library: Activate your virtual environment and install the MySQL client library using the following command: pip install mysqlclient ⠀ Ensure that you verify your installed version of MySQL and adjust the paths accordingly. Once these steps are completed, your MySQL setup should function smoothly on your Mac M3 Pro or Apple Silicon",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "mysql",
        "macos",
        "pip"
      ],
      "question_score": 11,
      "answer_score": 34,
      "created": "2023-08-10T14:43:17",
      "question_id": 76876823,
      "answer_id": 77086124
    }
  },
  {
    "question": "What&#39;s the polars equivalent to the pandas `.iloc` method?",
    "expected_answer": "This is a very good sheet by: @Liam Brannigan Credit to them. https://www.rhosignal.com/posts/polars-pandas-cheatsheet/ A glimse from the sheet: You can find other information related to Filtering Rows using iloc and its equivalent in polars in the sheet.",
    "context_chunks": [
      {
        "text": "I'm looking for the recommended way to select an individual row of a polars.DataFrame by row number: something largely equivalent to pandas.DataFrame's .iloc[[n]] method for a given integer n. For polars imported as pl and a polars DataFrame df, my current approach would be: # for example n = 3 # create row index, filter for individual row, drop the row index. new_df = ( df.with_row_index() .filter(pl.col('index') == n) .select(pl.exclude('index')) ) I'm migrating from Pandas, and I've read the Pandas-to-Polars migration guide, but a slick solution to this specific case wasn't addressed there. Edit: to clarify, I am looking for an approach that returns a polars.DataFrame object for the chosen row. Does anyone have something slicker?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is a very good sheet by: @Liam Brannigan Credit to them. https://www.rhosignal.com/posts/polars-pandas-cheatsheet/ A glimse from the sheet: You can find other information related to Filtering Rows using iloc and its equivalent in polars in the sheet.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I guess, the direct equivalent of .iloc from pandas is the .row() method in polars. If you have a dataframe, df: df = pl.DataFrame( { &quot;foo&quot;: [1, 2, 3], &quot;bar&quot;: [6, 7, 8], &quot;ham&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], } ) Here's how you'd access a row: df.row(2) (3, 8, 'c')",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pandas",
        "dataframe",
        "python-polars"
      ],
      "question_score": 11,
      "answer_score": 23,
      "created": "2024-01-25T17:42:34",
      "question_id": 77881942,
      "answer_id": 77882017
    }
  },
  {
    "question": "How to resolve `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead error from docplex?",
    "expected_answer": "Do: pip install &quot;numpy&lt;2&quot; Or import numpy as np np.float_ = np.float64",
    "context_chunks": [
      {
        "text": "I'm trying to rerun an older code (about a year old). I have installed academic version of CPLEX 22.1.1 version and Python API and docplex in python. However, the dependencies have changed and therefore I am unable to run the code now. Traceback (most recent call last): File &quot;C:\\Users\\...\\Desktop\\Jakhu\\iitk\\Sem3\\Codes\\Possibly_smart\\Master.py&quot;, line 4, in &lt;module&gt; from docplex.mp.model import Model File &quot;C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\docplex\\mp\\model.py&quot;, line 16, in &lt;module&gt; from docplex.mp.aggregator import ModelAggregator File &quot;C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\docplex\\mp\\aggregator.py&quot;, line 14, in &lt;module&gt; from docplex.mp.utils import is_number, is_iterable, is_iterator, is_pandas_series, \\ File &quot;C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\docplex\\mp\\utils.py&quot;, line 70, in &lt;module&gt; __float_types.add(numpy.float_) ^^^^^^^^^^^^ File &quot;C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\__init__.py&quot;, line 411, in __getattr__ raise AttributeError( AttributeError: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.. Did you mean: 'float16'? I tried installing numpy version using pip install --force-reinstall numpy==1.19.5 and pip install docplex==2.10.154 to make it compatible but it still throws the same error. I cannot change the aliases of numpy.float since they are arriving from docplex and are outside my control. What is happening here? How do I resolve this? Many thanks for reading!",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Do: pip install &quot;numpy&lt;2&quot; Or import numpy as np np.float_ = np.float64",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Please try to downgrade numpy (for example, version 1.26.4) while keeping docplex==2.27.239",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "numpy",
        "cplex",
        "docplex"
      ],
      "question_score": 11,
      "answer_score": 20,
      "created": "2024-04-18T15:47:39",
      "question_id": 78348773,
      "answer_id": 78721422
    }
  },
  {
    "question": "What is the most efficient way to fillna multiple columns with values from other columns in a way that they can be paired with a suffix?",
    "expected_answer": "What about using an Index to select all columns at once and set_axis to realign the DataFrame: cols = pd.Index(['x', 'y']) df[cols] = df[cols].fillna(df[cols+'_a'].set_axis(cols, axis=1)) NB. this is assuming all columns in cols and all '_a' columns exist. If you're not sure you could be safe and use intersection and reindex: cols = pd.Index(['x', 'y']).intersection(df.columns) df[cols] = df[cols].fillna(df.reindex(columns=cols+'_a').set_axis(cols, axis=1)) Or for an approach that is fully independent of explicitly passing input columns and just relying on the suffix (_a): suffix = '_a' # find columns &quot;xyz&quot; that have a &quot;xyz_a&quot; counterpart c1 = df.columns.intersection(df.columns+suffix) c2 = c1.str.removesuffix(suffix) # select, fillna, update df[c2] = df[c2].fillna(df[c1].set_axis(c2, axis=1)) Output: x y x_a y_a 0 1.0 6.0 1 6 1 2.0 7.0 2 7 2 3.0 8.0 3 8 3 4.0 9.0 4 9 4 5.0 10.0 5 10 Example for which the second approach would be needed: df = pd.DataFrame( { 'x': [1, np.nan, 3, np.nan, 5], 'z': [np.nan, 7, 8, 9, np.nan], 'p_a': [1, 2, 3, 4, 5], 'y_a': [6, 7, 8, 9, 10] } )",
    "context_chunks": [
      {
        "text": "This is my DataFrame: import pandas as pd import numpy as np df = pd.DataFrame( { 'x': [1, np.nan, 3, np.nan, 5], 'y': [np.nan, 7, 8, 9, np.nan], 'x_a': [1, 2, 3, 4, 5], 'y_a': [6, 7, 8, 9, 10] } ) Expected output is fill_na columns x and y: x y x_a y_a 0 1.0 6.0 1 6 1 2.0 7.0 2 7 2 3.0 8.0 3 8 3 4.0 9.0 4 9 4 5.0 10.0 5 10 Basically I want to fillna x with x_a and y with y_a. In other words each column should be paired with another column that has the suffix _a and the column name. I can get this output by using this code: for col in ['x', 'y']: df[col] = df[col].fillna(df[f'{col}_a']) But I wonder if it is the best/most efficient way? Suppose I got hundreds of columns like these",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "What about using an Index to select all columns at once and set_axis to realign the DataFrame: cols = pd.Index(['x', 'y']) df[cols] = df[cols].fillna(df[cols+'_a'].set_axis(cols, axis=1)) NB. this is assuming all columns in cols and all '_a' columns exist. If you're not sure you could be safe and use intersection and reindex: cols = pd.Index(['x', 'y']).intersection(df.columns) df[cols] = df[cols].fillna(df.reindex(columns=cols+'_a').set_axis(cols, axis=1)) Or for an approach that is fully independent of explicitly passing input columns and just relying on the suffix (_a): suffix = '_a' # find columns &quot;xyz&quot; that have a &quot;xyz_a&quot; counterpart c1 = df.columns.intersection(df.columns+suffix) c2 = c1.str.removesuffix(suffix) # select, fillna, update df[c2] = df[c2].fillna(df[c1].set_axis(c2, axis=1)) Output: x y x_a y_a 0 1.0 6.0 1 6 1 2.0 7.0 2 7 2 3.0 8.0 3 8 3 4.0 9.0 4 9 4 5.0 10.0 5 10 Example for which the second approach would be needed: df = pd.DataFrame( { 'x': [1, np.nan, 3, np.nan, 5], 'z': [np.nan, 7, 8, 9, np.nan], 'p_a': [1, 2, 3, 4, 5], 'y_a': [6, 7, 8, 9, 10] } )",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You could use df.combine_first: cols = ['x', 'y'] df[cols] = df[cols].combine_first( df.filter(regex='_a$').rename(lambda x: x.rstrip('_a'), axis=1) ) Output: x y x_a y_a 0 1.0 6.0 1 6 1 2.0 7.0 2 7 2 3.0 8.0 3 8 3 4.0 9.0 4 9 4 5.0 10.0 5 10 If you can expect extra suffix-columns (e.g. z_a), add [cols] to be safe: df[cols] = df[cols].combine_first( df.filter(regex='_a$').rename(lambda x: x.rstrip('_a'), axis=1) )[cols] Approach should also work if one of your columns isn't matched by a suffix-variant. I.e.: df = pd.DataFrame( { 'x': [1, np.nan, 3, np.nan, 5], 'y': [np.nan, 7, 8, 9, np.nan], 'x_a': [1, 2, 3, 4, 5], # 'y_a': [6, 7, 8, 9, 10], 'z_a': [11, 12, 13, 14, 15] } ) Output: x y x_a z_a 0 1.0 NaN 1 11 1 2.0 7.0 2 12 2 3.0 8.0 3 13 3 4.0 9.0 4 14 4 5.0 NaN 5 15 Edit: Scenario without a preset list of columns (cols). In this case, you could start from the suffix columns: df = (df .filter(regex='_a$') .rename(lambda x: x.rstrip('_a'), axis=1) .combine_first(df)[df.columns] )",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ],
      "question_score": 11,
      "answer_score": 10,
      "created": "2024-06-13T10:28:30",
      "question_id": 78617300,
      "answer_id": 78617351
    }
  },
  {
    "question": "VSCode Python: Error while enumerating installed packages",
    "expected_answer": "fixed after installing importlib-metadata and debugpy in my venv",
    "context_chunks": [
      {
        "text": "My long-successful VSCode Python setup broke inexplicably. Please, what does the error below mean? First the recommended Python 3.12 can't find _tkinter. I try an older installed version and get this. I can't use pip either, it says &quot;externally managed environment&quot;. I'm lost. % cd /Users/ken/Teaching/Python2024 ; /usr/bin/env /opt/local/bin/python3.9 /Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 64660 -- /Users/ken/Teaching/Python2024/turtle1.py E+00000.193: Error while enumerating installed packages. Traceback (most recent call last): File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/common/log.py&quot;, line 361, in get_environment_description report(&quot; {0}=={1}\\n&quot;, pkg.name, pkg.version) AttributeError: 'PathDistribution' object has no attribute 'name' Stack where logged: File &quot;/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py&quot;, line 197, in _run_module_as_main return _run_code(code, main_globals, None, File &quot;/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py&quot;, line 87, in _run_code exec(code, run_globals) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/__main__.py&quot;, line 91, in &lt;module&gt; main() File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/__main__.py&quot;, line 21, in main log.describe_environment(&quot;debugpy.launcher startup environment:&quot;) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/common/log.py&quot;, line 369, in describe_environment info(&quot;{0}&quot;, get_environment_description(header)) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/common/log.py&quot;, line 363, in get_environment_description swallow_exception(&quot;Error while enumerating installed packages.&quot;) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/common/log.py&quot;, line 215, in swallow_exception _exception(format_string, *args, **kwargs) E+00000.030: Error while enumerating installed packages. Traceback (most recent call last): File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/common/log.py&quot;, line 361, in get_environment_description report(&quot; {0}=={1}\\n&quot;, pkg.name, pkg.version) AttributeError: 'PathDistribution' object has no attribute 'name' Stack where logged: File &quot;/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py&quot;, line 197, in _run_module_as_main return _run_code(code, main_globals, None, File &quot;/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py&quot;, line 87, in _run_code exec(code, run_globals) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py&quot;, line 39, in &lt;module&gt; cli.main() File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py&quot;, line 415, in main api.ensure_logging() File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/api.py&quot;, line 61, in ensure_logging log.describe_environment(&quot;Initial environment:&quot;) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/common/log.py&quot;, line 369, in describe_environment info(&quot;{0}&quot;, get_environment_description(header)) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/common/log.py&quot;, line 363, in get_environment_description swallow_exception(&quot;Error while enumerating installed packages.&quot;) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/common/log.py&quot;, line 215, in swallow_exception _exception(format_string, *args, **kwargs) E+00000.225: Error while enumerating installed packages. Traceback (most recent call last): File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/common/log.py&quot;, line 361, in get_environment_description report(&quot; {0}=={1}\\n&quot;, pkg.name, pkg.version) AttributeError: 'PathDistribution' object has no attribute 'name' Stack where logged: File &quot;/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py&quot;, line 197, in _run_module_as_main return _run_code(code, main_globals, None, File &quot;/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py&quot;, line 87, in _run_code exec(code, run_globals) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py&quot;, line 39, in &lt;module&gt; cli.main() File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py&quot;, line 430, in main run() File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py&quot;, line 281, in run_file log.describe_environment(&quot;Pre-launch environment:&quot;) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/common/log.py&quot;, line 369, in describe_environment info(&quot;{0}&quot;, get_environment_description(header)) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/common/log.py&quot;, line 363, in get_environment_description swallow_exception(&quot;Error while enumerating installed packages.&quot;) File &quot;/Users/ken/.vscode/extensions/ms-python.debugpy-2023.3.13341006-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/common/log.py&quot;, line 215, in swallow_exception _exception(format_string, *args, **kwargs)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "fixed after installing importlib-metadata and debugpy in my venv",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This solved the issue for me (from https://github.com/microsoft/debugpy/issues/1379): Locate debugpy/common/log.py under site-packages (following the path you see in the output). Find the line in that file that says: swallow_exception(&quot;Error while enumerating installed packages.&quot;)} replace it with: swallow_exception(&quot;Error while enumerating installed packages.&quot;, level=&quot;info&quot;) Is incredible this hasn't been released yet.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code",
        "pip"
      ],
      "question_score": 11,
      "answer_score": 19,
      "created": "2024-01-18T14:21:53",
      "question_id": 77840099,
      "answer_id": 77976576
    }
  },
  {
    "question": "Topology-matching algorithm for finding 2D lattice in a 3D lattice",
    "expected_answer": "Introduction Let me rephrase the problem a little: We have a 3D pattern that fills the space with points. We have a reference plane that cuts this space basically anywhere, however we only know about the pattern the points in space create on the plane, nothing else. Find the plane that best matches the reference plane. To make matters worse, the pattern isn't fully 2D, it's more akin to a thin slice. Fortunately for us, the number of points is rather limited so we can go for some brute-forcish approaches. However, we can't just compute all possible 2D lattices and find the one that matches the best, combinatorics would kill us. But we can do something similar and only bit more complicated. Setting up the playground: First let's generate the space we will be working in. To make things simple, let's extend the 3D lattice along all axes to generate a space large enough to fit in any given orientation and shift of the 2D lattice. In our toy example, the 3D lattice vectors give us a cube and the 2D lattice vectors fit within this cube. So if we add a some padding on each side, this will cover all the possible rotations. Now if the reference 2D lattice is large or if the 3D lattice is skewed, this might not be enough so keep that in mind—it will likely require some math and stacking the lattices in a way that creates a space big enough to fit all possible rotations and translations of the reference 2D lattice. If we would miss the center 3D lattice, we can always snap back to it by claiming that whichever lattice we have biggest overlap with, is the new center piece. Hopefully not too much to invalidate our follow up work. Note: If one lattice has 32 points, then we can use up to say 300 lattices to create our space. So far the worst part of the matching algorithm was quadratic so 10K * 10K * 2d_lattice_points computations should be doable. 300 latices gives us something like a 6x6x6 lattice cube. Also since we're concerned about the &quot;flat and pointy&quot; case, we're looking for some rectangle instead (for example, if the lattice was a 3x3x10, then we could do 10x10x3 stacking to compensate). Note: If the 3D lattice is skewed, I suggest cutting off the skewed part and prepending it to create a rectangle instead. This should be ok due to the periodic nature. import numpy as np bulk_vecs = np.array([[5.44370237, 0.0, 0.0 ], [0.0, 5.44370237, 0.0 ], [0.0, 0.0, 5.44370237]]) bulk_coords = np.array([[ 1.27013575, 0.03307657, -0.02898786], [ 0.02039057, 0.02332397, 0.79145084], [ 0.0137774, 1.18012325, 1.85879242], [ 1.22267376, 1.27420799, 2.72875596], [ 1.20645874, -0.0488289, 3.52879259], [ 0.08487146, -0.06430577, 4.44147333]]) ref_vecs = np.array([[2.5178270133578393, 0.0, 0.0 ], [0.0, 2.5178270133578393, 0.0 ], [0.0, 0.0, 5.341117665]]) bulk_space = [] for point in [x for x in bulk_coords]: for i in range(3): for j in range(3): for k in range(3): shift = i * bulk_vecs[0] + j * bulk_vecs[1] + k * bulk_vecs[2] bulk_space.append(point + shift) This gives us some 162 points in the toy example which is fine. If this level of stacking is enough for the main case, this would give us some 432 points which is still ok. My hope is to do enough optimizations to be able to go up to low-thousands of points in the bulk space. (E.g., the 3D lattice generated playground.) Going much beyond that might be possible, but it will get annoying quickly—as I mentioned, I'm going to do some partial brute forcing. Let the voodoo begin First of all, let's get rid of all those pesky decimals. Computer's don't like them. To do that, let us agree, that three decimals are enough for the precision (for now) bulk_space = np.array(np.array(bulk_space) * 1000, dtype='longlong') Note that this might be either too little or too much—I'm not sure about the actual level of deformation/imprecision that happens within the 2D plane. I will try to do some matching—if there are too many matches I will increase the precision. If too little (or none) I will decrease it. This is a discretization at its finest—we slice the continuous space (or finer, we already don't have full continuum in this case) into boxes of fixed size, and then say that every point matches to a box it is contained within. Then we forget all about the original points and happily compute away with the boxes. Finally if we succeed in our endeavor, we go back to the points and try to upgrade the precision. Note: From what I got it seems that the chosen precision doesn't result in any overlaps of points (two points within the cube)—the space we're working in doesn't seem to be too densely populated. If two (or more) points overlap we don't mind too much. However it means that in the final check (while increasing the precision) we need to check every one of them with our solution. Plane approximation: Since we don't have knowledge about the 2D plane orientation, we will use the distance matching first. We will compute a distance matrix of the 3D space and store it away. Then we will compute the distance matrix for the 2D lattice, and take the longest distance, since it will repeat itself the least due to limited space in the 3D-lattice based box. Then we will take one of its endpoints and take all the distances to other points and sort them. Note: We're going to be comparing distances and we don't actually care about their values, only their comparisons, we can omit the square roots and work with distances squared. It is cheaper computation wise. Now we will search for matches of the largest distance in both 2D and 3D. For every match, we will then try to find a matching sequence of distances. Note 2: For checking the existence of the match, we can sort the distance array corresponding to the point, find the matching maximum (which we already know that it's there from the previous point) and then find the rest in order—if we don't find any given match, we can discard the whole sequence since it's sorted. Note that we either need to keep indices while sorting (which is sort of annoying) or brute-search for the matches after we confirm their existence to infer the indices from the matrix (which takes a bit time—but not too much, only number of 2D-based points times the number of 3D points). So we will do that. Note 3: If we don't find any match due to the imprecision of the 2D lattice cut, we can try again with lower precision until we find it. Note 4: We don't care about finding multiple matches at this points. We're just filtering/validating the points. We will handle these in the next step. for i in range(len(ref_space)): for j in range(i, len(ref_space)): diff = ref_space[i] - ref_space[j] dist = np.sum(diff * diff) distance_matrix_2d[i, j] = dist distance_matrix_2d[j, i] = dist if dist &gt; distance_max_2d: dmax_i = i dmax_j = j distance_max_2d = dist distance_matrix_3d = np.zeros(shape=[len(bulk_space), len(bulk_space)], dtype='longlong') match_list_i = [] match_list_j = [] point_match = [] dist_min = 1000000000 dist_max = 0 for i in range(len(bulk_space)): for j in range(i, len(bulk_space)): diff = bulk_space[i] - bulk_space[j] dist = np.sum(diff * diff) distance_matrix_3d[i, j] = dist distance_matrix_3d[j, i] = dist if distance_max_2d - dist == 0: match_list_i.append(i) match_list_j.append(j) point_match.append(i) point_match.append(j) point_matches = [] for i in point_match: x = [] for j in range(len(distance_matrix_3d)): x.append(distance_matrix_3d[i, j]) x = np.sort(x) point_matches.append((i, x)) # Note that we're currently not using the sort # and I'm simply brute forcing the next step. # If that is an issue, this sort can be used to # make the next step log(n) * 2d_pts_count # by checking first the existence of the # distance via binary search in the 3D # sorted matrix. # However since we only have few 2D points, this # is likely unnecessary unless we're solving # some big case, where there might be other # performance bottlenecks. point_matches_bruteforce = [] for i, matches in point_matches: # Check all the rows defined by points from the previous # step (selected by matching the longest distance) row_failed = False for dist2d in distance_matrix_2d[dmax_i]: # For every distance in 2D matrix, # try finding a matching one found = False for dist3d in matches: # If we find it, we qualify the distance if dist3d == dist2d: found = True #print(&quot;dist2d checked&quot;) if not found: # If we didn't find any match, we can discard # the point. There is at least one point from # the set without matching distance, e.g., # we can't match the structure. row_failed = True break if row_failed: # If any row failed (no matching 3D distance # for a 2D one, we discard the point) print(&quot;point &quot;, i, &quot;has failed to find matching set&quot;) else: # Otherwise, we keep it for the next, more bruteforcish round print(&quot;point &quot;, i, &quot;is a candidate&quot;) point_matches_bruteforce.append(i) Finding the points: We have now filtered the 3D candidate list and it's time to find whether any of them match our 2D lattice. For this we will use the triangle identity: two triangles are identical, if all their sides are identical. How will we do that? We already have the distances matrix for both 2D and 3D. Furthermore, we have the starting point with its possible neighbors. So for every such starting point, we will check whether the distances of neighbors are matching. This will take us some time—number_of_2d_points * number_or_3d_points if we don't do any optimizations. Note: There might be some pathological edge cases where this will find too many matches. There is always one optimization possible: sort the distances, then use binary search for matching, reducing the time from n to log(n) per point to be checked at the cost of the initial sort. Annoying part is that it will shuffle around indices so you will need to note them down and keep track of them. The algorithm: take the starting point find its potential neighbors via distance check (number_of_2d_points * number_of_3d_points) note that there might be duplicates. We will get rid of them ASAP to prevent combinatorial explosion go over (potential) neighbors one by one for every neighbor check distance to all other already checked neighbors if all match, continue onto next neighbor otherwise discard this branch (if there are more potential neighbors with this distance, try the next one, otherwise discard the starting point) if we matched all neighbors, we just found one potential set of matching points. (minus the imprecision due to the initial rounding, so we note it down) Note: I will try to get back to it and provide code and potentially benchmark and optimize. However, I'm afraid I already spent too much time and I have some duties to attend to. What now? Now we should have a set (or several) of 3D matching structures. If we didn't find one, we lower the precision and try again. If we found more than one, we can now increase the precision and look (or compute) which of the candidates is the best match—this should be rather straightforward as we already have 1:1 mapping between points from the previous step. (Keeping track of the indices will be a bit of a pain though.) When we have the best match, the last step to do is to find the 2D lattice orientation. We have: set of points in 3D (matrix P3d), the 2D lattice point coordinates (matrix P2d) and the lattice (matrix L2d). Dimension of matrices P2d and P3d are identical. So now we're looking for such a matrix L3d, how do we to that? We need to find a matrix X such that X * P2d = P3d Then we do X * L2d = L3d. Note: There is one edge case where this will not work—if you find a structure, where the 2D structure is a mirror image of the 3D one, rotation to find a match won't be possible. It should be reasonably easy to spot if you plot it though. Alternatively there is likely some math to find this case. And we're done. If I have more time, I will try to finish this up in code and test it properly and possibly optimize. Not sure when I can get to it though. Please let me know if this was sufficiently helpful or if you need me to explain any part more in depth. Also, if it works, please let me know how it went :-). Edit: To answer your questions: I already had some ideas about this one and listed them above, however now I'm uncertain due to the size differences. I will try to briefly sum them up. Normalize both lattices Take the 3D lattice and compute its bounding vertices and edges. Take the longest dimension of the 2D lattice 2d_long (e.g., length of the line segment connecting two vertices that are furthest one another) Compute the following bounding box: min_x = min_coord_x - 2d_long max_x = max_coord_x + 2d_long same for y and z Now fill this bounding box with copies of lattice 3D. This should be rather straightforward to compute. This should be enough and hopefully not too large. Ok, normalize by cell size, that shouldn't be a problem, rest should work as described. It will impact the precision so you might need to use only two decimals and subsequently will have more matches Neighbors in graph sense. I create a fully completed graph (clique) on the vertices in the 2D lattice and assign edges to have weight equal to length. Then for the 3D matrix I do the same, except I discard the edges that are not matched—e.g., have a length different from any of the 2D ones. Finally I check if there is a matching subgraph in the 3D space. You can consider the angle matching however if you can normalize with sufficient precision it is not needed—we're checking for identical triangles and those have matching angles by the virtue of being identical. We expect there are multiple matches—in fact, in the toy example, there are six matching &quot;starting&quot; points. That doesn't matter—we will simply check them and eliminate those that don't lead to the desired structure.",
    "context_chunks": [
      {
        "text": "I have a 3D lattice with a unit cell (i.e. the minimum repeating unit) of 16 points. Since it's a 3D lattice, it is periodic in all 3 dimensions (x, y, z). In detail, the unit cell looks like this: The 3 lattice vectors a1, a2, a3 are bulk_vecs = [[14.56578026795, 0.0, 0.0 ], # a1 [0.0, 8.919682340494, 0.0 ], # a2 [7.282890133975, 2.973227446831, 4.20477857933]] # a3 The Cartesian coordinates of the 16 points are bulk_coords = [[ 0.00000000, 0.00000000, 0.00000000], [ 10.9243352, 6.34420137, 2.66488775], [ 18.2072253, 6.34420137, 2.66488775], [ 12.7450577, 3.17210068, 1.33244387], [ 10.6807662, 8.91968234, 0.42187384], [ 16.1429338, 3.37097392, 1.19181926], [ 19.7843789, 9.31742882, 3.29420855], [ 9.34718165, 2.97322745, 1.47306849], [ 14.8093492, 6.34420137, 2.24301390], [ 9.10361267, 9.11855558, 3.43483316], [ 3.88501404, 0.39774648, 0.14062462], [ 7.03932116, 5.94645489, 2.52426313], [ 12.9886267, 8.91968234, 3.57545778], [ 7.28289013, 0.00000000, 0.00000000], [ 5.46216760, 3.17210068, 1.33244387], [ 16.3865028, 9.11855558, 3.43483316]] Now I have a reference 2D lattice that contains 32 points in unit cell. Note that here &quot;2D&quot; means it is only periodic in the x and y dimensions, but can still have thickness in the z direction. Its unit cell looks like this (this is only a simple example, the unit cell can be of any shape, not necessarily cubic): The 3 lattice vectors are given by ref_vecs = [[8.1968234, 0., 0. ], # x-direction (periodic) [0., 13.38535656, 0. ], # y-direction (periodic) [0., 0., 7.7280392142]] # z-thickness (non-periodic) The Cartesian coordinates of the 32 points are ref_coords = [[ 5.65852755, 9.84826406, 0.10024035], [ 5.66769587, 3.14583318, 0.03049278], [ 5.84383908, 6.16622816, 0.33687635], [ 3.06154746, 11.5036515, 0.89497370], [ 3.04533245, 4.74684988, 0.80482405], [ 2.81714593, 7.85388222, 0.95654678], [ 3.09409158, 1.66514480, 1.21902173], [-0.03484920, 9.98872688, 1.91238251], [ 0.07586672, 6.55689489, 2.02252838], [ 0.15624468, 13.1218508, 2.07583316], [ 0.30441248, 2.82022739, 2.21046382], [ 5.46226670, 11.2187312, 2.95396399], [ 5.83993074, 5.03326172, 3.11251461], [ 5.40963696, 8.13117939, 3.23409626], [ 5.45612978, 1.47718658, 3.32634157], [ 2.72591886, 6.68506441, 3.86756751], [ 2.91602855, 3.06649570, 3.90688764], [ 2.95946887, 9.79329729, 3.96116732], [ 3.11916042, 12.9792231, 4.17677610], [ 0.35863087, 4.72533071, 4.75876360], [ 0.31594175, 11.4937246, 4.67355664], [ 0.03089551, 1.37091216, 4.91348486], [ 0.39020723, 8.35223658, 5.11836201], [ 5.45925351, 3.34155067, 5.87036690], [ 5.62981527, 13.2212649, 5.93479016], [ 5.64259931, 6.46196620, 5.93435713], [ 5.83825398, 9.54653517, 6.08472919], [ 2.80592452, 4.61964086, 6.76956275], [ 3.15087591, 11.6754589, 6.98585963], [ 2.68542186, 1.40293444, 7.10791606], [ 2.73378423, 8.13529985, 7.12898617], [ 0.03114729, -0.01057378, 7.78083813]] This reference 2D lattice represents something similar to a &quot;thick layer&quot; extracted from a 3D lattice and its 2 lattice vectors in the x and y directions are parallel to a1 and a2 (i.e. parallel to the xy-plane). Now I want to find a parallel 2D lattice inside the current 3D lattice that best matches this reference 2D lattice. The similarity should depend on the topology of the two 2D lattice unit cells. For example, the reference unit cell could be thought as applying small perturbations to the fractional coordinates and lattice parameters of the best-matched 2D lattice unit cell. Below is an illustration for the 2D lattice extraction from the 3D lattice: To search in the space of all possible parallel 2D lattice unit cells in a 3D lattice space is difficult, especially considering that I might need to expand the 3D lattice unit cell to search in the whole space (but how much should I expand?). I would like to know if there's any algorithm that can find the best topology-matching 2D lattice unit cell in the whole 3D lattice space relatively fast？ Specifically, I want to obtain the lattice vectors and the 32 Cartesian coordinates of the best-matched 2D lattice unit cell. Any sugguestions or even ideas are welcome. Edit: OK I will try to explain the question in the simplest way possible. As you might have already guessed, I am working on crystallography. The 3D lattice represents a bulk crystal structure (i.e. periodic atom arrangement in all 3 dimensions) and a 2D lattice represents a surface structure (i.e. periodic atom arrangement in x and y directions, but has a non-periodic thick atom-layer in z direction). Obviously, a surface structure can be obtained by cutting the bulk structure. Let's use a simple Si bulk structure (downloaded from MaterialsProject) as an example, it has a cubic unit cell with 3 lattice vectors of bulk_vecs = [[5.44370237, 0.0, 0.0 ], [0.0, 5.44370237, 0.0 ], [0.0, 0.0, 5.44370237]] and atomic coordinates of bulk_coords = [[ 1.27013575, 0.03307657, -0.02898786], [ 0.02039057, 0.02332397, 0.79145084], [ 0.0137774, 1.18012325, 1.85879242], [ 1.22267376, 1.27420799, 2.72875596], [ 1.20645874, -0.0488289, 3.52879259], [ 0.08487146, -0.06430577, 4.44147333]] Diamond has the same type of crystal structure as Si, but slightly different lattice parameters and atomic coordinates. Now let's say I have a reference diamond(001) surface structure (the Miller index (001) basically means the surface plane is parallel to the xy-plane). It has a 6-atom unit cell, with 3 lattice vectors of ref_vecs = [[2.5178270133578393, 0.0, 0.0 ], [0.0, 2.5178270133578393, 0.0 ], [0.0, 0.0, 5.341117665]] and atomic coordinates of ref_coords = [[ 1.27013575, 0.03307657, -0.02898786], [ 0.02039057, 0.02332397, 0.79145084], [ 0.0137774, 1.18012325, 1.85879242], [ 1.22267376, 1.27420799, 2.72875596], [ 1.20645874, -0.0488289, 3.52879259], [ 0.08487146, -0.06430577, 4.44147333]] Since diamond has the same type of crystal structure as Si, one should be able to find a Si(001) surface unit cell from this reference diamond(001) surface unit cell. From what I could think of, a simple topology-matching algorithm could be to find a Si(001) surface unit cell in the bulk Si space, so that the sum of the fractional coordinate differences w.r.t. the reference diamond(001) surface unit cell is minimized. However, I am not sure how to search in such a large space, especially when the reference surface unit cell is large, one probably needs to expand the bulk unit cell by a lot, which means the search space becomes even larger. For benchmarking purposes, I will provide the expected solution for the best-matched Si(001) surface unit cell. It should have lattice vectors close to sol_vecs = [[3.8492788605882797, 0.0, 0.0 ], [0.0, 3.8492788605882797, 0.0 ], [0.0, 0.0, 8.165553555]] and atomic coordinates close to sol_coords = [[ 1.92463943, 0.0, 0.0 ], [ 0.0, 0.0, 1.36092562], [ 0.0, 1.92463943, 2.72185121], [ 1.92463943, 1.92463943, 4.08277680], [ 1.92463943, 0.0, 5.44370240], [ 0.0, 0.0, 6.80462799]] This is an illustration of this problem in terms of crystallography (cannot upload to SO for some reason). As you can see in the figure, the Si(001) surface unit cell solution that I provided matches well with the reference diamond(001) surface unit cell. Hint: working on the reciprocal space might be much easier than working on the real space. Final edit: OK so let me rephrase the question in the most clear and simple way: How to find a cell in the whole 3D lattice space where it contains the same number of points as the reference 2D lattice unit cell and has the minimum MAE/RMSE between the fractional coordinates of the two cells (i.e. Cartesian coordinates normalized by cell size)? The information we have is (1) the 3D lattice unit cell (2) the reference 2D lattice unit cell (3) the 2D lattice is parallel to the xy-plane of the 3D lattice",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Introduction Let me rephrase the problem a little: We have a 3D pattern that fills the space with points. We have a reference plane that cuts this space basically anywhere, however we only know about the pattern the points in space create on the plane, nothing else. Find the plane that best matches the reference plane. To make matters worse, the pattern isn't fully 2D, it's more akin to a thin slice. Fortunately for us, the number of points is rather limited so we can go for some brute-forcish approaches. However, we can't just compute all possible 2D lattices and find the one that matches the best, combinatorics would kill us. But we can do something similar and only bit more complicated. Setting up the playground: First let's generate the space we will be working in. To make things simple, let's extend the 3D lattice along all axes to generate a space large enough to fit in any given orientation and shift of the 2D lattice. In our toy example, the 3D lattice vectors give us a cube and the 2D lattice vectors fit within this cube. So if we add a some padding on each side, this will cover all the possible rotations. Now if the reference 2D lattice is large or if the 3D lattice is skewed, this might not be enough so keep that in mind—it will likely require some math and stacking the lattices in a way that creates a space big enough to fit all possible rotations and translations of the reference 2D lattice. If we would miss the center 3D lattice, we can always snap back to it by claiming that whichever lattice we have biggest overlap with, is the new center piece. Hopefully not too much to invalidate our follow up work. Note: If one lattice has 32 points, then we can use up to say 300 lattices to create our space. So far the worst part of the matching algorithm was quadratic so 10K * 10K * 2d_lattice_points computations should be doable. 300 latices gives us something like a 6x6x6 lattice cube. Also since we're concerned about the &quot;flat and pointy&quot; case, we're looking for some rectangle instead (for example, if the lattice was a 3x3x10, then we could do 10x10x3 stacking to compensate). Note: If the 3D lattice is skewed, I suggest cutting off the skewed part and prepending it to create a rectangle instead. This should be ok due to the periodic nature. import numpy as np bulk_vecs = np.array([[5.44370237, 0.0, 0.0 ], [0.0, 5.44370237, 0.0 ], [0.0, 0.0, 5.44370237]]) bulk_coords = np.array([[ 1.27013575, 0.03307657, -0.02898786], [ 0.02039057, 0.02332397, 0.79145084], [ 0.0137774, 1.18012325, 1.85879242], [ 1.22267376, 1.27420799, 2.72875596], [ 1.20645874, -0.0488289, 3.52879259], [ 0.08487146, -0.06430577, 4.44147333]]) ref_vecs = np.array([[2.5178270133578393, 0.0, 0.0 ], [0.0, 2.5178270133578393, 0.0 ], [0.0, 0.0, 5.341117665]]) bulk_space = [] for point in [x for x in bulk_coords]: for i in range(3): for j in range(3): for k in range(3): shift = i * bulk_vecs[0] + j * bulk_vecs[1] + k * bulk_vecs[2] bulk_space.append(point + shift) This gives us some 162 points in the toy example which is fine. If this level of stacking is enough for the main case, this would give us some 432 points which is still ok. My hope is to do enough optimizations to be able to go up to low-thousands of points in the bulk space. (E.g., the 3D lattice generated playground.) Going much beyond that might be possible, but it will get annoying quickly—as I mentioned, I'm going to do some partial brute forcing. Let the voodoo begin First of all, let's get rid of all those pesky decimals. Computer's don't like them. To do that, let us agree, that three decimals are enough for the precision (for now) bulk_space = np.array(np.array(bulk_space) * 1000, dtype='longlong') Note that this might be either too little or too much—I'm not sure about the actual level of deformation/imprecision that happens within the 2D plane. I will try to do some matching—if there are too many matches I will increase the precision. If too little (or none) I will decrease it. This is a discretization at its finest—we slice the continuous space (or finer, we already don't have full continuum in this case) into boxes of fixed size, and then say that every point matches to a box it is contained within. Then we forget all about the original points and happily compute away with the boxes. Finally if we succeed in our endeavor, we go back to the points and try to upgrade the precision. Note: From what I got it seems that the chosen precision doesn't result in any overlaps of points (two points within the cube)—the space we're working in doesn't seem to be too densely populated. If two (or more) points overlap we don't mind too much. However it means that in the final check (while increasing the precision) we need to check every one of them with our solution. Plane approximation: Since we don't have knowledge about the 2D plane orientation, we will use the distance matching first. We will compute a distance matrix of the 3D space and store it away. Then we will compute the distance matrix for the 2D lattice, and take the longest distance, since it will repeat itself the least due to limited space in the 3D-lattice based box. Then we will take one of its endpoints and take all the distances to other points and sort them. Note: We're going to be comparing distances and we don't actually care about their values, only their comparisons, we can omit the square roots and work with distances squared. It is cheaper computation wise. Now we will search for matches of the largest distance in both 2D and 3D. For every match, we will then try to find a matching sequence of distances. Note 2: For checking the existence of the match, we can sort the distance array corresponding to the point, find the matching maximum (which we already know that it's there from the previous point) and then find the rest in order—if we don't find any given match, we can discard the whole sequence since it's sorted. Note that we either need to keep indices while sorting (which is sort of annoying) or brute-search for the matches after we confirm their existence to infer the indices from the matrix (which takes a bit time—but not too much, only number of 2D-based points times the number of 3D points). So we will do that. Note 3: If we don't find any match due to the imprecision of the 2D lattice cut, we can try again with lower precision until we find it. Note 4: We don't care about finding multiple matches at this points. We're just filtering/validating the points. We will handle these in the next step. for i in range(len(ref_space)): for j in range(i, len(ref_space)): diff = ref_space[i] - ref_space[j] dist = np.sum(diff * diff) distance_matrix_2d[i, j] = dist distance_matrix_2d[j, i] = dist if dist &gt; distance_max_2d: dmax_i = i dmax_j = j distance_max_2d = dist distance_matrix_3d = np.zeros(shape=[len(bulk_space), len(bulk_space)], dtype='longlong') match_list_i = [] match_list_j = [] point_match = [] dist_min = 1000000000 dist_max = 0 for i in range(len(bulk_space)): for j in range(i, len(bulk_space)): diff = bulk_space[i] - bulk_space[j] dist = np.sum(diff * diff) distance_matrix_3d[i, j] = dist distance_matrix_3d[j, i] = dist if distance_max_2d - dist == 0: match_list_i.append(i) match_list_j.append(j) point_match.append(i) point_match.append(j) point_matches = [] for i in point_match: x = [] for j in range(len(distance_matrix_3d)): x.append(distance_matrix_3d[i, j]) x = np.sort(x) point_matches.append((i, x)) # Note that we're currently not using the sort # and I'm simply brute forcing the next step. # If that is an issue, this sort can be used to # make the next step log(n) * 2d_pts_count # by checking first the existence of the # distance via binary search in the 3D # sorted matrix. # However since we only have few 2D points, this # is likely unnecessary unless we're solving # some big case, where there might be other # performance bottlenecks. point_matches_bruteforce = [] for i, matches in point_matches: # Check all the rows defined by points from the previous # step (selected by matching the longest distance) row_failed = False for dist2d in distance_matrix_2d[dmax_i]: # For every distance in 2D matrix, # try finding a matching one found = False for dist3d in matches: # If we find it, we qualify the distance if dist3d == dist2d: found = True #print(&quot;dist2d checked&quot;) if not found: # If we didn't find any match, we can discard # the point. There is at least one point from # the set without matching distance, e.g., # we can't match the structure. row_failed = True break if row_failed: # If any row failed (no matching 3D distance # for a 2D one, we discard the point) print(&quot;point &quot;, i, &quot;has failed to find matching set&quot;) else: # Otherwise, we keep it for the next, more bruteforcish round print(&quot;point &quot;, i, &quot;is a candidate&quot;) point_matches_bruteforce.append(i) Finding the points: We have now filtered the 3D candidate list and it's time to find whether any of them match our 2D lattice. For this we will use the triangle identity: two triangles are identical, if all their sides are identical. How will we do that? We already have the distances matrix for both 2D and 3D. Furthermore, we have the starting point with its possible neighbors. So for every such starting point, we will check whether the distances of neighbors are matching. This will take us some time—number_of_2d_points * number_or_3d_points if we don't do any optimizations. Note: There might be some pathological edge cases where this will find too many matches. There is always one optimization possible: sort the distances, then use binary search for matching, reducing the time from n to log(n) per point to be checked at the cost of the initial sort. Annoying part is that it will shuffle around indices so you will need to note them down and keep track of them. The algorithm: take the starting point find its potential neighbors via distance check (number_of_2d_points * number_of_3d_points) note that there might be duplicates. We will get rid of them ASAP to prevent combinatorial explosion go over (potential) neighbors one by one for every neighbor check distance to all other already checked neighbors if all match, continue onto next neighbor otherwise discard this branch (if there are more potential neighbors with this distance, try the next one, otherwise discard the starting point) if we matched all neighbors, we just found one potential set of matching points. (minus the imprecision due to the initial rounding, so we note it down) Note: I will try to get back to it and provide code and potentially benchmark and optimize. However, I'm afraid I already spent too much time and I have some duties to attend to. What now? Now we should have a set (or several) of 3D matching structures. If we didn't find one, we lower the precision and try again. If we found more than one, we can now increase the precision and look (or compute) which of the candidates is the best match—this should be rather straightforward as we already have 1:1 mapping between points from the previous step. (Keeping track of the indices will be a bit of a pain though.) When we have the best match, the last step to do is to find the 2D lattice orientation. We have: set of points in 3D (matrix P3d), the 2D lattice point coordinates (matrix P2d) and the lattice (matrix L2d). Dimension of matrices P2d and P3d are identical. So now we're looking for such a matrix L3d, how do we to that? We need to find a matrix X such that X * P2d = P3d Then we do X * L2d = L3d. Note: There is one edge case where this will not work—if you find a structure, where the 2D structure is a mirror image of the 3D one, rotation to find a match won't be possible. It should be reasonably easy to spot if you plot it though. Alternatively there is likely some math to find this case. And we're done. If I have more time, I will try to finish this up in code and test it properly and possibly optimize. Not sure when I can get to it though. Please let me know if this was sufficiently helpful or if you need me to explain any part more in depth. Also, if it works, please let me know how it went :-). Edit: To answer your questions: I already had some ideas about this one and listed them above, however now I'm uncertain due to the size differences. I will try to briefly sum them up. Normalize both lattices Take the 3D lattice and compute its bounding vertices and edges. Take the longest dimension of the 2D lattice 2d_long (e.g., length of the line segment connecting two vertices that are furthest one another) Compute the following bounding box: min_x = min_coord_x - 2d_long max_x = max_coord_x + 2d_long same for y and z Now fill this bounding box with copies of lattice 3D. This should be rather straightforward to compute. This should be enough and hopefully not too large. Ok, normalize by cell size, that shouldn't be a problem, rest should work as described. It will impact the precision so you might need to use only two decimals and subsequently will have more matches Neighbors in graph sense. I create a fully completed graph (clique) on the vertices in the 2D lattice and assign edges to have weight equal to length. Then for the 3D matrix I do the same, except I discard the edges that are not matched—e.g., have a length different from any of the 2D ones. Finally I check if there is a matching subgraph in the 3D space. You can consider the angle matching however if you can normalize with sufficient precision it is not needed—we're checking for identical triangles and those have matching angles by the virtue of being identical. We expect there are multiple matches—in fact, in the toy example, there are six matching &quot;starting&quot; points. That doesn't matter—we will simply check them and eliminate those that don't lead to the desired structure.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Let's say you have a 3D point cloud (lattice or not) with N3 points and a 2D plane with N2 points. And let's also say they are at the same scale, not that one group can be &quot;zoomed&quot; to fit the other. And the goal is determine wether the N2 points can be found among the N3 points, after some translation and rotation common to all N2 points. Let's take the first point (p1) in 2D group and by the method you like (perhaps a triangulation fits well) you get the nearest neighbour p2 to p1. Calculate its distance d12 (or its square, to save sqrt calculations). Now suppose we have a point ss in 3D group that satisfies it has a 3D neighbour sn at that same d12 distance, with some tolerance error. It's not needed to be the closest point; in fact it can be any other point in 3D group, but likely it's somehow a neigbour of ss. Perhaps you have to traverse the whole 3D group to find such sspoint. If you can't find one, then no, the 2D slide doesn't belong to 3D group. Now we have a match you can align vector (p1, p2) with vector (ss, sn). This can be achieved by multiplicating two matrices, so any point in 2D system can be transformed into 3D coordinates system. We have aligned one axis but for the other two we need at least one more point. Get the next closest neigbour to p1, call it p3. Calculates distances d13, d23. Find neighbours in 3D that, again, satisfy the same distances. Again, no matching, no solution. If you have p1,p2,p3 and ss,sn,sc then you can rotate around the already aligned axis to get a third matrix, which multiplied by the others achieves the &quot;direct transformation&quot; from 2D to 3D systems. The rest is easy. Use that matrix with the rest of pj points in 2D and search in 3D for the same (now transformed) coordinates, always with some allowed error. Notice I've used closest neighbours, just for better understanding. But the method should work with any point sequence you like. NOTE When looking for points in 3D group you may find not only one, but several &quot;solutions&quot; (that's crystallography). You must store them, and check later, likey at &quot;rest of 2D&quot; points state. And you may also find that there are several 2D into 3D matches, again due to atoms configuration. Also, perhaps the 2D plane matches partially (eg, at the left and at the right, but not in the center). To avoid this I'd expand the unit 3D cell, so I use a 3x3x3 3D group repeating the unit cell. This way covers all 2D positions, overlapping in two different 3D unit cells.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "performance",
        "math",
        "pattern-matching"
      ],
      "question_score": 12,
      "answer_score": 2,
      "created": "2023-08-06T20:18:38",
      "question_id": 76847651,
      "answer_id": 76882441
    }
  },
  {
    "question": "How to solve RuntimeError: Couldn&#39;t find appropriate backend to handle uri in python",
    "expected_answer": "On my Windows 11 machine with: C:\\Users\\Foo&gt;python3 --version Python 3.12.2 C:\\Users\\Foo&gt;pip3 --version pip 24.0 from C:\\Users\\Foo\\PathToPython312\\site-packages\\pip (python 3.12) C:\\Users\\Foo&gt;pip3 show torch torchvision torchaudio PySoundFile WARNING: Package(s) not found: torch torchvision torchaudio PySoundFile No previous installs of torch, torchvision, torchaudio or PySoundFile I opened command prompt (not as administrator) and ran: pip3 install torch torchvision torchaudio Which installed: torch 2.2.1 torchaudio 2.2.1 torchvision 0.17.1 Note: several other dependency packages were installed along with the packages above. I then ran python3 ./test.py: # ./test.py import torchaudio print(str(torchaudio.list_audio_backends())) Which output an empty list: [] So I then ran: pip3 install PySoundFile Which installed: PySoundFile 0.9.0.post1 I then re-ran python3 ./test.py which output: ['soundfile'] I suggest uninstalling all related packages: pip3 uninstall torch torchvision torchaudio PySoundFile You'll probably also want to uninstall soundfile, since it's in the list of commands you tried. I would then inspect the output of pip3 list and ensure the packages are no longer there. Then follow the steps I took above, in order, and see if you can get ['soundfile'] as output. Note: I do not use anaconda, so if the steps above do not resolve your issue then I'd say there's a good chance that it's causing the problem.",
    "context_chunks": [
      {
        "text": "I want to work with audiofiles in pytorch. If I try running this line: metadata = torchaudio.info(SAMPLE_WAV_PATH) i get the error message RuntimeError: Couldn't find appropriate backend to handle uri _assets\\steam.wav and format None the internet told me to try str(torchaudio.list_audio_backends()) to see if I have soundfile installed. It appears I don't, because it returns an empty list. So I tried installing soundfile. Here is a list of commands i tried: conda install pytorch torchvision torchaudio -c pytorch conda install -c conda-forge pysoundfile pip install soundfile pip install PySoundFile none of it made a difference. According to the anaconda navigator I have both pysoundfile and soundfile installed, but I still get the same error message. I searched for similar problems and found questions like this cannot import torch audio &#39; No audio backend is available.&#39; But the only answeres there say I should install soundfile If somebody got an idea whats wrong I would appreciate some help, thanks in advance. EDIT I am using windows 11",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "On my Windows 11 machine with: C:\\Users\\Foo&gt;python3 --version Python 3.12.2 C:\\Users\\Foo&gt;pip3 --version pip 24.0 from C:\\Users\\Foo\\PathToPython312\\site-packages\\pip (python 3.12) C:\\Users\\Foo&gt;pip3 show torch torchvision torchaudio PySoundFile WARNING: Package(s) not found: torch torchvision torchaudio PySoundFile No previous installs of torch, torchvision, torchaudio or PySoundFile I opened command prompt (not as administrator) and ran: pip3 install torch torchvision torchaudio Which installed: torch 2.2.1 torchaudio 2.2.1 torchvision 0.17.1 Note: several other dependency packages were installed along with the packages above. I then ran python3 ./test.py: # ./test.py import torchaudio print(str(torchaudio.list_audio_backends())) Which output an empty list: [] So I then ran: pip3 install PySoundFile Which installed: PySoundFile 0.9.0.post1 I then re-ran python3 ./test.py which output: ['soundfile'] I suggest uninstalling all related packages: pip3 uninstall torch torchvision torchaudio PySoundFile You'll probably also want to uninstall soundfile, since it's in the list of commands you tried. I would then inspect the output of pip3 list and ensure the packages are no longer there. Then follow the steps I took above, in order, and see if you can get ['soundfile'] as output. Note: I do not use anaconda, so if the steps above do not resolve your issue then I'd say there's a good chance that it's causing the problem.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I had same issue with demucs which uses pytorch --- I needed to install both `torchaudio` and `soundfile` in conda isolated environment pip install torchaudio soundfile",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pytorch",
        "torchaudio"
      ],
      "question_score": 11,
      "answer_score": 8,
      "created": "2024-03-03T20:30:40",
      "question_id": 78097861,
      "answer_id": 78103260
    }
  },
  {
    "question": "AttributeError: Calling operator &quot;bpy.ops.import_scene.obj&quot; error, could not be found",
    "expected_answer": "Turns out bpy.ops.import_scene.obj was removed at bpy==4 which is the latest blender-api for python, hence the error. In bpy&gt;4 you have to use bpy.ops.wm.obj_import(filepath='') I just downgraded to bpy==3.60 to import object directly in the current scene. pip install bpy==3.6.0 I also modified my script to take input of .obj files in triangular-mesh and then convert the mesh to quadrilateral, then export as both stl and obj. Here's my working script: def convert_tris_to_quads(obj_path, export_folder): try: filename = os.path.basename(obj_path).split('.')[0] logging.info(f&quot;Importing {obj_path}&quot;) bpy.ops.object.select_all(action='DESELECT') bpy.ops.object.select_by_type(type='MESH') bpy.ops.object.delete() bpy.ops.import_scene.obj(filepath=obj_path) print(&quot;current objects in the scene: &quot;, [obj for obj in bpy.context.scene.objects]) for obj in bpy.context.selected_objects: bpy.context.view_layer.objects.active = obj logging.info(&quot;Converting mesh&quot;) bpy.ops.object.mode_set(mode='EDIT') bpy.ops.mesh.select_all(action='SELECT') bpy.ops.mesh.tris_convert_to_quads() bpy.ops.object.mode_set(mode='OBJECT') # Export to OBJ obj_export_path = export_folder + filename + '_quad.obj' logging.info(f&quot;Exporting OBJ to {obj_export_path}&quot;) bpy.ops.export_scene.obj(filepath=obj_export_path, use_selection=True) # Export to STL stl_export_path = export_folder + filename + '_quad.stl' logging.info(f&quot;Exporting STL to {stl_export_path}&quot;) bpy.ops.export_mesh.stl(filepath=stl_export_path, use_selection=True) except Exception as e: logging.error(f&quot;Error processing {obj_path}: {e}&quot;) return False This still might not be the best approach to this, so do let me know if anyone know any better approach.",
    "context_chunks": [
      {
        "text": "I am trying to write a python script that will convert triangular-mesh objects to quad-mesh objects. For example, image (a) will be my input (.obj/.stl) file and image (b) will be the output. I am a noob with mesh-algorithms or how they work all together. So, far this is the script I have written: import bpy inp = 'mushroom-shelve-1-merged.obj' # Load the triangle mesh OBJ file bpy.ops.import_scene.obj(filepath=inp, use_smooth_groups=False, use_image_search=False) # Get the imported mesh obj = bpy.context.selected_objects[0] # Convert triangles to quads # The `beauty` parameter can be set to False if desired bpy.ops.object.mode_set(mode='EDIT') bpy.ops.mesh.select_all(action='SELECT') bpy.ops.mesh.tris_convert_to_quads(beauty=True) bpy.ops.object.mode_set(mode='OBJECT') # Export to OBJ with quads bpy.ops.export_scene.obj(filepath='quad_mesh.obj') This results in the following error: Traceback (most recent call last): File &quot;/home/arrafi/mesh-convert-application/test.py&quot;, line 8, in &lt;module&gt; bpy.ops.import_scene.obj(filepath=inp, File &quot;/home/arrafi/mesh-convert-application/venv/lib/python3.10/site-packages/bpy/4.0/scripts/modules/bpy/ops.py&quot;, line 109, in __call__ ret = _op_call(self.idname_py(), kw) AttributeError: Calling operator &quot;bpy.ops.import_scene.obj&quot; error, could not be found Any help with what I am doing wrong here would be greatly appreciated. Also please provide your suggestions for if you know any better way to convert triangular-mesh to quad-mesh with Python. If you guys know of any API that I can call with python to do the conversion, that would work too.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Turns out bpy.ops.import_scene.obj was removed at bpy==4 which is the latest blender-api for python, hence the error. In bpy&gt;4 you have to use bpy.ops.wm.obj_import(filepath='') I just downgraded to bpy==3.60 to import object directly in the current scene. pip install bpy==3.6.0 I also modified my script to take input of .obj files in triangular-mesh and then convert the mesh to quadrilateral, then export as both stl and obj. Here's my working script: def convert_tris_to_quads(obj_path, export_folder): try: filename = os.path.basename(obj_path).split('.')[0] logging.info(f&quot;Importing {obj_path}&quot;) bpy.ops.object.select_all(action='DESELECT') bpy.ops.object.select_by_type(type='MESH') bpy.ops.object.delete() bpy.ops.import_scene.obj(filepath=obj_path) print(&quot;current objects in the scene: &quot;, [obj for obj in bpy.context.scene.objects]) for obj in bpy.context.selected_objects: bpy.context.view_layer.objects.active = obj logging.info(&quot;Converting mesh&quot;) bpy.ops.object.mode_set(mode='EDIT') bpy.ops.mesh.select_all(action='SELECT') bpy.ops.mesh.tris_convert_to_quads() bpy.ops.object.mode_set(mode='OBJECT') # Export to OBJ obj_export_path = export_folder + filename + '_quad.obj' logging.info(f&quot;Exporting OBJ to {obj_export_path}&quot;) bpy.ops.export_scene.obj(filepath=obj_export_path, use_selection=True) # Export to STL stl_export_path = export_folder + filename + '_quad.stl' logging.info(f&quot;Exporting STL to {stl_export_path}&quot;) bpy.ops.export_mesh.stl(filepath=stl_export_path, use_selection=True) except Exception as e: logging.error(f&quot;Error processing {obj_path}: {e}&quot;) return False This still might not be the best approach to this, so do let me know if anyone know any better approach.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I figured out the answer, and it does work for the newest version, which is 4.3; you will need Visual Studio to view the code because it is easier to see which line you need to change. You then need to copy this part of the code (bpy.ops.wm.obj_import), then go to line 440 and find this line: bpy.ops.import_scene.obj(filepath=self.properties.filepath, use_split_groups=True) After you locate this line, you need to change it to this: bpy.ops.wm.obj_import(filepath=self.properties.filepath, use_split_groups=True) After that it should work and happy animating!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "blender",
        "stl-format",
        "bpy"
      ],
      "question_score": 11,
      "answer_score": 12,
      "created": "2024-01-12T14:06:50",
      "question_id": 77807142,
      "answer_id": 77811716
    }
  },
  {
    "question": "TypeError: DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given",
    "expected_answer": "According to the pandas documentation, the source for drop would be DataFrame.drop(labels=None, *, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') The * represents the end of allowed positional arguments. This indicates that the positional arguments would be labels, and for python's error message this would likely be from the self positional argument that is not directly shown as it is implicitly supplied. Therefore, doing probe_df = probe_df.set_index(&quot;Chamber ID&quot;).drop(c_to_drop,1) would be feeding 2 positional arguments into a function which only takes 1 (not including self). By changing it to probe_df.set_index(&quot;Chamber ID&quot;).drop(c_to_drop, axis=1), we convert the 1 from a positional argument to a keyword argument as required by the function.",
    "context_chunks": [
      {
        "text": "I have a large file that I'm trying to reduce using dataframe.drop Here's my code: probe_df = pd.read_csv(csv_file,header = 9233, nrows = 4608) # Get rid of stuff c_to_drop = 'Unnamed: ' + str(count_tp+1) probe_df = probe_df.set_index(&quot;Chamber ID&quot;).drop(c_to_drop,1) When I ran this, I got this error message: probe_df = probe_df.set_index(&quot;Chamber ID&quot;).drop(c_to_drop,1) TypeError: DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given I don't understand why I got the error and how to fix it. Please help! I'm a newbie and I tried looking online for a solution but I'm still quite new and didn't see anything that matched my issue exactly.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "According to the pandas documentation, the source for drop would be DataFrame.drop(labels=None, *, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') The * represents the end of allowed positional arguments. This indicates that the positional arguments would be labels, and for python's error message this would likely be from the self positional argument that is not directly shown as it is implicitly supplied. Therefore, doing probe_df = probe_df.set_index(&quot;Chamber ID&quot;).drop(c_to_drop,1) would be feeding 2 positional arguments into a function which only takes 1 (not including self). By changing it to probe_df.set_index(&quot;Chamber ID&quot;).drop(c_to_drop, axis=1), we convert the 1 from a positional argument to a keyword argument as required by the function.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "If you want to drop column(s), you'll need to specify that using a keyword argument anyway (such as via axis=1 as in Shorn's answer), so instead of passing two arguments (labels and axis), you can directly pass the columns to drop to the columns= kwarg: df = df.drop(columns=c_to_drop) # &lt;--- pass the labels to `columns=` df.drop(columns=c_to_drop, inplace=True) # &lt;--- can also pass in-place",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "filter",
        "typeerror"
      ],
      "question_score": 11,
      "answer_score": 17,
      "created": "2023-06-06T02:21:31",
      "question_id": 76411055,
      "answer_id": 76411108
    }
  },
  {
    "question": "How do I perform pandas cumsum while skipping rows that are duplicated in another field?",
    "expected_answer": "Try: df[&quot;out&quot;] = ( df.groupby(&quot;id&quot;)[&quot;value&quot;].transform(&quot;diff&quot;).fillna(df[&quot;value&quot;]).cumsum().astype(int) ) print(df) Prints: id value cumsum_of_value desired_output out 0 a 12 12 12 12 1 b 14 26 26 26 2 c 3 29 29 29 3 a 13 42 30 30 4 b 16 58 32 32 5 e 7 65 39 39 6 f 4 69 43 43 7 a 6 75 36 36 8 b 10 85 30 30 9 k 18 103 48 48",
    "context_chunks": [
      {
        "text": "I am trying to use the pandas.cumsum() function, but in a way that ignores rows with a value in the ID column that is duplicated and specifically only adds the last value to the cumulative sum, ignoring all earlier values. Example code below (I couldn't share the real code, which is for work). import pandas as pd, numpy as np import random as rand id = ['a','b','c','a','b','e','f','a','b','k'] value = [12,14,3,13,16,7,4,6,10,18] df = pd.DataFrame({'id':id, 'value':value}) df[&quot;cumsum_of_value&quot;] = df['value'].cumsum() df[&quot;desired_output&quot;] = [ 12,26,29,30,32,39,43,36,30,48 ] df[&quot;comments&quot;] = [&quot;&quot;]*len(df) df.loc[df.index==0, &quot;comments&quot;]=&quot;standard cumsum&quot; df.loc[df.index==1, &quot;comments&quot;]=&quot;standard cumsum&quot; df.loc[df.index==2, &quot;comments&quot;]=&quot;standard cumsum&quot; df.loc[df.index==3, &quot;comments&quot;]=&quot;cumsum of rows 1-3, ignore row 0&quot; df.loc[df.index==4, &quot;comments&quot;]=&quot;cumsum of rows 2-4, ignore rows 0, 1&quot; df.loc[df.index==5, &quot;comments&quot;]=&quot;cumsum of rows 2-5, ignore rows 0, 1&quot; df.loc[df.index==6, &quot;comments&quot;]=&quot;cumsum of rows 2-6, ignore rows 0, 1&quot; df.loc[df.index==7, &quot;comments&quot;]=&quot;cumsum of rows 2,4-7, ignore rows 0, 1, 3&quot; df.loc[df.index==8, &quot;comments&quot;]=&quot;cumsum of rows 2,5-8, ignore rows 0, 1, 3, 4&quot; df.loc[df.index==9, &quot;comments&quot;]=&quot;cumsum of rows 2,5-9, ignore rows 0, 1, 3, 4&quot; print(df) In this example, there are seven (7) unique values in the ID column (a, b, c ,d, e, f, g), so the cumsum should only ever sum a max of seven (7) records as its output on any row. Is this possible using combinations of functions such as cumsum(), groupby(), duplicated(), drop_duplicates(), and avoiding the use of an iterative loop? I've tried the below df[&quot;duped&quot;] = np.where(df[&quot;id&quot;].duplicated(keep='last'),0,1) df[&quot;value_duped&quot;] = df[&quot;duped&quot;] * df[&quot;value&quot;] df[&quot;desired_output_attempt&quot;] = df[&quot;cumsum_of_value&quot;] - df[&quot;value_duped&quot;] But it doesn't come close to the correct answer. I can't think of how to get something like this to result in the desired output without iterating.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Try: df[&quot;out&quot;] = ( df.groupby(&quot;id&quot;)[&quot;value&quot;].transform(&quot;diff&quot;).fillna(df[&quot;value&quot;]).cumsum().astype(int) ) print(df) Prints: id value cumsum_of_value desired_output out 0 a 12 12 12 12 1 b 14 26 26 26 2 c 3 29 29 29 3 a 13 42 30 30 4 b 16 58 32 32 5 e 7 65 39 39 6 f 4 69 43 43 7 a 6 75 36 36 8 b 10 85 30 30 9 k 18 103 48 48",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Code If you don't have too many unique values for id, I think you can use pivot + ffill + sum. df[&quot;desired_output&quot;] = ( df.pivot(columns='id', values='value').ffill().sum(axis=1).astype('int') ) df: id value desired_output 0 a 12 12 1 b 14 26 2 c 3 29 3 a 13 30 4 b 16 32 5 e 7 39 6 f 4 43 7 a 6 36 8 b 10 30 9 k 18 48",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "duplicates",
        "cumsum"
      ],
      "question_score": 11,
      "answer_score": 8,
      "created": "2024-06-20T00:04:46",
      "question_id": 78645037,
      "answer_id": 78645074
    }
  },
  {
    "question": "tkinter extension was not compiled and GUI subsystem has been detected. Missing the Tk toolkit?",
    "expected_answer": "What fixed it for me on Ubuntu (22.04) was: sudo apt install tk-dev",
    "context_chunks": [
      {
        "text": "I am trying to install (through pyenv) Python-3.11.4 under CentOS-7. It installs but without GUI. I get the following error message: Installing Python-3.11.4... Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;/.../pyenv/versions/3.11.4/lib/python3.11/tkinter/__init__.py&quot;, line 38, in &lt;module&gt; import _tkinter # If this fails your Python may not be configured for Tk ^^^^^^^^^^^^^^^ ModuleNotFoundError: No module named '_tkinter' WARNING: The Python tkinter extension was not compiled and GUI subsystem has been detected. Missing the Tk toolkit? Installed Python-3.11.4 to /.../pyenv/versions/3.11.4 While Python-3.9.16 installs successfully on the same machine. According to Python 3.11 Build Changes, the requirement is to have &quot;Tcl/Tk version 8.5.12 or newer&quot; installed. I have $ rpm -q tk tk-devel tcl tcl-devel tk-8.5.13-6.el7.x86_64 tk-devel-8.5.13-6.el7.x86_64 tcl-8.5.13-8.el7.x86_64 tcl-devel-8.5.13-8.el7.x86_64 The same page says &quot;Tcl/Tk, and uuid flags are detected by pkg-config (when available). tkinter now requires a pkg-config command to detect development settings for Tcl/Tk headers and libraries.&quot;, which is also installed: $ rpm -q pkgconfig pkgconfig-0.27.1-4.el7.x86_64 Could you please help me to understand what might be the reason of the failure to install _tkinter? Thank you very much for your help!",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "What fixed it for me on Ubuntu (22.04) was: sudo apt install tk-dev",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "To build successfully Python-3.11.4 under CentOS-7, one needs to set the following environment variables: export CPPFLAGS=&quot;$(pkg-config --cflags openssl11) -I/usr/include&quot; export LDFLAGS=&quot;$(pkg-config --libs openssl11) -L/usr/lib64 -ltcl8.5 -ltk8.5&quot; where the openssl11 parts are needed for the _ssl module, and the rest is needed for the _tkinter module. The pieces needed to build Python with the Tk/Tcl were found in /usr/lib64/tclConfig.sh /usr/lib64/tkConfig.sh To check if tkinter was built successfully, do the following: /path/to/python/3.11.4/bin/python3 -m tkinter It seems, the root cause of the problem is that tcl and tk rpm packages in CentOS-7 do not provide the corresponding pkg-config files: /usr/lib64/pkgconfig/tcl.pc /usr/lib64/pkgconfig/tk.pc and so one has to provide the corresponding information manually.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "tkinter",
        "build",
        "tk-toolkit"
      ],
      "question_score": 11,
      "answer_score": 10,
      "created": "2023-08-10T19:13:16",
      "question_id": 76878713,
      "answer_id": 77684022
    }
  },
  {
    "question": "How to activate verbosity in Langchain",
    "expected_answer": "You can add a callback handler to the invoke method's configuration. Like this: from langchain.callbacks.tracers import ConsoleCallbackHandler # ...your code chain.invoke({&quot;topic&quot;: &quot;ice cream&quot;}, config={'callbacks': [ConsoleCallbackHandler()]}) Code with change incorporated: from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema.output_parser import StrOutputParser from langchain.callbacks.tracers import ConsoleCallbackHandler prompt = ChatPromptTemplate.from_template(&quot;tell me a joke about {topic}&quot;) model = ChatOpenAI() output_parser = StrOutputParser() chain = prompt | model | output_parser chain.invoke({&quot;topic&quot;: &quot;ice cream&quot;}, config={'callbacks': [ConsoleCallbackHandler()]}) The output isn't the same as the original &quot;verbose mode&quot;, but this is the closest alternative. Alternatives For more targeted output or less &quot;verbosity&quot; Try attaching a callback handler to specific objects. For example: ChatOpenAI().with_config({'callbacks': [ConsoleCallbackHandler()]}) You can learn more about customizing callbacks here For high verbosity Global debug still works with LCEL: from langchain.globals import set_debug set_debug(True) # your code For a GUI you can use weights and biases or langsmith",
    "context_chunks": [
      {
        "text": "I'm using Langchain 0.0.345. I cannot get a verbose output of what's going on under the hood using the LCEL approach to chain building. I have this code: from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema.output_parser import StrOutputParser from langchain.globals import set_verbose set_verbose(True) prompt = ChatPromptTemplate.from_template(&quot;tell me a joke about {topic}&quot;) model = ChatOpenAI() output_parser = StrOutputParser() chain = prompt | model | output_parser chain.invoke({&quot;topic&quot;: &quot;ice cream&quot;}) According to the documentation using set_verbose is the way to have a verbose output showing intermediate steps, prompt builds etc. But the output of this script is just a string without any intermediate steps. Actually, the module langchain.globals does not appear even mentioned in the API documentation. I have also tried setting the verbose=True parameter in the model creation, but it also does not work. This used to work with the former approach building with classes and so. How is the recommended and current approach to have the output logged so you can understand what's going on? Thanks!",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can add a callback handler to the invoke method's configuration. Like this: from langchain.callbacks.tracers import ConsoleCallbackHandler # ...your code chain.invoke({&quot;topic&quot;: &quot;ice cream&quot;}, config={'callbacks': [ConsoleCallbackHandler()]}) Code with change incorporated: from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema.output_parser import StrOutputParser from langchain.callbacks.tracers import ConsoleCallbackHandler prompt = ChatPromptTemplate.from_template(&quot;tell me a joke about {topic}&quot;) model = ChatOpenAI() output_parser = StrOutputParser() chain = prompt | model | output_parser chain.invoke({&quot;topic&quot;: &quot;ice cream&quot;}, config={'callbacks': [ConsoleCallbackHandler()]}) The output isn't the same as the original &quot;verbose mode&quot;, but this is the closest alternative. Alternatives For more targeted output or less &quot;verbosity&quot; Try attaching a callback handler to specific objects. For example: ChatOpenAI().with_config({'callbacks': [ConsoleCallbackHandler()]}) You can learn more about customizing callbacks here For high verbosity Global debug still works with LCEL: from langchain.globals import set_debug set_debug(True) # your code For a GUI you can use weights and biases or langsmith",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "According to the document (https://python.langchain.com/docs/guides/debugging), try this from langchain.globals import set_verbose, set_debug set_debug(True) set_verbose(True) All the intermediate message will be printed out",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "langchain",
        "large-language-model"
      ],
      "question_score": 11,
      "answer_score": 13,
      "created": "2023-12-08T09:14:30",
      "question_id": 77625508,
      "answer_id": 77629872
    }
  },
  {
    "question": "What&#39;s the Raku equivalent of the super keyword as used in JavaScript and Python?",
    "expected_answer": "What's the Raku equivalent of the super keyword as used in JavaScript and Python? One of Raku's re-dispatching functions.¹ Basics of redispatch First some code that does not include a redispatch function: class Rectangle { has ($.length, $.width) } Rectangle.new: length =&gt; 6, width =&gt; 4; The Rectangle declaration does not even include construction code, just a declaration of two attributes and that's it. So what is the Rectangle.new call doing? It's inheriting the default new method provided by Raku's Mu class which initializes any class attributes whose names match any named arguments. If you want a custom constructor that accepts positional arguments, then you typically write a new method which lists which arguments you want in its signature, and then have that method call suitably invoke the default new, which requires named arguments, by calling an appropriate redispatch function with the arguments converted to named arguments: class Rectangle { has ($.length, $.width); method new ($length, $width) { callwith length =&gt; $length, width =&gt; $width } } Rectangle.new: 6, 4; callwith is a redispatch function which does a: call of the next matching candidate based on the original call.² with a fresh set of arguments. In this simple case the original call was Rectangle.new: 6, 4, and the next candidate is the new method inherited from Mu. A Rectangle class based on yours Rather than mimic your code I'll write an idiomatic Raku translation of it and comment on it. class Rectangle { has ($!length, $!width) is required is built; method new ($length, $width) { callwith :$length, :$width } method shoutArea { put uc &quot;I am a {self.^name} and my area is {$!length * $!width}&quot; } method rectHello { 'Rectanglish: hello' } } constant rect = Rectangle.new: 6, 4; rect.shoutArea; #=&gt; I AM A RECTANGLE AND MY AREA IS 24 Commentary: It's a good habit to default to writing code that limits problems that can arise as code evolves. For this reason I've used $!length for the length attribute rather than $.length.³ I've added an is required annotation to the attributes. This means a failure to initialize attributes by the end of an instance's construction will mean an exception gets thrown. I've added an is built annotation to the attributes. This means that even an attribute without a public accessor -- as is the case for $!length and $!width due to my use of ! instead of . in the &quot;twigil&quot; -- can/will still be automatically initialized if there is a matching named argument in the construction call. :$length is short for length =&gt; $length. self.^name avoids unnecessary overhead. It's not important and quite possibly distracting to read about so feel free to ignore my footnote explaining it.⁴ A Square class based on yours I'll make the new for Square redispatch: class Square is Rectangle { method new ($side-length) { callwith $side-length, $side-length } method squaHello { &quot;Squarish: {self.rectHello.split(':')[1].trim}&quot; } } constant squa = Square.new: 5; squa.shoutArea; #=&gt; I AM A SQUARE AND MY AREA IS 25 put squa.squaHello; #=&gt; Squarish: hello Commentary: I picked the name $side-length for the Square's .new parameter, but the name doesn't matter because it's a positional parameter/argument. The redispatch is to the next candidate, just as it was before, abstractly speaking. Concretely speaking the next candidate this time is the method I had just defined in Rectangle (which in turn redispatches to the new of Mu). self.rectHello suffices because the method being called has a different name than the originally called method (squaHello). If you renamed the two methods in Rectangle and Square to have the same name Hello then a redispatch would again be appropriate, though this time I'd have written just callsame rather than callwith ... because callsame just redispatches to the next candidate using the same arguments that were provided in the original call, which would save bothering to write out the arguments again. Footnotes ¹ Redispatching is a generalization of features like super. Redispatch functions are used for a range of purposes, including ones that have nothing to do with object orientation. ² In Raku a function or method call may result in the compiler generating a list of possibly matching candidates taking into account factors such as invocants for method calls, and multiple dispatch and function wrappers for both functions and methods. Having constructed a candidate list it then dispatches to the leading candidate (or the next one in the case of redispatch to the next candidate). ³ If you really want a getter/setter to be automatically generated for a given attribute, then declare it with a ., eg $.length instead of $!length, and Raku will generate both a $!length attribute and a .length getter. (And a setter too if you add an is rw to the $.length declaration.) I did this in the first code example to keep things a bit simpler. ⁴ The ^ in a method call like foo.^bar means a bar method call is redirected &quot;upwards&quot; (hence the ^) to the Higher Order Workings object that knows how a foo functions as a particular kind of type. In this case a Rectangle is a class and the HOW object is an instance of Perl6::Metamodel::ClassHOW, which knows how classes work, including that each class has a distinct name and has a .name method that retrieves that name. And the name of the Rectangle class is of course 'Rectangle', so self.^name saves having to create something else with the class's name.",
    "context_chunks": [
      {
        "text": "Whenever you extend a class in JavaScript or Python, the derived class must use the super keyword in order to set attributes and/or invoke methods and constructor in the base class. For example: class Rectangle { constructor(length, width) { this.name = &quot;Rectangle&quot;; this.length = length; this.width = width; } shoutArea() { console.log( `I AM A ${this.name.toUpperCase()} AND MY AREA IS ${this.length * this.width}` ); } rectHello() { return &quot;Rectanglish: hello&quot;; } } class Square extends Rectangle { constructor(length) { super(length, length); this.name = &quot;Square&quot; } squaHello() { const h = super.rectHello(); return &quot;Squarish:&quot; + h.split(':')[1]; } } const rect = new Rectangle(6, 4); rect.shoutArea(); //=&gt; I AM A RECTANGLE AND MY AREA IS 24 const squa = new Square(5); squa.shoutArea(); //=&gt; I AM A SQUARE AND MY AREA IS 25 console.log(squa.squaHello()); //=&gt; Squarish: hello",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "What's the Raku equivalent of the super keyword as used in JavaScript and Python? One of Raku's re-dispatching functions.¹ Basics of redispatch First some code that does not include a redispatch function: class Rectangle { has ($.length, $.width) } Rectangle.new: length =&gt; 6, width =&gt; 4; The Rectangle declaration does not even include construction code, just a declaration of two attributes and that's it. So what is the Rectangle.new call doing? It's inheriting the default new method provided by Raku's Mu class which initializes any class attributes whose names match any named arguments. If you want a custom constructor that accepts positional arguments, then you typically write a new method which lists which arguments you want in its signature, and then have that method call suitably invoke the default new, which requires named arguments, by calling an appropriate redispatch function with the arguments converted to named arguments: class Rectangle { has ($.length, $.width); method new ($length, $width) { callwith length =&gt; $length, width =&gt; $width } } Rectangle.new: 6, 4; callwith is a redispatch function which does a: call of the next matching candidate based on the original call.² with a fresh set of arguments. In this simple case the original call was Rectangle.new: 6, 4, and the next candidate is the new method inherited from Mu. A Rectangle class based on yours Rather than mimic your code I'll write an idiomatic Raku translation of it and comment on it. class Rectangle { has ($!length, $!width) is required is built; method new ($length, $width) { callwith :$length, :$width } method shoutArea { put uc &quot;I am a {self.^name} and my area is {$!length * $!width}&quot; } method rectHello { 'Rectanglish: hello' } } constant rect = Rectangle.new: 6, 4; rect.shoutArea; #=&gt; I AM A RECTANGLE AND MY AREA IS 24 Commentary: It's a good habit to default to writing code that limits problems that can arise as code evolves. For this reason I've used $!length for the length attribute rather than $.length.³ I've added an is required annotation to the attributes. This means a failure to initialize attributes by the end of an instance's construction will mean an exception gets thrown. I've added an is built annotation to the attributes. This means that even an attribute without a public accessor -- as is the case for $!length and $!width due to my use of ! instead of . in the &quot;twigil&quot; -- can/will still be automatically initialized if there is a matching named argument in the construction call. :$length is short for length =&gt; $length. self.^name avoids unnecessary overhead. It's not important and quite possibly distracting to read about so feel free to ignore my footnote explaining it.⁴ A Square class based on yours I'll make the new for Square redispatch: class Square is Rectangle { method new ($side-length) { callwith $side-length, $side-length } method squaHello { &quot;Squarish: {self.rectHello.split(':')[1].trim}&quot; } } constant squa = Square.new: 5; squa.shoutArea; #=&gt; I AM A SQUARE AND MY AREA IS 25 put squa.squaHello; #=&gt; Squarish: hello Commentary: I picked the name $side-length for the Square's .new parameter, but the name doesn't matter because it's a positional parameter/argument. The redispatch is to the next candidate, just as it was before, abstractly speaking. Concretely speaking the next candidate this time is the method I had just defined in Rectangle (which in turn redispatches to the new of Mu). self.rectHello suffices because the method being called has a different name than the originally called method (squaHello). If you renamed the two methods in Rectangle and Square to have the same name Hello then a redispatch would again be appropriate, though this time I'd have written just callsame rather than callwith ... because callsame just redispatches to the next candidate using the same arguments that were provided in the original call, which would save bothering to write out the arguments again. Footnotes ¹ Redispatching is a generalization of features like super. Redispatch functions are used for a range of purposes, including ones that have nothing to do with object orientation. ² In Raku a function or method call may result in the compiler generating a list of possibly matching candidates taking into account factors such as invocants for method calls, and multiple dispatch and function wrappers for both functions and methods. Having constructed a candidate list it then dispatches to the leading candidate (or the next one in the case of redispatch to the next candidate). ³ If you really want a getter/setter to be automatically generated for a given attribute, then declare it with a ., eg $.length instead of $!length, and Raku will generate both a $!length attribute and a .length getter. (And a setter too if you add an is rw to the $.length declaration.) I did this in the first code example to keep things a bit simpler. ⁴ The ^ in a method call like foo.^bar means a bar method call is redirected &quot;upwards&quot; (hence the ^) to the Higher Order Workings object that knows how a foo functions as a particular kind of type. In this case a Rectangle is a class and the HOW object is an instance of Perl6::Metamodel::ClassHOW, which knows how classes work, including that each class has a distinct name and has a .name method that retrieves that name. And the name of the Rectangle class is of course 'Rectangle', so self.^name saves having to create something else with the class's name.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Here is my cut at your example in raku: class Rectangle { has $.name = &quot;Rectangle&quot;; has $.length; has $.width; method area { $!length * $!width } method shoutArea { say &quot;I AM A {$.name.uc} AND MY AREA IS {$.area}&quot;; } method hello-prefix { &quot;Rectanglish:&quot; } method hello { $.hello-prefix ~ ' hello' } } class Square is Rectangle { has $.name = &quot;Square&quot;; has $.side; method area { $!side ** 2 } method hello-prefix { &quot;Squarish:&quot; } } my $rect = Rectangle.new(length =&gt; 6, width =&gt; 4); $rect.shoutArea(); # =&gt; I AM A RECTANGLE AND MY AREA IS 24 my $squa = Square.new(side =&gt; 5); $squa.shoutArea(); # =&gt; I AM A SQUARE AND MY AREA IS 25 say $squa.hello; # =&gt; Squarish: hello To answer your question on 'super', as mentioned by @raiph, this is not needed in raku due to the mutli dispatch mechanism. It seems that the 'super' keyword needs methods to &quot;know&quot; where they sit (in the child or the parent). In raku, this can be handled more generally with nextsame, nextwith, callsame, callwith. But, as my rewrite shows, often you can avoid this by rephrasing the methods to maintain encapsulation.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "javascript",
        "python",
        "oop",
        "inheritance",
        "raku"
      ],
      "question_score": 11,
      "answer_score": 11,
      "created": "2023-07-05T22:43:52",
      "question_id": 76624477,
      "answer_id": 76625096
    }
  },
  {
    "question": "Performance results differ between run_in_threadpool() and run_in_executor() in FastAPI",
    "expected_answer": "Using run_in_threadpool() FastAPI is fully compatible with (and based on) Starlette, and hence, with FastAPI you get all of Starlette's features, such as run_in_threadpool(). Starlette's run_in_threadpool(), which uses anyio.to_thread.run_sync() behind the scenes, &quot;will run the sync blocking function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked&quot;—see this answer and AnyIO's Working with threads documentation for more details. Calling run_in_threadpool()—which internally calls anyio.to_thread.run_sync(), and subsequently, AsyncIOBackend.run_sync_in_worker_thread()—will return a coroutine that will then be awaited to get the eventual result of the sync function (e.g., result = await run_in_threadpool(...)), and hence, FastAPI will still work asynchronously (instead of calling that synchronous function directly in the event loop, in which case, would block the event loop, and hence, block the main thread). As can be seen in Starlette's source code (link is given above), the run_in_threadpool() function simply looks like this (supporting both sequence and keyword arguments): async def run_in_threadpool( func: typing.Callable[P, T], *args: P.args, **kwargs: P.kwargs ) -&gt; T: if kwargs: # pragma: no cover # run_sync doesn't accept 'kwargs', so bind them in here func = functools.partial(func, **kwargs) return await anyio.to_thread.run_sync(func, *args) As described in AnyIO's documentation: Adjusting the default maximum worker thread count The default AnyIO worker thread limiter has a value of 40, meaning that any calls to to_thread.run_sync() without an explicit limiter argument will cause a maximum of 40 threads to be spawned. You can adjust this limit like this: from anyio import to_thread async def foo(): # Set the maximum number of worker threads to 60 to_thread.current_default_thread_limiter().total_tokens = 60 Note AnyIO’s default thread pool limiter does not affect the default thread pool executor on asyncio. Since FastAPI uses Startlette's concurrency module to run blocking functions in an external threadpool (the same threadpool is also used by FastAPI to run endpoints defined with normal def instead of async def, as described in this answer), the default value of the thread limiter, as shown above, is applied here as well, i.e., 40 threads maximum—see the relevant AsyncIOBackend.current_default_thread_limiter() method that returns the CapacityLimiter with the default number of threads. Hence, sending 50 requests simultaneously, as in your case, would lead to threadpool starvation, meaning that there wouldn't be enough threads available in the threadpool to handle all incoming requests concurrently. As described earlier, one could adjust that value, by increasing the number of threads, which might lead to an improvement in performance results—always depending on the number of requests to def endpoints (or async def endpoints that make calls to run_in_threadpool() inside) that your API is expected to serve concurrently. For instance, if you expect the API to serve no more than 50 requests at a time to such endpoints, then set the maximum number of threads to 50. Note that if, in addition to def endpoints, your FastAPI application uses synchronous/blocking background tasks and/or StreamingResponse's generators and/or Dependencies (synchronous/blocking functions refer to those defined with normal def instead of async def), or even UploadFile's async methods, such as await file.read()/await file.close()/etc. (which they all call the corresponding synchronous def file methods, using run_in_threadpool() behind the scenes), you could then increase the number of threads as required, as FastAPI actually runs all those functions in the same external threadpool as well—it is all explained in this answer in details. Note that using the approach below, which was described here, would have the same effect on adjusting the number of worker threads: from anyio.lowlevel import RunVar from anyio import CapacityLimiter RunVar(&quot;_default_thread_limiter&quot;).set(CapacityLimiter(60)) But, it would be best to follow the approach provided by AnyIO's official documentation (as shown earlier). It is also a good idea to have this done when the application starts up, using a lifespan event handler, as demonstrated here. In the working example below, since the /sync endpoint is defined with normal def instead of async def, FastAPI will run it in a separate thread from the external threadpool and then await it, thus ensuring the event loop (and hence, the main thread and the entire server) does not get blocked due to the blocking operations (either blocking IO-bound or CPU-bound) that will be performed inside that endpoint. Working Example 1 from fastapi import FastAPI from contextlib import asynccontextmanager from anyio import to_thread import time @asynccontextmanager async def lifespan(app: FastAPI): to_thread.current_default_thread_limiter().total_tokens = 60 yield app = FastAPI(lifespan=lifespan) @app.get(&quot;/sync&quot;) def test_sync() -&gt; None: time.sleep(3) print(&quot;sync&quot;) @app.get('/get_available_threads') async def get_available_threads(): return to_thread.current_default_thread_limiter().available_tokens Using ApacheBench, you could test the example above as follows, which will send 1000 requests in total with 50 being sent simultaneously at a time (-n: Number of requests, -c : Number of concurrent requests): ab -n 1000 -c 50 &quot;http://localhost:8000/sync&quot; While running a performance test on the example above, if you call the /get_available_threads endpoint from your browser, e.g., http://localhost:8000/get_available_threads, you would see that the amount of threads available is always 10 or above (since only 50 threads are used at a time in this test, but the thread limiter was set to 60), meaning that setting the maximum number of threads on AnyIO's thread limiter to a number that is well above your needs, like 200 as shown in some other answer and in your recent example, wouldn't bring about any improvements in the performance; on the contrary, you would end up with a number of threads &quot;sitting&quot; there without being used. As explained earlier, the number of maximum threads should depend on (1) the number of requests your API is expected to serve concurrently (i.e., number of calls to def endpoints, or async def endpoints that call run_in_threadpool() inside), (2) any additional blocking tasks/functions that would run in the threadpool by FastAPI itself under the hood, as well as (3) the server machine's resources available. The example below is the same as the one above, but instead of letting FastAPI itself to handle the blocking operation(s) inside the def endpoint (by running the def endpoint in the external threadpool and awaiting it), the endpoint is now defined with async def (meaning that FastAPI will run it directly in the event loop), and inside the endpoint, run_in_threadpool() (which returns an awaitable) is used to run the blocking operation (i.e., time.sleep() in the example). Performing a benchmark test on the example below would yield similar results to the previous example. Working Example 2 from fastapi import FastAPI from fastapi.concurrency import run_in_threadpool from contextlib import asynccontextmanager from anyio import to_thread import time @asynccontextmanager async def lifespan(app: FastAPI): to_thread.current_default_thread_limiter().total_tokens = 60 yield app = FastAPI(lifespan=lifespan) @app.get(&quot;/sync_async_run_in_tp&quot;) async def test_sync_async_with_run_in_threadpool() -&gt; None: await run_in_threadpool(time.sleep, 3) print(&quot;sync_async using FastAPI's run_in_threadpool&quot;) @app.get('/get_available_threads') async def get_available_threads(): return to_thread.current_default_thread_limiter().available_tokens Using ApacheBench, you could test the example above as follows: ab -n 1000 -c 50 &quot;http://localhost:8000/sync_async_run_in_tp&quot; Using loop.run_in_executor() with ThreadPoolExecutor When using asyncio's loop.run_in_executor()—after obtaining the running event loop using asyncio.get_running_loop()—one could pass None to the executor argument, which would lead to the default executor being used; that is, a ThreadPoolExecutor. Note that when calling loop.run_in_executor() and passing None to the executor argument, this does not create a new instance of a ThreadPoolExecutor every time you call, for instance, await loop.run_in_executor(None, time.sleep, 3). Instead, the default ThreadPoolExecutor is only initialized once the first time you do that, but for subsequent calls to loop.run_in_executor() with the executor argument set None, Python reuses that very same instance of ThreadPoolExecutor (hence, the default executor). This can been seen in the source code of loop.run_in_executor(). That means, the number of threads that can be created, when calling await loop.run_in_executor(None, ...), is limited to the default number of thread workers in the ThreadPoolExecutor class. As described in the documentation of ThreadPoolExecutor—and as shown in its implementation here—by default, the max_workers argument is set to None, in which case, the number of worker threads is set based on the following equation: min(32, os.cpu_count() + 4) (In Python 3.13 this is changed to min(32, (os.process_cpu_count() or 1) + 4)). The os.cpu_count() function (see os.process_cpu_count() for Python 3.13 and later versions) reutrns the number of logical CPUs in the current system. As explained in this article, physical cores refers to the number of CPU cores provided in the hardware (e.g., the chips), while logical cores is the number of CPU cores after hyperthreading is taken into account. If, for instance, your machine has 4 physical cores, each with hyperthreading (most modern CPUs have this), then Python will see 8 CPUs and will allocate 12 threads (8 CPUs + 4) to the pool by default (Python limits the number of threads to 32 to &quot;avoid consuming surprisingly large resources on multi-core machines&quot;; however, one could always adjust the max_workers argument on their own when using a custom ThreadPoolExecutor, instead of using the default one). You could check the default number of worker threads on your system as follows: import concurrent.futures # create a thread pool with the default number of worker threads pool = concurrent.futures.ThreadPoolExecutor() # report the number of worker threads chosen by default # Note: `_max_workers` is a protected variable and may change in the future print(pool._max_workers) Now, as shown in your original example, you are not using a custom ThreadPoolExecutor; instead, you are using the default ThreadPoolExecutor every time a request arrives, by calling await loop.run_in_executor(None, time.sleep, 3) (inside the sync_async_func() function, which is triggered by the /test/sync_async endpoint). Assuming your machine has 4 physical cores with hyperthreading enabled (as explained in the example earlier), then the default number of worker threads for the default ThreadPoolExecutor would be 12. That means, based on your original example and the /test/sync_async endpoint that triggers the await loop.run_in_executor(None, time.sleep, 3) function, your application could only handle 12 concurrent requests at a time. That is the main reason for the difference observed in the performance results when compared to using run_in_threadpool(), which comes with 40 allocated threads by default. Even though, in both cases, a threadpool starvation was caused when sending 50 requests simultaneously, the endpoint (in your example) that uses run_in_threadpool() performed better only because the default number of threads created was greater than the one used by the default ThreadPoolExecutor (in your other endpoint). One way to solve this is to create a new instance of ThreadPoolExecutor (on your own, instead of using the default executor) every time a request arrives and have it terminated once the task is completed (using the with statement), as shown below: import concurrent.futures import asyncio loop = asyncio.get_running_loop() with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool: await loop.run_in_executor(pool, time.sleep, 3) Although the above should wok just fine, it would be best to instantiate a ThreadPoolExecutor once at application startup, adjust the number of worker threads as needed, and reuse that executor when required. Having said that, if, for any reason, you ever encounter a memory leak—i.e., memory that is no longer needed, but is not released—after tasks are completed when reusing a ThreadPoolExecutor (maybe due to external libraries that you might be using for that blocking task, which do not properly release the memory), you might find creating a new instance of ThreadPoolExecutor each time, as shown above, more suitable. Note, however, that if this was a ProcessPoolExecutor instead, creating and destroying many processes over and over could become computationally expensive. Creating and destroying too many threads could consume huge memory as well. Below is a complete working example, demonstrating how to create a reusable custom ThreadPoolExecutor. Calling the /get_active_threads endpoint from your browser, e.g., http://localhost:8000/get_active_threads, while running a performance test with ApacheBench (using 50 concurrent requests, as described in your question and as shown below), you would see that the number of active threads never goes above 51 (50 concurrent threads + 1, which is the main thread), despite setting the max_workers argument to 60 in the example below. This is simply because, in this performance test, the application is never required to serve more than 50 requests at the same time. Also, ThreadPoolExecutor won't spawn new threads, if idle threads are available (thus, saving resources)—see the relevant implementation part. Hence, again, initializing the ThreadPoolExecutor with max_workers=100, as shown in your recent update, would be unecessary, if you never expect your FastAPI application to serve more than 50 requests at a time (to endpoints where that ThreadPoolExecutor is used). Working Example from fastapi import FastAPI, Request from contextlib import asynccontextmanager import concurrent.futures import threading import asyncio import time @asynccontextmanager async def lifespan(app: FastAPI): pool = concurrent.futures.ThreadPoolExecutor(max_workers=60) yield {'pool': pool} pool.shutdown() app = FastAPI(lifespan=lifespan) @app.get(&quot;/sync_async&quot;) async def test_sync_async(request: Request) -&gt; None: loop = asyncio.get_running_loop() await loop.run_in_executor(request.state.pool, time.sleep, 3) print(&quot;sync_async&quot;) @app.get('/get_active_threads') async def get_active_threads(): return threading.active_count() Using ApacheBench, you could test the example above as follows: ab -n 1000 -c 50 &quot;http://localhost:8000/sync_async&quot; Final Notes In general, you should always aim at using asynchronous code (i.e., using async/await), wherever is possible, as async code, also known as coroutines, runs directly in the event loop—the event loop runs in a thread (typically the main thread) and executes all callbacks and Tasks in its thread. That means there is only one thread that can take a lock on the interpreter; thus, avoiding the additional overhead of context switching (i.e., the CPU jumping from one thread of execution to another). When dealing with sync blocking IO-bound tasks though, you could either (1) define your endpoint with def and let FastAPI handle it behind the scenes as described earlier, as well as in this answer, or (2) define your endpoint with async def and use run_in_threadpool() on your own to run that blocking task in a separate thread and await it, or (3) define your endpoint with async def and use asyncio's loop.run_in_executor() with a custom (preferably reusable) ThreadPoolExecutor, adjusting the number of worker threads as required. When required to perform blocking CPU-bound tasks, while running such tasks in a separate thread from an external threadpool and then awaiting them would successfully prevent the event loop from getting blocked, it wouldn't, however, provide the performance improvement you would expect from running code in parallel. Thus, for CPU-bound tasks, one may choose to use loop.run_in_executor() with ProcessPoolExecutor instead—relevant example could be found in this answer (Note that when using processes in general, you need to explicitly protect the entry point with if __name__ == '__main__'). To run tasks in the background, without waiting for them to complete, in order to proceed with executing the rest of the code in an endpoint, you could use FastAPI's BackgroundTasks, as shown here and here. If the background task function is defined with async def, FastAPI will run it directly in the event loop, whereas if it is defined with normal def, FastAPI will use run_in_threadpool() and await the returned coroutine (same concept as API endpoints). Another option when you need to run an async def function in the background, but not necessarily having it trigerred after returning a FastAPI response (which is the case in BackgroundTasks), is to use asyncio.create_task(), as shown in this answer and this answer. If you need to perform heavy background computation and you don't necessarily need it to be run by the same process, you may benefit from using other bigger tools such as Celery. Finally, regarding the optimal/maximum number of worker threads, I would suggest reading this article (have a look at this article as well for more details on ThreadPoolExecutor in general). As explained in the article: It is important to limit the number of worker threads in the thread pools to the number of asynchronous tasks you wish to complete, based on the resources in your system, or on the number of resources you intend to use within your tasks. Alternately, you may wish to increase the number of worker threads dramatically, given the greater capacity in the resources you intend to use. [...] It is common to have more threads than CPUs (physical or logical) in your system. The reason for this is that threads are used for IO-bound tasks, not CPU-bound tasks. This means that threads are used for tasks that wait for relatively slow resources to respond, like hard drives, DVD drives, printers, network connections, and much more. Therefore, it is not uncommon to have tens, hundreds and even thousands of threads in your application, depending on your specific needs. It is unusual to have more than one or a few thousand threads. If you require this many threads, then alternative solutions may be preferred, such as AsyncIO. Also, in the same article: Does the Number of Threads in the ThreadPoolExecutor Match the Number of CPUs or Cores? The number of worker threads in the ThreadPoolExecutor is not related to the number of CPUs or CPU cores in your system. You can configure the number of threads based on the number of tasks you need to execute, the amount of local system resources you have available (e.g., memory), and the limitations of resources you intend to access within your tasks (e.g., connections to remote servers). How Many Threads Should I Use? If you have hundreds of tasks, you should probably set the number of threads to be equal to the number of tasks. If you have thousands of tasks, you should probably cap the number of threads at hundreds or 1,000. If your application is intended to be executed multiple times in the future, you can test different numbers of threads and compare overall execution time, then choose a number of threads that gives approximately the best performance. You may want to mock the task in these tests with a random sleep operation. What Is the Maximum Number of Worker Threads in the ThreadPoolExecutor? There is no maximum number of worker threads in the ThreadPoolExecutor. Nevertheless, your system will have an upper limit of the number of threads you can create based on how much main memory (RAM) you have available. Before you exceed main memory, you will reach a point of diminishing returns in terms of adding new threads and executing more tasks. This is because your operating system must switch between the threads, called context switching. With too many threads active at once, your program may spend more time context switching than actually executing tasks. A sensible upper limit for many applications is hundreds of threads to perhaps a few thousand threads. More than a few thousand threads on a modern system may result in too much context switching, depending on your system and on the types of tasks that are being executed.",
    "context_chunks": [
      {
        "text": "Here's a minimal reproducible example of my FastAPI app. I have a strange behavior and I'm not sure I understand the reason. I'm using ApacheBench (ab) to send multiple requests as follows: ab -n 1000 -c 50 -H 'accept: application/json' -H 'x-data-origin: source' 'http://localhost:8001/test/async' FastAPI app import time import asyncio import enum from typing import Any from fastapi import FastAPI, Path, Body from starlette.concurrency import run_in_threadpool app = FastAPI() loop = asyncio.get_running_loop() def sync_func() -&gt; None: time.sleep(3) print(&quot;sync func&quot;) async def sync_async_with_fastapi_thread() -&gt; None: await run_in_threadpool( time.sleep, 3) print(&quot;sync async with fastapi thread&quot;) async def sync_async_func() -&gt; None: await loop.run_in_executor(None, time.sleep, 3) async def async_func() -&gt; Any: await asyncio.sleep(3) print(&quot;async func&quot;) @app.get(&quot;/test/sync&quot;) def test_sync() -&gt; None: sync_func() print(&quot;sync&quot;) @app.get(&quot;/test/async&quot;) async def test_async() -&gt; None: await async_func() print(&quot;async&quot;) @app.get(&quot;/test/sync_async&quot;) async def test_sync_async() -&gt; None: await sync_async_func() print(&quot;sync async&quot;) @app.get(&quot;/test/sync_async_fastapi&quot;) async def test_sync_async_with_fastapi_thread() -&gt; None: await sync_async_with_fastapi_thread() print(&quot;sync async with fastapi thread&quot;) Here's the ApacheBench results: async with (asyncio.sleep) : *Concurrency Level: 50 Time taken for tests: 63.528 seconds Complete requests: 1000 Failed requests: 0 Total transferred: 128000 bytes HTML transferred: 4000 bytes Requests per second: 15.74 [#/sec] (mean) Time per request: 3176.407 [ms] (mean) Time per request: 63.528 [ms] (mean, across all concurrent requests) Transfer rate: 1.97 [Kbytes/sec] received* sync (with time.sleep): Concurrency Level: 50 *Time taken for tests: 78.615 seconds Complete requests: 1000 Failed requests: 0 Total transferred: 128000 bytes HTML transferred: 4000 bytes Requests per second: 12.72 [#/sec] (mean) Time per request: 3930.751 [ms] (mean) Time per request: 78.615 [ms] (mean, across all concurrent requests) Transfer rate: 1.59 [Kbytes/sec] received* sync_async (time sleep with run_in_executor) : *Concurrency Level: 50 Time taken for tests: 256.201 seconds Complete requests: 1000 Failed requests: 0 Total transferred: 128000 bytes HTML transferred: 4000 bytes Requests per second: 3.90 [#/sec] (mean) Time per request: 12810.038 [ms] (mean) Time per request: 256.201 [ms] (mean, across all concurrent requests) Transfer rate: 0.49 [Kbytes/sec] received* sync_async_fastapi (time sleep with run_in threadpool): *Concurrency Level: 50 Time taken for tests: 78.877 seconds Complete requests: 1000 Failed requests: 0 Total transferred: 128000 bytes HTML transferred: 4000 bytes Requests per second: 12.68 [#/sec] (mean) Time per request: 3943.841 [ms] (mean) Time per request: 78.877 [ms] (mean, across all concurrent requests) Transfer rate: 1.58 [Kbytes/sec] received* In conclusion, I'm experiencing a surprising disparity in results; especially, when using run_in_executor, where I'm encountering significantly higher average times (12 seconds). I don't understand this outcome. --- EDIT --- After AKX answer. Here the code working as expected: import time import asyncio from anyio import to_thread to_thread.current_default_thread_limiter().total_tokens = 200 loop = asyncio.get_running_loop() executor = ThreadPoolExecutor(max_workers=100) def sync_func() -&gt; None: time.sleep(3) print(&quot;sync func&quot;) async def sync_async_with_fastapi_thread() -&gt; None: await run_in_threadpool( time.sleep, 3) print(&quot;sync async with fastapi thread&quot;) async def sync_async_func() -&gt; None: await loop.run_in_executor(executor, time.sleep, 3) async def async_func() -&gt; Any: await asyncio.sleep(3) print(&quot;async func&quot;) @app.get(&quot;/test/sync&quot;) def test_sync() -&gt; None: sync_func() print(&quot;sync&quot;) @app.get(&quot;/test/async&quot;) async def test_async() -&gt; None: await async_func() print(&quot;async&quot;) @app.get(&quot;/test/sync_async&quot;) async def test_sync_async() -&gt; None: await sync_async_func() print(&quot;sync async&quot;) @app.get(&quot;/test/sync_async_fastapi&quot;) async def test_sync_async_with_fastapi_thread() -&gt; None: await sync_async_with_fastapi_thread() print(&quot;sync async with fastapi thread&quot;)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Using run_in_threadpool() FastAPI is fully compatible with (and based on) Starlette, and hence, with FastAPI you get all of Starlette's features, such as run_in_threadpool(). Starlette's run_in_threadpool(), which uses anyio.to_thread.run_sync() behind the scenes, &quot;will run the sync blocking function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked&quot;—see this answer and AnyIO's Working with threads documentation for more details. Calling run_in_threadpool()—which internally calls anyio.to_thread.run_sync(), and subsequently, AsyncIOBackend.run_sync_in_worker_thread()—will return a coroutine that will then be awaited to get the eventual result of the sync function (e.g., result = await run_in_threadpool(...)), and hence, FastAPI will still work asynchronously (instead of calling that synchronous function directly in the event loop, in which case, would block the event loop, and hence, block the main thread). As can be seen in Starlette's source code (link is given above), the run_in_threadpool() function simply looks like this (supporting both sequence and keyword arguments): async def run_in_threadpool( func: typing.Callable[P, T], *args: P.args, **kwargs: P.kwargs ) -&gt; T: if kwargs: # pragma: no cover # run_sync doesn't accept 'kwargs', so bind them in here func = functools.partial(func, **kwargs) return await anyio.to_thread.run_sync(func, *args) As described in AnyIO's documentation: Adjusting the default maximum worker thread count The default AnyIO worker thread limiter has a value of 40, meaning that any calls to to_thread.run_sync() without an explicit limiter argument will cause a maximum of 40 threads to be spawned. You can adjust this limit like this: from anyio import to_thread async def foo(): # Set the maximum number of worker threads to 60 to_thread.current_default_thread_limiter().total_tokens = 60 Note AnyIO’s default thread pool limiter does not affect the default thread pool executor on asyncio. Since FastAPI uses Startlette's concurrency module to run blocking functions in an external threadpool (the same threadpool is also used by FastAPI to run endpoints defined with normal def instead of async def, as described in this answer), the default value of the thread limiter, as shown above, is applied here as well, i.e., 40 threads maximum—see the relevant AsyncIOBackend.current_default_thread_limiter() method that returns the CapacityLimiter with the default number of threads. Hence, sending 50 requests simultaneously, as in your case, would lead to threadpool starvation, meaning that there wouldn't be enough threads available in the threadpool to handle all incoming requests concurrently. As described earlier, one could adjust that value, by increasing the number of threads, which might lead to an improvement in performance results—always depending on the number of requests to def endpoints (or async def endpoints that make calls to run_in_threadpool() inside) that your API is expected to serve concurrently. For instance, if you expect the API to serve no more than 50 requests at a time to such endpoints, then set the maximum number of threads to 50. Note that if, in addition to def endpoints, your FastAPI application uses synchronous/blocking background tasks and/or StreamingResponse's generators and/or Dependencies (synchronous/blocking functions refer to those defined with normal def instead of async def), or even UploadFile's async methods, such as await file.read()/await file.close()/etc. (which they all call the corresponding synchronous def file methods, using run_in_threadpool() behind the scenes), you could then increase the number of threads as required, as FastAPI actually runs all those functions in the same external threadpool as well—it is all explained in this answer in details. Note that using the approach below, which was described here, would have the same effect on adjusting the number of worker threads: from anyio.lowlevel import RunVar from anyio import CapacityLimiter RunVar(&quot;_default_thread_limiter&quot;).set(CapacityLimiter(60)) But, it would be best to follow the approach provided by AnyIO's official documentation (as shown earlier). It is also a good idea to have this done when the application starts up, using a lifespan event handler, as demonstrated here. In the working example below, since the /sync endpoint is defined with normal def instead of async def, FastAPI will run it in a separate thread from the external threadpool and then await it, thus ensuring the event loop (and hence, the main thread and the entire server) does not get blocked due to the blocking operations (either blocking IO-bound or CPU-bound) that will be performed inside that endpoint. Working Example 1 from fastapi import FastAPI from contextlib import asynccontextmanager from anyio import to_thread import time @asynccontextmanager async def lifespan(app: FastAPI): to_thread.current_default_thread_limiter().total_tokens = 60 yield app = FastAPI(lifespan=lifespan) @app.get(&quot;/sync&quot;) def test_sync() -&gt; None: time.sleep(3) print(&quot;sync&quot;) @app.get('/get_available_threads') async def get_available_threads(): return to_thread.current_default_thread_limiter().available_tokens Using ApacheBench, you could test the example above as follows, which will send 1000 requests in total with 50 being sent simultaneously at a time (-n: Number of requests, -c : Number of concurrent requests): ab -n 1000 -c 50 &quot;http://localhost:8000/sync&quot; While running a performance test on the example above, if you call the /get_available_threads endpoint from your browser, e.g., http://localhost:8000/get_available_threads, you would see that the amount of threads available is always 10 or above (since only 50 threads are used at a time in this test, but the thread limiter was set to 60), meaning that setting the maximum number of threads on AnyIO's thread limiter to a number that is well above your needs, like 200 as shown in some other answer and in your recent example, wouldn't bring about any improvements in the performance; on the contrary, you would end up with a number of threads &quot;sitting&quot; there without being used. As explained earlier, the number of maximum threads should depend on (1) the number of requests your API is expected to serve concurrently (i.e., number of calls to def endpoints, or async def endpoints that call run_in_threadpool() inside), (2) any additional blocking tasks/functions that would run in the threadpool by FastAPI itself under the hood, as well as (3) the server machine's resources available. The example below is the same as the one above, but instead of letting FastAPI itself to handle the blocking operation(s) inside the def endpoint (by running the def endpoint in the external threadpool and awaiting it), the endpoint is now defined with async def (meaning that FastAPI will run it directly in the event loop), and inside the endpoint, run_in_threadpool() (which returns an awaitable) is used to run the blocking operation (i.e., time.sleep() in the example). Performing a benchmark test on the example below would yield similar results to the previous example. Working Example 2 from fastapi import FastAPI from fastapi.concurrency import run_in_threadpool from contextlib import asynccontextmanager from anyio import to_thread import time @asynccontextmanager async def lifespan(app: FastAPI): to_thread.current_default_thread_limiter().total_tokens = 60 yield app = FastAPI(lifespan=lifespan) @app.get(&quot;/sync_async_run_in_tp&quot;) async def test_sync_async_with_run_in_threadpool() -&gt; None: await run_in_threadpool(time.sleep, 3) print(&quot;sync_async using FastAPI's run_in_threadpool&quot;) @app.get('/get_available_threads') async def get_available_threads(): return to_thread.current_default_thread_limiter().available_tokens Using ApacheBench, you could test the example above as follows: ab -n 1000 -c 50 &quot;http://localhost:8000/sync_async_run_in_tp&quot; Using loop.run_in_executor() with ThreadPoolExecutor When using asyncio's loop.run_in_executor()—after obtaining the running event loop using asyncio.get_running_loop()—one could pass None to the executor argument, which would lead to the default executor being used; that is, a ThreadPoolExecutor. Note that when calling loop.run_in_executor() and passing None to the executor argument, this does not create a new instance of a ThreadPoolExecutor every time you call, for instance, await loop.run_in_executor(None, time.sleep, 3). Instead, the default ThreadPoolExecutor is only initialized once the first time you do that, but for subsequent calls to loop.run_in_executor() with the executor argument set None, Python reuses that very same instance of ThreadPoolExecutor (hence, the default executor). This can been seen in the source code of loop.run_in_executor(). That means, the number of threads that can be created, when calling await loop.run_in_executor(None, ...), is limited to the default number of thread workers in the ThreadPoolExecutor class. As described in the documentation of ThreadPoolExecutor—and as shown in its implementation here—by default, the max_workers argument is set to None, in which case, the number of worker threads is set based on the following equation: min(32, os.cpu_count() + 4) (In Python 3.13 this is changed to min(32, (os.process_cpu_count() or 1) + 4)). The os.cpu_count() function (see os.process_cpu_count() for Python 3.13 and later versions) reutrns the number of logical CPUs in the current system. As explained in this article, physical cores refers to the number of CPU cores provided in the hardware (e.g., the chips), while logical cores is the number of CPU cores after hyperthreading is taken into account. If, for instance, your machine has 4 physical cores, each with hyperthreading (most modern CPUs have this), then Python will see 8 CPUs and will allocate 12 threads (8 CPUs + 4) to the pool by default (Python limits the number of threads to 32 to &quot;avoid consuming surprisingly large resources on multi-core machines&quot;; however, one could always adjust the max_workers argument on their own when using a custom ThreadPoolExecutor, instead of using the default one). You could check the default number of worker threads on your system as follows: import concurrent.futures # create a thread pool with the default number of worker threads pool = concurrent.futures.ThreadPoolExecutor() # report the number of worker threads chosen by default # Note: `_max_workers` is a protected variable and may change in the future print(pool._max_workers) Now, as shown in your original example, you are not using a custom ThreadPoolExecutor; instead, you are using the default ThreadPoolExecutor every time a request arrives, by calling await loop.run_in_executor(None, time.sleep, 3) (inside the sync_async_func() function, which is triggered by the /test/sync_async endpoint). Assuming your machine has 4 physical cores with hyperthreading enabled (as explained in the example earlier), then the default number of worker threads for the default ThreadPoolExecutor would be 12. That means, based on your original example and the /test/sync_async endpoint that triggers the await loop.run_in_executor(None, time.sleep, 3) function, your application could only handle 12 concurrent requests at a time. That is the main reason for the difference observed in the performance results when compared to using run_in_threadpool(), which comes with 40 allocated threads by default. Even though, in both cases, a threadpool starvation was caused when sending 50 requests simultaneously, the endpoint (in your example) that uses run_in_threadpool() performed better only because the default number of threads created was greater than the one used by the default ThreadPoolExecutor (in your other endpoint). One way to solve this is to create a new instance of ThreadPoolExecutor (on your own, instead of using the default executor) every time a request arrives and have it terminated once the task is completed (using the with statement), as shown below: import concurrent.futures import asyncio loop = asyncio.get_running_loop() with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool: await loop.run_in_executor(pool, time.sleep, 3) Although the above should wok just fine, it would be best to instantiate a ThreadPoolExecutor once at application startup, adjust the number of worker threads as needed, and reuse that executor when required. Having said that, if, for any reason, you ever encounter a memory leak—i.e., memory that is no longer needed, but is not released—after tasks are completed when reusing a ThreadPoolExecutor (maybe due to external libraries that you might be using for that blocking task, which do not properly release the memory), you might find creating a new instance of ThreadPoolExecutor each time, as shown above, more suitable. Note, however, that if this was a ProcessPoolExecutor instead, creating and destroying many processes over and over could become computationally expensive. Creating and destroying too many threads could consume huge memory as well. Below is a complete working example, demonstrating how to create a reusable custom ThreadPoolExecutor. Calling the /get_active_threads endpoint from your browser, e.g., http://localhost:8000/get_active_threads, while running a performance test with ApacheBench (using 50 concurrent requests, as described in your question and as shown below), you would see that the number of active threads never goes above 51 (50 concurrent threads + 1, which is the main thread), despite setting the max_workers argument to 60 in the example below. This is simply because, in this performance test, the application is never required to serve more than 50 requests at the same time. Also, ThreadPoolExecutor won't spawn new threads, if idle threads are available (thus, saving resources)—see the relevant implementation part. Hence, again, initializing the ThreadPoolExecutor with max_workers=100, as shown in your recent update, would be unecessary, if you never expect your FastAPI application to serve more than 50 requests at a time (to endpoints where that ThreadPoolExecutor is used). Working Example from fastapi import FastAPI, Request from contextlib import asynccontextmanager import concurrent.futures import threading import asyncio import time @asynccontextmanager async def lifespan(app: FastAPI): pool = concurrent.futures.ThreadPoolExecutor(max_workers=60) yield {'pool': pool} pool.shutdown() app = FastAPI(lifespan=lifespan) @app.get(&quot;/sync_async&quot;) async def test_sync_async(request: Request) -&gt; None: loop = asyncio.get_running_loop() await loop.run_in_executor(request.state.pool, time.sleep, 3) print(&quot;sync_async&quot;) @app.get('/get_active_threads') async def get_active_threads(): return threading.active_count() Using ApacheBench, you could test the example above as follows: ab -n 1000 -c 50 &quot;http://localhost:8000/sync_async&quot; Final Notes In general, you should always aim at using asynchronous code (i.e., using async/await), wherever is possible, as async code, also known as coroutines, runs directly in the event loop—the event loop runs in a thread (typically the main thread) and executes all callbacks and Tasks in its thread. That means there is only one thread that can take a lock on the interpreter; thus, avoiding the additional overhead of context switching (i.e., the CPU jumping from one thread of execution to another). When dealing with sync blocking IO-bound tasks though, you could either (1) define your endpoint with def and let FastAPI handle it behind the scenes as described earlier, as well as in this answer, or (2) define your endpoint with async def and use run_in_threadpool() on your own to run that blocking task in a separate thread and await it, or (3) define your endpoint with async def and use asyncio's loop.run_in_executor() with a custom (preferably reusable) ThreadPoolExecutor, adjusting the number of worker threads as required. When required to perform blocking CPU-bound tasks, while running such tasks in a separate thread from an external threadpool and then awaiting them would successfully prevent the event loop from getting blocked, it wouldn't, however, provide the performance improvement you would expect from running code in parallel. Thus, for CPU-bound tasks, one may choose to use loop.run_in_executor() with ProcessPoolExecutor instead—relevant example could be found in this answer (Note that when using processes in general, you need to explicitly protect the entry point with if __name__ == '__main__'). To run tasks in the background, without waiting for them to complete, in order to proceed with executing the rest of the code in an endpoint, you could use FastAPI's BackgroundTasks, as shown here and here. If the background task function is defined with async def, FastAPI will run it directly in the event loop, whereas if it is defined with normal def, FastAPI will use run_in_threadpool() and await the returned coroutine (same concept as API endpoints). Another option when you need to run an async def function in the background, but not necessarily having it trigerred after returning a FastAPI response (which is the case in BackgroundTasks), is to use asyncio.create_task(), as shown in this answer and this answer. If you need to perform heavy background computation and you don't necessarily need it to be run by the same process, you may benefit from using other bigger tools such as Celery. Finally, regarding the optimal/maximum number of worker threads, I would suggest reading this article (have a look at this article as well for more details on ThreadPoolExecutor in general). As explained in the article: It is important to limit the number of worker threads in the thread pools to the number of asynchronous tasks you wish to complete, based on the resources in your system, or on the number of resources you intend to use within your tasks. Alternately, you may wish to increase the number of worker threads dramatically, given the greater capacity in the resources you intend to use. [...] It is common to have more threads than CPUs (physical or logical) in your system. The reason for this is that threads are used for IO-bound tasks, not CPU-bound tasks. This means that threads are used for tasks that wait for relatively slow resources to respond, like hard drives, DVD drives, printers, network connections, and much more. Therefore, it is not uncommon to have tens, hundreds and even thousands of threads in your application, depending on your specific needs. It is unusual to have more than one or a few thousand threads. If you require this many threads, then alternative solutions may be preferred, such as AsyncIO. Also, in the same article: Does the Number of Threads in the ThreadPoolExecutor Match the Number of CPUs or Cores? The number of worker threads in the ThreadPoolExecutor is not related to the number of CPUs or CPU cores in your system. You can configure the number of threads based on the number of tasks you need to execute, the amount of local system resources you have available (e.g., memory), and the limitations of resources you intend to access within your tasks (e.g., connections to remote servers). How Many Threads Should I Use? If you have hundreds of tasks, you should probably set the number of threads to be equal to the number of tasks. If you have thousands of tasks, you should probably cap the number of threads at hundreds or 1,000. If your application is intended to be executed multiple times in the future, you can test different numbers of threads and compare overall execution time, then choose a number of threads that gives approximately the best performance. You may want to mock the task in these tests with a random sleep operation. What Is the Maximum Number of Worker Threads in the ThreadPoolExecutor? There is no maximum number of worker threads in the ThreadPoolExecutor. Nevertheless, your system will have an upper limit of the number of threads you can create based on how much main memory (RAM) you have available. Before you exceed main memory, you will reach a point of diminishing returns in terms of adding new threads and executing more tasks. This is because your operating system must switch between the threads, called context switching. With too many threads active at once, your program may spend more time context switching than actually executing tasks. A sensible upper limit for many applications is hundreds of threads to perhaps a few thousand threads. More than a few thousand threads on a modern system may result in too much context switching, depending on your system and on the types of tasks that are being executed.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "starlette.concurrency.run_in_threadpool uses anyio.to_thread.run_sync() under the hood. By default, the concurrency there is limited to 40, so 50 concurrent requests will starve the threadpool; you can increase that limit with from anyio import to_thread to_thread.current_default_thread_limiter().total_tokens = 200 Similarly, run_in_executor uses a default ThreadPoolExecutor if you don't pass in one; the default worker count for the default executor is min(32, os.cpu_count() + 4), so depending on your configuration, that too may be way too little.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-asyncio",
        "fastapi",
        "starlette",
        "apachebench"
      ],
      "question_score": 10,
      "answer_score": 31,
      "created": "2024-02-04T09:46:30",
      "question_id": 77935269,
      "answer_id": 77941425
    }
  },
  {
    "question": "I got this error! OSError: [WinError 193] %1 is not a valid Win32 application",
    "expected_answer": "If you construct the service like this: service = ChromeService(ChromeDriverManager().install()) I noticed ChromeDriverManager().install() returns &lt;path stuff&gt;\\chromedriver-win32\\THIRD_PARTY_NOTICES.chromedriver instead of the chromedrive.exe. This works for me: chrome_install = ChromeDriverManager().install() folder = os.path.dirname(chrome_install) chromedriver_path = os.path.join(folder, &quot;chromedriver.exe&quot;) service = ChromeService(chromedriver_path)",
    "context_chunks": [
      {
        "text": "I trying to run an Python file today and got this error below! Anyone know what's issue and the solution to fix it? Traceback (most recent call last): File &quot;C:\\Users\\Al PC\\PycharmProjects\\Fe\\report_auto-final-v2.7.py&quot;, line 60, in &lt;module&gt; driver = webdriver.Chrome(service=chrome_service, options=options) # ChromeDriverManager() File &quot;C:\\Users\\Al PC\\PycharmProjects\\SocialMedia\\venv\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py&quot;, line 45, in __init__ super().__init__( File &quot;C:\\Users\\Al PC\\PycharmProjects\\SocialMedia\\venv\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py&quot;, line 53, in __init__ self.service.start() File &quot;C:\\Users\\Al PC\\PycharmProjects\\SocialMedia\\venv\\lib\\site-packages\\selenium\\webdriver\\common\\service.py&quot;, line 105, in start self._start_process(self._path) File &quot;C:\\Users\\Al PC\\PycharmProjects\\SocialMedia\\venv\\lib\\site-packages\\selenium\\webdriver\\common\\service.py&quot;, line 206, in _start_process self.process = subprocess.Popen( File &quot;C:\\Users\\Al PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py&quot;, line 966, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File &quot;C:\\Users\\Al PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py&quot;, line 1435, in _execute_child hp, ht, pid, tid = _winapi.CreateProcess(executable, args, OSError: [WinError 193] %1 is not a valid Win32 application Process finished with exit code 1",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "If you construct the service like this: service = ChromeService(ChromeDriverManager().install()) I noticed ChromeDriverManager().install() returns &lt;path stuff&gt;\\chromedriver-win32\\THIRD_PARTY_NOTICES.chromedriver instead of the chromedrive.exe. This works for me: chrome_install = ChromeDriverManager().install() folder = os.path.dirname(chrome_install) chromedriver_path = os.path.join(folder, &quot;chromedriver.exe&quot;) service = ChromeService(chromedriver_path)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Everyone, I'm sure you are facing a sudden problem and are having a lot of trouble, but it turns out that this problem can be solved by updating the version of webdriver_manager. My coding import wx from selenium import webdriver from selenium.webdriver.chrome.options import Options from webdriver_manager.chrome import ChromeDriverManager # Required for dialog box display app = wx.App(0) # Creating options option = Options() # To avoid &quot;unknown error: net::ERR_CONNECTION_CLOSED&quot; option.add_argument('--disable-dev-shm-usage') option.add_argument('no-sandbox') option.add_argument('--disable-extensions') option.add_argument('--disable-gpu') option.add_argument('disable-infobars') option.add_argument('--incognito') # secret mode # Enable headless mode. option.add_argument('--headless') # Create a Chrome WebDriver object. try: # Process to automatically install the driver of the same version as Chrome on your PC. driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=option) except Exception as e: print(&quot;error:&quot;, e) wx.MessageBox(f'{e}', u'error', wx.ICON_ERROR) Error message dialog C:\\Users\\iorin\\OneDrive\\画像\\スクリーンショット\\2024-08-06 (6).png What is output to the system's error.log Traceback (most recent call last): File &quot;c:\\Users\\iorin\\OneDrive\\Python_Scripts\\formdemo\\modules\\chrome_driver.py&quot;, line 34, in &lt;module&gt; driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=option) File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py&quot;, line 73, in __init__ self.service.start() File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\common\\service.py&quot;, line 72, in start self.process = subprocess.Popen(cmd, env=self.env, File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py&quot;, line 951, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py&quot;, line 1420, in _execute_child hp, ht, pid, tid = _winapi.CreateProcess(executable, args, OSError: [WinError 193] %1 is not a valid Win32 application During handling of the above exception, another exception occurred: Traceback (most recent call last): File &quot;c:\\Users\\iorin\\OneDrive\\Python_Scripts\\formdemo\\main.py&quot;, line 12, in &lt;module&gt; from modules import (full_name, File &quot;c:\\Users\\iorin\\OneDrive\\Python_Scripts\\formdemo\\modules\\__init__.py&quot;, line 1, in &lt;module&gt; from . import ( File &quot;c:\\Users\\iorin\\OneDrive\\Python_Scripts\\formdemo\\modules\\full_name.py&quot;, line 5, in &lt;module&gt; from . import (chrome_driver, File &quot;c:\\Users\\iorin\\OneDrive\\Python_Scripts\\formdemo\\modules\\chrome_driver.py&quot;, line 46, in &lt;module&gt; driver = webdriver.Chrome(ChromeDriverManager(res.text).install(),options=option) File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py&quot;, line 76, in __init__ RemoteWebDriver.__init__( File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py&quot;, line 157, in __init__ self.start_session(capabilities, browser_profile) File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py&quot;, line 252, in start_session response = self.execute(Command.NEW_SESSION, parameters) File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py&quot;, line 321, in execute self.error_handler.check_response(response) File &quot;C:\\Users\\iorin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py&quot;, line 242, in check_response raise exception_class(message, screen, stacktrace) selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 114 Current browser version is 127.0.6533.89 with binary path C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe Step 1. Run the command pip show webdriver-manager. Name: webdriver-manager Version: 4.0.0 Summary: Library provides the way to automatically manage drivers for different browsers Home-page: https://github.com/SergeyPirogov/webdriver_manager Author: Sergey Pirogov Author-email: automationremarks@gmail.com License: Location: c:\\users\\iorin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages Requires: packaging, python-dotenv, requests Required-by: Step 2. Visit the author's site. Step 3. Run the command pip install webdriver-manager==4.0.2 on See what changed . Questions that were helpful▶ ChromerDriverManager not returning correct path for the chrome driver #665",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver"
      ],
      "question_score": 10,
      "answer_score": 29,
      "created": "2024-07-26T07:47:28",
      "question_id": 78796828,
      "answer_id": 78797164
    }
  },
  {
    "question": "How to use DataFrameMapper to delete rows with a null value in a specific column?",
    "expected_answer": "I would recommend an approach that filters before transformation, otherwise you lose efficiency if your dataset contains a lot of null values: import pandas as pd df = df.dropna(subset=['xxx']) Then proceed with DataFrameMapper similar to the following: from sklearn_pandas import DataFrameMapper from sklearn.preprocessing import StandardScaler mapper = DataFrameMapper([ ('xxx', StandardScaler()), ], df_out=True) # Transform the data new_data = mapper.fit_transform(df.copy())",
    "context_chunks": [
      {
        "text": "I am using sklearn-pandas.DataFrameMapper to preprocess my data. I don't want to impute for a specific column. I just want to drop the row if this column is Null. Is there a way to do that?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I would recommend an approach that filters before transformation, otherwise you lose efficiency if your dataset contains a lot of null values: import pandas as pd df = df.dropna(subset=['xxx']) Then proceed with DataFrameMapper similar to the following: from sklearn_pandas import DataFrameMapper from sklearn.preprocessing import StandardScaler mapper = DataFrameMapper([ ('xxx', StandardScaler()), ], df_out=True) # Transform the data new_data = mapper.fit_transform(df.copy())",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This can be done very easily before or after using the DataFrameMapper: df_filtered = df [~df['specific column name'].isnull()] To do it using the DataFrameMapper itself, you would need to build a transformer as so: class DropNullTransformer(BaseEstimator, TransformerMixin): def __init__(self, column): self.column = column def fit(self, X, y=None): return self def transform(self, X): return X.dropna(subset=[self.column]) From there, you include this transformer when building the DataFrameMapper: dfm = DataFrameMapper([ ([specificColumnName], DropNullTransformer(specificColumnName)) ]) Then, the fit and transformation function will perform the drop for you. To learn more about custom transformers, you can read the Sklearn guide.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "sklearn-pandas",
        "sklearn2pmml"
      ],
      "question_score": 11,
      "answer_score": 8,
      "created": "2024-07-13T15:34:57",
      "question_id": 78744276,
      "answer_id": 78790789
    }
  },
  {
    "question": "Find the optimal clipped circle",
    "expected_answer": "Start by creating a cumulative frequency table, or a fenwick tree. You'll have a record for each radius of circle, with value corresponding to explored weights at that distance from the origin. Then, begin a BFS from the origin. For each diagonal &quot;frontier&quot;, you'll need to update your table/tree with the radius:weight key-value pair (add weight to existing value). You'll also need to then query the table/tree for the current cumulative sum at each radius just added, noting the maximum and updating a global running maximum accordingly. Once your search terminates, you'll have the maximum sum for your clipped-circle. If you want to reconstruct the circle, just store the max radius and BFS depth along with the global max sum itself. This will give you your solution in O(N^2 log N) time, as there will be N^2 updates and queries, which are O(log N) each. The intuition behind this solution is that by exploring along this diagonal &quot;frontier&quot; outward, you implicitly clip all your circles you query since the weights above/right of it haven't been added yet. By calculating the max (at each search depth) for just the radii that were just updated, you also enforce the constraint that the circles intersect the clipping line at an integer coordinate. Update Here is python code showing this in action. It needs cleaned up, but at least it shows the process. I opted to use cumulative frequency / max arrays, instead of trees, since that'll probably lend itself to vectorization with numpy for OP. def solve(matrix): n = len(matrix) max_radius_sqr = 2 * (n - 1) ** 2 num_bins = max_radius_sqr.bit_length() + 1 frontier = [(0, 0)] csum_arr = [[0] * 2 ** i for i in range(num_bins)[::-1]] cmax_arr = [[0] * 2 ** i for i in range(num_bins)[::-1]] max_csum = -float(&quot;inf&quot;) max_csum_depth = None max_csum_radius_sqr = None depth = 0 while frontier: next_frontier = [] if depth + 1 &lt; n: # BFS up next_frontier.append((0, depth + 1)) # explore frontier, updating csums and maximums per each for x, y in frontier: if x + 1 &lt; n: # BFS right next_frontier.append((x + 1, y)) index = x ** 2 + y ** 2 # index is initially the radius squared for i in range(num_bins): csum_arr[i][index] += matrix[y][x] # update csums if i != 0: # skip first, since no children to take max of sum_left = csum_arr[i-1][index &lt;&lt; 1] # left/right is tree notation of the array max_left = cmax_arr[i-1][index &lt;&lt; 1] max_right = cmax_arr[i-1][index &lt;&lt; 1 | 1] cmax_arr[i][index] = max(max_left, sum_left + max_right) # update csum maximums index &gt;&gt;= 1 # shift off last bit, update sums/maxs again, log2 times # after entire frontier is explored, query for overall max csum over all radii # update running global max and associated values if cmax_arr[-1][0] &gt; max_csum: max_csum = cmax_arr[-1][0] max_csum_depth = depth index = 0 for i in range(num_bins-1)[::-1]: # reconstruct max radius (this could just as well be stored) sum_left = csum_arr[i][index &lt;&lt; 1] max_left = cmax_arr[i][index &lt;&lt; 1] max_right = cmax_arr[i][index &lt;&lt; 1 | 1] index &lt;&lt;= 1 if sum_left + max_right &gt; max_left: index |= 1 max_csum_radius_sqr = index depth += 1 frontier = next_frontier # total max sum, dx + dy of diagonal cut, radius ** 2 return max_csum, max_csum_depth, max_csum_radius_sqr Calling this with the given test case produces the expected output: matrix = [ [-1, 3, -3, -2, 0, -1, -2, -1, -1, 2], [ 0, -3, 0, 3, 2, -2, 3, -2, 3, 3], [-3, 1, 3, 3, 0, -3, -3, 2, -2, 1], [ 3, 2, 2, -1, 0, -3, 1, 1, -2, 2], [-3, 3, 2, 0, -3, -2, -1, -3, 0, -3], [-1, -2, -1, 2, 3, 3, -3, -3, 2, 0], [-2, 0, -3, 3, 0, 2, -1, 1, 3, 3], [ 2, 2, -3, 2, -2, -1, 2, 2, -2, 0], [-2, -1, 0, 1, 0, -2, 0, 0, 1, -3], [ 1, 1, -3, 0, 0, 3, -1, 3, -3, 2], ][::-1] print(solve(matrix)) # output: 13 9 54 In other words, it says the maximum total sum is 13, with a diagonal cut stagger (dx + dy) of 9, and radius squared of 54. If I have some time tonight or this weekend, I'll clean up the code a bit.",
    "context_chunks": [
      {
        "text": "Given a NxN integer lattice, I want to find the clipped circle which maximizes the sum of its interior lattice point values. Each lattice point (i,j) has a value V(i,j) and are stored in the following matrix V: [[ 1, 1, -3, 0, 0, 3, -1, 3, -3, 2], [-2, -1, 0, 1, 0, -2, 0, 0, 1, -3], [ 2, 2, -3, 2, -2, -1, 2, 2, -2, 0], [-2, 0, -3, 3, 0, 2, -1, 1, 3, 3], [-1, -2, -1, 2, 3, 3, -3, -3, 2, 0], [-3, 3, 2, 0, -3, -2, -1, -3, 0, -3], [ 3, 2, 2, -1, 0, -3, 1, 1, -2, 2], [-3, 1, 3, 3, 0, -3, -3, 2, -2, 1], [ 0, -3, 0, 3, 2, -2, 3, -2, 3, 3], [-1, 3, -3, -2, 0, -1, -2, -1, -1, 2]] The goal is to maximize the sum of values V(i,j) of the lattice points lying on the boundary and within interior of a (clipped) circle with radius R, with the assumptions and conditions: the circle has center at (0,0) the circle can have any positive radius (not necessarily an integer radius, i.e., rational). the circle may be clipped at two lattice points, resulting in a diagonal line as shown in the picture. This diagonal line has a slope of -45 degrees. Some additional details: The score for a clipped circle is the sum of all the integers that are both within the circle (or on the border) and on the side of the diagonal line including (0,0). The values on (or near) the border are -3, 1, 3, -1, -3, 3, -1, 2, 0, 3. Even though the circle can have any radius, we need only consider circles that intersect a grid point precisely so there are n^2 different relevant radiuses. Further, we need only record one position where the circle intersects with the diagonal line to fully specify the clipped circle. Note that this intersection with the diagonal does not need to be at an integer coordinate. If the optimal solution doesn't have the diagonal clipping the circle at all then we need only return the radius of the circle. What I have found so far: If we only wanted to find the optimal circle we could do that quickly in time proportional to the input size with: import numpy as np from math import sqrt np.random.seed(40) def find_max(A): n = A.shape[0] sum_dist = np.zeros(2 * n * n, dtype=np.int32) for i in range(n): for j in range(n): dist = i**2 + j**2 sum_dist[dist] += A[i, j] cusum = np.cumsum(sum_dist) # returns optimal radius with its score return sqrt(np.argmax(cusum)), np.max(cusum) A = np.random.randint(-3, 4, (10, 10)) print(find_max(A)) How quickly can the optimal clipped circle be found?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Start by creating a cumulative frequency table, or a fenwick tree. You'll have a record for each radius of circle, with value corresponding to explored weights at that distance from the origin. Then, begin a BFS from the origin. For each diagonal &quot;frontier&quot;, you'll need to update your table/tree with the radius:weight key-value pair (add weight to existing value). You'll also need to then query the table/tree for the current cumulative sum at each radius just added, noting the maximum and updating a global running maximum accordingly. Once your search terminates, you'll have the maximum sum for your clipped-circle. If you want to reconstruct the circle, just store the max radius and BFS depth along with the global max sum itself. This will give you your solution in O(N^2 log N) time, as there will be N^2 updates and queries, which are O(log N) each. The intuition behind this solution is that by exploring along this diagonal &quot;frontier&quot; outward, you implicitly clip all your circles you query since the weights above/right of it haven't been added yet. By calculating the max (at each search depth) for just the radii that were just updated, you also enforce the constraint that the circles intersect the clipping line at an integer coordinate. Update Here is python code showing this in action. It needs cleaned up, but at least it shows the process. I opted to use cumulative frequency / max arrays, instead of trees, since that'll probably lend itself to vectorization with numpy for OP. def solve(matrix): n = len(matrix) max_radius_sqr = 2 * (n - 1) ** 2 num_bins = max_radius_sqr.bit_length() + 1 frontier = [(0, 0)] csum_arr = [[0] * 2 ** i for i in range(num_bins)[::-1]] cmax_arr = [[0] * 2 ** i for i in range(num_bins)[::-1]] max_csum = -float(&quot;inf&quot;) max_csum_depth = None max_csum_radius_sqr = None depth = 0 while frontier: next_frontier = [] if depth + 1 &lt; n: # BFS up next_frontier.append((0, depth + 1)) # explore frontier, updating csums and maximums per each for x, y in frontier: if x + 1 &lt; n: # BFS right next_frontier.append((x + 1, y)) index = x ** 2 + y ** 2 # index is initially the radius squared for i in range(num_bins): csum_arr[i][index] += matrix[y][x] # update csums if i != 0: # skip first, since no children to take max of sum_left = csum_arr[i-1][index &lt;&lt; 1] # left/right is tree notation of the array max_left = cmax_arr[i-1][index &lt;&lt; 1] max_right = cmax_arr[i-1][index &lt;&lt; 1 | 1] cmax_arr[i][index] = max(max_left, sum_left + max_right) # update csum maximums index &gt;&gt;= 1 # shift off last bit, update sums/maxs again, log2 times # after entire frontier is explored, query for overall max csum over all radii # update running global max and associated values if cmax_arr[-1][0] &gt; max_csum: max_csum = cmax_arr[-1][0] max_csum_depth = depth index = 0 for i in range(num_bins-1)[::-1]: # reconstruct max radius (this could just as well be stored) sum_left = csum_arr[i][index &lt;&lt; 1] max_left = cmax_arr[i][index &lt;&lt; 1] max_right = cmax_arr[i][index &lt;&lt; 1 | 1] index &lt;&lt;= 1 if sum_left + max_right &gt; max_left: index |= 1 max_csum_radius_sqr = index depth += 1 frontier = next_frontier # total max sum, dx + dy of diagonal cut, radius ** 2 return max_csum, max_csum_depth, max_csum_radius_sqr Calling this with the given test case produces the expected output: matrix = [ [-1, 3, -3, -2, 0, -1, -2, -1, -1, 2], [ 0, -3, 0, 3, 2, -2, 3, -2, 3, 3], [-3, 1, 3, 3, 0, -3, -3, 2, -2, 1], [ 3, 2, 2, -1, 0, -3, 1, 1, -2, 2], [-3, 3, 2, 0, -3, -2, -1, -3, 0, -3], [-1, -2, -1, 2, 3, 3, -3, -3, 2, 0], [-2, 0, -3, 3, 0, 2, -1, 1, 3, 3], [ 2, 2, -3, 2, -2, -1, 2, 2, -2, 0], [-2, -1, 0, 1, 0, -2, 0, 0, 1, -3], [ 1, 1, -3, 0, 0, 3, -1, 3, -3, 2], ][::-1] print(solve(matrix)) # output: 13 9 54 In other words, it says the maximum total sum is 13, with a diagonal cut stagger (dx + dy) of 9, and radius squared of 54. If I have some time tonight or this weekend, I'll clean up the code a bit.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The previous version was wrong! Here is the code of an algorithm that yields an exact solution. Update. This timed version is faster. def clippedCircleMax(A): startime=time.time() N = A.shape[0] dN=(N-1)**2 cumsom = np.zeros((N,N),dtype=np.int32) imins = np.zeros(2*N*N, dtype=np.int32) maxsom=A[0,0] for diagonal in range(2*N-1): d=diagonal//2 for j in range(d,-1,-1): i=diagonal-j if i&lt;N: d2 = i**2 + j**2 jmin=0 if d2&lt;=dN: imin=int(sqrt(d2)) if j&gt;0: if imins[d2]==0: jmin=int(sqrt(d2-imin**2)) if jmin&gt;=j: jmin=j-1 else: imin=imins[d2] jmin=int(sqrt(d2-imin**2)) imin+=1 else: jmin=int(sqrt(d2-dN)) if jmin&gt;=j: jmin=j-1 imin=N som=cumsom[imin-1,jmin] dmin=imin+jmin dmax=i+j+1 som=getClippedSum(A,N,d2,dmin,dmax,som) cumsom[i,j]=som imins[d2]=i if som&gt;=maxsom: maxsom=som imax=i jmax=j dtime=time.time()-startime return maxsom,imax,jmax,dtime,N def getClippedSum(A,N,d2,dmin,dmax,som): for d in range(dmin,dmax): xmax=min(d+1,N) k,p=divmod(d,2) xmin=k+p for x in range(xmin,xmax): y=d-x r2=x**2+y**2 if r2&lt;=d2 : som+=A[x,y] if x!=y: som+=A[y,x] else: break return som",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "performance",
        "optimization"
      ],
      "question_score": 11,
      "answer_score": 6,
      "created": "2024-03-10T17:45:45",
      "question_id": 78136859,
      "answer_id": 78151598
    }
  },
  {
    "question": "How to find all grid points that correspond to non-reduced fractions in a square?",
    "expected_answer": "The algorithm of Weeble runs in O(n² m²) where m is the size of integers in bits (using a naive multiplication). Since we can assume the multiplication of numbers to be done in constant time (due to bounded native integers used by Numpy), this means O(n²) but with a significant hidden constant which should not be neglected. Performance wise, the algorithm is bounded by inefficient page fault operations and the filling of big temporary arrays. It is far from being optimal. The algorithm of Pete Kirkham should run in O(n²) (hard to prove formally though) with a relatively small hidden constant. This is is a good approach. However, is is very slow because of inefficient scalar Numpy operations instead of vectorised ones. Fortunately, it can be easily vectorised: array = np.full((N,N), 255, np.uint8) for d in range(2, N+1): array[d-1:N:d, d-1:N:d] = 0 Note I corrected the implementation to return correct results (with values 0 and 255). A very simple alternative solution is just to use Numba so to speed up the code of Pete Kirkham. That being said, the code is not efficient because it iterates on items of different rows in the inner most loop. We can easily fix that by swapping variables: import numba as nb @nb.njit('(int32,)') def compute(N): array = np.full((N,N), 255, np.uint8) for denominator in range(2, N+1): for i in range(denominator, N+1, denominator): for j in range(denominator, N+1, denominator): array[i-1, j-1] = 0 return array Faster alternative approach Note that the output matrix is symmetric so we don't even need to compute the bottom-left part. Indeed, gcd(a, b) == gcd(b, a). Unfortunately, I do not think the can use this property to make the Numpy vectorised code but we can probably make the Numba code faster. Moreover, the diagonal can be trivially set to 0 (except the first item) since gcd(a, a) == a so gcd(a, a) &gt; 1 if a &gt; 1. Technically, we can also trivially set the direct neighbourhood of the diagonal (i.e. array.diagonal(1)) to 255 since gcd(a, a-1) = 1. array.diagonal(1) should be filled with alternating values (i.e. [255, 0, 255, 0, ...]) since gcd(a, a-2) = gcd(a, 2) = 2 - (a % 2). A similar strategy can be applied for array.diagonal(2). For other diagonal, it starts to be more complex since we certainly need to factorise numbers. Factorising numbers is known to be expensive, but this cost is amortised here since we need to do that O(n) times. Another symmetry of the gcd is gcd(a, b) = gcd(a, b-a) = gcd(a-b, b). We can leverage all these symmetry of the gcd so to write a significantly faster implementation using dynamic programming. A naive implementation (combining all the symmetries rather efficiently) is the following: @nb.njit('(int32,)') def compute_v2(n): arr = np.empty((n,n), np.uint8) arr[:, 0] = 255 arr[0, :] = 255 for i in range(1, n): for j in range(1, i): arr[i, j] = arr[j, i-j-1] # &lt;--- very slow part arr[i, i] = 0 for j in range(i+1, n): arr[i, j] = arr[i, j-i-1] return arr Unfortunately the transposition is pretty inefficient and take nearly all the time... Optimizing is possible but not easy. We can divide the computation in dependent tiles (similar to how block LU decomposition algorithm work). This makes the code more complex much much faster thanks to a more efficient access pattern: # Compute the tile arr[start:stop,start:stop]. # Assume arr[:start,:start] has been already computed. # Assume start and stop are valid. @nb.njit('(uint8[:,::1], uint32, uint32)', inline='always') def compute_diag_tile(arr, start, stop): for i in range(start, stop): for j in range(start, i): arr[i, j] = arr[j, i-j-1] arr[i, i] = 0 for j in range(i+1, stop): arr[i, j] = arr[i, j-i-1] # Compute the tile arr[start:stop,stop:]. # Assume arr[start:stop,:stop] has been already computed. # Assume start and stop are valid. @nb.njit('(uint8[:,::1], uint32, uint32)', inline='always') def compute_upper_right_tile(arr, start, stop): n = np.uint32(arr.shape[1]) for i in range(start, stop): for j in range(stop, n): arr[i, j] = arr[i, np.uint64(j-i-1)] # Compute the tile arr[stop:,start:stop]. # Assume arr[start:stop,stop:] has been already computed; that is to say # compute_upper_right_tile has been called on the associated diag tile. # This function transposes the tile written by compute_upper_right_tile. # Assume start and stop are valid. @nb.njit('(uint8[:,::1], uint32, uint32)', inline='always') def compute_bottom_left_tile(arr, start, stop): n = np.uint32(arr.shape[0]) for i in range(stop, n): for j in range(start, stop): arr[i, j] = arr[j, i] @nb.njit('(uint8[:,::1], uint32, uint32)', inline='always') def compute_tile_group(arr, start, stop): compute_diag_tile(arr, start, stop) compute_upper_right_tile(arr, start, stop) compute_bottom_left_tile(arr, start, stop) @nb.njit('(uint32,)') def compute_v3(n): chunk_size = 32 arr = np.empty((n, n), np.uint8) arr[0, :] = 255 arr[:, 0] = 255 for start in range(1, n, chunk_size): if start + chunk_size &lt;= n: compute_tile_group(arr, start, start + chunk_size) else: compute_tile_group(arr, start, n) return arr The transposition is still the slowest part of this code. It can be optimized further but at the expense of a significantly bigger code. I prefer to keep this reasonably simple, but note that one way to make the transposition much faster is to use SIMD intrinsics (certainly at least &gt;2 faster). Benchmark Here are results for N=1024 on my machine (i5-9600KF CPU): find_complex_points: 173 ms PeteKirkham's code: 108 ms find_complex_points_1: 99 ms Weeble's code: 70 ms find_complex_points_2: 38 ms Vectorized Numpy code: 4.0 ms &lt;----- PeteKirkham's code with Numba: 2.5 ms Numba code `compute_v2`: 0.70 ms Numba code `compute`: 0.66 ms Numba code `compute_v3`: 0.45 ms &lt;----- find_complex_points_3: 0.44 ms The vectorised Numba code is much faster than the other implementation and the optimised Numba codes outperforms all implementation by a large margin (except the new find_complex_points_3)! One can parallelise some of the Numba codes to make it even faster but this is not trivial and it is certainly fast enough anyway, not to mention is will not scale well because the code is rather memory-bound for large N. Actually, a basic Numpy copy takes about 0.3 ms, which can be considered as a lower bound execution time.",
    "context_chunks": [
      {
        "text": "Given a positive integer N, we can label all grid points in the square N x N, starting at 1, the total number of grid points is N x N, and the grid points are list(itertools.product(range(1, N + 1), repeat=2)). Now, I want to find all tuples (x, y) that satisfies the condition x/y is a non-reduced fraction, the following is a bruteforce implementation that is guaranteed to be correct, but it is very inefficient: import math from itertools import product def find_complex_points(lim: int) -&gt; list[tuple[int, int]]: return [ (x, y) for x, y in product(range(1, lim + 1), repeat=2) if math.gcd(x, y) &gt; 1 ] Now the next function is slightly smarter, but it generates duplicates and as a result is only noticeably faster but not by much: def find_complex_points_1(lim: int) -&gt; set[tuple[int, int]]: lim += 1 return { (x, y) for mult in range(2, lim) for x, y in product(range(mult, lim, mult), repeat=2) } In [255]: %timeit find_complex_points(1024) 233 ms ± 4.44 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) In [256]: %timeit find_complex_points_1(1024) 194 ms ± 1.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Is there a better way to accomplish this? (My goal is simple, I want to create a NumPy 2D array of uint8 type with shape (N, N), fill it with 255, and make all pixels (x, y) 0 if (x+1)/(y+1) is a non-reduced fraction) I have devised a method that is smarter than both my previous ones by a wide margin, and also tremendously faster, but it still generates duplicates, I have opt to not to use a set here so that you can copy-paste the code as is and run some tests and see the exact output in the order they are generated: def find_complex_points_2(lim: int) -&gt; set[tuple[int, int]]: stack = dict.fromkeys(range(lim, 1, -1)) lim += 1 points = [] while stack: x, _ = stack.popitem() points.append((x, x)) mults = [] for y in range(x * 2, lim, x): stack.pop(y, None) mults.append(y) points.extend([(x, y), (y, x)]) for i, x in enumerate(mults): points.append((x, x)) for y in mults[i + 1:]: points.extend([(x, y), (y, x)]) return points In [292]: sorted(set(find_complex_points_2(1024))) == find_complex_points(1024) Out[292]: True In [293]: %timeit find_complex_points_2(1024) 58.9 ms ± 580 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) In [294]: %timeit find_complex_points(1024) 226 ms ± 3.24 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) To clarify, the output of find_complex_points_2(10) is: In [287]: find_complex_points_2(10) Out[287]: [(2, 2), (2, 4), (4, 2), (2, 6), (6, 2), (2, 8), (8, 2), (2, 10), (10, 2), (4, 4), (4, 6), (6, 4), (4, 8), (8, 4), (4, 10), (10, 4), (6, 6), (6, 8), (8, 6), (6, 10), (10, 6), (8, 8), (8, 10), (10, 8), (10, 10), (3, 3), (3, 6), (6, 3), (3, 9), (9, 3), (6, 6), (6, 9), (9, 6), (9, 9), (5, 5), (5, 10), (10, 5), (10, 10), (7, 7)] As you can see, (10, 10) shows up twice. I want to avoid redundant computations. Also this happens in find_complex_points_1, if I don't use a set, then many duplicates will be included, because the method used will inevitably generate them repeatedly, by using a set there still is unnecessary computation, it just doesn't collect the duplicates. And no, I actually want the coordinates to be replaced by the sum of all numbers before it, so N is replaced by (N2 + N) / 2. I just implemented the image generation to better illustrate what I want: import numpy as np import numba as nb @nb.njit(cache=True) def resize_img(img: np.ndarray, h_scale: int, w_scale: int) -&gt; np.ndarray: height, width = img.shape result = np.empty((height, h_scale, width, w_scale), np.uint8) result[...] = img[:, None, :, None] return result.reshape((height * h_scale, width * w_scale)) def find_composite_points(lim: int) -&gt; set[tuple[int, int]]: stack = dict.fromkeys(range(lim, 1, -1)) lim += 1 points = set() while stack: x, _ = stack.popitem() points.add((x, x)) mults = [] for y in range(x * 2, lim, x): stack.pop(y, None) mults.append(y) points.update([(x, y), (y, x)]) for i, x in enumerate(mults): points.add((x, x)) for y in mults[i + 1 :]: points.update([(x, y), (y, x)]) return points def natural_sum(n: int) -&gt; int: return (n + 1) * n // 2 def composite_image(lim: int, scale: int) -&gt; np.ndarray: length = natural_sum(lim) img = np.full((length, length), 255, dtype=np.uint8) for x, y in find_composite_points(lim): x1, y1 = natural_sum(x - 1), natural_sum(y - 1) img[x1 : x1 + x, y1 : y1 + y] = 0 return resize_img(img, scale, scale) composite_image(12, 12)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The algorithm of Weeble runs in O(n² m²) where m is the size of integers in bits (using a naive multiplication). Since we can assume the multiplication of numbers to be done in constant time (due to bounded native integers used by Numpy), this means O(n²) but with a significant hidden constant which should not be neglected. Performance wise, the algorithm is bounded by inefficient page fault operations and the filling of big temporary arrays. It is far from being optimal. The algorithm of Pete Kirkham should run in O(n²) (hard to prove formally though) with a relatively small hidden constant. This is is a good approach. However, is is very slow because of inefficient scalar Numpy operations instead of vectorised ones. Fortunately, it can be easily vectorised: array = np.full((N,N), 255, np.uint8) for d in range(2, N+1): array[d-1:N:d, d-1:N:d] = 0 Note I corrected the implementation to return correct results (with values 0 and 255). A very simple alternative solution is just to use Numba so to speed up the code of Pete Kirkham. That being said, the code is not efficient because it iterates on items of different rows in the inner most loop. We can easily fix that by swapping variables: import numba as nb @nb.njit('(int32,)') def compute(N): array = np.full((N,N), 255, np.uint8) for denominator in range(2, N+1): for i in range(denominator, N+1, denominator): for j in range(denominator, N+1, denominator): array[i-1, j-1] = 0 return array Faster alternative approach Note that the output matrix is symmetric so we don't even need to compute the bottom-left part. Indeed, gcd(a, b) == gcd(b, a). Unfortunately, I do not think the can use this property to make the Numpy vectorised code but we can probably make the Numba code faster. Moreover, the diagonal can be trivially set to 0 (except the first item) since gcd(a, a) == a so gcd(a, a) &gt; 1 if a &gt; 1. Technically, we can also trivially set the direct neighbourhood of the diagonal (i.e. array.diagonal(1)) to 255 since gcd(a, a-1) = 1. array.diagonal(1) should be filled with alternating values (i.e. [255, 0, 255, 0, ...]) since gcd(a, a-2) = gcd(a, 2) = 2 - (a % 2). A similar strategy can be applied for array.diagonal(2). For other diagonal, it starts to be more complex since we certainly need to factorise numbers. Factorising numbers is known to be expensive, but this cost is amortised here since we need to do that O(n) times. Another symmetry of the gcd is gcd(a, b) = gcd(a, b-a) = gcd(a-b, b). We can leverage all these symmetry of the gcd so to write a significantly faster implementation using dynamic programming. A naive implementation (combining all the symmetries rather efficiently) is the following: @nb.njit('(int32,)') def compute_v2(n): arr = np.empty((n,n), np.uint8) arr[:, 0] = 255 arr[0, :] = 255 for i in range(1, n): for j in range(1, i): arr[i, j] = arr[j, i-j-1] # &lt;--- very slow part arr[i, i] = 0 for j in range(i+1, n): arr[i, j] = arr[i, j-i-1] return arr Unfortunately the transposition is pretty inefficient and take nearly all the time... Optimizing is possible but not easy. We can divide the computation in dependent tiles (similar to how block LU decomposition algorithm work). This makes the code more complex much much faster thanks to a more efficient access pattern: # Compute the tile arr[start:stop,start:stop]. # Assume arr[:start,:start] has been already computed. # Assume start and stop are valid. @nb.njit('(uint8[:,::1], uint32, uint32)', inline='always') def compute_diag_tile(arr, start, stop): for i in range(start, stop): for j in range(start, i): arr[i, j] = arr[j, i-j-1] arr[i, i] = 0 for j in range(i+1, stop): arr[i, j] = arr[i, j-i-1] # Compute the tile arr[start:stop,stop:]. # Assume arr[start:stop,:stop] has been already computed. # Assume start and stop are valid. @nb.njit('(uint8[:,::1], uint32, uint32)', inline='always') def compute_upper_right_tile(arr, start, stop): n = np.uint32(arr.shape[1]) for i in range(start, stop): for j in range(stop, n): arr[i, j] = arr[i, np.uint64(j-i-1)] # Compute the tile arr[stop:,start:stop]. # Assume arr[start:stop,stop:] has been already computed; that is to say # compute_upper_right_tile has been called on the associated diag tile. # This function transposes the tile written by compute_upper_right_tile. # Assume start and stop are valid. @nb.njit('(uint8[:,::1], uint32, uint32)', inline='always') def compute_bottom_left_tile(arr, start, stop): n = np.uint32(arr.shape[0]) for i in range(stop, n): for j in range(start, stop): arr[i, j] = arr[j, i] @nb.njit('(uint8[:,::1], uint32, uint32)', inline='always') def compute_tile_group(arr, start, stop): compute_diag_tile(arr, start, stop) compute_upper_right_tile(arr, start, stop) compute_bottom_left_tile(arr, start, stop) @nb.njit('(uint32,)') def compute_v3(n): chunk_size = 32 arr = np.empty((n, n), np.uint8) arr[0, :] = 255 arr[:, 0] = 255 for start in range(1, n, chunk_size): if start + chunk_size &lt;= n: compute_tile_group(arr, start, start + chunk_size) else: compute_tile_group(arr, start, n) return arr The transposition is still the slowest part of this code. It can be optimized further but at the expense of a significantly bigger code. I prefer to keep this reasonably simple, but note that one way to make the transposition much faster is to use SIMD intrinsics (certainly at least &gt;2 faster). Benchmark Here are results for N=1024 on my machine (i5-9600KF CPU): find_complex_points: 173 ms PeteKirkham's code: 108 ms find_complex_points_1: 99 ms Weeble's code: 70 ms find_complex_points_2: 38 ms Vectorized Numpy code: 4.0 ms &lt;----- PeteKirkham's code with Numba: 2.5 ms Numba code `compute_v2`: 0.70 ms Numba code `compute`: 0.66 ms Numba code `compute_v3`: 0.45 ms &lt;----- find_complex_points_3: 0.44 ms The vectorised Numba code is much faster than the other implementation and the optimised Numba codes outperforms all implementation by a large margin (except the new find_complex_points_3)! One can parallelise some of the Numba codes to make it even faster but this is not trivial and it is certainly fast enough anyway, not to mention is will not scale well because the code is rather memory-bound for large N. Actually, a basic Numpy copy takes about 0.3 ms, which can be considered as a lower bound execution time.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Since what you want is a numpy array, a different approach would be to start with the array and use something like the sieve of Eratosthenes to mark the ones which are non-reduced: array = np.full((N,N), 1, np.uint8) for denominator in range(2, N+1): for i in range(denominator, N+1, denominator): for j in range(denominator, N+1, denominator): array[j-1, i-1] = 0",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "math",
        "number-theory"
      ],
      "question_score": 10,
      "answer_score": 11,
      "created": "2025-04-09T14:19:47",
      "question_id": 79564589,
      "answer_id": 79565492
    }
  },
  {
    "question": "Chrome 122 - How to allow insecure content? (Insecure download blocked)",
    "expected_answer": "Okay, so found two solutions: --unsafely-treat-insecure-origin-as-secure=* This is an experimental flag that allows you to list which domains to treat as secure so the download is no longer blocked. --disable-features=InsecureDownloadWarnings This is a more stable flag that disables the insecure download blocking feature for all domains. -- This is what worked for me: from selenium import webdriver from selenium.webdriver.chrome.options import Options chrome_options = Options() chrome_options.add_argument(&quot;--window-size=1920,1080&quot;) chrome_options.add_argument(&quot;--allow-running-insecure-content&quot;) # Allow insecure content chrome_options.add_argument(&quot;--unsafely-treat-insecure-origin-as-secure=http://example.com&quot;) # Replace example.com with your site's domain chrome_options.add_experimental_option(&quot;prefs&quot;, { &quot;download.default_directory&quot;: download_path, &quot;download.prompt_for_download&quot;: False, &quot;download.directory_upgrade&quot;: True, &quot;safebrowsing.enabled&quot;: True }) driver = webdriver.Chrome(options=chrome_options)",
    "context_chunks": [
      {
        "text": "I'm unable to test file download with Selenium (python), after Chrome update to the version '122.0.6261.70'. Previously running Chrome with the '--allow-running-insecure-content' arg did a trick. The same is suggested over the net. On some sites one additional arg is suggested: '--disable-web-security'. But both change nothing for me (the warning keeps appearing). Does anybody know if something has been changed between the 121 and 122 versions? Is there some arg or pref that I'm missing? Warning image for the reference: Driver creation (simplified): from selenium import webdriver from selenium.webdriver.chrome.options import Options options = Options() for arg in [&quot;--allow-running-insecure-content&quot;, &quot;--disable-web-security&quot;]: options.add_argument(arg) driver = webdriver.Chrome(options=options)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Okay, so found two solutions: --unsafely-treat-insecure-origin-as-secure=* This is an experimental flag that allows you to list which domains to treat as secure so the download is no longer blocked. --disable-features=InsecureDownloadWarnings This is a more stable flag that disables the insecure download blocking feature for all domains. -- This is what worked for me: from selenium import webdriver from selenium.webdriver.chrome.options import Options chrome_options = Options() chrome_options.add_argument(&quot;--window-size=1920,1080&quot;) chrome_options.add_argument(&quot;--allow-running-insecure-content&quot;) # Allow insecure content chrome_options.add_argument(&quot;--unsafely-treat-insecure-origin-as-secure=http://example.com&quot;) # Replace example.com with your site's domain chrome_options.add_experimental_option(&quot;prefs&quot;, { &quot;download.default_directory&quot;: download_path, &quot;download.prompt_for_download&quot;: False, &quot;download.directory_upgrade&quot;: True, &quot;safebrowsing.enabled&quot;: True }) driver = webdriver.Chrome(options=chrome_options)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For people stumbling upon this issue AGAIN with Chrome 124 With 122, I used chromeOptions.addArguments(&quot;--disable-features=InsecureDownloadWarnings&quot;); and this did suffice. Adding the other argument mentioned in the answer helped me to fix the problem occurring once again in Chrome 124: chromeOptions.addArguments(&quot;--unsafely-treat-insecure-origin-as-secure=http://my-download-origin:1234&quot;); Hope it helps! Kudos to the original answer!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver",
        "selenium-chromedriver",
        "ui-automation"
      ],
      "question_score": 10,
      "answer_score": 21,
      "created": "2024-02-25T21:22:00",
      "question_id": 78057740,
      "answer_id": 78065424
    }
  },
  {
    "question": "Matplotlib - Seaborn-whitegrid is not a valid package style",
    "expected_answer": "I solved the problem using one of the available styles. You can show available styles using: plt.style.available In my case I can use &quot;seaborn-v0_8-whitegrid&quot;: plt.style.use(&quot;seaborn-v0_8-whitegrid&quot;)",
    "context_chunks": [
      {
        "text": "I am using ADTK for anomaly detection and matplotlib for the visualization, but I am getting errors trying to run my program. This is my code and I run using python3 import pandas as pd import matplotlib.pyplot as plt import matplotlib.dates as mdates from adtk.data import validate_series from adtk.visualization import plot from adtk.detector import PersistAD data = pd.read_csv('SQL_bytes_sent_ferdig.csv', index_col=&quot;time&quot;, parse_dates=True) # Convert the index to a DatetimeIndex explicitly if it's not already data.index = pd.to_datetime(data.index, utc=True) # Add 1 hour to the index data.index = data.index + pd.Timedelta(hours=1) data_valid = validate_series(data) persist_ad = PersistAD(c=3.0, side='positive') anomalies = persist_ad.fit_detect(data_valid) plot(data_valid, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_markersize=3, anomaly_color='red', anomaly_tag=&quot;marker&quot;); Running this gives the following error: /home/ubuntu/ad/sql_ad.py:1: DeprecationWarning: Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0), (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries) but was not found to be installed on your system. If this would cause problems for you, please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466 import pandas as pd /home/ubuntu/.local/lib/python3.10/site-packages/adtk/detector/_detector_1d.py:270: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first. predicted[s.isna()] = np.nan /home/ubuntu/.local/lib/python3.10/site-packages/adtk/detector/_detector_1d.py:141: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first. predicted[s.isna()] = np.nan /home/ubuntu/.local/lib/python3.10/site-packages/adtk/aggregator/_aggregator.py:211: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first. predicted[predicted &amp; lists.isna().any(axis=1)] = float(&quot;nan&quot;) Traceback (most recent call last): File &quot;/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/style/core.py&quot;, line 137, in use style = _rc_params_in_file(style) File &quot;/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/__init__.py&quot;, line 866, in _rc_params_in_file with _open_file_or_url(fname) as fd: File &quot;/usr/lib/python3.10/contextlib.py&quot;, line 135, in __enter__ return next(self.gen) File &quot;/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/__init__.py&quot;, line 843, in _open_file_or_url with open(fname, encoding='utf-8') as f: FileNotFoundError: [Errno 2] No such file or directory: 'seaborn-whitegrid' The above exception was the direct cause of the following exception: Traceback (most recent call last): File &quot;/home/ubuntu/ad/sql_ad.py&quot;, line 22, in &lt;module&gt; plot(data_valid, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_markersize=3, anomaly_color='red', anomaly_tag=&quot;marker&quot;); File &quot;/home/ubuntu/.local/lib/python3.10/site-packages/adtk/visualization/_visualization.py&quot;, line 201, in plot plt.style.use(&quot;seaborn-whitegrid&quot;) File &quot;/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/style/core.py&quot;, line 139, in use raise OSError( OSError: 'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`) I have looked into other posts with similar problems and tried to specify which style to use in my code, where I have the following styles: import matplotlib.pyplot as plt # plt.style.use(['science', 'notebook']) print(plt.style.available) Gives: ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10'] But none of them changes the error. Running it in Jupytor Notebook it works and I dont even have to specify which plot to use. I tried installing seaborn using pip but still nothing works.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I solved the problem using one of the available styles. You can show available styles using: plt.style.available In my case I can use &quot;seaborn-v0_8-whitegrid&quot;: plt.style.use(&quot;seaborn-v0_8-whitegrid&quot;)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I would recommend attempting to uninstall and then reinstall Matplotlib and Seaborn. This often resolves many issues.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "matplotlib"
      ],
      "question_score": 10,
      "answer_score": 13,
      "created": "2024-02-19T10:13:23",
      "question_id": 78019854,
      "answer_id": 78339240
    }
  },
  {
    "question": "How to validate access token from AzureAD in python?",
    "expected_answer": "Microsoft does not have a Python library to validate access tokens. Nevertheless, I found this official sample. You can check the requires_auth() function, which is used to validate the access token.",
    "context_chunks": [
      {
        "text": "What is the recommended way to validate the access token in backend? Any library that handles it? Another team has implemented the frontend they send the access token in the Bearer attributed in the header. I found https://github.com/odwyersoftware/azure-ad-verify-token but it has only 17 Stars. I thought microsoft should have support for it in MSAL (https://github.com/AzureAD/microsoft-authentication-library-for-python) but seems not. Any suggestions on how to implement it in a secure way? Or any good libs that handles the validation. I have tried write the code my self but I get problems but worried its not secured and the code got messy. Also tried above lib but should like to have some more popular so its not a security risk.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Microsoft does not have a Python library to validate access tokens. Nevertheless, I found this official sample. You can check the requires_auth() function, which is used to validate the access token.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "We can further simplify the @justin-tanner's code by using PyJWKClient: import jwt from jwt import PyJWKClient def token_is_valid(tenant_id, audience, token): jwks_url = f&quot;https://login.microsoftonline.com/{tenant_id}/discovery/v2.0/keys&quot; issuer_url = f&quot;https://login.microsoftonline.com/{tenant_id}/v2.0&quot; jwks_client = PyJWKClient( jwks_url, ) signing_key = jwks_client.get_signing_key_from_jwt(token) return jwt.decode( token, signing_key.key, verify=True, algorithms=[&quot;RS256&quot;], audience=audience, issuer=issuer_url, )",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "jwt",
        "single-sign-on",
        "fastapi",
        "azure-ad-msal"
      ],
      "question_score": 10,
      "answer_score": 9,
      "created": "2023-08-11T19:23:54",
      "question_id": 76886257,
      "answer_id": 76969732
    }
  },
  {
    "question": "langchain_community &amp; langchain packages giving error: Missing 1 required keyword-only argument: &#39;recursive_guard&#39;",
    "expected_answer": "I am having the same issue. The stack is different, but the error comes from the same line pydantic\\v1\\typing.py&quot;, line 66 This is referring to the python typing module (v3.12.4) that has an additional mandatory parameter 'recursive_guard'. There are other areas of the code in pydantic where this has been fixed (recursive_gurard=set()). Check this out --&gt; https://github.com/pydantic/pydantic-core/issues/1292 Within this thread, they mention that using python v3.12.3 could temporarily solve the issue in 1292, probably because this additional attribute in v3.12.4 (I am guessing here). This is not an option for me as my google alpha functions local deploy is not recognizing the --runtime=python311 and always take the latest runtime (v3.12.4). I hope that they fix this too",
    "context_chunks": [
      {
        "text": "All of sudden langchain_community &amp; langchain packages started throwing error: TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard' The error getting generated somewhere in pydantic I strongly suspect it is version mismatch. So I tried upgrading packages langchain, langchain_community, pydantic, langsmith etc. But no luck. My current installed versions shows as under: Python 3.12.4 langchain: 0.2.3 langchain_community: 0.2.4 langsmith: 0.1.75 pydantic: 2.7.3 typing_extensions: 4.11.0 Pip check also not showing any conflict. Here is complete trace of error. Any help would be really appreciated. TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard' File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\streamlit\\runtime\\scriptrunner\\script_runner.py&quot;, line 600, in _run_script exec(code, module.__dict__) File &quot;C:\\MyProject\\MyScript.py&quot;, line 20, in &lt;module&gt; from langchain_community.vectorstores import Chroma File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1412, in _handle_fromlist File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_community\\vectorstores\\__init__.py&quot;, line 509, in __getattr__ module = importlib.import_module(_module_lookup[name]) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py&quot;, line 90, in import_module return _bootstrap._gcd_import(name[level:], package, level) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_community\\vectorstores\\chroma.py&quot;, line 20, in &lt;module&gt; from langchain_core.documents import Document File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\documents\\__init__.py&quot;, line 6, in &lt;module&gt; from langchain_core.documents.compressor import BaseDocumentCompressor File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\documents\\compressor.py&quot;, line 6, in &lt;module&gt; from langchain_core.callbacks import Callbacks File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\callbacks\\__init__.py&quot;, line 22, in &lt;module&gt; from langchain_core.callbacks.manager import ( File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\callbacks\\manager.py&quot;, line 29, in &lt;module&gt; from langsmith.run_helpers import get_run_tree_context File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\run_helpers.py&quot;, line 40, in &lt;module&gt; from langsmith import client as ls_client File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\client.py&quot;, line 52, in &lt;module&gt; from langsmith import env as ls_env File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\env\\__init__.py&quot;, line 3, in &lt;module&gt; from langsmith.env._runtime_env import ( File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\env\\_runtime_env.py&quot;, line 10, in &lt;module&gt; from langsmith.utils import get_docker_compose_command File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\utils.py&quot;, line 31, in &lt;module&gt; from langsmith import schemas as ls_schemas File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\schemas.py&quot;, line 69, in &lt;module&gt; class Example(ExampleBase): File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pydantic\\v1\\main.py&quot;, line 286, in __new__ cls.__try_update_forward_refs__() File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pydantic\\v1\\main.py&quot;, line 807, in __try_update_forward_refs__ update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,)) File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pydantic\\v1\\typing.py&quot;, line 554, in update_model_forward_refs update_field_forward_refs(f, globalns=globalns, localns=localns) File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pydantic\\v1\\typing.py&quot;, line 520, in update_field_forward_refs field.type_ = evaluate_forwardref(field.type_, globalns, localns or None) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pydantic\\v1\\typing.py&quot;, line 66, in evaluate_forwardref return cast(Any, type_)._evaluate(globalns, localns, set()) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I am having the same issue. The stack is different, but the error comes from the same line pydantic\\v1\\typing.py&quot;, line 66 This is referring to the python typing module (v3.12.4) that has an additional mandatory parameter 'recursive_guard'. There are other areas of the code in pydantic where this has been fixed (recursive_gurard=set()). Check this out --&gt; https://github.com/pydantic/pydantic-core/issues/1292 Within this thread, they mention that using python v3.12.3 could temporarily solve the issue in 1292, probably because this additional attribute in v3.12.4 (I am guessing here). This is not an option for me as my google alpha functions local deploy is not recognizing the --runtime=python311 and always take the latest runtime (v3.12.4). I hope that they fix this too",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I fixed this issue by reinstalling these: pip install -U pydantic pydantic_core",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pydantic",
        "langchain",
        "langsmith"
      ],
      "question_score": 10,
      "answer_score": 11,
      "created": "2024-06-07T19:21:12",
      "question_id": 78593700,
      "answer_id": 78594372
    }
  },
  {
    "question": "Hydra: How to express None in config files?",
    "expected_answer": "Try null: benchmarking: seed_number: null",
    "context_chunks": [
      {
        "text": "I have a very simple Python script: import hydra from omegaconf import DictConfig, OmegaConf @hydra.main(version_base=&quot;1.3&quot;, config_path=&quot;.&quot;, config_name=&quot;config&quot;) def main(cfg: DictConfig) -&gt; None: if cfg.benchmarking.seed_number is None: raise ValueError() if __name__ == &quot;__main__&quot;: main() And here the config file: benchmarking: seed_number: None Unfortunately, the Python script does not raise an error. Instead, when I print the type of cfg.benchmarking.seed_number, it is str. How can I pass None instead?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Try null: benchmarking: seed_number: null",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In YAML format, it represents None as a string (&quot;None&quot;). To pass None as the value instead, you can replace it with !!null benchmarking: seed_number: !!null Using !!null in the YAML configuration, it explicitly represents a null value. But as this configuration is loaded into the Python, cfg.benchmarking.seed_number will be None instead of a string.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "fb-hydra"
      ],
      "question_score": 10,
      "answer_score": 17,
      "created": "2023-06-27T18:24:12",
      "question_id": 76567692,
      "answer_id": 76568404
    }
  },
  {
    "question": "Is there a way to interpolate variables into a python string WITHOUT using the print function?",
    "expected_answer": "Ok...so, it's kind of easy. The old method: num = 6 mystr = 'number is %s' % num print(mystr) # number is 6 The newer .format method: num = 6 mystr = &quot;number is {}&quot;.format(num) print(mystr) # number is 6 The .format method using named variable (useful for when sequence can't be depended upon): num = 6 mystr = &quot;number is {num}&quot;.format(num=num) print(mystr) # number is 6 The shorter f-string method: (thank you @mayur) num = 6 mystr = f&quot;number is {num}&quot; print(mystr) # number is 6",
    "context_chunks": [
      {
        "text": "Every example I have seen of Python string variable interpolation uses the print function. For example: num = 6 # str.format method print(&quot;number is {}&quot;.format(num)) # % placeholders print(&quot;number is %s&quot;%(num)) # named .format method print(&quot;number is {num}&quot;.format(num=num)) Can you interpolate variables into strings without using print?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Ok...so, it's kind of easy. The old method: num = 6 mystr = 'number is %s' % num print(mystr) # number is 6 The newer .format method: num = 6 mystr = &quot;number is {}&quot;.format(num) print(mystr) # number is 6 The .format method using named variable (useful for when sequence can't be depended upon): num = 6 mystr = &quot;number is {num}&quot;.format(num=num) print(mystr) # number is 6 The shorter f-string method: (thank you @mayur) num = 6 mystr = f&quot;number is {num}&quot; print(mystr) # number is 6",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In each of your cases you have a str object. Its .format method returns the formatted string as a new object. Its __mod__ method, which python calls when it sees the % operator, also returns a formatted string. Functions and methods return anonymous objects. The context where they are called decide what happens next. &quot;number is {num}&quot;.format(num=num) throws the result away. some_variable = &quot;number is {num}&quot;.format(num=num) assigns the result. some_function(&quot;number is {num}&quot;.format(num=num)) calls the function with the result as a parameter. print is not a special case - python doesn't do anything special with print. Interestingly, f-strings like f&quot;number is {num}&quot; is compiled into a series of instructions that build the string dynamically.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "string",
        "variables",
        "format",
        "string-interpolation"
      ],
      "question_score": 10,
      "answer_score": 14,
      "created": "2023-10-31T02:11:28",
      "question_id": 77392792,
      "answer_id": 77392847
    }
  },
  {
    "question": "What datatype is considered &#39;list-like&#39; in Python?",
    "expected_answer": "&quot;List-like&quot; isn't a standard Python term. Googling pandas list-like turns up pandas.api.types.is_list_like, but the documentation for that just says Check if the object is list-like. Objects that are considered list-like are for example Python lists, tuples, sets, NumPy arrays, and Pandas Series. Strings and datetime objects, however, are not considered list-like. which isn't really much of a spec. So, as a last resort, we turn to the source code, and after following a lot of imports and aliasing, we eventually find this function: cdef bint c_is_list_like(object obj, bint allow_sets) except -1: # first, performance short-cuts for the most common cases if util.is_array(obj): # exclude zero-dimensional numpy arrays, effectively scalars return not cnp.PyArray_IsZeroDim(obj) elif isinstance(obj, list): return True # then the generic implementation return ( # equiv: `isinstance(obj, abc.Iterable)` getattr(obj, &quot;__iter__&quot;, None) is not None and not isinstance(obj, type) # we do not count strings/unicode/bytes as list-like # exclude Generic types that have __iter__ and not isinstance(obj, (str, bytes, _GenericAlias)) # exclude zero-dimensional duck-arrays, effectively scalars and not (hasattr(obj, &quot;ndim&quot;) and obj.ndim == 0) # exclude sets if allow_sets is False and not (allow_sets is False and isinstance(obj, abc.Set)) ) So Pandas considers an object list-like if it passes this complicated series of checks. If an object is a 0-dimensional NumPy array, it's not list-like. Otherwise, if it's a list, it's list-like. Otherwise, it needs to pass all the following checks to be list-like: It needs to have an __iter__ attribute that's not None. It needs to not be a type. It needs to not be a string, a bytestring, or a &quot;generic alias&quot; (a type used for some typing module things). It needs to not have an ndim attribute equal to 0. In some cases, Pandas will disallow instances of collections.abc.Set, which are sets, frozensets, and certain other set-like objects. (abc is collections.abc here.) That means Pandas considers most iterable objects to be list-like. Strings, bytestrings, generic aliases, and iterable type objects (like Enum classes) are excluded, with that part about excluding iterable type objects probably being a bug - the code is trying to exclude non-iterable type objects whose instances are iterable. The 0-dimensional array and ndim==0 checks attempt to exclude objects for which positive-dimensional instances of their type would be iterable, but 0-dimensional instances aren't. Sets and other collections.abc.Set subclasses are sometimes excluded, but Series.isin doesn't pass the flag to exclude them.",
    "context_chunks": [
      {
        "text": "In the Pandas documentation here for Series.isin(values), they state: values : set or list-like What is considered list-like? For a Python dictionary temp_dict, would temp_dict.keys() and temp_dict.values() be considered list-like?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "&quot;List-like&quot; isn't a standard Python term. Googling pandas list-like turns up pandas.api.types.is_list_like, but the documentation for that just says Check if the object is list-like. Objects that are considered list-like are for example Python lists, tuples, sets, NumPy arrays, and Pandas Series. Strings and datetime objects, however, are not considered list-like. which isn't really much of a spec. So, as a last resort, we turn to the source code, and after following a lot of imports and aliasing, we eventually find this function: cdef bint c_is_list_like(object obj, bint allow_sets) except -1: # first, performance short-cuts for the most common cases if util.is_array(obj): # exclude zero-dimensional numpy arrays, effectively scalars return not cnp.PyArray_IsZeroDim(obj) elif isinstance(obj, list): return True # then the generic implementation return ( # equiv: `isinstance(obj, abc.Iterable)` getattr(obj, &quot;__iter__&quot;, None) is not None and not isinstance(obj, type) # we do not count strings/unicode/bytes as list-like # exclude Generic types that have __iter__ and not isinstance(obj, (str, bytes, _GenericAlias)) # exclude zero-dimensional duck-arrays, effectively scalars and not (hasattr(obj, &quot;ndim&quot;) and obj.ndim == 0) # exclude sets if allow_sets is False and not (allow_sets is False and isinstance(obj, abc.Set)) ) So Pandas considers an object list-like if it passes this complicated series of checks. If an object is a 0-dimensional NumPy array, it's not list-like. Otherwise, if it's a list, it's list-like. Otherwise, it needs to pass all the following checks to be list-like: It needs to have an __iter__ attribute that's not None. It needs to not be a type. It needs to not be a string, a bytestring, or a &quot;generic alias&quot; (a type used for some typing module things). It needs to not have an ndim attribute equal to 0. In some cases, Pandas will disallow instances of collections.abc.Set, which are sets, frozensets, and certain other set-like objects. (abc is collections.abc here.) That means Pandas considers most iterable objects to be list-like. Strings, bytestrings, generic aliases, and iterable type objects (like Enum classes) are excluded, with that part about excluding iterable type objects probably being a bug - the code is trying to exclude non-iterable type objects whose instances are iterable. The 0-dimensional array and ndim==0 checks attempt to exclude objects for which positive-dimensional instances of their type would be iterable, but 0-dimensional instances aren't. Sets and other collections.abc.Set subclasses are sometimes excluded, but Series.isin doesn't pass the flag to exclude them.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "If you check the Pandas documentation, you find a function that determines whether something is list like. If you do a bunch of searching and searching you eventually end up at a pyx file that defines a C-ish version of the function: cdef bint c_is_list_like(object obj, bint allow_sets) except -1: # first, performance short-cuts for the most common cases if util.is_array(obj): # exclude zero-dimensional numpy arrays, effectively scalars return not cnp.PyArray_IsZeroDim(obj) elif isinstance(obj, list): return True # then the generic implementation return ( # equiv: `isinstance(obj, abc.Iterable)` getattr(obj, &quot;__iter__&quot;, None) is not None and not isinstance(obj, type) # we do not count strings/unicode/bytes as list-like # exclude Generic types that have __iter__ and not isinstance(obj, (str, bytes, _GenericAlias)) # exclude zero-dimensional duck-arrays, effectively scalars and not (hasattr(obj, &quot;ndim&quot;) and obj.ndim == 0) # exclude sets if allow_sets is False and not (allow_sets is False and isinstance(obj, abc.Set)) ) It's a number of conditions. It excluded zero dimensional numpy arrays before allowing all lists. What remains must be iterable but not a string, bytes, or generic, not ndim == 0, and not a set if that flag is set.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pandas",
        "dictionary"
      ],
      "question_score": 10,
      "answer_score": 15,
      "created": "2023-08-30T04:17:01",
      "question_id": 77004957,
      "answer_id": 77004991
    }
  },
  {
    "question": "ERROR: Failed to build installable wheels for some pyproject.toml based projects (pycryptodome)",
    "expected_answer": "I think you should install Pyrebase4 pip install Pyrebase4 or pip3 install Pyrebase4 https://pypi.org/project/Pyrebase4/ A simple python wrapper for the Firebase API with current deps This is a more recent one with last released on Apr 30, 2024 The old one was : pip install Pyrebase it was last released on Jan 7, 2017 so it will not be supported by new python versions. https://pypi.org/project/Pyrebase/",
    "context_chunks": [
      {
        "text": "I am trying to install Pyrebase to my NewLoginApp Project using PyCharm IDE and Python. I checked and upgraded the version of the software and I selected the project as my interpreter, but I still get this error: ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pycryptodome) Below is the screenshot of the error that I am getting: Below is the whole code I wrote to the terminal to fix the problem:",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I think you should install Pyrebase4 pip install Pyrebase4 or pip3 install Pyrebase4 https://pypi.org/project/Pyrebase4/ A simple python wrapper for the Firebase API with current deps This is a more recent one with last released on Apr 30, 2024 The old one was : pip install Pyrebase it was last released on Jan 7, 2017 so it will not be supported by new python versions. https://pypi.org/project/Pyrebase/",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Usually this kind of errors are related to the use of an obsolete &quot;setuptools&quot; python dependency. Try upgrade the &quot;setuptools&quot; dependency in your virtual environment pip install setuptools --upgrade",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pip",
        "pyrebase"
      ],
      "question_score": 10,
      "answer_score": 6,
      "created": "2024-07-02T10:51:37",
      "question_id": 78696575,
      "answer_id": 78697188
    }
  },
  {
    "question": "How to get maps in GeoPandas after datasets are removed?",
    "expected_answer": "You can read it from Nacis : import geopandas as gpd url = &quot;https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip&quot; gdf = gpd.read_file(url) old answer: The simplest solution would be to download/store the shapefile somewhere. That being said, if (for some reason), you need to read it from the source, you can do it this way : import fsspec url = &quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/&quot; \\ &quot;download/110m/cultural/ne_110m_admin_0_countries.zip&quot; with fsspec.open(f&quot;simplecache::{url}&quot;) as file: gdf = gpd.read_file(file) Output : featurecla scalerank ... FCLASS_UA geometry 0 Admin-0 country 1 ... None MULTIPOLYGON (((180.00000 -16.0... 1 Admin-0 country 1 ... None POLYGON ((33.90371 -0.95000, 34... 2 Admin-0 country 1 ... None POLYGON ((-8.66559 27.65643, -8... .. ... ... ... ... ... 174 Admin-0 country 1 ... Unrecognized POLYGON ((20.59025 41.85541, 20... 175 Admin-0 country 1 ... None POLYGON ((-61.68000 10.76000, -... 176 Admin-0 country 1 ... None POLYGON ((30.83385 3.50917, 29.... [177 rows x 169 columns]",
    "context_chunks": [
      {
        "text": "I have found very easy and useful to load world map from geopandas datasets, as probably many others, for example: import geopandas as gpd world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) However, this gives a FutureWarning that the dataset module is deprecated and will be removed in the future. There are maps available for download, for example from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/ but the files are zipped and it does not seem like a convenient workflow to either get and process files from there or neither include processed files with the source. Is there an alternative? What is the best way to do this, especially if I want my code to work with future versions of GeoPandas?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can read it from Nacis : import geopandas as gpd url = &quot;https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip&quot; gdf = gpd.read_file(url) old answer: The simplest solution would be to download/store the shapefile somewhere. That being said, if (for some reason), you need to read it from the source, you can do it this way : import fsspec url = &quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/&quot; \\ &quot;download/110m/cultural/ne_110m_admin_0_countries.zip&quot; with fsspec.open(f&quot;simplecache::{url}&quot;) as file: gdf = gpd.read_file(file) Output : featurecla scalerank ... FCLASS_UA geometry 0 Admin-0 country 1 ... None MULTIPOLYGON (((180.00000 -16.0... 1 Admin-0 country 1 ... None POLYGON ((33.90371 -0.95000, 34... 2 Admin-0 country 1 ... None POLYGON ((-8.66559 27.65643, -8... .. ... ... ... ... ... 174 Admin-0 country 1 ... Unrecognized POLYGON ((20.59025 41.85541, 20... 175 Admin-0 country 1 ... None POLYGON ((-61.68000 10.76000, -... 176 Admin-0 country 1 ... None POLYGON ((30.83385 3.50917, 29.... [177 rows x 169 columns]",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For my Jupyter notebooks, I have a tools.py where I store repeatedly used methods. One of them is get_shapes(), for getting frequently needed base-shapes (world-countries, us-states, de-states). Working folder relative paths: tools/tools.py tools/__init__.py (create an empty file) an absolute path would work, too - just replace str(Path.cwd() / &quot;tools&quot;) below Use as: from pathlib import Path module_path = str(Path.cwd() / &quot;tools&quot;) if module_path not in sys.path: sys.path.append(module_path) from tools import tools CRS_PROJ = &quot;esri:54009&quot; # Mollweide (e.g.) world = tools.get_shapes( &quot;world&quot;, shape_dir=Path.cwd() / &quot;shapes&quot;) world.to_crs(CRS_PROJ, inplace=True) world.plot() Can be (definitely) improved, but it is a start. Extend, if needed: import io import csv import requests import geopandas as gp import zipfile from pathlib import Path from IPython.display import clear_output from typing import List, Optional, Dict, Tuple from IPython.core.display import display def return_total(headers: Dict[str, str]): &quot;&quot;&quot;Return total length from requests header&quot;&quot;&quot; if not headers: return total_length = headers.get('content-length') if not total_length: return try: total_length = int(total_length) except: total_length = None return total_length def stream_progress(total_length: int, loaded: int): &quot;&quot;&quot;Stream progress report&quot;&quot;&quot; clear_output(wait=True) perc_str = &quot;&quot; if total_length: total = total_length/1000000 perc = loaded/(total/100) perc_str = f&quot;of {total:.2f} ({perc:.0f}%)&quot; print( f&quot;Loaded {loaded:.2f} MB &quot; f&quot;{perc_str}..&quot;) def stream_progress_basic(total: int, loaded: int): &quot;&quot;&quot;Stream progress report&quot;&quot;&quot; clear_output(wait=True) perc_str = &quot;&quot; if total: perc = loaded/(total/100) perc_str = f&quot;of {total:.0f} ({perc:.0f}%)&quot; print( f&quot;Processed {loaded:.0f} &quot; f&quot;{perc_str}..&quot;) def get_stream_file(url: str, path: Path): &quot;&quot;&quot;Download file from url and save to path&quot;&quot;&quot; chunk_size = 8192 with requests.get(url, stream=True) as r: r.raise_for_status() total_length = return_total(r.headers) with open(path, 'wb') as f: for ix, chunk in enumerate(r.iter_content(chunk_size=chunk_size)): f.write(chunk) loaded = (ix*chunk_size)/1000000 if (ix % 100 == 0): stream_progress( total_length, loaded) stream_progress( total_length, loaded) def get_stream_bytes(url: str): &quot;&quot;&quot;Stream file from url to bytes object (in-memory)&quot;&quot;&quot; chunk_size = 8192 content = bytes() with requests.get(url, stream=True) as r: r.raise_for_status() total_length = return_total(r.headers) for ix, chunk in enumerate(r.iter_content( chunk_size=chunk_size)): content += bytes(chunk) loaded = (ix*chunk_size)/1000000 if (ix % 100 == 0): stream_progress( total_length, loaded) stream_progress( total_length, loaded) return content def get_folder_size(folder: Path): &quot;&quot;&quot;Return size of all files in folder in MegaBytes&quot;&quot;&quot; if not folder.exists(): raise Warning( f&quot;Folder {folder} does not exist&quot;) return size_mb = 0 for file in folder.glob('*'): size_mb += file.stat().st_size / (1024*1024) return size_mb def get_zip_extract( uri: str, filename: str, output_path: Path, create_path: bool = True, skip_exists: bool = True, report: bool = True, filter_files: List[str] = None, write_intermediate: bool = None): &quot;&quot;&quot;Get Zip file and extract to output_path. Create Path if not exists.&quot;&quot;&quot; if write_intermediate is None: write_intermediate = False if create_path: output_path.mkdir( exist_ok=True) if skip_exists and Path( output_path / filename.replace(&quot;.zip&quot;, &quot;.csv&quot;)).exists(): if report: print(&quot;File already exists.. skipping download..&quot;) return if write_intermediate: out_file = output_path / filename get_stream_file(f'{uri}{filename}', out_file) z = zipfile.ZipFile(out_file) else: content = get_stream_bytes( f'{uri}{filename}') z = zipfile.ZipFile(io.BytesIO(content)) print(&quot;Extracting zip..&quot;) if filter_files: file_names = z.namelist() for filename in file_names: if filename in filter_files: z.extract(filename, output_path) else: z.extractall(output_path) if write_intermediate: if out_file.is_file(): out_file.unlink() if report: raw_size_mb = get_folder_size(output_path) print( f&quot;Retrieved {filename}, &quot; f&quot;extracted size: {raw_size_mb:.2f} MB&quot;) def drop_cols_except(df: pd.DataFrame, columns_keep: List[str]): &quot;&quot;&quot;Drop all columns from DataFrame except those specified in cols_except&quot;&quot;&quot; df.drop( df.columns.difference(columns_keep), axis=1, inplace=True) def get_shapes( reference: str, shape_dir: Path, clean_cols: Optional[bool] = None, normalize_cols: Optional[bool] = None, set_index: Optional[bool] = None) -&gt; gp.GeoDataFrame: &quot;&quot;&quot;Custom method to get frequently used shapes (DE Bundesländer, US States) and return a geopandas.GeoDataFrame (WGS1984). Will only download shapes if not already downloaded. reference: str - , &quot;world&quot;, &quot;us&quot; and &quot;de&quot; are currently supported clean_cols: will remove all columns except geometry and state-reference. Defaults to True. normalize_cols: will rename columns to sane defaults. Defaults to True. set_index: will set state-reference as index column. Defaults to True. &quot;&quot;&quot; if clean_cols is None: clean_cols = True if normalize_cols is None: normalize_cols = True if set_index is None: set_index = True target_name = &quot;state&quot; if reference == &quot;us&quot;: source_zip = &quot;https://www2.census.gov/geo/tiger/GENZ2018/shp/&quot; filename = &quot;cb_2018_us_state_5m.zip&quot; shapes_name = &quot;cb_2018_us_state_5m.shp&quot; col_name = &quot;NAME&quot; elif reference == &quot;de&quot;: source_zip = &quot;https://daten.gdz.bkg.bund.de/produkte/vg/vg2500/aktuell/&quot; filename = &quot;vg2500_12-31.utm32s.shape.zip&quot; shapes_name = &quot;vg2500_12-31.utm32s.shape/vg2500/VG2500_LAN.shp&quot; col_name = &quot;GEN&quot; elif reference == &quot;world&quot;: source_zip = &quot;https://naciscdn.org/naturalearth/110m/cultural/&quot; filename = &quot;ne_110m_admin_0_countries.zip&quot; shapes_name = &quot;ne_110m_admin_0_countries.shp&quot; col_name = &quot;SOVEREIGNT&quot; target_name = &quot;country&quot; # create temporary storage folder, if not exists already shape_dir.mkdir(exist_ok=True) # test if file already downloaded if not (shape_dir / shapes_name).exists(): get_zip_extract( uri=source_zip, filename=filename, output_path=shape_dir) else: print(&quot;Already exists&quot;) shapes = gp.read_file(shape_dir / shapes_name) if clean_cols: drop_cols_except(df=shapes, columns_keep=[&quot;geometry&quot;, col_name]) if normalize_cols: shapes.rename(columns={col_name: target_name}, inplace=True) col_name = target_name if set_index: shapes.set_index(col_name, inplace=True) return shapes.to_crs(&quot;EPSG:4326&quot;)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visualization",
        "geospatial",
        "geopandas"
      ],
      "question_score": 10,
      "answer_score": 9,
      "created": "2023-06-24T22:36:45",
      "question_id": 76548222,
      "answer_id": 76549743
    }
  },
  {
    "question": "PyTorch UserWarning: Failed to initialize NumPy: _ARRAY_API not found and BERTModel weight initialization issue",
    "expected_answer": "pip install --force-reinstall -v &quot;numpy==1.25.2&quot; Fixed the issue for me. This was following this github discussion from : https://github.com/stitionai/devika/issues/606 All thanks to @HOBE for the comment above",
    "context_chunks": [
      {
        "text": "I am working with PyTorch and the Hugging Face Transformers library to fine-tune a BERT model (UFNLP/gatortron-base) for a downstream task. I received a warning related to NumPy initialization: C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\storage.py:321: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.) My code: type himport torch from transformers import BertTokenizer, BertModel tokenizer = BertTokenizer.from_pretrained('UFNLP/gatortron-base') model = BertModel.from_pretrained('UFNLP/gatortron-base') model.eval() def prepare_input(text): tokens = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True, max_length=512, truncation=True) return tokens['input_ids'], tokens['attention_mask'] def get_response(input_ids, attention_mask): with torch.no_grad(): outputs = model(input_ids=input_ids, attention_mask=attention_mask) if 'logits' in outputs: predictions = torch.argmax(outputs['logits'], dim=-1) else: # Adjust this based on the actual structure of `outputs` predictions = torch.argmax(outputs[0], dim=-1) # predictions = torch.argmax(outputs.logits, dim=-1) return tokenizer.decode(predictions[0], skip_special_tokens=True) input_text = &quot;Hello, how are you?&quot; input_ids, attention_mask = prepare_input(input_text) response = get_response(input_ids, attention_mask) print(&quot;Response from the model:&quot;, response)ere Python: 3.12 NumPy: 1.19.5",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "pip install --force-reinstall -v &quot;numpy==1.25.2&quot; Fixed the issue for me. This was following this github discussion from : https://github.com/stitionai/devika/issues/606 All thanks to @HOBE for the comment above",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I had this same problem, and I sorted it out by upgrading my torch version from 1.x.x to 2.x.x",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "pytorch"
      ],
      "question_score": 10,
      "answer_score": 13,
      "created": "2024-06-28T07:25:01",
      "question_id": 78681145,
      "answer_id": 78730054
    }
  },
  {
    "question": "ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based",
    "expected_answer": "Either use python 3.11 or pip install aiohttp==3.9.0b0 installs their current beta release that supports python 3.12.x then try openai installation Link to git :https://github.com/KillianLucas/open-interpreter/issues/581",
    "context_chunks": [
      {
        "text": "Newbie here. I have been trying to installen the openai library into python, but I keep running into problems. I have already installed C++ libraries. It seems to have problems specific with aio http, and I get the error below. I a running a Windows 11 laptop without admin restrictions. Error &quot;C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.37.32822\\bin\\HostX86\\x64\\cl.exe&quot; /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\sande\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\sande\\AppData\\Local\\Programs\\Python\\Python312\\Include &quot;-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.37.32822\\include&quot; &quot;-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include&quot; &quot;-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt&quot; &quot;-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um&quot; &quot;-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared&quot; &quot;-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt&quot; &quot;-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt&quot; /Tcaiohttp/_websocket.c /Fobuild\\temp.win-amd64-cpython-312\\Release\\aiohttp/_websocket.obj _websocket.c aiohttp/_websocket.c(1475): warning C4996: 'Py_OptimizeFlag': deprecated in 3.12 aiohttp/_websocket.c(3042): error C2039: 'ob_digit': is not a member of '_longobject' [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for aiohttp Failed to build aiohttp ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Either use python 3.11 or pip install aiohttp==3.9.0b0 installs their current beta release that supports python 3.12.x then try openai installation Link to git :https://github.com/KillianLucas/open-interpreter/issues/581",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As of February the 1st 2024, multidict released a wheel for Python 3.12. You can download it here, but it will also be updated over the coming days in the affected libraries. aiohttp as an example updated theirs today (02-02-2024). Official Github link",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pip",
        "openai-api",
        "aiohttp"
      ],
      "question_score": 10,
      "answer_score": 11,
      "created": "2023-10-30T13:08:50",
      "question_id": 77388920,
      "answer_id": 77388956
    }
  },
  {
    "question": "Split on regex (more than a character, maybe variable width) and keep the separator like GNU awk",
    "expected_answer": "With splitting you always have one more field than the delimiters, which is why you have to fill in an empty string as the delimiter for the last field. A simpler way to achieve the filling would be to always append an empty string to the list returned by the split so that you can use the itertools.batched function (available since Python 3.12, or as a recipe beforehand) to produce easy pairings: import re from io import StringIO from itertools import batched file = StringIO('''a\\t b c\\td _ e a b c\\td _ e a b c\\td _ e a b_c\\td _ e\\t abcd''') for line in file: print(list(batched(re.split(r&quot;([\\s_]+)&quot;, line.rstrip('\\r\\n')) + [''], 2))) This outputs: [('a', '\\t '), ('b', ' '), ('c', '\\t'), ('d', ' _ '), ('e', '')] [('a', ' '), ('b', ' '), ('c', '\\t'), ('d', ' _ '), ('e', '')] [('', ' '), ('a', ' '), ('b', ' '), ('c', '\\t'), ('d', ' _ '), ('e', '')] [('a', ' '), ('b', '_'), ('c', '\\t'), ('d', ' _ '), ('e', '\\t'), ('', '')] [('abcd', '')] Demo here",
    "context_chunks": [
      {
        "text": "In GNU awk, there is a four argument version of split that can optionally keep all the separators from the split in a second array. This is useful if you want to reconstruct a select subset of columns from a file where the delimiter may be more complicated than just a single character. Suppose I have the following file: # sed makes the invisibles visible... # ∙ is a space; \\t is a literal tab; $ is line end $ sed -E 's/\\t/\\\\t/g; s/ /∙/g; s/$/\\$/' f.txt a\\t∙∙b∙c\\td∙_∙e$ a∙∙∙b∙c\\td∙_∙e$ ∙∙∙a∙∙∙b∙c\\td∙_∙e$ a∙∙∙b_c\\td∙_∙e\\t$ abcd$ Here I have a field comprised of anything other than the delimiter character set, and a delimiter of one or more characters of the set [\\s_]. With gawk, you can do: gawk '{ printf &quot;[&quot; n=split($0, flds, /[[:space:]_]+/, seps) for(i=1; i&lt;=n; i++) printf &quot;[\\&quot;%s\\&quot;, \\&quot;%s\\&quot;]%s&quot;, flds[i], seps[i], i&lt;n ? &quot;, &quot; : &quot;]&quot; ORS } ' f.txt Prints (where the first element is the field, the second is the match to the delimiter regexp): [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot; &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot; &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;&quot;, &quot; &quot;], [&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot; &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot;_&quot;], [&quot;c&quot;, &quot; &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot; &quot;], [&quot;&quot;, &quot;&quot;]] [[&quot;abcd&quot;, &quot;&quot;]] Ruby's str.split, unfortunately, does not have the same functionality. (Neither does Python's or Perl's.) What you can do is capture the match string from the delimiter regexp: irb(main):053&gt; s=&quot;a b c d _ e&quot; =&gt; &quot;a b c d _ e&quot; irb(main):054&gt; s.split(/([\\s_]+)/) =&gt; [&quot;a&quot;, &quot; &quot;, &quot;b&quot;, &quot; &quot;, &quot;c&quot;, &quot; &quot;, &quot;d&quot;, &quot; _ &quot;, &quot;e&quot;] Then use that result with .each_slice(2) and replace the nil's with '': irb(main):055&gt; s.split(/([\\s_]+)/).each_slice(2).map{|a,b| [a,b]} =&gt; [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot; &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, nil]] irb(main):056&gt; s.split(/([\\s_]+)/).each_slice(2).map{|a,b| [a,b]}.map{|sa| sa.map{|e| e.nil? ? &quot;&quot; : e} } =&gt; [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot; &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] Which allows gawk's version of split to be replicated: ruby -ne 'p $_.gsub(/\\r?\\n$/,&quot;&quot;).split(/([\\s_]+)/).each_slice(2). map{|a,b| [a,b]}.map{|sa| sa.map{|e| e.nil? ? &quot;&quot; : e} }' f.txt Prints: [[&quot;a&quot;, &quot;\\t &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;&quot;, &quot; &quot;], [&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot;_&quot;], [&quot;c&quot;, &quot;\\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;\\t&quot;]] [[&quot;abcd&quot;, &quot;&quot;]] So the same output (other than the line with trailing \\t which gawk has as an empty field, delimiter combination.) In Python, roughly the same method also works: python3 -c ' import sys, re from itertools import zip_longest with open(sys.argv[1]) as f: for line in f: lp=re.split(r&quot;([\\s_]+)&quot;, line.rstrip(&quot;\\r\\n&quot;)) print(list(zip_longest(*[iter(lp)]*2, fillvalue=&quot;&quot;)) ) ' f.txt I am looking for a general algorithm to replicate the functionality of gawk's four argument split in Ruby/Python/Perl/etc. The Ruby and Python I have here works. Most of solutions (other than for gawk) to I want to split on this delimiter and keep the delimiter? involve a unique regex more complex than simply matching the delimiter. Most seem to be either scanning for a field, delimiter combination or use lookarounds. I am specifically trying to use a simple regexp that matches the delimiter only without lookarounds. With roughly the same regexp I would have used with GNU awk. So stated generally: Take a regexp matching the delimiter fields (without having to think much about the data fields) and put inside a capturing group; Take the resulting array of [field1, delimiter1, field2, delimiter2, ...] and create array of [[field1, delimiter1], [field2, delimiter2], ...] That method is easily used in Ruby (see above) and Python (see above) and Perl (I was too lazy to write that one...) Is this the best way to do this?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "With splitting you always have one more field than the delimiters, which is why you have to fill in an empty string as the delimiter for the last field. A simpler way to achieve the filling would be to always append an empty string to the list returned by the split so that you can use the itertools.batched function (available since Python 3.12, or as a recipe beforehand) to produce easy pairings: import re from io import StringIO from itertools import batched file = StringIO('''a\\t b c\\td _ e a b c\\td _ e a b c\\td _ e a b_c\\td _ e\\t abcd''') for line in file: print(list(batched(re.split(r&quot;([\\s_]+)&quot;, line.rstrip('\\r\\n')) + [''], 2))) This outputs: [('a', '\\t '), ('b', ' '), ('c', '\\t'), ('d', ' _ '), ('e', '')] [('a', ' '), ('b', ' '), ('c', '\\t'), ('d', ' _ '), ('e', '')] [('', ' '), ('a', ' '), ('b', ' '), ('c', '\\t'), ('d', ' _ '), ('e', '')] [('a', ' '), ('b', '_'), ('c', '\\t'), ('d', ' _ '), ('e', '\\t'), ('', '')] [('abcd', '')] Demo here",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I appreciate @blhsing answer and all the comments. Especially the (Duh!) observation that &quot;With splitting you always have one more field than the delimiters, which is why you have to fill in an empty string as the delimiter for the last field.&quot; With @engineersmnky comment this is a better Ruby: ruby -lne 'p $_.split(/([\\s_]+)/,-1).&lt;&lt;(&quot;&quot;).each_slice(2).to_a' f.txt # needed to catch last match ^ (Note the -1 argument needed to split as a flag to catch the potential last match on the end of the line with Perl and Ruby. On awk and Python, this is the default.) With @blhsing answer, this is a better Python: python3 -c ' import sys, re from itertools import batched with open(sys.argv[1]) as f: for line in f: print(list(batched(re.split(r&quot;([\\s_]+)&quot;, line.rstrip(&quot;\\r\\n&quot;) )+[&quot;&quot;], 2))) ' f.txt And I finally wrote that Perl: perl -lnE ' BEGIN { use strict; use warnings; use DDP; # only for printing use List::Util qw(pairs); } my @r; foreach my $pair ( pairs (split(/([\\h_]+)/, $_, -1), (&quot;&quot;)) ) { my @e = ($pair-&gt;key, $pair-&gt;value); push (@r, \\@e); } my $out=np(@r, multiline=&gt;0); $out=~s/(?&lt;=[\\[\\]]) (?=[\\[\\]])//g; $out=~s/\\t/\\\\t/g; # only for printing $out=~s/(?:(?&lt;=[\\[\\]]) (?=&quot;))|(?:(?&lt;=&quot;) (?=[\\[\\]]))//g; # only for printing say $out; ' f.txt Again, the goal was to split relatively more complex [field]?[delimiter][field][delimiter][field]|[delimiter] with a simple regex that targeted the delimiter alone and keep the result. I wanted to be able to prototype something on gawk because it is so universal. Then be able to confidently use the same regex in Perl, Python, Ruby or anything else that can capture what caused the split (Rust, Swift? Go cannot.) Given the example, all (except Python prints a list of tuples) the above print: [[&quot;a&quot;, &quot;\\t &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;&quot;, &quot; &quot;], [&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]] [[&quot;a&quot;, &quot; &quot;], [&quot;b&quot;, &quot;_&quot;], [&quot;c&quot;, &quot;\\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;\\t&quot;], [&quot;&quot;, &quot;&quot;]] [[&quot;abcd&quot;, &quot;&quot;]] The same approach can be inverted to target the fields with a regex also.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "regex",
        "ruby",
        "algorithm",
        "split"
      ],
      "question_score": 10,
      "answer_score": 7,
      "created": "2024-09-15T20:32:14",
      "question_id": 78988304,
      "answer_id": 78988700
    }
  },
  {
    "question": "What is wrong with the function below for calculating simple moving average?",
    "expected_answer": "The first time the else block is executed, is when i == w - 1. This means the argument passed to mean is t[-1:w-1]. This is wrong. The first slice you want to get the mean of is t[:w]. So you need to add one to both start and end indices of the slice: g[i] = np.mean(t[i-w+1:i+1])",
    "context_chunks": [
      {
        "text": "I have written the function below in order to find the SMA of a csv file according to the desired SMA formula, However, something is wrong with my formula which I can't figure it out. def SMA_calculation(t, w): s = np.size(t) g = np.zeros(s) for i in range(0, s): if i &lt; w-1: g[i] = np.NaN else: g[i] = np.mean(t[i-w:i]) return g",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The first time the else block is executed, is when i == w - 1. This means the argument passed to mean is t[-1:w-1]. This is wrong. The first slice you want to get the mean of is t[:w]. So you need to add one to both start and end indices of the slice: g[i] = np.mean(t[i-w+1:i+1])",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "To understand the error I change the code like this: import numpy as np t = np.array([1,2,3,4,5,6]) def SMA_calculation(t, w): s = np.size(t) g = np.zeros(s) for i in range(0, s): if i &lt; w-1: # g[i] = np.NaN print(i) else: # g[i] = np.mean(t[i-w:i]) print(f&quot;0{i}&quot;) print(t[i-w:i]) w = 3 SMA_calculation(t,w) &gt;&gt; 0 1 02 [] 03 [1 2 3] 04 [2 3 4] 05 [3 4 5] you can see your function has a null array. to solve this issue you can convert t[i-w:I] to t[i-w+1:i]. good luck ;)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "moving-average"
      ],
      "question_score": 10,
      "answer_score": 3,
      "created": "2024-01-25T21:06:05",
      "question_id": 77882942,
      "answer_id": 77883012
    }
  },
  {
    "question": "UserWarning: The figure layout has changed to tight self._figure.tight_layout(*args, **kwargs)",
    "expected_answer": "As mentioned in the comments, this is a matplotlib bug. It was fixed in version 3.7.3, so you can avoid it by upgrading Matplotlib. One of the comments suggests calling plt.figure(..., layout='constrained'), instead of tight_layout(), and that matches a few comments I found in the docs, like the Constrained Layout Guide: Constrained layout is similar to Tight layout, but is substantially more flexible. I saw this warning, because I was calling subplots(), then repeatedly plotting, calling tight_layout(), saving the figure, and calling cla(). I fixed it by removing the call to tight_layout() from the loop, and calling subplots(..., layout='tight'). Mind that you may need Python 3.11 for matplotlib 3.7.3, as a test with Python 3.10 shows: conda install matplotlib[version='&gt;=3.7.3'] Output: Retrieving notices: ...working... done Channels: - defaults Platform: linux-64 Collecting package metadata (repodata.json): done Solving environment: \\ warning libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE failed LibMambaUnsatisfiableError: Encountered problems while solving: - package matplotlib-3.8.0-py311h06a4308_0 requires matplotlib-base &gt;=3.8.0,&lt;3.8.1.0a0, but none of the providers can be installed Could not solve for environment specs The following packages are incompatible ├─ matplotlib-base 3.7.2.* is installable with the potential options │ ├─ matplotlib-base 3.7.2, which can be installed; │ ├─ matplotlib-base [3.7.2|3.8.0] would require │ │ └─ python &gt;=3.10,&lt;3.11.0a0 , which can be installed; │ ├─ matplotlib-base 3.7.2 would require │ │ └─ python &gt;=3.8,&lt;3.9.0a0 , which can be installed; │ └─ matplotlib-base [3.7.2|3.8.0] would require │ └─ python &gt;=3.9,&lt;3.10.0a0 , which can be installed; ├─ matplotlib &gt;=3.7.3 is installable with the potential options │ ├─ matplotlib 3.8.0 would require │ │ └─ matplotlib-base &gt;=3.8.0,&lt;3.8.1.0a0 with the potential options │ │ ├─ matplotlib-base [3.7.2|3.8.0], which can be installed (as previously explained); │ │ ├─ matplotlib-base [3.7.2|3.8.0], which can be installed (as previously explained); │ │ ├─ matplotlib-base 3.8.0 conflicts with any installable versions previously reported; │ │ └─ matplotlib-base 3.8.0 would require │ │ └─ python &gt;=3.12,&lt;3.13.0a0 , which can be installed; │ ├─ matplotlib 3.8.0 would require │ │ └─ python &gt;=3.10,&lt;3.11.0a0 , which can be installed; │ ├─ matplotlib 3.8.0 would require │ │ └─ python &gt;=3.12,&lt;3.13.0a0 , which can be installed; │ └─ matplotlib 3.8.0 would require │ └─ python &gt;=3.9,&lt;3.10.0a0 , which can be installed; └─ pin-1 is not installable because it requires └─ python 3.11.* , which conflicts with any installable versions previously reported. Pins seem to be involved in the conflict. Currently pinned specs: - python 3.11.* (labeled as 'pin-1')",
    "context_chunks": [
      {
        "text": "Why do I keep getting this warning whenever I try to use FacetGrid from seaborn? UserWarning: The figure layout has changed to tight. self._figure.tight_layout(*args, **kwargs) I understand it's a warning and not an error and I also understand it is changing the layout to tight. My question is why does it appear in the first place? Am I missing something? Example code: import seaborn as sns penguins = sns.load_dataset(&quot;penguins&quot;) g = sns.FacetGrid(penguins, col=&quot;island&quot;) g.map_dataframe(sns.histplot, x=&quot;bill_length_mm&quot;) This code throws that warning. What am I doing wrong? I know I can hide them with warnings module but I don't wanna do that.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "As mentioned in the comments, this is a matplotlib bug. It was fixed in version 3.7.3, so you can avoid it by upgrading Matplotlib. One of the comments suggests calling plt.figure(..., layout='constrained'), instead of tight_layout(), and that matches a few comments I found in the docs, like the Constrained Layout Guide: Constrained layout is similar to Tight layout, but is substantially more flexible. I saw this warning, because I was calling subplots(), then repeatedly plotting, calling tight_layout(), saving the figure, and calling cla(). I fixed it by removing the call to tight_layout() from the loop, and calling subplots(..., layout='tight'). Mind that you may need Python 3.11 for matplotlib 3.7.3, as a test with Python 3.10 shows: conda install matplotlib[version='&gt;=3.7.3'] Output: Retrieving notices: ...working... done Channels: - defaults Platform: linux-64 Collecting package metadata (repodata.json): done Solving environment: \\ warning libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE failed LibMambaUnsatisfiableError: Encountered problems while solving: - package matplotlib-3.8.0-py311h06a4308_0 requires matplotlib-base &gt;=3.8.0,&lt;3.8.1.0a0, but none of the providers can be installed Could not solve for environment specs The following packages are incompatible ├─ matplotlib-base 3.7.2.* is installable with the potential options │ ├─ matplotlib-base 3.7.2, which can be installed; │ ├─ matplotlib-base [3.7.2|3.8.0] would require │ │ └─ python &gt;=3.10,&lt;3.11.0a0 , which can be installed; │ ├─ matplotlib-base 3.7.2 would require │ │ └─ python &gt;=3.8,&lt;3.9.0a0 , which can be installed; │ └─ matplotlib-base [3.7.2|3.8.0] would require │ └─ python &gt;=3.9,&lt;3.10.0a0 , which can be installed; ├─ matplotlib &gt;=3.7.3 is installable with the potential options │ ├─ matplotlib 3.8.0 would require │ │ └─ matplotlib-base &gt;=3.8.0,&lt;3.8.1.0a0 with the potential options │ │ ├─ matplotlib-base [3.7.2|3.8.0], which can be installed (as previously explained); │ │ ├─ matplotlib-base [3.7.2|3.8.0], which can be installed (as previously explained); │ │ ├─ matplotlib-base 3.8.0 conflicts with any installable versions previously reported; │ │ └─ matplotlib-base 3.8.0 would require │ │ └─ python &gt;=3.12,&lt;3.13.0a0 , which can be installed; │ ├─ matplotlib 3.8.0 would require │ │ └─ python &gt;=3.10,&lt;3.11.0a0 , which can be installed; │ ├─ matplotlib 3.8.0 would require │ │ └─ python &gt;=3.12,&lt;3.13.0a0 , which can be installed; │ └─ matplotlib 3.8.0 would require │ └─ python &gt;=3.9,&lt;3.10.0a0 , which can be installed; └─ pin-1 is not installable because it requires └─ python 3.11.* , which conflicts with any installable versions previously reported. Pins seem to be involved in the conflict. Currently pinned specs: - python 3.11.* (labeled as 'pin-1')",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Simply update seaborn: pip install seaborn --upgrade",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "matplotlib",
        "seaborn"
      ],
      "question_score": 10,
      "answer_score": 5,
      "created": "2023-08-14T20:07:33",
      "question_id": 76901874,
      "answer_id": 77290689
    }
  },
  {
    "question": "How to prevent error message when importing import cv2?",
    "expected_answer": "I encountered this issue when I was using numpy 1.21.5 along with cv2 4.8.0 in Python 3.9.13. Upgrading to numpy 1.25.2 resolved the issue for me. pip install --upgrade numpy",
    "context_chunks": [
      {
        "text": "what does this error mean? I have opencv installed using pip install opencv-python. I’m using Python 3 and am in Jupyter Notebook. import cv2 TypeError: 'numpy._DTypeMeta' object is not subscriptable I tried importing opencv but it produced an error message",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I encountered this issue when I was using numpy 1.21.5 along with cv2 4.8.0 in Python 3.9.13. Upgrading to numpy 1.25.2 resolved the issue for me. pip install --upgrade numpy",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Change your numpy version pip install opencv-python==4.8.0.74",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "opencv",
        "jupyter-notebook",
        "pip",
        "jupyter-lab"
      ],
      "question_score": 10,
      "answer_score": 5,
      "created": "2023-08-10T23:44:00",
      "question_id": 76879942,
      "answer_id": 77019823
    }
  },
  {
    "question": "How to use the new gpt-3.5-16k model with langchain?",
    "expected_answer": "Update 20th Auguest, 2023 OpenAIChat is deprecated. Use ChatOpenAI() model instead. Also one side note, the model name should pass through the model_name parameter. from langchain.chat_models import ChatOpenAI chat = ChatOpenAI( model_name='gpt-3.5-turbo-16k', temperature = self.config.llm.temperature, openai_api_key = self.config.llm.openai_api_key, max_tokens=self.config.llm.max_tokens ) Old Answer: The gpt-3.5-turbo-16k is supposed to be used with the chat completion API endpoint. See the ref below: ENDPOINT MODEL NAME /v1/chat/completions gpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613 /v1/completions text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001 So, instead of using the OpenAI() llm, which uses text completion API under the hood, try using OpenAIChat(). See the below example with ref to your sample code: from langchain.llms import OpenAIChat self.llm = OpenAIChat( model_name='gpt-3.5-turbo-16k', temperature = self.config.llm.temperature, openai_api_key = self.config.llm.openai_api_key, max_tokens=self.config.llm.max_tokens )",
    "context_chunks": [
      {
        "text": "I have written an application in langchain that passes a number of chains to a Sequential Chain to run. The problem I'm having is that the prompts are so large that they are exceeding the 4K token limit size. I saw that OpenAI has released a new 16K token window sized model for ChatGPT, but I can't seem to access it from the API. When I try, I get the following error: openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions? Here is how I'm attempting to instantiate the model: self.llm = OpenAI(model='gpt-3.5-turbo-16k',temperature = self.config.llm.temperature, openai_api_key = self.config.llm.openai_api_key, max_tokens=self.config.llm.max_tokens ) Anybody know how I can fix this?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Update 20th Auguest, 2023 OpenAIChat is deprecated. Use ChatOpenAI() model instead. Also one side note, the model name should pass through the model_name parameter. from langchain.chat_models import ChatOpenAI chat = ChatOpenAI( model_name='gpt-3.5-turbo-16k', temperature = self.config.llm.temperature, openai_api_key = self.config.llm.openai_api_key, max_tokens=self.config.llm.max_tokens ) Old Answer: The gpt-3.5-turbo-16k is supposed to be used with the chat completion API endpoint. See the ref below: ENDPOINT MODEL NAME /v1/chat/completions gpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613 /v1/completions text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001 So, instead of using the OpenAI() llm, which uses text completion API under the hood, try using OpenAIChat(). See the below example with ref to your sample code: from langchain.llms import OpenAIChat self.llm = OpenAIChat( model_name='gpt-3.5-turbo-16k', temperature = self.config.llm.temperature, openai_api_key = self.config.llm.openai_api_key, max_tokens=self.config.llm.max_tokens )",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "from langchain.llms import OpenAI llm = OpenAI(temperature=0.1, model_name=&quot;gpt-3.5-turbo-16k&quot;) Works fine for me.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "langchain",
        "py-langchain"
      ],
      "question_score": 10,
      "answer_score": 7,
      "created": "2023-06-21T12:52:17",
      "question_id": 76523509,
      "answer_id": 76524372
    }
  },
  {
    "question": "Why is black hole null geodesic not printing zero? Are my trajectories correct?",
    "expected_answer": "You are missing a factor of 2 in the expression for the acceleration component dv_p. Change that line to dv_p = - 2 * Γ_p_rp * v_r * v_p This is because Γ_p_pr = Γ_p_rp and you need to include that term as well in the summation. Neil Butcher's post also picks out another non-zero Christoffel symbol Γ_t_rt that I missed and would be necessary to get v_t right and then affect the trajectories. BUT ... I believe the metric check is a red herring - you initialise v_t at the start with that metric and I think you can (and should) do the same again each time the metric components are recalculated from a new r. That is then bound to keep you on a geodesic. For completeness, I believe that you are solving systems of the form or, in terms of your quasi-velocities, If you do have to print the null-geodesic error then I think it should be normalised. You do have to see it in the context of c2, which is very large. Anyway, making the factor-of-2 change does reduce this error by 10 orders of magnitude. You will have to tell me the outcome of the plotting. I don't have, or know how to use, Blender. As you define a variable horizon it would be useful to use that (or the more common rs), rather than repeating 2 * G * M / c**2 In summary: put the extra factor of 2 in the expression for dv_p update v_t from the metric coefficients every time you recalculate the metric from a new r.",
    "context_chunks": [
      {
        "text": "Using matplotlib, I am path tracing some 2D photons bending due to a non-rotating standard black hole defined by Schwarzschild metric. I set my initial velocities in terms of r (radius), phi (angle), and t (time) with respect to the affine parameter lambda and then iteratively update the space-time vector based on the Christoffel symbols at the respective point. The main concerns are is why the null geodesic statements don't print something closer to zero. Everything scaled down by 1e6 to plot import numpy as np import matplotlib.pyplot as plt from math import sin, cos, sqrt, atan2, pi # Constants M = 9e32 # Mass of the black hole in kg c = 299792458 # Speed of light in m/s G = 6.6743e-11 # Gravitational constant in N m^2/kg^2 # Simulation parameters curve_num = 2000 # Resolution of the curve d_lambda = 1e-4 # Step size for integration # Black hole parameters horizon = 2 * G * M / c**2 # Event horizon radius photon_sphere = 3 * G * M / c**2 # Photon sphere radius # Test point parameters num = 30 # Number of photons x_i = 7.5 y_i = 2.5 y_f = 4 # Initial velocity in Cartesian coordinates v_x = -c # Speed of light in negative x-direction v_y = 0 # No velocity in the y-direction # Prepare for matplotlib plotting trajectories = [] for i in range(num): trajectory = [] # Define initial test point if num &gt; 1: test_point = np.array([x_i, y_i + i / (num - 1) * (y_f - y_i), 0]) # linear interpolation else: test_point = np.array([x_i, y_i, 0]) # Convert to polar coordinates r = np.linalg.norm(test_point) * 1e6 # Radius in meters p = atan2(test_point[1], test_point[0]) # Initial planar angle # Metric coefficients g_tt = -(1 - 2 * G * M / (r * c**2)) g_rr = 1 / (1 - 2 * G * M / (r * c**2)) g_pp = r**2 # Initial velocities v_r = (v_x * cos(p) + v_y * sin(p)) # Radial velocity v_p = (-v_x * sin(p) + v_y * cos(p)) / r # Angular velocity v_t = sqrt(-(g_rr * v_r**2 + g_pp * v_p**2) / g_tt) # Check the null geodesic condition for the initial point #print(g_tt * v_t**2 + g_rr * v_r**2 + g_pp * v_p**2) # Integrate geodesics for j in range(curve_num): if r &gt; horizon + 10000: # Precompute common terms term1 = G * M / (r**2 * c**2) # Common term: GM / r^2c^2 term2 = 1 - 2 * G * M / (r * c**2) # Common term: 1 - 2GM / rc^2 # Christoffel symbols using common terms Γ_r_tt = term1 * term2 Γ_r_rr = -term1 / term2 Γ_r_pp = -r * term2 Γ_p_rp = 1 / r #Γ_t_rt = term1 / term2 # ignoring time marching # Update change in velocities dv_r = -Γ_r_tt * v_t**2 - Γ_r_rr * v_r**2 - Γ_r_pp * v_p**2 dv_p = -2 * Γ_p_rp * v_r * v_p #dv_t = -2 * Γ_t_rt * v_r * v_t # ignoring time marching # Update velocities v_r += dv_r * d_lambda v_p += dv_p * d_lambda #v_t += dv_t * d_lambda # ignoring time marching # Update positions r += v_r * d_lambda p += v_p * d_lambda # Metric tensor components (update for new r) g_tt = -term2 g_rr = 1 / term2 g_pp = r**2 # Recalculate v_t from the metric components v_t = sqrt(-(g_rr * v_r**2 + g_pp * v_p**2) / g_tt) # Check the null geodesic condition at each step #print(g_tt * v_t**2 + g_rr * v_r**2 + g_pp * v_p**2) # Store Cartesian coordinates x = (r / 1e6) * cos(p) y = (r / 1e6) * sin(p) # Only store points within the -10 to 10 range if -10 &lt;= x &lt;= 10 and -10 &lt;= y &lt;= 10: trajectory.append((x, y)) else: break trajectories.append(trajectory) # Plot using matplotlib plt.figure(figsize=(8, 8)) # Plot each trajectory for trajectory in trajectories: trajectory = np.array(trajectory) if len(trajectory) &gt; 0: # Plot only if there are points plt.plot(trajectory[:, 0], trajectory[:, 1]) # Plot the event horizon circle = plt.Circle((0, 0), horizon / 1e6, color='black', fill=True, label=&quot;Event Horizon&quot;) plt.gca().add_artist(circle) # Plot the photon sphere photon_sphere_circle = plt.Circle((0, 0), photon_sphere / 1e6, color='red', fill=False, linestyle='--', linewidth=1.5, label=&quot;Photon Sphere&quot;) plt.gca().add_artist(photon_sphere_circle) # Set plot limits explicitly plt.xlim(-10, 10) plt.ylim(-10, 10) # Configure plot plt.title(&quot;Photon Trajectories Around a Black Hole&quot;) plt.xlabel(&quot;x (scaled by 1e6 meters)&quot;) plt.ylabel(&quot;y (scaled by 1e6 meters)&quot;) plt.axhline(0, color=&quot;gray&quot;, linewidth=0.5) plt.axvline(0, color=&quot;gray&quot;, linewidth=0.5) plt.axis('equal') plt.grid() plt.legend() plt.show() I expect my null geodesic print statement inside the for loop to be either 0 or a relatively small integer. For example, the first null geodesic print statement may be 0, 16, -8, etc. due to a small amount of imprecision with the large float addition (e17 magnitudes). I have tried to debug by replacing my &quot;else&quot; statement with &quot;elif j == 1&quot; and looking at the very next iteration, it can be seen the null geodesic prints a much larger float. I believe figuring out the null geodesic error will reveal if my trajectories are incorrect.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You are missing a factor of 2 in the expression for the acceleration component dv_p. Change that line to dv_p = - 2 * Γ_p_rp * v_r * v_p This is because Γ_p_pr = Γ_p_rp and you need to include that term as well in the summation. Neil Butcher's post also picks out another non-zero Christoffel symbol Γ_t_rt that I missed and would be necessary to get v_t right and then affect the trajectories. BUT ... I believe the metric check is a red herring - you initialise v_t at the start with that metric and I think you can (and should) do the same again each time the metric components are recalculated from a new r. That is then bound to keep you on a geodesic. For completeness, I believe that you are solving systems of the form or, in terms of your quasi-velocities, If you do have to print the null-geodesic error then I think it should be normalised. You do have to see it in the context of c2, which is very large. Anyway, making the factor-of-2 change does reduce this error by 10 orders of magnitude. You will have to tell me the outcome of the plotting. I don't have, or know how to use, Blender. As you define a variable horizon it would be useful to use that (or the more common rs), rather than repeating 2 * G * M / c**2 In summary: put the extra factor of 2 in the expression for dv_p update v_t from the metric coefficients every time you recalculate the metric from a new r.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I know nothing about the plotting but there are a few omissions with the terms of the accelerations You need to double any a asymmetric terms You calculate Γ_p_rp and don't bother to calculate Γ_p_pr (due to the symmetry). However you do need to compensate for that in your accelerations. eg 2.0*Γ_p_rp * v_r * v_p You need to include the 'acceleration' of time (I don't know what to call dv_t, but you need to include it) There is a symbol totally omitted Γ_t_rt = (G * M / (r**2 * c**2)) / (1 - 2 * G * M / (r * c**2)) this needs to be included and it causes to updates in v_t dv_t = 2.0*Γ_t_rt * v_t * v_r v_t += dv_t * d_lambda Putting this together gives the updated code # Relevant Christoffel symbols Γ_t_rt = (G * M / (r**2 * c**2)) / (1 - 2 * G * M / (r * c**2)) Γ_r_tt = (G * M / (r**2 * c**2)) * (1 - 2 * G * M / (r * c**2)) Γ_r_rr = -(G * M / (r**2 * c**2)) / (1 - 2 * G * M / (r * c**2)) Γ_r_pp = -r * (1 - 2 * G * M / (r * c**2)) Γ_p_rp = 1 / r # Get accelerations dv_t = -2.0*Γ_t_rt * v_t * v_r dv_r = -Γ_r_tt * v_t**2 - Γ_r_rr * v_r**2 - Γ_r_pp * v_p**2 dv_p = -2.0*Γ_p_rp * v_r * v_p # Update velocities v_t += dv_t * d_lambda v_r += dv_r * d_lambda v_p += dv_p * d_lambda",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "math",
        "simulation",
        "physics"
      ],
      "question_score": 10,
      "answer_score": 1,
      "created": "2025-01-15T07:46:07",
      "question_id": 79357401,
      "answer_id": 79361179
    }
  },
  {
    "question": "Dash &amp; polars; RAM-use keeps increasing",
    "expected_answer": "I found the answer at the polars-github. Just add os.environ['MIMALLOC_ABANDONED_PAGE_RESET'] = '1' before importing polars.",
    "context_chunks": [
      {
        "text": "I have made a local dash-app to allow students to efficiently find/study their (measurement) data. To allow for further development, I tried to make a transition from pandas &amp; duckdb to polars. After weeks of work to integrate it into this extensive app, I realized that I have run into a major problem. The app was stable before, but now with polars, the RAM-footprint (of the pythonw.exe process) balloons with each successive callback. While the app starts out around 100 MB; each callback adds something like 5MB. I doesn’t seem to stabilize; at 1500 MB it was still growing. I’m sort of stuck and would really appreciate some pointers how to resolve this. I made a minimum example to illustrate the issue. If I run it with “polars_check=True”, then I start with 98MB and after 100 iterations it has become 261 MB. If I do it with “polars_check”=False (i.e. pandas), then I start and end with 98MB. import pathlib, os, shutil import polars as pl, pandas as pd, numpy as np, datetime as dt from dash import Dash, dcc, html, Input, Output import plotly.graph_objects as go #Check-input polars_check = True ### Whether the example returns with polars or with pandas. if polars_check: #To accomdate the slower data retrieval with pandas. interval_time = 3E3 else: interval_time = 3E3 #Constants folder = pathlib.Path(r'C:\\PerovskiteCell example') n_files = 100 #Number of files in folder n_lines = 500000 #Number of total lines in folder n_cols = 25 #Generating sample data in example folder (Only once). if not folder.exists(): size = int(n_lines / n_files) col = np.linspace(-1E3, 1E3, num=size) df = pl.DataFrame({f'col{n}': col for n in range(n_cols)}) # Creating folder &amp; files os.makedirs(folder) f_path0 = folder.joinpath('0.csv') df.write_csv(f_path0) for n in range(1, n_files): shutil.copy2(f_path0, folder.joinpath(f'{n}.csv')) #Functions def pl_data(): &quot;&quot;&quot;Retrieves data via the polars route&quot;&quot;&quot; lf = (pl.scan_csv(folder.joinpath(f'{n}.csv'), schema={f'col{n}': pl.Float64 for n in range(n_cols)}) .select(pl.all().get(n)) for n in range(n_files)) lf = pl.concat(lf) lf = lf.select('col0', 'col1') return lf.collect() def pd_data(): &quot;&quot;&quot;Retrieves data via the pandas route&quot;&quot;&quot; dfs = (pd.read_csv(folder.joinpath(f'{n}.csv'), usecols=['col0', 'col1']).iloc[n:n+1] for n in range(n_files)) return pd.concat(dfs, ignore_index=True) #App (initialization) app = Dash() app.layout = html.Div([dcc.Graph(id='graph'), dcc.Interval(id = 'check', interval = interval_time, max_intervals = 100)]) @app.callback( Output('graph', 'figure'), Input('check', 'n_intervals')) def plot(_): #Data retrieval if polars_check: df = pl_data() else: df = pd_data() #Plotting fig = go.Figure() trace = go.Scattergl(x = list(df['col0']), y=list(df['col1']), mode='lines+markers') fig.add_trace(trace) fig.update_xaxes(title = str(dt.datetime.now())) return fig if __name__ == '__main__': app.run(debug=False, port = 8050)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I found the answer at the polars-github. Just add os.environ['MIMALLOC_ABANDONED_PAGE_RESET'] = '1' before importing polars.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can try to use python package of garbage collector to empty the unused ram. Hope it helps. gc.enable() gc.collect(generation=2)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "flask",
        "plotly-dash",
        "python-polars"
      ],
      "question_score": 10,
      "answer_score": 2,
      "created": "2024-08-05T11:50:42",
      "question_id": 78834305,
      "answer_id": 78886894
    }
  },
  {
    "question": "Why is the code executed multiple times whenever Streamlit is started?",
    "expected_answer": "It seems to be because streamlit is running your code on different threads, when available. My guess is that it has something to do with the way the underlying webserver works. You can confirm it with the following: import streamlit as st from streamlit.logger import get_logger import logging import threading LOGGER = get_logger(__file__) LOGGER.setLevel(logging.DEBUG) LOGGER.debug(f'start of streamlit_test, {threading.get_ident()}') st.write(&quot;hellos&quot;) LOGGER.debug(f'end of streamlit_test, {threading.get_ident()}') And the output on my machine: 2024-01-05 14:51:30.542 start of streamlit_test, 140329869112896 2024-01-05 14:51:30.563 start of streamlit_test, 140329860720192 2024-01-05 14:51:30.564 end of streamlit_test, 140329860720192 2024-01-05 14:51:30.781 start of streamlit_test, 140329860720192 2024-01-05 14:51:30.783 end of streamlit_test, 140329860720192 2024-01-05 14:51:31.003 start of streamlit_test, 140329860720192 2024-01-05 14:51:31.004 end of streamlit_test, 140329860720192 2024-01-05 14:51:31.230 start of streamlit_test, 140329860720192 2024-01-05 14:51:31.233 end of streamlit_test, 140329860720192 2024-01-05 14:51:31.355 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.414 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.415 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.568 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.569 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.681 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.681 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.789 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.790 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.887 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.888 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.978 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.978 end of streamlit_test, 140329869112896 2024-01-05 14:51:32.079 start of streamlit_test, 140329869112896 2024-01-05 14:51:32.083 end of streamlit_test, 140329869112896 2024-01-05 14:51:32.180 start of streamlit_test, 140329869112896 2024-01-05 14:51:32.181 end of streamlit_test, 140329869112896",
    "context_chunks": [
      {
        "text": "When I launch my Streamlit application using the command &quot;streamlit run streamlit_test.py&quot;, I noticed that the logs are printed multiple times. However, when I refresh the browser, the logs are only printed once. Here is my code: import streamlit as st from streamlit.logger import get_logger import logging LOGGER = get_logger(__name__) LOGGER.setLevel(logging.DEBUG) LOGGER.debug(f'start of streamlit_test') st.write(&quot;hello&quot;) LOGGER.debug(f'end of streamlit_test') When I start Streamlit, the logs in my code are printed multiple times. Here are the logs: 2023-06-14 22:09:19.993 start of streamlit_test 2023-06-14 22:09:19.998 start of streamlit_test 2023-06-14 22:09:20.008 end of streamlit_test 2023-06-14 22:09:20.285 end of streamlit_test 2023-06-14 22:09:20.831 start of streamlit_test 2023-06-14 22:09:20.833 end of streamlit_test 2023-06-14 22:09:23.266 start of streamlit_test 2023-06-14 22:09:23.268 end of streamlit_test 2023-06-14 22:09:23.752 start of streamlit_test 2023-06-14 22:09:23.754 end of streamlit_test When I refresh the browser, the logs are only printed once.Here are the logs: 2023-06-14 22:30:08.388 start of streamlit_test 2023-06-14 22:30:08.391 end of streamlit_test I would like to know the reason behind this. Can someone help me? I would greatly appreciate it.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "It seems to be because streamlit is running your code on different threads, when available. My guess is that it has something to do with the way the underlying webserver works. You can confirm it with the following: import streamlit as st from streamlit.logger import get_logger import logging import threading LOGGER = get_logger(__file__) LOGGER.setLevel(logging.DEBUG) LOGGER.debug(f'start of streamlit_test, {threading.get_ident()}') st.write(&quot;hellos&quot;) LOGGER.debug(f'end of streamlit_test, {threading.get_ident()}') And the output on my machine: 2024-01-05 14:51:30.542 start of streamlit_test, 140329869112896 2024-01-05 14:51:30.563 start of streamlit_test, 140329860720192 2024-01-05 14:51:30.564 end of streamlit_test, 140329860720192 2024-01-05 14:51:30.781 start of streamlit_test, 140329860720192 2024-01-05 14:51:30.783 end of streamlit_test, 140329860720192 2024-01-05 14:51:31.003 start of streamlit_test, 140329860720192 2024-01-05 14:51:31.004 end of streamlit_test, 140329860720192 2024-01-05 14:51:31.230 start of streamlit_test, 140329860720192 2024-01-05 14:51:31.233 end of streamlit_test, 140329860720192 2024-01-05 14:51:31.355 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.414 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.415 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.568 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.569 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.681 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.681 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.789 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.790 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.887 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.888 end of streamlit_test, 140329869112896 2024-01-05 14:51:31.978 start of streamlit_test, 140329869112896 2024-01-05 14:51:31.978 end of streamlit_test, 140329869112896 2024-01-05 14:51:32.079 start of streamlit_test, 140329869112896 2024-01-05 14:51:32.083 end of streamlit_test, 140329869112896 2024-01-05 14:51:32.180 start of streamlit_test, 140329869112896 2024-01-05 14:51:32.181 end of streamlit_test, 140329869112896",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Make sure you don't have multiple browser tabs with streamlit open",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "streamlit"
      ],
      "question_score": 10,
      "answer_score": 2,
      "created": "2023-06-14T14:38:19",
      "question_id": 76474732,
      "answer_id": 77765085
    }
  },
  {
    "question": "Difference between numpy power and ** for certain values",
    "expected_answer": "The results are different because f**2 calls numpy.square, while f[0]**2 and numpy.power(f, 2) call numpy.power. numpy.ndarray.__pow__ is written in C. It looks like this: static PyObject * array_power(PyObject *a1, PyObject *o2, PyObject *modulo) { PyObject *value = NULL; if (modulo != Py_None) { /* modular exponentiation is not implemented (gh-8804) */ Py_INCREF(Py_NotImplemented); return Py_NotImplemented; } BINOP_GIVE_UP_IF_NEEDED(a1, o2, nb_power, array_power); if (fast_scalar_power(a1, o2, 0, &amp;value) != 0) { value = PyArray_GenericBinaryFunction(a1, o2, n_ops.power); } return value; } The value = PyArray_GenericBinaryFunction(a1, o2, n_ops.power); is a Python function call to the numpy.power ufunc object, but first, it tries a fast_scalar_power function. This function tries to optimize exponentiation with common scalar powers, such as 2. For the f**2 operation, fast_scalar_power detects the exponent of 2, and delegates the operation to numpy.square: else if (exponent == 2.0) { fastop = n_ops.square; } For numpy.power(f, 2), this is of course a direct call to numpy.power. numpy.power doesn't go through fast_scalar_power, and doesn't have any special handling for an exponent of 2. (Depending on what underlying power implementation it hits, that implementation might still have special handling for 2, though.) For scalars, I believe numpy.float64.__pow__ actually just calls array_power: static PyObject * gentype_power(PyObject *m1, PyObject *m2, PyObject *modulo) { if (modulo != Py_None) { /* modular exponentiation is not implemented (gh-8804) */ Py_INCREF(Py_NotImplemented); return Py_NotImplemented; } BINOP_GIVE_UP_IF_NEEDED(m1, m2, nb_power, gentype_power); return PyArray_Type.tp_as_number-&gt;nb_power(m1, m2, Py_None); } so it hits fast_scalar_power, but one of the first checks in fast_scalar_power is if (PyArray_Check(o1) &amp;&amp; An instance of numpy.float64 does not pass PyArray_Check, which checks for objects whose type is exactly numpy.ndarray. Thus, the scalar goes through the general numpy.power code path.",
    "context_chunks": [
      {
        "text": "I have a numpy array where the entries in f**2 differ from f[i]**2, but only for some specific value. import numpy as np np.set_printoptions(precision = 16) f = np.array([ -40709.6555510835, -40708.6555510835, -33467.081758611654, -27653.379955714125]) f2 = f**2 # f2 = np.power(f,2) print(&quot;outside loop&quot;, np.abs(f[1]**2 - f2[1]), np.abs(1.0 - f2[1] / f[1]**2), f.dtype, f[1].dtype, f2.dtype, f2[1].dtype) for i, val in enumerate(f): print(&quot;inside loop&quot;, i, np.abs(val**2 - f2[i]), np.abs(1.0 - f2[i] / val**2), val.dtype, f2.dtype, f2[i].dtype) Produces output: outside loop 2.384185791015625e-07 2.220446049250313e-16 float64 float64 float64 float64 inside loop 0 0.0 0.0 float64 float64 float64 inside loop 1 2.384185791015625e-07 2.220446049250313e-16 float64 float64 float64 inside loop 2 0.0 0.0 float64 float64 float64 inside loop 3 0.0 0.0 float64 float64 float64 I do note that this is a relative error on the order of epsilon. This issue goes away when using np.power instead of ** in the definition of f2. Even so, why is f[i]**2 not the same as the ith value of f**2 (even if only for certain values in f). I'm using python 3.10.6 and the latest numpy 1.26.4. Edit: The fundamental issue is captured in: import numpy as np f = np.array([-40708.6555510835]) print((f[0])**2 - (f**2)[0]) which displays a value of -2.384185791015625e-07 I would like to know why that specific number has this specific issue. If you'd like confirmation, or to try different values for f, see this demo.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The results are different because f**2 calls numpy.square, while f[0]**2 and numpy.power(f, 2) call numpy.power. numpy.ndarray.__pow__ is written in C. It looks like this: static PyObject * array_power(PyObject *a1, PyObject *o2, PyObject *modulo) { PyObject *value = NULL; if (modulo != Py_None) { /* modular exponentiation is not implemented (gh-8804) */ Py_INCREF(Py_NotImplemented); return Py_NotImplemented; } BINOP_GIVE_UP_IF_NEEDED(a1, o2, nb_power, array_power); if (fast_scalar_power(a1, o2, 0, &amp;value) != 0) { value = PyArray_GenericBinaryFunction(a1, o2, n_ops.power); } return value; } The value = PyArray_GenericBinaryFunction(a1, o2, n_ops.power); is a Python function call to the numpy.power ufunc object, but first, it tries a fast_scalar_power function. This function tries to optimize exponentiation with common scalar powers, such as 2. For the f**2 operation, fast_scalar_power detects the exponent of 2, and delegates the operation to numpy.square: else if (exponent == 2.0) { fastop = n_ops.square; } For numpy.power(f, 2), this is of course a direct call to numpy.power. numpy.power doesn't go through fast_scalar_power, and doesn't have any special handling for an exponent of 2. (Depending on what underlying power implementation it hits, that implementation might still have special handling for 2, though.) For scalars, I believe numpy.float64.__pow__ actually just calls array_power: static PyObject * gentype_power(PyObject *m1, PyObject *m2, PyObject *modulo) { if (modulo != Py_None) { /* modular exponentiation is not implemented (gh-8804) */ Py_INCREF(Py_NotImplemented); return Py_NotImplemented; } BINOP_GIVE_UP_IF_NEEDED(m1, m2, nb_power, gentype_power); return PyArray_Type.tp_as_number-&gt;nb_power(m1, m2, Py_None); } so it hits fast_scalar_power, but one of the first checks in fast_scalar_power is if (PyArray_Check(o1) &amp;&amp; An instance of numpy.float64 does not pass PyArray_Check, which checks for objects whose type is exactly numpy.ndarray. Thus, the scalar goes through the general numpy.power code path.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This might be a numpy bug, and might have been introduced in https://github.com/numpy/numpy/pull/7496. I found this by searching through the changelog of numpy for &quot;pow&quot;. It appears that both python and numpy use the standard library pow for doubles. So I was looking for differences on numpy's handling of ** on scalar values vs arrays. I don't know if this is the actual cause of the behavior or not, but it's the only place that I've found so far where numpy doesn't use the standard library pow() function. And it lead me to a test that seems to duplicate the behavior you're seeing. I see the exact same results on my core i7 iMac and on my M1 MBP. &gt;&gt;&gt; x = -40708.6555510835 &gt;&gt;&gt; x**2 1657194636.7767615 &gt;&gt;&gt; x * x 1657194636.7767618 &gt;&gt;&gt; x**2 - x*x -2.384185791015625e-07 So if I'm right, then saying f[0]**2 uses the math lib pow function, and (f**2)[0] skips pow and multiplies the values together.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy"
      ],
      "question_score": 10,
      "answer_score": 6,
      "created": "2024-03-07T16:11:35",
      "question_id": 78122836,
      "answer_id": 78130903
    }
  },
  {
    "question": "Python build error with Stable Diffusion repository: `ERROR: Could not find a version that satisfies the requirement triton==2.0.0`",
    "expected_answer": "This works on windows. Below versions on which it worked for me : Python : 3.10.9 Windows : 10 pro pip install https://huggingface.co/r4ziel/xformers_pre_built/resolve/main/triton-2.0.0-cp310-cp310-win_amd64.whl refer : https://github.com/openai/triton/issues/1057",
    "context_chunks": [
      {
        "text": "I'm trying to run the Python build for the Stable Diffusion SDXL project: https://github.com/Stability-AI/generative-models I'm mostly following the simple instructions provided on the github page. The project page says it supports Python 3.8 and Python 3.10. I tried Python 3.10, got an error that some dependency required Requires-Python &gt;=3.7,&lt;3.10, so I tried Python 3.8, and I get a different error, which I show below: git clone git@github.com:Stability-AI/generative-models.git cd generative-models rm -rf .pt2 /opt/homebrew/opt/python@3.8/bin/python3.8 -m venv .pt2 source .pt2/bin/activate pip3 install -r requirements/pt2.txt I'm snipping out some of the logs and including the error and a few lines before the error. I'm not sure what is wrong and what can I do to get this working? &lt;snip&gt; Collecting torchmetrics&gt;=1.0.1 Using cached torchmetrics-1.1.2-py3-none-any.whl (764 kB) Collecting torchvision&gt;=0.15.2 Using cached torchvision-0.15.2-cp38-cp38-macosx_11_0_arm64.whl (1.4 MB) Collecting tqdm&gt;=4.65.0 Using cached tqdm-4.66.1-py3-none-any.whl (78 kB) Collecting transformers==4.19.1 Using cached transformers-4.19.1-py3-none-any.whl (4.2 MB) ERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python &lt;3.5; 1.11.0 Requires-Python &lt;3.13,&gt;=3.9; 1.11.0rc1 Requires-Python &lt;3.13,&gt;=3.9; 1.11.0rc2 Requires-Python &lt;3.13,&gt;=3.9; 1.11.1 Requires-Python &lt;3.13,&gt;=3.9; 1.11.2 Requires-Python &lt;3.13,&gt;=3.9; 1.25.0 Requires-Python &gt;=3.9; 1.25.0rc1 Requires-Python &gt;=3.9; 1.25.1 Requires-Python &gt;=3.9; 1.25.2 Requires-Python &gt;=3.9; 1.26.0 Requires-Python &lt;3.13,&gt;=3.9; 1.26.0b1 Requires-Python &lt;3.13,&gt;=3.9; 1.26.0rc1 Requires-Python &lt;3.13,&gt;=3.9; 2.1.0 Requires-Python &gt;=3.9; 2.1.0rc0 Requires-Python &gt;=3.9; 2.1.1 Requires-Python &gt;=3.9; 3.8.0 Requires-Python &gt;=3.9; 3.8.0rc1 Requires-Python &gt;=3.9 ERROR: Could not find a version that satisfies the requirement triton==2.0.0 (from versions: none) ERROR: No matching distribution found for triton==2.0.0",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This works on windows. Below versions on which it worked for me : Python : 3.10.9 Windows : 10 pro pip install https://huggingface.co/r4ziel/xformers_pre_built/resolve/main/triton-2.0.0-cp310-cp310-win_amd64.whl refer : https://github.com/openai/triton/issues/1057",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I meet the same problem, it seems that triton doesn't support Windows platform, so you have to switch to Linux. See the official homepage here.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x"
      ],
      "question_score": 10,
      "answer_score": 2,
      "created": "2023-09-21T16:55:35",
      "question_id": 77152235,
      "answer_id": 78105458
    }
  },
  {
    "question": "How can I divide a list of electrical loads into 3 groups with near-equal total sum using Python?",
    "expected_answer": "This is an instance of the &quot;multiple subset sums problem&quot; (MSSP). A search engine will turn up mountains of information. Solving it exactly is generally intractable as the number of inputs grows, so it's crucial you say more about how many inputs there may be. You only showed an example with 10 inputs, and a computer can handle that very quickly via exhaustive brute force. The number of ways to partition n inputs into k subsets, S(n, k), is a &quot;Stirling number of the second kind&quot;. Here's a function to compute it: import functools @functools.cache def stirling2(n, k): if n &lt; 0 or k &lt; 0: raise ValueError(&quot;n and k must be &gt;= 0&quot;) if n == k: return 1 if n * k == 0: return 0 return stirling2(n - 1, k - 1) + k * stirling2(n - 1, k) For your example, &gt;&gt;&gt; stirling2(10, 3) 9330 You don't want to do that &quot;by hand&quot;, but generating and scoring 10K partitions is trivial for a computer. But it doesn't scale! Double the number of inputs, and there are over half a billion partitions to check: stirling2(20, 3) 580606446 But in the absence of more information from you, I'm not going to write a book ;-) Happy to share code to generate all possible ways, but it won't do you any real good unless stirling2() convinces you that's practical for the input sizes you expect. The literature on faster approximation methods is vast. Here's a best-possible partition for your example, obtained via such brute force; all the other 9329 ways of partitioning the loads have a larger spread than 300 between the max and min sums: (2000, 5500) (2500, 1200, 1300, 2700) (1500, 1700, 3000, 1600) sums: 7500, 7700, 7800",
    "context_chunks": [
      {
        "text": "I'm an electrician working toward becoming an electrical engineer. I have some basic knowledge of Python and am trying to automate a tedious process we often face in the field. In 3-phase electrical installations (phases R, S, and T), we need to distribute loads across the phases such that the total power in each phase is as balanced as possible. For example, given the following list of loads (in watts): loads = [2000, 2500, 1500, 1700, 5500, 3000, 1200, 1300, 1600, 2700] The total load is 23,000 W, and the ideal distribution would be roughly 7666.67 W per phase. Of course, exact balance is often impossible, but I want to get as close as possible. I wrote this basic Python script to assign each load to the phase with the lowest current total: loads = [2000, 2500, 1500, 1700, 5500, 3000, 1200, 1300, 1600, 2700] loads.sort(reverse=True) phase_R, phase_S, phase_T = [], [], [] sum_R = sum_S = sum_T = 0 for load in loads: if sum_R &lt;= sum_S and sum_R &lt;= sum_T: phase_R.append(load) sum_R += load elif sum_S &lt;= sum_T: phase_S.append(load) sum_S += load else: phase_T.append(load) sum_T += load total = sum_R + sum_S + sum_T imbalance = (max(sum_R, sum_S, sum_T) - min(sum_R, sum_S, sum_T)) / max(sum_R, sum_S, sum_T) * 100 print(&quot;Phase R:&quot;, phase_R, &quot;-&gt;&quot;, sum_R, &quot;W&quot;) print(&quot;Phase S:&quot;, phase_S, &quot;-&gt;&quot;, sum_S, &quot;W&quot;) print(&quot;Phase T:&quot;, phase_T, &quot;-&gt;&quot;, sum_T, &quot;W&quot;) print(f&quot;Imbalance: {imbalance:.2f}%&quot;) This greedy approach works decently for many inputs but performs poorly in some cases, especially when the number of loads is not a multiple of 3 or when certain values dominate the list. Is there an existing algorithm that can help distribute a list of numbers into 3 groups such that the sums of the groups are as close as possible? Ideally, the solution should be able to handle lists of arbitrary length and not assume the number of elements is divisible by 3.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is an instance of the &quot;multiple subset sums problem&quot; (MSSP). A search engine will turn up mountains of information. Solving it exactly is generally intractable as the number of inputs grows, so it's crucial you say more about how many inputs there may be. You only showed an example with 10 inputs, and a computer can handle that very quickly via exhaustive brute force. The number of ways to partition n inputs into k subsets, S(n, k), is a &quot;Stirling number of the second kind&quot;. Here's a function to compute it: import functools @functools.cache def stirling2(n, k): if n &lt; 0 or k &lt; 0: raise ValueError(&quot;n and k must be &gt;= 0&quot;) if n == k: return 1 if n * k == 0: return 0 return stirling2(n - 1, k - 1) + k * stirling2(n - 1, k) For your example, &gt;&gt;&gt; stirling2(10, 3) 9330 You don't want to do that &quot;by hand&quot;, but generating and scoring 10K partitions is trivial for a computer. But it doesn't scale! Double the number of inputs, and there are over half a billion partitions to check: stirling2(20, 3) 580606446 But in the absence of more information from you, I'm not going to write a book ;-) Happy to share code to generate all possible ways, but it won't do you any real good unless stirling2() convinces you that's practical for the input sizes you expect. The literature on faster approximation methods is vast. Here's a best-possible partition for your example, obtained via such brute force; all the other 9329 ways of partitioning the loads have a larger spread than 300 between the max and min sums: (2000, 5500) (2500, 1200, 1300, 2700) (1500, 1700, 3000, 1600) sums: 7500, 7700, 7800",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You could utilize the OR-Tools package: pip install ortools &quot;&quot;&quot;Load Balancer for 3-Phase Electrical Installations using OR-Tools CP-SAT.&quot;&quot;&quot; import dataclasses import time from ortools.sat.python import cp_model _PHASE_NAMES = ['R', 'S', 'T'] @dataclasses.dataclass class DistributionResult: &quot;&quot;&quot;Result of the load distribution process.&quot;&quot;&quot; phase_assignments: dict[str, list[int]] phase_sums: dict[str, int] imbalance_percentage: float def balance_loads(loads: list[int]) -&gt; DistributionResult: &quot;&quot;&quot;Distribute loads across three phases to minimize imbalance. Args: loads: A list of integer loads in watts. Returns: A DistributionResult containing: - phase_assignments: Mapping of phase name to list of loads assigned. - phase_sums: Mapping of phase name to total load. - imbalance_percentage: Ratio of (max_sum - min_sum) to max_sum * 100. &quot;&quot;&quot; model = cp_model.CpModel() num_loads = len(loads) num_phases = len(_PHASE_NAMES) # Decision variables: x[i][j] is True if load i is assigned to phase j. x = [[model.NewBoolVar(f'x_{i}_{j}') for j in range(num_phases)] for i in range(num_loads)] # Integer variables for the sum of loads on each phase. phase_sums_vars = [ model.NewIntVar(0, sum(loads), f'sum_{phase}') for phase in _PHASE_NAMES ] # Each load must be assigned to exactly one phase. for i in range(num_loads): model.Add(sum(x[i][j] for j in range(num_phases)) == 1) # Define phase sums based on assignments. for j in range(num_phases): model.Add(phase_sums_vars[j] == sum(x[i][j] * loads[i] for i in range(num_loads))) # Variables for tracking max and min phase sums. max_sum = model.NewIntVar(0, sum(loads), 'max_sum') min_sum = model.NewIntVar(0, sum(loads), 'min_sum') model.AddMaxEquality(max_sum, phase_sums_vars) model.AddMinEquality(min_sum, phase_sums_vars) # Objective: minimize the difference between max and min phase sums. model.Minimize(max_sum - min_sum) # Solve with a time limit. solver = cp_model.CpSolver() solver.parameters.max_time_in_seconds = 30.0 status = solver.Solve(model) if status not in (cp_model.OPTIMAL, cp_model.FEASIBLE): raise RuntimeError('No feasible solution found for load balancing.') # Extract results. assignments: dict[str, list[int]] = {phase: [] for phase in _PHASE_NAMES} sums: dict[str, int] = {} for j, phase in enumerate(_PHASE_NAMES): sums[phase] = solver.Value(phase_sums_vars[j]) for i, load in enumerate(loads): if solver.Value(x[i][j]): assignments[phase].append(load) imbalance = (solver.Value(max_sum) - solver.Value(min_sum)) / solver.Value(max_sum) * 100.0 return DistributionResult( phase_assignments=assignments, phase_sums=sums, imbalance_percentage=imbalance, ) def main() -&gt; None: sample_loads = [2000, 2500, 1500, 1700, 5500, 3000, 1200, 1300, 1600, 2700] start = time.perf_counter() result = balance_loads(sample_loads) end = time.perf_counter() print(f'Time taken: {end - start:.2f} seconds') print('Balanced Distribution:') for phase in _PHASE_NAMES: loads_assigned = result.phase_assignments[phase] total = result.phase_sums[phase] print(f'Phase {phase}: {loads_assigned} -&gt; {total} W') print(f'Imbalance: {result.imbalance_percentage:.2f}%') if __name__ == '__main__': main() Output: Time taken: 0.02 seconds Balanced Distribution: Phase R: [1500, 1700, 3000, 1600] -&gt; 7800 W Phase S: [2500, 1200, 1300, 2700] -&gt; 7700 W Phase T: [2000, 5500] -&gt; 7500 W Imbalance: 3.85%",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "partitioning",
        "greedy"
      ],
      "question_score": 9,
      "answer_score": 12,
      "created": "2025-05-17T16:50:50",
      "question_id": 79626775,
      "answer_id": 79627210
    }
  },
  {
    "question": "Python request-html is not downloading Chromium",
    "expected_answer": "UPDATE Thanks to @Abdul Aziz Barkat's comment, it turns out that you can specify chromium version through environment variables, and pyppeteer will use it. PYPPETEER_CHROMIUM_REVISION = '1263111' requests-html uses pyppeteer library to download chromium, and it looks like version 1181205 of chromium which is hardcoded in pyppeteer has been removed from google storage. Since requests-html installs pyppeteer with it, a simple workaround can be updating line 20 of pyppeteer's __init__.py file in your env: __chromium_revision__ = '1181205' -&gt; __chromium_revision__ = '1263111' Note: I used version 1263111 because it's the latest for Win_x64 at the time of answering, and it works fine.",
    "context_chunks": [
      {
        "text": "import requests from bs4 import BeautifulSoup from requests_html import HTMLSession url=&quot;https://dmarket.com/ingame-items/item-list/csgo-skins?title=recoil%20case&quot; sesion = HTMLSession() response = sesion.get(url) response.html.render() soup = BeautifulSoup(response.html.html, features=&quot;html.parser&quot;) print(soup) After run it said [INFO] Starting Chromium download. After that crashes with this in VS Code: Chromium downloadable not found at https://storage.googleapis.com/chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip: Received &lt;?xml version='1.0' encoding='UTF-8'?&gt;&lt;Error&gt;&lt;Code&gt;NoSuchKey&lt;/Code&gt;&lt;Message&gt;The specified key does not exist.&lt;/Message&gt;&lt;Details&gt;No such object: chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip&lt;/Details&gt;&lt;/Error&gt; I tried installing different versions of requests_html",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "UPDATE Thanks to @Abdul Aziz Barkat's comment, it turns out that you can specify chromium version through environment variables, and pyppeteer will use it. PYPPETEER_CHROMIUM_REVISION = '1263111' requests-html uses pyppeteer library to download chromium, and it looks like version 1181205 of chromium which is hardcoded in pyppeteer has been removed from google storage. Since requests-html installs pyppeteer with it, a simple workaround can be updating line 20 of pyppeteer's __init__.py file in your env: __chromium_revision__ = '1181205' -&gt; __chromium_revision__ = '1263111' Note: I used version 1263111 because it's the latest for Win_x64 at the time of answering, and it works fine.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I tried the solution by @Lk4m1. This setup worked for me : import asyncio import os PYPPETEER_CHROMIUM_REVISION = '1263111' os.environ['PYPPETEER_CHROMIUM_REVISION'] = PYPPETEER_CHROMIUM_REVISION from pyppeteer import launch async def generate_pdf(url, pdf_path): browser = await launch() page = await browser.newPage() await page.goto(url) await page.pdf({'path': pdf_path, 'format': 'A4'}) await browser.close() # Run the function asyncio.get_event_loop().run_until_complete(generate_pdf('https://example.com', 'example.pdf'))",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "web-scraping",
        "python-requests-html"
      ],
      "question_score": 9,
      "answer_score": 21,
      "created": "2024-02-19T21:04:01",
      "question_id": 78023508,
      "answer_id": 78031678
    }
  },
  {
    "question": "How to resolve &quot;cannot import name &#39;_MissingValues&#39; from &#39;sklearn.utils._param_validation&#39;&quot; issue when trying to import imblearn?",
    "expected_answer": "I was having the same issue, downgrading to scikit-learn 1.2.2 fixed it for me",
    "context_chunks": [
      {
        "text": "I am trying to import imblearn into my python notebook after installing the required modules. However, I am getting the following error: Additional info: I am using a virtual environment in Visual Studio Code. I've made sure that venv was selected as interpreter and as the notebook kernel. I've reloaded the window and restarted the kernel several times. I have also uninstalled and installed imbalanced-learn and scikit-learn several times, with and without &quot;--upgrade&quot;. I'm still getting the same error. Edit: Full traceback of error { &quot;name&quot;: &quot;ImportError&quot;, &quot;message&quot;: &quot;cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (c:\\\\Users\\\\wen\\\\OneDrive\\\\Desktop\\\\Colab_Notebooks\\\\.venv\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\_param_validation.py)&quot;, &quot;stack&quot;: &quot;\\u001b[1;31m---------------------------------------------------------------------------\\u001b[0m\\n\\u001b[1;31mImportError\\u001b[0m Traceback (most recent call last)\\nCell \\u001b[1;32mIn[1], line 1\\u001b[0m\\n\\u001b[1;32m----&gt; 1\\u001b[0m \\u001b[39mimport\\u001b[39;00m \\u001b[39mimblearn\\u001b[39;00m\\n\\u001b[0;32m 2\\u001b[0m \\u001b[39m# Data Processing\\u001b[39;00m\\n\\u001b[0;32m 3\\u001b[0m \\u001b[39mimport\\u001b[39;00m \\u001b[39mpandas\\u001b[39;00m \\u001b[39mas\\u001b[39;00m \\u001b[39mpd\\u001b[39;00m\\n\\nFile \\u001b[1;32mc:\\\\Users\\\\wen\\\\OneDrive\\\\Desktop\\\\Colab_Notebooks\\\\.venv\\\\Lib\\\\site-packages\\\\imblearn\\\\__init__.py:52\\u001b[0m\\n\\u001b[0;32m 48\\u001b[0m sys\\u001b[39m.\\u001b[39mstderr\\u001b[39m.\\u001b[39mwrite(\\u001b[39m\\&quot;\\u001b[39m\\u001b[39mPartial import of imblearn during the build process.\\u001b[39m\\u001b[39m\\\\n\\u001b[39;00m\\u001b[39m\\&quot;\\u001b[39m)\\n\\u001b[0;32m 49\\u001b[0m \\u001b[39m# We are not importing the rest of scikit-learn during the build\\u001b[39;00m\\n\\u001b[0;32m 50\\u001b[0m \\u001b[39m# process, as it may not be compiled yet\\u001b[39;00m\\n\\u001b[0;32m 51\\u001b[0m \\u001b[39melse\\u001b[39;00m:\\n\\u001b[1;32m---&gt; 52\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m (\\n\\u001b[0;32m 53\\u001b[0m combine,\\n\\u001b[0;32m 54\\u001b[0m ensemble,\\n\\u001b[0;32m 55\\u001b[0m exceptions,\\n\\u001b[0;32m 56\\u001b[0m metrics,\\n\\u001b[0;32m 57\\u001b[0m over_sampling,\\n\\u001b[0;32m 58\\u001b[0m pipeline,\\n\\u001b[0;32m 59\\u001b[0m tensorflow,\\n\\u001b[0;32m 60\\u001b[0m under_sampling,\\n\\u001b[0;32m 61\\u001b[0m utils,\\n\\u001b[0;32m 62\\u001b[0m )\\n\\u001b[0;32m 63\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39m_version\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m __version__\\n\\u001b[0;32m 64\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39mbase\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m FunctionSampler\\n\\nFile \\u001b[1;32mc:\\\\Users\\\\wen\\\\OneDrive\\\\Desktop\\\\Colab_Notebooks\\\\.venv\\\\Lib\\\\site-packages\\\\imblearn\\\\combine\\\\__init__.py:5\\u001b[0m\\n\\u001b[0;32m 1\\u001b[0m \\u001b[39m\\&quot;\\&quot;\\&quot;The :mod:`imblearn.combine` provides methods which combine\\u001b[39;00m\\n\\u001b[0;32m 2\\u001b[0m \\u001b[39mover-sampling and under-sampling.\\u001b[39;00m\\n\\u001b[0;32m 3\\u001b[0m \\u001b[39m\\&quot;\\&quot;\\&quot;\\u001b[39;00m\\n\\u001b[1;32m----&gt; 5\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39m_smote_enn\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m SMOTEENN\\n\\u001b[0;32m 6\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39m_smote_tomek\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m SMOTETomek\\n\\u001b[0;32m 8\\u001b[0m __all__ \\u001b[39m=\\u001b[39m [\\u001b[39m\\&quot;\\u001b[39m\\u001b[39mSMOTEENN\\u001b[39m\\u001b[39m\\&quot;\\u001b[39m, \\u001b[39m\\&quot;\\u001b[39m\\u001b[39mSMOTETomek\\u001b[39m\\u001b[39m\\&quot;\\u001b[39m]\\n\\nFile \\u001b[1;32mc:\\\\Users\\\\wen\\\\OneDrive\\\\Desktop\\\\Colab_Notebooks\\\\.venv\\\\Lib\\\\site-packages\\\\imblearn\\\\combine\\\\_smote_enn.py:12\\u001b[0m\\n\\u001b[0;32m 9\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39msklearn\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mbase\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m clone\\n\\u001b[0;32m 10\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39msklearn\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mutils\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m check_X_y\\n\\u001b[1;32m---&gt; 12\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mbase\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m BaseSampler\\n\\u001b[0;32m 13\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mover_sampling\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m SMOTE\\n\\u001b[0;32m 14\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mover_sampling\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mbase\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m BaseOverSampler\\n\\nFile \\u001b[1;32mc:\\\\Users\\\\wen\\\\OneDrive\\\\Desktop\\\\Colab_Notebooks\\\\.venv\\\\Lib\\\\site-packages\\\\imblearn\\\\base.py:21\\u001b[0m\\n\\u001b[0;32m 18\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39msklearn\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mutils\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mmulticlass\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m check_classification_targets\\n\\u001b[0;32m 20\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39mutils\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m check_sampling_strategy, check_target_type\\n\\u001b[1;32m---&gt; 21\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39mutils\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39m_param_validation\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m validate_parameter_constraints\\n\\u001b[0;32m 22\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39m.\\u001b[39;00m\\u001b[39mutils\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39m_validation\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m ArraysTransformer\\n\\u001b[0;32m 25\\u001b[0m \\u001b[39mclass\\u001b[39;00m \\u001b[39mSamplerMixin\\u001b[39;00m(BaseEstimator, metaclass\\u001b[39m=\\u001b[39mABCMeta):\\n\\nFile \\u001b[1;32mc:\\\\Users\\\\wen\\\\OneDrive\\\\Desktop\\\\Colab_Notebooks\\\\.venv\\\\Lib\\\\site-packages\\\\imblearn\\\\utils\\\\_param_validation.py:908\\u001b[0m\\n\\u001b[0;32m 906\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39msklearn\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mutils\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39m_param_validation\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m generate_valid_param \\u001b[39m# noqa\\u001b[39;00m\\n\\u001b[0;32m 907\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39msklearn\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mutils\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39m_param_validation\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m validate_parameter_constraints \\u001b[39m# noqa\\u001b[39;00m\\n\\u001b[1;32m--&gt; 908\\u001b[0m \\u001b[39mfrom\\u001b[39;00m \\u001b[39msklearn\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39mutils\\u001b[39;00m\\u001b[39m.\\u001b[39;00m\\u001b[39m_param_validation\\u001b[39;00m \\u001b[39mimport\\u001b[39;00m (\\n\\u001b[0;32m 909\\u001b[0m HasMethods,\\n\\u001b[0;32m 910\\u001b[0m Hidden,\\n\\u001b[0;32m 911\\u001b[0m Interval,\\n\\u001b[0;32m 912\\u001b[0m Options,\\n\\u001b[0;32m 913\\u001b[0m StrOptions,\\n\\u001b[0;32m 914\\u001b[0m _ArrayLikes,\\n\\u001b[0;32m 915\\u001b[0m _Booleans,\\n\\u001b[0;32m 916\\u001b[0m _Callables,\\n\\u001b[0;32m 917\\u001b[0m _CVObjects,\\n\\u001b[0;32m 918\\u001b[0m _InstancesOf,\\n\\u001b[0;32m 919\\u001b[0m _IterablesNotString,\\n\\u001b[0;32m 920\\u001b[0m _MissingValues,\\n\\u001b[0;32m 921\\u001b[0m _NoneConstraint,\\n\\u001b[0;32m 922\\u001b[0m _PandasNAConstraint,\\n\\u001b[0;32m 923\\u001b[0m _RandomStates,\\n\\u001b[0;32m 924\\u001b[0m _SparseMatrices,\\n\\u001b[0;32m 925\\u001b[0m _VerboseHelper,\\n\\u001b[0;32m 926\\u001b[0m make_constraint,\\n\\u001b[0;32m 927\\u001b[0m validate_params,\\n\\u001b[0;32m 928\\u001b[0m )\\n\\n\\u001b[1;31mImportError\\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (c:\\\\Users\\\\wen\\\\OneDrive\\\\Desktop\\\\Colab_Notebooks\\\\.venv\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\_param_validation.py)&quot; } The versions of the modules are as follows: scikit-learn 1.3.0 imblearn 0.0 imbalanced-learn 0.10.1",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I was having the same issue, downgrading to scikit-learn 1.2.2 fixed it for me",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For those who came across this issue on Jupyter notebook, here's how I solved it !pip uninstall scikit-learn --yes !pip uninstall imblearn --yes !pip install scikit-learn==1.2.2 !pip install imblearn Restart anaconda navigator It should work",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "scikit-learn",
        "imblearn"
      ],
      "question_score": 9,
      "answer_score": 11,
      "created": "2023-07-01T08:52:15",
      "question_id": 76593906,
      "answer_id": 76596074
    }
  },
  {
    "question": "import torch: How to fix OSError WinError 126, error loading fbgemm.dll or dependencies",
    "expected_answer": "I've faced this problem and reinstalled pytorch and installed VC_Redist after trying many methods but all failed to run Pytorch properly. But the issue solved by simply downloading libomp140.x86_64.dll and place it in 'C:\\Windows\\System32' and it finally works after long pain of trying this and that!",
    "context_chunks": [
      {
        "text": "I installed the modules below: conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 -c pytorch Then installed numpy, pandas, matplotlib, seaborn, sickit-learn, pyedflib in this environment. Yet upon import it seems some files are missing: OSError Traceback (most recent call last) Cell In[3], line 1 ----&gt; 1 import torch 2 from torch import nn 3 import numpy as np File d:\\anaconda3\\envs\\RN\\lib\\site-packages\\torch\\__init__.py:141 139 err = ctypes.WinError(ctypes.get_last_error()) 140 err.strerror += f' Error loading &quot;{dll}&quot; or one of its dependencies.' --&gt; 141 raise err 143 kernel32.SetErrorMode(prev_error_mode) 146 def _preload_cuda_deps(lib_folder, lib_name): OSError: [WinError 126] can't find this module. Error loading &quot;d:\\anaconda3\\envs\\RN\\lib\\site-packages\\torch\\lib\\fbgemm.dll&quot; or one of its dependencies. I reinstalled torch, but I can't solve it. And I even checked the file 'fbgemm.dll', it is there.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I've faced this problem and reinstalled pytorch and installed VC_Redist after trying many methods but all failed to run Pytorch properly. But the issue solved by simply downloading libomp140.x86_64.dll and place it in 'C:\\Windows\\System32' and it finally works after long pain of trying this and that!",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I was also facing the same issue. But the issue got resolved when I uninstalled torch 2.4.0 and installed torch 2.3.1. The issue is because of compatibility.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pytorch",
        "conda",
        "python-import",
        "oserror"
      ],
      "question_score": 9,
      "answer_score": 34,
      "created": "2024-03-06T12:32:10",
      "question_id": 78114412,
      "answer_id": 78838829
    }
  },
  {
    "question": "No module named &#39;keras.wrappers&#39;",
    "expected_answer": "This works for me pip install keras==2.12.0 Another Approach you can try pip uninstall tensorflow pip install tensorflow==2.12.0",
    "context_chunks": [
      {
        "text": "I have this code on google colab which allows me to optimise an LSTM model using gridsearchCV, but recently an error message has appeared: ModuleNotFoundError: No module named 'keras.wrappers'. is there another module other than 'keras.wrappers' that allows the code to be restarted? Code: from keras.layers import Dense, LSTM, Dropout from keras import optimizers from sklearn.model_selection import GridSearchCV from keras.wrappers.scikit_learn import KerasRegressor def create_model(unit, dropout_rate, lr ): model=Sequential() model.add(LSTM(unit,return_sequences=True, input_shape=(1,5))) model.add(Dropout(dropout_rate)) model.add(LSTM(unit)) model.add(Dropout(dropout_rate)) model.add(Dense(1)) adam= optimizers.Adam(lr) model.compile(optimizer=adam, loss='mean_squared_error') return model my_regressor = KerasRegressor(build_fn=create_model, verbose=2) grid_param_LSTM = { 'unit': [50, 70, 120], 'batch_size': [12, 24, 48], 'epochs': [200], 'lr': [0.001, 0.01, 0.1], 'dropout_rate':[0.1, 0.2, 0.3] } grid_GBR = GridSearchCV(estimator=my_regressor, param_grid = grid_param_LSTM, scoring = 'neg_root_mean_squared_error', cv = 2) grid_GBR.fit(X_train, y_train) print(&quot;Best: %f using %s&quot; % (grid_GBR.best_score_, grid_GBR.best_params_))",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This works for me pip install keras==2.12.0 Another Approach you can try pip uninstall tensorflow pip install tensorflow==2.12.0",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I've had the same problem and I solved adding above import an instruction to install scikeras has KerasClassifier !pip install scikeras from scikeras.wrappers import KerasClassifier https://adriangb.com/scikeras/stable/migration.html",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "tensorflow",
        "keras",
        "lstm"
      ],
      "question_score": 9,
      "answer_score": 12,
      "created": "2023-09-14T10:42:00",
      "question_id": 77104125,
      "answer_id": 77285146
    }
  },
  {
    "question": "Reverse (flip) right-left(anti-)diagonals of a non-square numpy array",
    "expected_answer": "A simple way to break down this problem, without using any loops: Step 1: Transpose the upper and lower triangles (red and blue) Step 2: Flip the &quot;parallelogram&quot; in the middle (purple) When the matrix is square (h=w), just transpose is enough. I'm showing an example when the matrix is &quot;tall&quot; (h &gt; w): def flip_along_diag(a): h, w = a.shape out = np.zeros_like(a) # step 1: flip upper and lower triangles u, v = np.triu_indices(w, 0) p, q = np.tril_indices(w, 0) out[u, w-v-1] = a[w-v-1, u] out[h-w+p, w-1-q] = a[h-1-q, p] # step 2: flip the &quot;parallelogram&quot; of size (r, s) in the middle r, s = h - w - 1, w if r &gt;= 1: i, j = np.mgrid[:r, :s] i = i + np.arange(s)[::-1] + 1 out[i, j] = np.fliplr(a[i, j]) return out I believe the same algorithm can be extended when the matrix is wide (w &gt; h) instead of tall, with a little modification. Update: For handling the case of wide (w &gt; h) matrices, we can simply first check if w &gt; h, and then simply modify the indices as needed. def flip_along_diag(a): h, w = a.shape if h == w: # for square matrix, a.T is enough return a.T out = np.zeros_like(a) k, d = min(h, w), abs(h - w) # step 1: flip two triangles u, v = np.triu_indices(k, 0) p, q = np.tril_indices(k, 0) if h &gt; w: # h &gt; w: upper and lower triangle out[u, k-v-1] = a[k-v-1, u] out[d+p, w-q-1] = a[h-q-1, p] else: # w &gt; h: left and right triangle out[u, k-v-1] = a[k-v-1, u] out[p, w-q-1] = a[h-q-1, d+p] # step 2: flip parallelogram if h &gt; w: # h &gt; w: flip left-right r, s = h - w - 1, w if r &gt;= 1: i, j = np.mgrid[:r, :s] i = i + np.arange(s)[::-1] + 1 out[i, j] = np.fliplr(a[i, j]) else: # w &gt; h: flip up-down r, s = h, w - h - 1 if s &gt;= 1: i, j = np.mgrid[:r, :s] j = j + np.arange(r)[::-1, None] + 1 out[i, j] = np.flipud(a[i, j]) return out",
    "context_chunks": [
      {
        "text": "What I am after is Python code able to reverse the order of the values in each of the array anti-diagonals in a numpy array. I have already tried various combinations of np.rot90, np.fliplr, np.transpose, np.flipud but none is able to give me the original shape of the 5x3 array with all the anti-diagonals reversed. Any idea how to accomplish this? Example: [[ 1 2 4] [ 3 5 7] [ 6 8 10] [ 9 11 13] [12 14 15]] Should become: [[ 1 3 6] [ 2 5 9] [ 4 8 12] [ 7 11 14] [10 13 15]] I suppose it must be easy, but somehow I have yet failed to find how to do it efficiently on arrays with millions of values. Inspired by the already provided answers (status 2024-05-23 11:37 CET) and re-thinking what would be the most efficient way of getting the required transformation done it seems that giving a simple function taking two indices : iRow, jColumn of a value in an array and returning the required i,j indices to access the array as if it were flipped/reversed over the diagonals will provide fastest results. With such function for the over the diagonals flipped version of the array would be getting the right values without operating on the array as easy as in a trivial case of one-based and column/row based access to array values demonstrated below: import numpy as np srcArr = np.array([[ 1, 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24]]) def ijOfArrayValueGivenOneBasedColumnRowBasedIndices(i, j): return ( j - 1, i - 1 ) print( srcArr[ ijOfArrayValueGivenOneBasedColumnRowBasedIndices( 3,4)] ) # gives 21 print( srcArr[3,4] ) # gives 23 From this perspective the question comes down to providing a function ijIndicesToSourceArray_gettingValueOfSourceArrayWithReversedRightLeftAntiDiagonalsAt(i,j,arrShapeRows,arrShapeColumns)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "A simple way to break down this problem, without using any loops: Step 1: Transpose the upper and lower triangles (red and blue) Step 2: Flip the &quot;parallelogram&quot; in the middle (purple) When the matrix is square (h=w), just transpose is enough. I'm showing an example when the matrix is &quot;tall&quot; (h &gt; w): def flip_along_diag(a): h, w = a.shape out = np.zeros_like(a) # step 1: flip upper and lower triangles u, v = np.triu_indices(w, 0) p, q = np.tril_indices(w, 0) out[u, w-v-1] = a[w-v-1, u] out[h-w+p, w-1-q] = a[h-1-q, p] # step 2: flip the &quot;parallelogram&quot; of size (r, s) in the middle r, s = h - w - 1, w if r &gt;= 1: i, j = np.mgrid[:r, :s] i = i + np.arange(s)[::-1] + 1 out[i, j] = np.fliplr(a[i, j]) return out I believe the same algorithm can be extended when the matrix is wide (w &gt; h) instead of tall, with a little modification. Update: For handling the case of wide (w &gt; h) matrices, we can simply first check if w &gt; h, and then simply modify the indices as needed. def flip_along_diag(a): h, w = a.shape if h == w: # for square matrix, a.T is enough return a.T out = np.zeros_like(a) k, d = min(h, w), abs(h - w) # step 1: flip two triangles u, v = np.triu_indices(k, 0) p, q = np.tril_indices(k, 0) if h &gt; w: # h &gt; w: upper and lower triangle out[u, k-v-1] = a[k-v-1, u] out[d+p, w-q-1] = a[h-q-1, p] else: # w &gt; h: left and right triangle out[u, k-v-1] = a[k-v-1, u] out[p, w-q-1] = a[h-q-1, d+p] # step 2: flip parallelogram if h &gt; w: # h &gt; w: flip left-right r, s = h - w - 1, w if r &gt;= 1: i, j = np.mgrid[:r, :s] i = i + np.arange(s)[::-1] + 1 out[i, j] = np.fliplr(a[i, j]) else: # w &gt; h: flip up-down r, s = h, w - h - 1 if s &gt;= 1: i, j = np.mgrid[:r, :s] j = j + np.arange(r)[::-1, None] + 1 out[i, j] = np.flipud(a[i, j]) return out",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I also have not found a Numpy method to do this. However, the indices can be generated using this function: import numpy as np def flip_inds(a): m, n = a.shape i = np.arange(m).reshape(-1, 1) j = np.arange(n).reshape(1, -1) jp = np.where( i + j &lt; min(m, n), i, np.where( i + j &lt; max(m, n), ( i + j - (m - 1 - i) if n &gt; m else n - 1 - j ), n - (m - i) ) ) ip = i + j - jp return ip, jp ip, jp = flip_inds(a) a[ip, jp] # diagonals flipped but I would recommend Numba for this because it will be more efficient: import numba as nb @nb.njit def flip(a): m, n = a.shape b = np.empty_like(a) for i in range(m): for j in range(n): # Along a diagonal i + j is constant # denote by (ip, jp) the indices of the value in the input that should be put in the (i, j) entry of the output # We know that: i + j = ip + jp jp = ( # Case 1: (i, j) in upper left triangle # Here we just do the regular transpose i if i + j &lt; min(m, n) else ( # Case 2: (i, j) neither in upper left or lower right triangle # in this case what we do depends on if n &gt; m or not # If n &gt; m we are bounded by m, locate ip at i steps from the maximum i index: # ip = m - 1 - i # from that we can obtain jp as i + j - ip: i + j - (m - 1 - i) if n &gt; m else # If m &gt; n locate the jp index j steps from the max j index n - 1 - j ) if i + j &lt; max(m, n) else # Case 3: Lower right corner, here we can do a transpose again, but with indices going backwards n - (m - i) ) ip = i + j - jp b[i, j] = a[ip, jp] return b flip(a) # diagonals flipped Without comments (easier to copy): @nb.njit def flip(a): m, n = a.shape b = np.empty_like(a) for i in range(m): for j in range(n): jp = ( i if i + j &lt; min(m, n) else ( i + j - (m - 1 - i) if n &gt; m else n - 1 - j ) if i + j &lt; max(m, n) else n - (m - i) ) ip = i + j - jp b[i, j] = a[ip, jp] return b",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "matrix",
        "diagonal"
      ],
      "question_score": 9,
      "answer_score": 9,
      "created": "2024-05-22T16:32:29",
      "question_id": 78518903,
      "answer_id": 78545910
    }
  },
  {
    "question": "The following argument(s) are not supported with the native Keras format: [&#39;options&#39;]",
    "expected_answer": "As I mentioned in the comments, there seems to be a weird behaviour related to keras saving and also versioning of TF/Keras. I could replicate your error when running TF/Keras with version 2.13 (newest right now) on colab. Standard install on colab is 2.12, where the error doesn't come up. So one solution would be to downgrade TF/Keras to 2.12.x, or change keras.callbacks.ModelCheckpoint( filepath=&quot;convnet_from_scratch.keras&quot;, ..) to keras.callbacks.ModelCheckpoint( filepath=&quot;convnet_from_scratch.x&quot;, ..) where x stands for whatever you fancy (NOT &quot;keras&quot;) to not save in the .keras format.",
    "context_chunks": [
      {
        "text": "I am building a Keras deep learning Algorithm on dogs vs cats dataset. I am able to run my code in colab. But in Jupyter lab I am getting this error. The following argument(s) are not supported with the native Keras format: ['options'] Below is the code: import os import shutil import pathlib original_dir = pathlib.Path(&quot;/content/drive/MyDrive/Reva/dogs_vs_cats/train/train&quot;) new_base_dir = pathlib.Path(&quot;/content/drive/MyDrive/Reva/dogs_vs_cats/&quot;) def make_subset(subset_name, start_index, end_index): for category in (&quot;cat&quot;, &quot;dog&quot;): dir = new_base_dir / subset_name / category # Check if the folder exists and delete it if it does if os.path.exists(dir): shutil.rmtree(dir) # Create the folder again os.makedirs(dir) fnames = [f&quot;{category}.{i}.jpg&quot; for i in range(start_index, end_index)] for fname in fnames: shutil.copyfile(src=original_dir / fname, dst=dir / fname) make_subset(&quot;train&quot;, start_index=0, end_index=1000) make_subset(&quot;validation&quot;, start_index=1000, end_index=1500) make_subset(&quot;test&quot;, start_index=1500, end_index=2500) from tensorflow.keras.utils import image_dataset_from_directory train_dataset = image_dataset_from_directory( new_base_dir / &quot;train&quot;, image_size=(180, 180), batch_size=32) validation_dataset = image_dataset_from_directory( new_base_dir / &quot;validation&quot;, image_size=(180, 180), batch_size=32) test_dataset = image_dataset_from_directory( new_base_dir / &quot;test&quot;, image_size=(180, 180), batch_size=32) from tensorflow import keras from tensorflow.keras import layers inputs = keras.Input(shape=(180, 180, 3)) x = layers.Rescaling(1./255)(inputs) x = layers.Conv2D(filters=32, kernel_size=3, activation=&quot;relu&quot;)(x) x = layers.MaxPooling2D(pool_size=2)(x) x = layers.Conv2D(filters=64, kernel_size=3, activation=&quot;relu&quot;)(x) x = layers.MaxPooling2D(pool_size=2)(x) x = layers.Conv2D(filters=128, kernel_size=3, activation=&quot;relu&quot;)(x) x = layers.MaxPooling2D(pool_size=2)(x) x = layers.Conv2D(filters=256, kernel_size=3, activation=&quot;relu&quot;)(x) x = layers.MaxPooling2D(pool_size=2)(x) x = layers.Conv2D(filters=256, kernel_size=3, activation=&quot;relu&quot;)(x) x = layers.Flatten()(x) outputs = layers.Dense(1, activation=&quot;sigmoid&quot;)(x) model = keras.Model(inputs=inputs, outputs=outputs) model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;, metrics=[&quot;accuracy&quot;]) callbacks = [ keras.callbacks.ModelCheckpoint( filepath=&quot;convnet_from_scratch.keras&quot;, save_best_only=True, monitor=&quot;val_loss&quot;) ] history = model.fit( train_dataset, epochs=30, validation_data=validation_dataset, callbacks=callbacks) I need to know how to resolve the above code. Any suggestions to improve the time required to run the code is also welcome.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "As I mentioned in the comments, there seems to be a weird behaviour related to keras saving and also versioning of TF/Keras. I could replicate your error when running TF/Keras with version 2.13 (newest right now) on colab. Standard install on colab is 2.12, where the error doesn't come up. So one solution would be to downgrade TF/Keras to 2.12.x, or change keras.callbacks.ModelCheckpoint( filepath=&quot;convnet_from_scratch.keras&quot;, ..) to keras.callbacks.ModelCheckpoint( filepath=&quot;convnet_from_scratch.x&quot;, ..) where x stands for whatever you fancy (NOT &quot;keras&quot;) to not save in the .keras format.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Changing the format from .keras to .tf works for me (note the save format line): callbacks = [ keras.callbacks.ModelCheckpoint( filepath=&quot;convnet_from_scratch.tf&quot;, save_best_only=True, monitor=&quot;val_loss&quot;, save_format=&quot;tf&quot;) ] history = model.fit( train_dataset, epochs=30, validation_data=validation_dataset, callbacks=callbacks) and later you'd want: test_model = keras.models.load_model(&quot;convnet_from_scratch.tf&quot;) test_loss, test_acc = test_model.evaluate(test_dataset) print(f&quot;Test accuracy: {test_acc:.3f}&quot;)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "tensorflow",
        "keras"
      ],
      "question_score": 9,
      "answer_score": 15,
      "created": "2023-07-17T05:04:27",
      "question_id": 76701617,
      "answer_id": 76723205
    }
  },
  {
    "question": "How to see the Embedding of the documents with Chroma (or any other DB) saved in Lang Chain?",
    "expected_answer": "You just need to specify that you want the embeddings as well when using .get # Get all embeddings db._collection.get(include=['embeddings']) # Get embeddings by document_id db._collection.get(ids=['doc0', ..., 'docN'], include=['embeddings'])",
    "context_chunks": [
      {
        "text": "I can see everything but the Embedding of the documents when I used Chroma with Langchain and OpenAI embeddings. It always show me None for that Here is the code: for db_collection_name in tqdm([&quot;class1-sub2-chap3&quot;, &quot;class2-sub3-chap4&quot;]): documents = [] doc_ids = [] for doc_index in range(3): cl, sub, chap = db_collection_name.split(&quot;-&quot;) content = f&quot;This is {db_collection_name}-doc{doc_index}&quot; doc = Document(page_content=content, metadata={&quot;chunk_num&quot;: doc_index, &quot;chapter&quot;:chap, &quot;class&quot;:cl, &quot;subject&quot;:sub}) documents.append(doc) doc_ids.append(str(doc_index)) # # Initialize a Chroma instance with the original document db = Chroma.from_documents( collection_name=db_collection_name, documents=documents, ids=doc_ids, embedding=embeddings, persist_directory=&quot;./data&quot;) db.persist() when I do db.get(), I see everything as expected except embedding is None. {'ids': ['0', '1', '2'], 'embeddings': None, 'documents': ['This is class1-sub2-chap3-doc0', 'This is class1-sub2-chap3-doc1', 'This is class1-sub2-chap3-doc2'], 'metadatas': [{'chunk_num': 0, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'}, {'chunk_num': 1, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'}, {'chunk_num': 2, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'}]} My embeddings is also working fine as it returns: len(embeddings.embed_documents([&quot;EMBED THIS&quot;])[0]) &gt;&gt; 1536 also, in my ./data directory I have Embedding file as chroma-embeddings.parquet I tried the example with example given in document but it shows None too # Import Document class from langchain.docstore.document import Document # Initial document content and id initial_content = &quot;This is an initial document content&quot; document_id = &quot;doc1&quot; # Create an instance of Document with initial content and metadata original_doc = Document(page_content=initial_content, metadata={&quot;page&quot;: &quot;0&quot;}) # Initialize a Chroma instance with the original document new_db = Chroma.from_documents( collection_name=&quot;test_collection&quot;, documents=[original_doc], embedding=OpenAIEmbeddings(), # using the same embeddings as before ids=[document_id], ) Here also new_db.get() gives me None",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You just need to specify that you want the embeddings as well when using .get # Get all embeddings db._collection.get(include=['embeddings']) # Get embeddings by document_id db._collection.get(ids=['doc0', ..., 'docN'], include=['embeddings'])",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Here is the solution loader = DirectoryLoader(&quot;document&quot;, glob=&quot;**/*.*&quot;) files = loader.load() text_splitter = RecursiveCharacterTextSplitter( chunk_size = 1500, chunk_overlap = 150 ) docs = text_splitter.split_documents(files) documents = [Document(page_content=doc.page_content, metadata={&quot;topic&quot;:f&quot;John's story{i}&quot;}) for i, doc in enumerate(docs)] db = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=&quot;db&quot;) Please try to run this code. I have checked that the metadata has added. Here is the result image.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "nlp",
        "openai-api",
        "langchain",
        "chromadb"
      ],
      "question_score": 9,
      "answer_score": 21,
      "created": "2023-06-01T07:07:40",
      "question_id": 76379440,
      "answer_id": 76386231
    }
  },
  {
    "question": "Python: fastest way of checking if there are more than x files in a folder",
    "expected_answer": "There is indeed another function introduced by PEP471 : os.scandir(path) As it returns a generator, no list will be created and worse case scenario (huge directory) will still be lightweight. Its higher level interface os.walk(path) will allow you to go through a directory without having to list all of it. Here is a code example for your specific case : import os MINIMUM_SIZE = 2 file_count = 0 for entry in os.scandir('.'): if entry.is_file(): file_count += 1 if file_count == MINIMUM_SIZE: break enough_files = (file_count == MINIMUM_SIZE)",
    "context_chunks": [
      {
        "text": "I am looking for a very rapid way to check whether a folder contains more than 2 files. I worry that len(os.listdir('/path/')) &gt; 2 may become very slow if there are a lot of files in /path/, especially since this function will be called frequently by multiple processes at a time.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "There is indeed another function introduced by PEP471 : os.scandir(path) As it returns a generator, no list will be created and worse case scenario (huge directory) will still be lightweight. Its higher level interface os.walk(path) will allow you to go through a directory without having to list all of it. Here is a code example for your specific case : import os MINIMUM_SIZE = 2 file_count = 0 for entry in os.scandir('.'): if entry.is_file(): file_count += 1 if file_count == MINIMUM_SIZE: break enough_files = (file_count == MINIMUM_SIZE)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "To get the fastest it's probably something hacky. My guess was: def iterdir_approach(path): iter_of_files = (x for x in Path(path).iterdir() if x.isfile()) try: next(iter_of_files) next(iter_of_files) next(iter_of_files) return True except: return False We create a generator and try to exhaust it, catching the thrown exception if necessary. To profile the approaches we create a bunch of directories with a bunch of files in them : import shutil import tempfile import timeit import matplotlib.pyplot as plt from pathlib import Path def create_temp_directory(num_directories): temp_dir = tempfile.mkdtemp() for i in range(num_directories): dir_path = os.path.join(temp_dir, f&quot;subdir_{i}&quot;) os.makedirs(dir_path) for j in range(random.randint(0,i)): file_path = os.path.join(dir_path, f&quot;file_{j}.txt&quot;) with open(file_path, 'w') as file: file.write(&quot;Sample content&quot;) return temp_dir We define the various approaches (Copied the other two from the answers to the question: def iterdir_approach(path): #@swozny iter_of_files = (x for x in Path(path).iterdir() if x.isfile()) try: next(iter_of_files) next(iter_of_files) next(iter_of_files) return True except: return False def len_os_dir_approach(path): #@bluppfisk return len(os.listdir(path)) &gt; 2 def check_files_os_scandir_approach(path): #@PoneyUHC MINIMUM_SIZE = 3 file_count = 0 for entry in os.scandir(path): if entry.is_file(): file_count += 1 if file_count == MINIMUM_SIZE: return True return False def path_resolve_approach(path): #@matleg directory_path = Path(path).resolve() nb_files = 0 enough_files = False for file_path in directory_path.glob(&quot;*&quot;): if file_path.is_file(): nb_files += 1 if nb_files &gt; 2: return True return False def dilettant_approach(path): #@dilettant gen = os.scandir(path) # OP states only files in folder /path/ enough = 3 # At least 2 files has_enough = len(list(itertools.islice(gen, enough))) &gt;= enough return has_enough def adrian_ang_approach(path): #@adrian_ang count = 0 with os.scandir(path) as entries: for entry in entries: if entry.is_file(): count += 1 if count &gt; 2: return True return False Then we profile the code using timeit.timeit and plot the execution times for various amounts of directories: num_directories_list = [10, 50, 100, 200, 500,1000] approach1_times = [] approach2_times = [] approach3_times = [] approach4_times = [] approach5_times = [] approach6_times = [] for num_directories in num_directories_list: temp_dir = create_temp_directory(num_directories) subdir_paths = [str(p) for p in Path(create_temp_directory(num_directories)).iterdir()] approach1_time = timeit.timeit(lambda: [iterdir_approach(path)for path in subdir_paths], number=5) approach2_time = timeit.timeit(lambda: [check_files_os_scandir_approach(path)for path in subdir_paths], number=5) approach3_time = timeit.timeit(lambda: [path_resolve_approach(path)for path in subdir_paths], number=5) approach4_time = timeit.timeit(lambda: [len_os_dir_approach(path)for path in subdir_paths], number=5) approach5_time = timeit.timeit(lambda: [dilettant_approach(path)for path in subdir_paths], number=5) approach6_time = timeit.timeit(lambda: [adrian_ang_approach(path)for path in subdir_paths], number=5) approach1_times.append(approach1_time) approach2_times.append(approach2_time) approach3_times.append(approach3_time) approach4_times.append(approach4_time) approach5_times.append(approach5_time) approach6_times.append(approach6_time) shutil.rmtree(temp_dir) Visualization of the results plt.plot(num_directories_list, approach1_times, label='iterdir_approach') plt.plot(num_directories_list, approach2_times, label='check_files_os_scandir_approach') plt.plot(num_directories_list, approach3_times, label='path_resolve_approach') plt.plot(num_directories_list, approach4_times, label='os_dir_approach') plt.plot(num_directories_list, approach5_times, label='dilettant_approach') plt.plot(num_directories_list, approach6_times, label='adrian_ang_approach') plt.xlabel('Number of Directories') plt.ylabel('Execution Time (seconds)') plt.title('Performance Comparison') plt.legend() plt.show() Closeup of best 3 solutions:",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 9,
      "answer_score": 10,
      "created": "2023-07-04T14:33:11",
      "question_id": 76613672,
      "answer_id": 76613784
    }
  },
  {
    "question": "How to plot polygons from categorical grid points in matplotlib? (phase-diagram generation)",
    "expected_answer": "I am not sure if you can easily get a representation with contiguous polygons, however you could easily get the bounding polygon from a set of points using shapely.convex_hull: import shapely import matplotlib.pyplot as plt f, ax = plt.subplots(figsize=(8, 8)) for (name, color), coords in df.groupby(['label', 'color'])[['x', 'y']]: polygon = shapely.convex_hull(shapely.MultiPoint(coords.to_numpy())) ax.fill(*polygon.exterior.xy, color=color) ax.annotate(name, polygon.centroid.coords[0], ha='center', va='center') If you want the shapely polygons: polygons = {k: shapely.convex_hull(shapely.MultiPoint(g.to_numpy())) for k, g in df.groupby(['label', 'color'])[['x', 'y']]} Output: contiguous polygons To have contiguous polygons you can use the same strategy after adding points with a greater density and assigning them to their closest counterpart with a KDTree: from scipy.spatial import KDTree # interpolate points on the initial polygons polygons = {k: shapely.convex_hull(shapely.MultiPoint(g.to_numpy())) for k, g in df.groupby('label')[['x', 'y']]} def interp_ext(shape): try: return np.c_[shape.xy].T except NotImplementedError: pass e = shape.exterior if hasattr(shape, 'exterior') else shape points = e.interpolate(np.linspace(0, e.length, 1000)) return np.c_[Polygon(points).exterior.xy].T df2 = (pd.DataFrame([(l, *interp_ext(p)) for l, p in polygons.items()], columns=['label', 'x', 'y']) .merge(df[['label', 'color']], on='label') .explode(['x', 'y']) ) # get bounding values xmin, ymin, xmax, ymax = df[['x', 'y']].agg(['min', 'max']).values.ravel() # create a grid with a higher density (here 10x) Xs = np.arange(xmin, xmax, 0.1) Ys = np.arange(ymin, ymax, 0.1) Xs, Ys = (x.ravel() for x in np.meshgrid(Xs, Ys)) grid = np.c_[Xs, Ys] # indentify closest reference point _, idx = KDTree(df2[['x', 'y']]).query(grid) # create new DataFrame with labels/colors df3 = pd.DataFrame(np.c_[grid, df2[['label', 'color']].to_numpy()[idx]], columns=['x', 'y', 'label', 'color'] ) # plot f, ax = plt.subplots(figsize=(8, 8)) for (name, color), coords in df3.groupby(['label', 'color'])[['x', 'y']]: polygon = shapely.convex_hull(shapely.MultiPoint(coords.to_numpy())) ax.fill(*polygon.exterior.xy, color=color) ax.annotate(name, polygon.centroid.coords[0], ha='center', va='center') Output: Another, faster, option could be to use a Voronoi diagram based on the original shapes. I found a library (voronoi-diagram-for-polygons) that does this but requires GeoPandas: import geopandas as gpd from longsgis import voronoiDiagram4plg from shapely import Polygon, convex_hull, coverage_union # create the initial convex hulls tmp = (df.groupby(['label', 'color']) .apply(lambda x: convex_hull(Polygon(x[['x', 'y']].to_numpy()))) .reset_index(name='geometry') ) # convert to geodataframe gdf = gpd.GeoDataFrame(tmp, geometry='geometry') # Split using a Voronoi diagram mask = Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)]) tmp = voronoiDiagram4plg(gdf, mask, densify=True) # plot tmp.plot(color=gdf['color']) Output:",
    "context_chunks": [
      {
        "text": "I have a dataframe that contains 1681 evenly distributed 2D grid points. Each data point has its x and y coordinates, a label representing its category (or phase), and a color for that category. x y label color 0 -40.0 -30.0 Fe #660066 1 -40.0 -29.0 Fe #660066 2 -40.0 -28.0 FeS #ff7f50 3 -40.0 -27.0 FeS #ff7f50 4 -40.0 -26.0 FeS #ff7f50 ... ... ... ... ... 1676 0.0 6.0 Fe2(SO4)3 #8a2be2 1677 0.0 7.0 Fe2(SO4)3 #8a2be2 1678 0.0 8.0 Fe2(SO4)3 #8a2be2 1679 0.0 9.0 Fe2(SO4)3 #8a2be2 1680 0.0 10.0 Fe2(SO4)3 #8a2be2 [1681 rows x 4 columns] I want to generate a polygon diagram that shows the linear boundary of each category (in my case also known as a &quot;phase diagram&quot;). Sor far I can only show this kind of diagram in a simple scatter plot like this: import matplotlib.pyplot as plt import pandas as pd plt.figure(figsize=(8., 8.)) for color in df.color.unique(): df_color = df[df.color==color] plt.scatter( x=df_color.x, y=df_color.y, c=color, s=100, label=df_color.label.iloc[0] ) plt.xlim([-40., 0.]) plt.ylim([-30., 10.]) plt.xlabel('Log pO2(g)') plt.ylabel('Log pSO2(g)') plt.legend(bbox_to_anchor=(1.05, 1.)) plt.show() However, what I want is a phase diagram with clear linear boundaries that looks something like this: Is there any way I can generate such phase diagram using matplotlib? Note that the boundary is not deterministic, especially when the grid points are not dense enough. Hence there needs to be some kind of heuristics, for example the boundary line should always lie in the middle of two neighboring points with different categories. I imagine there will be some sort of line fitting or interpolation needed, and matplotlib.patches.Polygon is probably useful here. For easy testing, I attach a code snippet for generating the data, but the polygon information shown below are not supposed to be used for generating the phase diagram import numpy as np import pandas as pd from shapely.geometry import Point, Polygon labels = ['Fe', 'Fe3O4', 'FeS', 'Fe2O3', 'FeS2', 'FeSO4', 'Fe2(SO4)3'] colors = ['#660066', '#b6fcd5', '#ff7f50', '#ffb6c1', '#c6e2ff', '#d3ffce', '#8a2be2'] polygons = [] polygons.append(Polygon([(-26.7243,-14.7423), (-26.7243,-30.0000), (-40.0000,-30.0000), (-40.0000,-28.0181)])) polygons.append(Polygon([(-18.1347,-0.4263), (-16.6048,1.6135), (-16.6048,-30.0000), (-26.7243,-30.0000), (-26.7243,-14.7423), (-18.1347,-0.4263)])) polygons.append(Polygon([(-18.1347,-0.4263), (-26.7243,-14.7423), (-40.0000,-28.0181), (-40.0000,-22.2917), (-18.1347,-0.4263)])) polygons.append(Polygon([(0.0000,-20.2615), (0.0000,-30.0000), (-16.6048,-30.0000), (-16.6048,1.6135), (-16.5517,1.6865), (-6.0517,-0.9385), (0.0000,-3.9643)])) polygons.append(Polygon([(-14.2390,10.0000), (-14.5829,7.5927), (-16.5517,1.6865), (-16.6048,1.6135), (-18.1347,-0.4263), (-40.0000,-22.2917), (-40.0000,10.0000)])) polygons.append(Polygon([(-6.0517,-0.9385), (-16.5517,1.6865), (-14.5829,7.5927), (-6.0517,-0.9385)])) polygons.append(Polygon([(0.0000,-3.9643), (-6.0517,-0.9385), (-14.5829,7.5927), (-14.2390,10.0000), (0.0000,10.0000)])) x_grid = np.arange(-40., 0.01, 1.) y_grid = np.arange(-30., 10.01, 1.) xy_grid = np.array(np.meshgrid(x_grid, y_grid)).T.reshape(-1, 2).tolist() data = [] for coords in xy_grid: point = Point(coords) for i, poly in enumerate(polygons): if poly.buffer(1e-3).contains(point): data.append({ 'x': point.x, 'y': point.y, 'label': labels[i], 'color': colors[i] }) break df = pd.DataFrame(data)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I am not sure if you can easily get a representation with contiguous polygons, however you could easily get the bounding polygon from a set of points using shapely.convex_hull: import shapely import matplotlib.pyplot as plt f, ax = plt.subplots(figsize=(8, 8)) for (name, color), coords in df.groupby(['label', 'color'])[['x', 'y']]: polygon = shapely.convex_hull(shapely.MultiPoint(coords.to_numpy())) ax.fill(*polygon.exterior.xy, color=color) ax.annotate(name, polygon.centroid.coords[0], ha='center', va='center') If you want the shapely polygons: polygons = {k: shapely.convex_hull(shapely.MultiPoint(g.to_numpy())) for k, g in df.groupby(['label', 'color'])[['x', 'y']]} Output: contiguous polygons To have contiguous polygons you can use the same strategy after adding points with a greater density and assigning them to their closest counterpart with a KDTree: from scipy.spatial import KDTree # interpolate points on the initial polygons polygons = {k: shapely.convex_hull(shapely.MultiPoint(g.to_numpy())) for k, g in df.groupby('label')[['x', 'y']]} def interp_ext(shape): try: return np.c_[shape.xy].T except NotImplementedError: pass e = shape.exterior if hasattr(shape, 'exterior') else shape points = e.interpolate(np.linspace(0, e.length, 1000)) return np.c_[Polygon(points).exterior.xy].T df2 = (pd.DataFrame([(l, *interp_ext(p)) for l, p in polygons.items()], columns=['label', 'x', 'y']) .merge(df[['label', 'color']], on='label') .explode(['x', 'y']) ) # get bounding values xmin, ymin, xmax, ymax = df[['x', 'y']].agg(['min', 'max']).values.ravel() # create a grid with a higher density (here 10x) Xs = np.arange(xmin, xmax, 0.1) Ys = np.arange(ymin, ymax, 0.1) Xs, Ys = (x.ravel() for x in np.meshgrid(Xs, Ys)) grid = np.c_[Xs, Ys] # indentify closest reference point _, idx = KDTree(df2[['x', 'y']]).query(grid) # create new DataFrame with labels/colors df3 = pd.DataFrame(np.c_[grid, df2[['label', 'color']].to_numpy()[idx]], columns=['x', 'y', 'label', 'color'] ) # plot f, ax = plt.subplots(figsize=(8, 8)) for (name, color), coords in df3.groupby(['label', 'color'])[['x', 'y']]: polygon = shapely.convex_hull(shapely.MultiPoint(coords.to_numpy())) ax.fill(*polygon.exterior.xy, color=color) ax.annotate(name, polygon.centroid.coords[0], ha='center', va='center') Output: Another, faster, option could be to use a Voronoi diagram based on the original shapes. I found a library (voronoi-diagram-for-polygons) that does this but requires GeoPandas: import geopandas as gpd from longsgis import voronoiDiagram4plg from shapely import Polygon, convex_hull, coverage_union # create the initial convex hulls tmp = (df.groupby(['label', 'color']) .apply(lambda x: convex_hull(Polygon(x[['x', 'y']].to_numpy()))) .reset_index(name='geometry') ) # convert to geodataframe gdf = gpd.GeoDataFrame(tmp, geometry='geometry') # Split using a Voronoi diagram mask = Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)]) tmp = voronoiDiagram4plg(gdf, mask, densify=True) # plot tmp.plot(color=gdf['color']) Output:",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I suggest the following algorithm to construct straight lines along borders between phases: Find points in between sample grid points that have different phases on each side. Store these points ALONG with the pair of phases that they separate. Sort the boundary points by their phase pairs. ( This ensures that all the points that separate any particular phase pair are stored in adjacent memory locations ) LOOP over each phase pair that share boundary points LOOP over points that separate this phase pair Find the two points between phase pair that are the furthest apart from each other construct boundary line connecting two furthest apart points merge close boundary line endpoints at their centroid Applying this algorithm to your sample data gives I implemented the algorithm in C++ ( my MATLAB coding skills are atrophied ). Here is the routine that detects the boundaries and constructs the straight lines void cSampleGrid::findBoundaries() { // locate grid points on boundaries between phases for (int r = 0; r &lt; Grid.size() - 1; r++) for (int c = 0; c &lt; Grid[0].size() - 1; c++) { if (Grid[r][c] != Grid[r][c + 1]) addBoundaryPoint(c + 0.5, r, Grid[r][c], Grid[r][c + 1]); if (Grid[r][c] != Grid[r + 1][c]) addBoundaryPoint(c, r + 0.5, Grid[r][c], Grid[r + 1][c]); } // sort boundary points by the phases they are between std::sort( myBoundary.begin(), myBoundary.end(), [](const sBoundary &amp;a, const sBoundary &amp;b) { return a.phase &lt; b.phase; }); // calculate end points of straight line though // all points between each pair of phases std::pair&lt;int, int&gt; prevPhase(INT_MAX, INT_MAX); int i1 = 0; for (int bk = 0; bk &lt; myBoundary.size(); bk++) { if (bk == 0) { prevPhase = myBoundary[bk].phase; continue; } if (myBoundary[bk].phase != prevPhase) { // found all points between two particular phases // create line between too points with greatest separation addBoundaryLine( i1, bk ); // setup looking for points between next two phases prevPhase = myBoundary[bk].phase; i1 = bk; } } // draw line for seperation of last two phases addBoundaryLine(i1,myBoundary.size()); } The complete code for the application that generates the sample grid, detects the boundaries, generates the boundary lines and displays the result is at https://codeberg.org/JamesBremner/Phaser ========================================================= Thank you for including the defined polygons to be used for generating a test dataset - they make this question both easier to work with and more interesting. In the above answer I have used the same grid resolution as in your code snippet, i.e. 40 by 40 = 1,600 data points. That is a lot of data points! I would guess that finding the stable phase at each of 1600 different conditions would be tedious, arduous and very expensive. Would so many data points be available in the real world? I tried lowering the resolution and thus reducing the data requirements. So, if the resolution is significantly reduced, things begin to fall apart. Yet, even asking your lab people to do 200 odd measurements seems to me like a big ask. I conclude that while my algorithm is successful in answering your question as posted, I have doubts that it will be useful IRL. Please let me know if you are interested in developing code that will produce useful phase diagram boundaries for lower resolution data sets.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "algorithm",
        "matplotlib",
        "plot"
      ],
      "question_score": 9,
      "answer_score": 12,
      "created": "2025-02-13T11:42:43",
      "question_id": 79436039,
      "answer_id": 79436096
    }
  },
  {
    "question": "Surprising lack of speedup in caching numpy calculations",
    "expected_answer": "TL;DR: page faults explain why the cache-based version is significantly slower than the one without a cache when num_iter is small. This is a side effect of creating many new Numpy arrays and deleted only at the end. When num_iter is big, the cache becomes more effective (as explained in the JonSG's answer). Using another system allocator like TCMalloc can strongly reduce this overhead. When you create many new temporary arrays, Numpy requests some memory to the system allocator which request relatively large buffers to the operating system (OS). The first touch to memory pages causes a page fault enabling the OS to actually setup the pages (actual page fault): the virtual pages are mapped to a physical one and the target pages are filled with zeros for security reasons (information should not leak from one process to another). When all arrays are deleted, Numpy free the memory space and the underlying memory allocator has a good chance to give the memory back to the OS (so other processes can use it). Page faults are very expensive because the CPU needs to switch from the user-land to kernel one (with elevated privileges). The kernel needs to setup many data structures and call a lot of functions to do that. Profiling &amp; Analysis To prove page faults are the issue and how bad page faults are performance-wise, here is the low-level breakdown of the time when the cache is enabled (using perf): 13,75% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] DOUBLE_add_AVX2 7,47% [kernel] [k] __irqentry_text_end 6,94% _mt19937.cpython-311-x86_64-linux-gnu.so [.] __pyx_f_5numpy_6random_8_mt19937_mt19937_double 3,63% [kernel] [k] clear_page_erms 3,22% [kernel] [k] error_entry 2,98% [kernel] [k] native_irq_return_iret 2,88% libpython3.11.so.1.0 [.] _PyEval_EvalFrameDefault 2,35% [kernel] [k] sync_regs 2,28% [kernel] [k] __list_del_entry_valid_or_report 2,27% [kernel] [k] unmap_page_range 1,62% [kernel] [k] __handle_mm_fault 1,45% [kernel] [k] __mod_memcg_lruvec_state 1,43% mtrand.cpython-311-x86_64-linux-gnu.so [.] random_standard_uniform_fill 1,10% [kernel] [k] do_anonymous_page 1,10% _mt19937.cpython-311-x86_64-linux-gnu.so [.] mt19937_gen 0,98% [kernel] [k] mas_walk 0,93% libpython3.11.so.1.0 [.] PyObject_GenericGetAttr 0,91% [kernel] [k] get_mem_cgroup_from_mm 0,89% [kernel] [k] get_page_from_freelist 0,79% libpython3.11.so.1.0 [.] _PyObject_Malloc 0,77% [kernel] [k] lru_gen_add_folio 0,72% [nvidia] [k] _nv039919rm 0,65% [kernel] [k] lru_gen_del_folio.constprop.0 0,63% [kernel] [k] blk_cgroup_congested 0,62% [kernel] [k] handle_mm_fault 0,59% [kernel] [k] __alloc_pages_noprof 0,57% [kernel] [k] lru_add 0,57% [kernel] [k] folio_batch_move_lru 0,56% [kernel] [k] __rcu_read_lock 0,52% [kernel] [k] do_user_addr_fault [...] (many others functions taking &lt;0.52% each) As we can see, there are a lot of [kernel] functions called and most of them are due to page faults. For example, __irqentry_text_end and native_irq_return_iret (taking ~10% of the time) is caused by CPU interrupts triggered when the CPython process access to pages for the first time. clear_page_erms is the function clearing a memory page during a first touch. Several functions are related to the virtual to physical memory mapping (e.g. AFAIK ones related to the LRU cache). Note that DOUBLE_add_AVX2 is the internal native Numpy function actually summing the two arrays. In comparison, here is the breakdown with the cache disabled: 20,85% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] DOUBLE_add_AVX2 17,39% _mt19937.cpython-311-x86_64-linux-gnu.so [.] __pyx_f_5numpy_6random_8_mt19937_mt19937_double 5,69% libpython3.11.so.1.0 [.] _PyEval_EvalFrameDefault 3,35% mtrand.cpython-311-x86_64-linux-gnu.so [.] random_standard_uniform_fill 2,46% _mt19937.cpython-311-x86_64-linux-gnu.so [.] mt19937_gen 2,15% libpython3.11.so.1.0 [.] PyObject_GenericGetAttr 1,76% [kernel] [k] __irqentry_text_end 1,46% libpython3.11.so.1.0 [.] _PyObject_Malloc 1,07% libpython3.11.so.1.0 [.] PyUnicode_FromFormatV 1,03% libc.so.6 [.] printf_positional 0,93% libpython3.11.so.1.0 [.] _PyObject_Free 0,88% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] NpyIter_AdvancedNew 0,79% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] ufunc_generic_fastcall 0,77% [kernel] [k] error_entry 0,74% _bounded_integers.cpython-311-x86_64-linux-gnu.so [.] __pyx_f_5numpy_6random_17_bounded_integers__rand_int64 0,72% [nvidia] [k] _nv039919rm 0,69% [kernel] [k] native_irq_return_iret 0,66% [kernel] [k] clear_page_erms 0,55% libpython3.11.so.1.0 [.] PyType_IsSubtype 0,55% [kernel] [k] sync_regs 0,52% mtrand.cpython-311-x86_64-linux-gnu.so [.] __pyx_pw_5numpy_6random_6mtrand_11RandomState_31randint 0,48% [kernel] [k] unmap_page_range 0,46% libpython3.11.so.1.0 [.] PyDict_GetItemWithError 0,43% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] PyArray_NewFromDescr_int 0,40% libpython3.11.so.1.0 [.] _PyFunction_Vectorcall 0,38% [kernel] [k] __mod_memcg_lruvec_state 0,38% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] promote_and_get_ufuncimpl 0,37% libpython3.11.so.1.0 [.] PyDict_SetItem 0,36% libpython3.11.so.1.0 [.] PyObject_RichCompareBool 0,36% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] PyUFunc_GenericReduction [...] (many others functions taking &lt;0.35% each) There are clearly less kernel function called in the top 30 most expensive functions. We can still see the interrupt related functions, but note that this low-level profiling is global to my whole machine and so these interrupt-related function likely comes from other processes (e.g. perf itself, the graphical desktop environment, and Firefox running as I write this answer). There is another reason proving page faults are the main culprit: during my tests, the system allocator suddenly changed its behavior because I allocated many arrays before running the same commands, and this results in only few kernel calls (in perf) as well as very close timings between the two analyzed variants (with/without cache): # First execution: In [3]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 20.2 ms ± 63.7 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) 46.4 ms ± 81.9 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) # Second execution: In [4]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 19.8 ms ± 43.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) 46.3 ms ± 155 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) # After creating many Numpy arrays (not deleted since) with no change of `numpy_comparison`: In [95]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 18.4 ms ± 15.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) 19.9 ms ± 26.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) &lt;----- We can clearly see that the overhead of using a cache is now pretty small. This also means the performance could be very different in a real-world application (because of a different allocator state), or even on different platforms. Solution to mitigate page faults You can use alternative system allocators like TCMalloc which requests a big chunk of memory to the OS at startup time so not to often pay page faults. Here are results with it (using the command line LD_PRELOAD=libtcmalloc_minimal.so.4 ipython): In [3]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 16.9 ms ± 51.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) 19.5 ms ± 29.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) The overhead related to the cache which was actually due the creation of many temporary arrays and more specifically page faults is now &gt;10 times smaller! I think the remaining overhead is due to the larger memory working set as pointed out by NickODell in comments (this cause more cache misses due to cash trashing and more data to be loaded from the slow DRAM). Put it shortly, the cache version is simply less cache friendly. Here are results with num_item=100_000 with/without TCMalloc: # Default system allocator (glibc) In [97]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 1.31 s ± 4.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 859 ms ± 3.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) # TCMalloc allocator In [3]: %timeit -n 1 numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit -n 1 numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 1.28 s ± 13.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 774 ms ± 83.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) This behavior is expected: the cache starts to be useful with more hits. Note that TCMalloc makes the overall execution faster in most case!",
    "context_chunks": [
      {
        "text": "I need to do a lot of calculations on numpy arrays, with some of the calculations being repeated. I had the idea of caching the results, but observe that In most cases, the cached version is slower than just carrying out all calculations. Not only is the cached version slower, line profiling also indicates that the absolute time spent on numpy operations increase, even though there are fewer of them. I can accept the first observation by some combined magic of numpy and the python interpreter, but the second observation makes no sense to me. I also see similar behavior when operating on scipy sparse matrices. The full application is complex, but the behavior can be reproduced by the following: import numpy as np from time import time def numpy_comparison(do_cache: bool, array_size: int, num_arrays: int, num_iter: int): # Create random arrays arrays: dict[int, np.ndarray] = {} for i in range(num_arrays): arrays[i] = np.random.rand(array_size) if do_cache: # Set up the cache if needed - I cannot use lru_cache or similar in practice cache: dict[tuple[int, int], np.ndarray] = {} for _ in range(num_iter): # Loop over random pairs of array, add, store if relevant i, j = np.random.randint(num_arrays, size=2) if do_cache and (i, j) in cache: a = cache[(i, j)] # a is not used further here, but would be in the real case else: a = arrays[i] + arrays[j] if do_cache: cache[(i, j)] = a Now running (with no multithreading) %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) gives the following results num_iter No caching With caching 100 10.3ms 13.7ms 1000 28.8ms 62.7ms 10000 225ms 392ms 100000 2.12s 1.62s Varying the array size and number of arrays give similar behavior. When num_iter is sufficiently high, retrieving from cache is most efficient, but in the regime relevant for my application, num_iter=1000 when the average chance of hitting a cached value is about 5%. Line profiling indicates this is not caused by working on cache, but on the addition of the arrays being slow. Can anyone give a hint of what is going on here?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "TL;DR: page faults explain why the cache-based version is significantly slower than the one without a cache when num_iter is small. This is a side effect of creating many new Numpy arrays and deleted only at the end. When num_iter is big, the cache becomes more effective (as explained in the JonSG's answer). Using another system allocator like TCMalloc can strongly reduce this overhead. When you create many new temporary arrays, Numpy requests some memory to the system allocator which request relatively large buffers to the operating system (OS). The first touch to memory pages causes a page fault enabling the OS to actually setup the pages (actual page fault): the virtual pages are mapped to a physical one and the target pages are filled with zeros for security reasons (information should not leak from one process to another). When all arrays are deleted, Numpy free the memory space and the underlying memory allocator has a good chance to give the memory back to the OS (so other processes can use it). Page faults are very expensive because the CPU needs to switch from the user-land to kernel one (with elevated privileges). The kernel needs to setup many data structures and call a lot of functions to do that. Profiling &amp; Analysis To prove page faults are the issue and how bad page faults are performance-wise, here is the low-level breakdown of the time when the cache is enabled (using perf): 13,75% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] DOUBLE_add_AVX2 7,47% [kernel] [k] __irqentry_text_end 6,94% _mt19937.cpython-311-x86_64-linux-gnu.so [.] __pyx_f_5numpy_6random_8_mt19937_mt19937_double 3,63% [kernel] [k] clear_page_erms 3,22% [kernel] [k] error_entry 2,98% [kernel] [k] native_irq_return_iret 2,88% libpython3.11.so.1.0 [.] _PyEval_EvalFrameDefault 2,35% [kernel] [k] sync_regs 2,28% [kernel] [k] __list_del_entry_valid_or_report 2,27% [kernel] [k] unmap_page_range 1,62% [kernel] [k] __handle_mm_fault 1,45% [kernel] [k] __mod_memcg_lruvec_state 1,43% mtrand.cpython-311-x86_64-linux-gnu.so [.] random_standard_uniform_fill 1,10% [kernel] [k] do_anonymous_page 1,10% _mt19937.cpython-311-x86_64-linux-gnu.so [.] mt19937_gen 0,98% [kernel] [k] mas_walk 0,93% libpython3.11.so.1.0 [.] PyObject_GenericGetAttr 0,91% [kernel] [k] get_mem_cgroup_from_mm 0,89% [kernel] [k] get_page_from_freelist 0,79% libpython3.11.so.1.0 [.] _PyObject_Malloc 0,77% [kernel] [k] lru_gen_add_folio 0,72% [nvidia] [k] _nv039919rm 0,65% [kernel] [k] lru_gen_del_folio.constprop.0 0,63% [kernel] [k] blk_cgroup_congested 0,62% [kernel] [k] handle_mm_fault 0,59% [kernel] [k] __alloc_pages_noprof 0,57% [kernel] [k] lru_add 0,57% [kernel] [k] folio_batch_move_lru 0,56% [kernel] [k] __rcu_read_lock 0,52% [kernel] [k] do_user_addr_fault [...] (many others functions taking &lt;0.52% each) As we can see, there are a lot of [kernel] functions called and most of them are due to page faults. For example, __irqentry_text_end and native_irq_return_iret (taking ~10% of the time) is caused by CPU interrupts triggered when the CPython process access to pages for the first time. clear_page_erms is the function clearing a memory page during a first touch. Several functions are related to the virtual to physical memory mapping (e.g. AFAIK ones related to the LRU cache). Note that DOUBLE_add_AVX2 is the internal native Numpy function actually summing the two arrays. In comparison, here is the breakdown with the cache disabled: 20,85% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] DOUBLE_add_AVX2 17,39% _mt19937.cpython-311-x86_64-linux-gnu.so [.] __pyx_f_5numpy_6random_8_mt19937_mt19937_double 5,69% libpython3.11.so.1.0 [.] _PyEval_EvalFrameDefault 3,35% mtrand.cpython-311-x86_64-linux-gnu.so [.] random_standard_uniform_fill 2,46% _mt19937.cpython-311-x86_64-linux-gnu.so [.] mt19937_gen 2,15% libpython3.11.so.1.0 [.] PyObject_GenericGetAttr 1,76% [kernel] [k] __irqentry_text_end 1,46% libpython3.11.so.1.0 [.] _PyObject_Malloc 1,07% libpython3.11.so.1.0 [.] PyUnicode_FromFormatV 1,03% libc.so.6 [.] printf_positional 0,93% libpython3.11.so.1.0 [.] _PyObject_Free 0,88% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] NpyIter_AdvancedNew 0,79% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] ufunc_generic_fastcall 0,77% [kernel] [k] error_entry 0,74% _bounded_integers.cpython-311-x86_64-linux-gnu.so [.] __pyx_f_5numpy_6random_17_bounded_integers__rand_int64 0,72% [nvidia] [k] _nv039919rm 0,69% [kernel] [k] native_irq_return_iret 0,66% [kernel] [k] clear_page_erms 0,55% libpython3.11.so.1.0 [.] PyType_IsSubtype 0,55% [kernel] [k] sync_regs 0,52% mtrand.cpython-311-x86_64-linux-gnu.so [.] __pyx_pw_5numpy_6random_6mtrand_11RandomState_31randint 0,48% [kernel] [k] unmap_page_range 0,46% libpython3.11.so.1.0 [.] PyDict_GetItemWithError 0,43% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] PyArray_NewFromDescr_int 0,40% libpython3.11.so.1.0 [.] _PyFunction_Vectorcall 0,38% [kernel] [k] __mod_memcg_lruvec_state 0,38% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] promote_and_get_ufuncimpl 0,37% libpython3.11.so.1.0 [.] PyDict_SetItem 0,36% libpython3.11.so.1.0 [.] PyObject_RichCompareBool 0,36% _multiarray_umath.cpython-311-x86_64-linux-gnu.so [.] PyUFunc_GenericReduction [...] (many others functions taking &lt;0.35% each) There are clearly less kernel function called in the top 30 most expensive functions. We can still see the interrupt related functions, but note that this low-level profiling is global to my whole machine and so these interrupt-related function likely comes from other processes (e.g. perf itself, the graphical desktop environment, and Firefox running as I write this answer). There is another reason proving page faults are the main culprit: during my tests, the system allocator suddenly changed its behavior because I allocated many arrays before running the same commands, and this results in only few kernel calls (in perf) as well as very close timings between the two analyzed variants (with/without cache): # First execution: In [3]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 20.2 ms ± 63.7 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) 46.4 ms ± 81.9 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) # Second execution: In [4]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 19.8 ms ± 43.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) 46.3 ms ± 155 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) # After creating many Numpy arrays (not deleted since) with no change of `numpy_comparison`: In [95]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 18.4 ms ± 15.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) 19.9 ms ± 26.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) &lt;----- We can clearly see that the overhead of using a cache is now pretty small. This also means the performance could be very different in a real-world application (because of a different allocator state), or even on different platforms. Solution to mitigate page faults You can use alternative system allocators like TCMalloc which requests a big chunk of memory to the OS at startup time so not to often pay page faults. Here are results with it (using the command line LD_PRELOAD=libtcmalloc_minimal.so.4 ipython): In [3]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 16.9 ms ± 51.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) 19.5 ms ± 29.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) The overhead related to the cache which was actually due the creation of many temporary arrays and more specifically page faults is now &gt;10 times smaller! I think the remaining overhead is due to the larger memory working set as pointed out by NickODell in comments (this cause more cache misses due to cash trashing and more data to be loaded from the slow DRAM). Put it shortly, the cache version is simply less cache friendly. Here are results with num_item=100_000 with/without TCMalloc: # Default system allocator (glibc) In [97]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 1.31 s ± 4.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 859 ms ± 3.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) # TCMalloc allocator In [3]: %timeit -n 1 numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter) ...: %timeit -n 1 numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter) 1.28 s ± 13.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 774 ms ± 83.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) This behavior is expected: the cache starts to be useful with more hits. Note that TCMalloc makes the overall execution faster in most case!",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I believe part of the issue here is that the work involved in the example is very quick compared to testing for and fetching the value from the cache. So I think it reasonable to see that initially there is a penalty for enabling the cache. Here is a quick workup allowing you to vary the &quot;amount of work done&quot; as well as the size of the cache. Hopefully this will allow you to explore in more detail how the work and cache hit ratio are impacting performance. If this is not of any value, just drop me a note here and I will delete this. import numpy as np import cachetools import timeit def build_tester(array_size, num_arrays, num_iter, work_size, cache_size): arrays: dict[int, np.ndarray] = {} for i in range(num_arrays): arrays[i] = np.random.rand(array_size) def _delay(work_size): for _ in range(work_size): pass return 0 def _work_uncached(i, j): return arrays[i] + arrays[j] + _delay(work_size) @cachetools.cached(cachetools.LRUCache(maxsize=cache_size), info=True) def _work_cached(i, j): return _work_uncached(i, j) _work = _work_cached if cache_size else _work_uncached def _test(): if hasattr(_work, &quot;cache_info&quot;): _work.cache_clear() a = 0 for _ in range(num_iter): # Loop over random pairs of array, add, store if relevant i, j = np.random.randint(num_arrays, size=2) a += _work(i, j) if hasattr(_work, &quot;cache_info&quot;): print(f&quot;\\t\\t{_work.cache_info()}&quot;) return a return _test for work_amount in [10, 100, 500, 1_000, 2_000, 4_000, 8_000]: print(f&quot;{work_amount=}&quot;) for cache_size in [0, 10, 100, 1_000, 2_000, 4_000, 8_000, 16_000]: print(f&quot;\\t{cache_size=}&quot;) tester = build_tester(10_000, 100, 100_000, work_amount, cache_size) seconds = timeit.timeit(tester, number=1) print(f&quot;\\t\\t{seconds=:0.4f}&quot;) That gives me a result like: work_amount=10 cache_size=0 seconds=2.6566 cache_size=10 CacheInfo(hits=112, misses=99888, maxsize=10, currsize=10) seconds=3.8053 cache_size=100 CacheInfo(hits=954, misses=99046, maxsize=100, currsize=100) seconds=4.7884 cache_size=1000 CacheInfo(hits=9791, misses=90209, maxsize=1000, currsize=1000) seconds=4.7839 cache_size=2000 CacheInfo(hits=19702, misses=80298, maxsize=2000, currsize=2000) seconds=4.6119 cache_size=4000 CacheInfo(hits=39029, misses=60971, maxsize=4000, currsize=4000) seconds=4.2463 cache_size=8000 CacheInfo(hits=75199, misses=24801, maxsize=8000, currsize=8000) seconds=3.2509 cache_size=16000 CacheInfo(hits=90001, misses=9999, maxsize=16000, currsize=9999) seconds=2.8887 work_amount=100 cache_size=0 seconds=2.7871 cache_size=10 CacheInfo(hits=95, misses=99905, maxsize=10, currsize=10) seconds=4.1492 cache_size=100 CacheInfo(hits=1017, misses=98983, maxsize=100, currsize=100) seconds=5.4277 cache_size=1000 CacheInfo(hits=9923, misses=90077, maxsize=1000, currsize=1000) seconds=5.6435 cache_size=2000 CacheInfo(hits=19928, misses=80072, maxsize=2000, currsize=2000) seconds=5.5402 cache_size=4000 CacheInfo(hits=38916, misses=61084, maxsize=4000, currsize=4000) seconds=4.9680 cache_size=8000 CacheInfo(hits=75266, misses=24734, maxsize=8000, currsize=8000) seconds=3.3910 cache_size=16000 CacheInfo(hits=90001, misses=9999, maxsize=16000, currsize=9999) seconds=2.8241 work_amount=500 cache_size=0 seconds=3.4740 cache_size=10 CacheInfo(hits=90, misses=99910, maxsize=10, currsize=10) seconds=4.8242 cache_size=100 CacheInfo(hits=1005, misses=98995, maxsize=100, currsize=100) seconds=5.9154 cache_size=1000 CacheInfo(hits=10124, misses=89876, maxsize=1000, currsize=1000) seconds=6.2072 cache_size=2000 CacheInfo(hits=19756, misses=80244, maxsize=2000, currsize=2000) seconds=6.0852 cache_size=4000 CacheInfo(hits=39104, misses=60896, maxsize=4000, currsize=4000) seconds=5.2798 cache_size=8000 CacheInfo(hits=75312, misses=24688, maxsize=8000, currsize=8000) seconds=3.6092 cache_size=16000 CacheInfo(hits=90000, misses=10000, maxsize=16000, currsize=10000) seconds=2.8147 work_amount=1000 cache_size=0 seconds=4.4938 cache_size=10 CacheInfo(hits=92, misses=99908, maxsize=10, currsize=10) seconds=6.3360 cache_size=100 CacheInfo(hits=970, misses=99030, maxsize=100, currsize=100) seconds=6.7533 cache_size=1000 CacheInfo(hits=10069, misses=89931, maxsize=1000, currsize=1000) seconds=7.1395 cache_size=2000 CacheInfo(hits=19799, misses=80201, maxsize=2000, currsize=2000) seconds=7.1510 cache_size=4000 CacheInfo(hits=39254, misses=60746, maxsize=4000, currsize=4000) seconds=6.0416 cache_size=8000 CacheInfo(hits=74989, misses=25011, maxsize=8000, currsize=8000) seconds=3.8283 cache_size=16000 CacheInfo(hits=90000, misses=10000, maxsize=16000, currsize=10000) seconds=2.9638 work_amount=2000 cache_size=0 seconds=6.0297 cache_size=10 CacheInfo(hits=108, misses=99892, maxsize=10, currsize=10) seconds=8.0671 cache_size=100 CacheInfo(hits=1014, misses=98986, maxsize=100, currsize=100) seconds=8.3695 cache_size=1000 CacheInfo(hits=9978, misses=90022, maxsize=1000, currsize=1000) seconds=9.6132 cache_size=2000 CacheInfo(hits=19729, misses=80271, maxsize=2000, currsize=2000) seconds=9.1182 cache_size=4000 CacheInfo(hits=39244, misses=60756, maxsize=4000, currsize=4000) seconds=7.2789 cache_size=8000 CacheInfo(hits=75272, misses=24728, maxsize=8000, currsize=8000) seconds=4.3689 cache_size=16000 CacheInfo(hits=90000, misses=10000, maxsize=16000, currsize=10000) seconds=3.0879 work_amount=4000 cache_size=0 seconds=10.8127 cache_size=10 CacheInfo(hits=105, misses=99895, maxsize=10, currsize=10) seconds=11.5326 cache_size=100 CacheInfo(hits=1008, misses=98992, maxsize=100, currsize=100) seconds=12.3085 cache_size=1000 CacheInfo(hits=9846, misses=90154, maxsize=1000, currsize=1000) seconds=12.2700 cache_size=2000 CacheInfo(hits=19831, misses=80169, maxsize=2000, currsize=2000) seconds=11.5156 cache_size=4000 CacheInfo(hits=39071, misses=60929, maxsize=4000, currsize=4000) seconds=9.7006 cache_size=8000 CacheInfo(hits=75259, misses=24741, maxsize=8000, currsize=8000) seconds=5.2042 cache_size=16000 CacheInfo(hits=90000, misses=10000, maxsize=16000, currsize=10000) seconds=3.4366 work_amount=8000 cache_size=0 seconds=16.8940 cache_size=10 CacheInfo(hits=84, misses=99916, maxsize=10, currsize=10) seconds=18.3884 cache_size=100 CacheInfo(hits=992, misses=99008, maxsize=100, currsize=100) seconds=19.4869 cache_size=1000 CacheInfo(hits=9929, misses=90071, maxsize=1000, currsize=1000) seconds=19.1156 cache_size=2000 CacheInfo(hits=19845, misses=80155, maxsize=2000, currsize=2000) seconds=18.9519 cache_size=4000 CacheInfo(hits=38982, misses=61018, maxsize=4000, currsize=4000) seconds=14.2407 cache_size=8000 CacheInfo(hits=75141, misses=24859, maxsize=8000, currsize=8000) seconds=7.4209 cache_size=16000 CacheInfo(hits=90000, misses=10000, maxsize=16000, currsize=10000) seconds=4.2533",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "performance",
        "caching"
      ],
      "question_score": 9,
      "answer_score": 10,
      "created": "2025-01-27T16:19:08",
      "question_id": 79391458,
      "answer_id": 79392502
    }
  },
  {
    "question": "AttributeError: module &#39;numpy&#39; has no attribute &#39;long&#39;",
    "expected_answer": "Sadly, numpy.long was deprecated in numpy 1.20 and it is removed in numpy 1.24 If you wan the result you have to try numpy.longlong import numpy as np np.longlong(9**19) #output 1350851717672992089 https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
    "context_chunks": [
      {
        "text": "I am trying to find 9 raise to power 19 using numpy. I am using numpy 1.24.3 This is the code I am trying: import numpy as np np.long(9**19) This is the error I am getting: AttributeError: module 'numpy' has no attribute 'long'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Sadly, numpy.long was deprecated in numpy 1.20 and it is removed in numpy 1.24 If you wan the result you have to try numpy.longlong import numpy as np np.longlong(9**19) #output 1350851717672992089 https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "If you encounter an error in NumPy like AttributeError: module 'numpy' has no attribute 'long', indicating that there is no 'long' attribute in the 'numpy' module, it may be due to the absence of the 'long' type in the latest version of NumPy. To resolve this issue, you can roll back to a previous version(before version 1.23.0) using the following command: pip install numpy==1.23.0 Now you can execute the code in your current Python environment after performing this installation.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "numpy"
      ],
      "question_score": 9,
      "answer_score": 15,
      "created": "2023-06-02T10:36:57",
      "question_id": 76389395,
      "answer_id": 76389396
    }
  },
  {
    "question": "ERROR: Could not build wheels for lxml, which is required to install pyproject.toml-based projects",
    "expected_answer": "This issue is getting resolved for me after doing the upgrade for setuptools. pip install --upgrade setuptools pip install lxml I am using the python version 3.8.16. Hope this helps.",
    "context_chunks": [
      {
        "text": "I'm new with python and im trying to install the module of yfinance for my uni class, but I keep getting this error when trying to install it... anyone knows how to solve this problem?enter image description here Wanted to install yfinance module because when I try to install it in vs code i get the following error: ModuleNotFoundError: No module named 'yfinance",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This issue is getting resolved for me after doing the upgrade for setuptools. pip install --upgrade setuptools pip install lxml I am using the python version 3.8.16. Hope this helps.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "pip install lxml==4.8.0 Try to install lxml 4.8 instead of lxml 4.9. Version 4.9 has this issue",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "module",
        "yfinance"
      ],
      "question_score": 9,
      "answer_score": 13,
      "created": "2023-06-06T22:25:03",
      "question_id": 76418822,
      "answer_id": 76808408
    }
  },
  {
    "question": "Azure Functions: Can&#39;t open lib &#39;ODBC Driver 17 for SQL Server&#39;",
    "expected_answer": "This happened to me when I deployed a python 3.11 azure function using azure functions core tools: func azure functionapp publish &lt;name_of_azure_function&gt;. When I switched back to python 3.10, the ODBC Driver 17 for SQL Server driver was installed. Now that python 3.11 is fully supported for azure functions, I've discovered that they come with &quot;ODBC Driver 18 for SQL Server&quot; installed, instead of the version 17 which is installed on python 3.10 azure functions. So in your conn_str, you need to replace 17 with 18 in order for it to work on python 3.11.",
    "context_chunks": [
      {
        "text": "I've written a Python script that connects to SQL Server housed in a Virtual Machine hosted in our Azure Environment. I've been able to successfully connect and run the query locally within the Virtual Machine but when I deploy to Azure Functions I'm getting the following error: ('01000', &quot;[01000] [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 17 for SQL Server' : file not found (0) (SQLDriverConnect)&quot;) I successfully ran the script and connected to the database a few days ago, but for some reason, it stopped working and this error now appears. import pyodbc DatabaseServer = 'Server' DatabaseName = 'databasename' conn_str = &quot;Driver={ODBC Driver 17 for SQL Server };Server=&quot;+str(DatabaseServer)+';Database='+str(DatabaseName)+&quot;;'Trusted_Connection=yes;&quot; try: # Connect to the SQL Server conn = pyodbc.connect(conn_str) cursor = conn.cursor() # Execute the query cursor.execute(&quot;SELECT TOP 10 hmy FROM Table&quot;) # Fetch and print the results rows = cursor.fetchall() results_str = &quot;&quot; for row in rows: results_str += str(row) + &quot;\\n&quot; # Close the cursor and connection cursor.close() conn.close() print(&quot;Connection to SQL Server Succesful&quot;) except pyodbc.Error as e: print(f&quot;Error connecting to SQL Server {str(e)}&quot;) Pyodbc is included in the requirements.txt file which is deployed to Azure Functions. If somebody could help that would be great. I believe it could be something to do with Azure functions not having the correct ODBC library but I've read that it is pre-installed so this shouldn't be a problem.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This happened to me when I deployed a python 3.11 azure function using azure functions core tools: func azure functionapp publish &lt;name_of_azure_function&gt;. When I switched back to python 3.10, the ODBC Driver 17 for SQL Server driver was installed. Now that python 3.11 is fully supported for azure functions, I've discovered that they come with &quot;ODBC Driver 18 for SQL Server&quot; installed, instead of the version 17 which is installed on python 3.10 azure functions. So in your conn_str, you need to replace 17 with 18 in order for it to work on python 3.11.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Azure Functions Python will have PYODBC module installed by default. Make sure to add pyodbc in requirements.txt. I used the below code to connect and query Select statement with Azure SQL using Azure Functions and it worked successfully, Refer below:- My init.py:- import logging from multiprocessing import connection import pyodbc import os import azure.functions as func def main(req: func.HttpRequest) -&gt; func.HttpResponse: logging.info('Python HTTP trigger function processed a request.') connectionstring = os.environ[&quot;connectionstring&quot;] conn = pyodbc.connect(connectionstring) cursor = conn.cursor() cursor.execute(&quot;SELECT * FROM StudentReviews&quot;) conn.commit() conn.commit() cursor.close() conn.close() # Prepare &amp; Return the HTTP Response return func.HttpResponse( body=&quot;Your request is processed&quot;, status_code=202 ) requirements.txt:- azure-functions pyodbc local.settings.json:- { &quot;IsEncrypted&quot;: false, &quot;Values&quot;: { &quot;AzureWebJobsStorage&quot;: &quot;DefaultEndpointsProtocol=https;AccountName=siliconrg8c29;AccountKey=xxxxxxxxxqvo9mCwMuHlTpFk5yzn/Wk/bu3Wy1rxlxxxxx==;EndpointSuffix=core.windows.net&quot;, &quot;FUNCTIONS_WORKER_RUNTIME&quot;: &quot;python&quot;, &quot;connectionstring&quot; : &quot;DRIVER={ODBC Driver 17 for SQL Server};SERVER=tcp:sqlserver.database.windows.net;PORT=1433;DATABASE=silicondb;UID=username;PWD=Password&quot; } } I deployed the above function code in my Azure Function app created with Runtime set to Python 3.10 and Linux OS like below:- Commands to deploy the Function:- az login az account set --subscription &quot;SID Subscription&quot; func azure functionapp publish siliconfunc430 Added connectionstring setting in the configuration as settings from local.settings.json is not added in the Function app while deployment. Make sure you check the Function Outbound Ip's and whitelist these Ip's in your SQL as mentioned in this SO thread answer. One alternative is to deploy your Function app in a dedicated plan, Either Premium plan or App Service plan and then run the commands from this Document to install ODBC Driver manually in your Function app. Commands:- Checked the OS version that was Debian and than ran the script from the document specific to Debian. cat /etc/os-release sudo su curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - #Download appropriate package for the OS version #Choose only ONE of the following, corresponding to your OS version #Debian 9 curl https://packages.microsoft.com/config/debian/9/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list #Debian 10 curl https://packages.microsoft.com/config/debian/10/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list #Debian 11 curl https://packages.microsoft.com/config/debian/11/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list exit sudo apt-get update sudo ACCEPT_EULA=Y apt-get install -y msodbcsql17 # optional: for bcp and sqlcmd sudo ACCEPT_EULA=Y apt-get install -y mssql-tools echo 'export PATH=&quot;$PATH:/opt/mssql-tools/bin&quot;' &gt;&gt; ~/.bashrc source ~/.bashrc # optional: for unixODBC development headers sudo apt-get install -y unixodbc-dev # optional: kerberos library for debian-slim distributions sudo apt-get install -y libgssapi-krb5-2",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "sql-server",
        "azure",
        "azure-functions",
        "pyodbc"
      ],
      "question_score": 9,
      "answer_score": 19,
      "created": "2023-07-25T15:51:55",
      "question_id": 76764452,
      "answer_id": 76887960
    }
  },
  {
    "question": "how to convert a dict to a dataclass (reverse of asdict)?",
    "expected_answer": "You can use make_dataclass: from dataclasses import make_dataclass my_dict = {&quot;a&quot;: 100, &quot;b&quot;: 200} make_dataclass( &quot;MyDynamicallyCreatedDataclass&quot;, ((k, type(v)) for k, v in my_dict.items()) )(**my_dict)",
    "context_chunks": [
      {
        "text": "the dataclasses module lets users make a dict from a dataclass reall conveniently, like this: from dataclasses import dataclass, asdict @dataclass class MyDataClass: ''' description of the dataclass ''' a: int b: int # create instance c = MyDataClass(100, 200) print(c) # turn into a dict d = asdict(c) print(d) But i am trying to do the reverse process: dict -&gt; dataclass. The best that i can do is unpack a dict back into the predefined dataclass. # is there a way to convert this dict to a dataclass ? my_dict = {'a': 100, 'b': 200} e = MyDataClass(**my_dict) print(e) How can i achieve this without having to pre-define the dataclass (if it is possible) ?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can use make_dataclass: from dataclasses import make_dataclass my_dict = {&quot;a&quot;: 100, &quot;b&quot;: 200} make_dataclass( &quot;MyDynamicallyCreatedDataclass&quot;, ((k, type(v)) for k, v in my_dict.items()) )(**my_dict)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I'd go with unpacking the dict as you showed. from dataclasses import dataclass, asdict @dataclass class MyDataClass: a: int b: int c = MyDataClass(100, 200) print(c) &gt;&gt;&gt; MyDataClass(a=100, b=200) d = asdict(c) print(d) &gt;&gt;&gt; {'a': 100, 'b': 200} e = MyDataClass(**d) print(e) &gt;&gt;&gt; MyDataClass(a=100, b=200) You can't turn a dictionary into a dataclass class that doesn't yet exist. You'll have to create the class before transforming a dict into it. If you really would rather have a class than a dictionary you could use an Enum. Otherwise you're just left with shortcut ways to make classes and then turning a dict into those classes.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-dataclasses"
      ],
      "question_score": 9,
      "answer_score": 10,
      "created": "2023-10-19T15:35:31",
      "question_id": 77325233,
      "answer_id": 77325321
    }
  },
  {
    "question": "Integrate Keycloak with FastAPI",
    "expected_answer": "I crafted some Python code for fastAPI with keycloak integration, it may be helpful to share it. Declare auth functions #/auth.py from fastapi.security import OAuth2AuthorizationCodeBearer from keycloak import KeycloakOpenID # pip require python-keycloak from config import settings from fastapi import Security, HTTPException, status,Depends from pydantic import Json from models import User # This is used for fastapi docs authentification oauth2_scheme = OAuth2AuthorizationCodeBearer( authorizationUrl=settings.authorization_url, # https://sso.example.com/auth/ tokenUrl=settings.token_url, # https://sso.example.com/auth/realms/example-realm/protocol/openid-connect/token ) # This actually does the auth checks # client_secret_key is not mandatory if the client is public on keycloak keycloak_openid = KeycloakOpenID( server_url=settings.server_url, # https://sso.example.com/auth/ client_id=settings.client_id, # backend-client-id realm_name=settings.realm, # example-realm client_secret_key=settings.client_secret, # your backend client secret verify=True ) async def get_idp_public_key(): return ( &quot;-----BEGIN PUBLIC KEY-----\\n&quot; f&quot;{keycloak_openid.public_key()}&quot; &quot;\\n-----END PUBLIC KEY-----&quot; ) # Get the payload/token from keycloak async def get_payload(token: str = Security(oauth2_scheme)) -&gt; dict: try: return keycloak_openid.decode_token( token, key= await get_idp_public_key(), options={ &quot;verify_signature&quot;: True, &quot;verify_aud&quot;: False, &quot;exp&quot;: True } ) except Exception as e: raise HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=str(e), # &quot;Invalid authentication credentials&quot;, headers={&quot;WWW-Authenticate&quot;: &quot;Bearer&quot;}, ) # Get user infos from the payload async def get_user_info(payload: dict = Depends(get_payload)) -&gt; User: try: return User( id=payload.get(&quot;sub&quot;), username=payload.get(&quot;preferred_username&quot;), email=payload.get(&quot;email&quot;), first_name=payload.get(&quot;given_name&quot;), last_name=payload.get(&quot;family_name&quot;), realm_roles=payload.get(&quot;realm_access&quot;, {}).get(&quot;roles&quot;, []), client_roles=payload.get(&quot;realm_access&quot;, {}).get(&quot;roles&quot;, []) ) except Exception as e: raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail=str(e), # &quot;Invalid authentication credentials&quot;, headers={&quot;WWW-Authenticate&quot;: &quot;Bearer&quot;}, ) Define your models , you can use what ever model you need : #/models.py from pydantic import BaseModel, EmailStr class User(BaseModel): id: str username: str email: str first_name: str last_name: str realm_roles: list client_roles: list class authConfiguration(BaseModel): server_url: str realm: str client_id: str client_secret: str authorization_url: str token_url: str Define your config: #/config.py from models import authConfiguration settings = authConfiguration( server_url=&quot;http://localhost:8080/&quot;, realm=&quot;roc&quot;, client_id=&quot;rns:roc:portal&quot;, client_secret=&quot;&quot;, authorization_url=&quot;http://localhost:8080/realms/roc/protocol/openid-connect/auth&quot;, token_url=&quot;http://localhost:8080/realms/roc/protocol/openid-connect/token&quot;, ) and finally define your routes and secure them: #/main.py import uvicorn from fastapi import FastAPI,Depends from models import User from auth import get_user_info app = FastAPI() @app.get(&quot;/&quot;) async def root(): return {&quot;message&quot;: &quot;Hello World&quot;} @app.get(&quot;/secure&quot;) async def root(user: User = Depends(get_user_info)): return {&quot;message&quot;: f&quot;Hello {user.username} you have the following service: {user.realm_roles}&quot;} if __name__ == '__main__': uvicorn.run('main:app', host=&quot;127.0.0.1&quot;, port=8081) On the keycloak side create a realm and within this real create a client. Create a test user to use it for authentification and that's it! References: https://github.com/tiangolo/fastapi/discussions/9066 https://pypi.org/project/python-keycloak/",
    "context_chunks": [
      {
        "text": "I have an app that is written with FastAPI and SvelteKit, both these is running in separate containers, the current solution is Svelte sends the username and password to the FastAPI server, the FastAPI server then returns a signed JWT to svelte which is used to authenticate with FastAPI. Heres the login flow Svelte -&gt; username + password -&gt; FastAPI FastAPI -&gt; JWT -&gt; Svelte(stored in cookie) Here's what happens when a request is made Svelte(Authorization: Bearer) -&gt; FastAPI I want to get rid of my own username and password and use KeyCloak for my authentication. I'm super new to OAuth so I have no idea what I'm supposed to do or even what terms to search. Here's what I understand I want: Svelte(goto &quot;/login&quot;) -&gt; redirects to keycloak -&gt; login and get a token (somehow get the token to FastAPI so I can sign my own token and send it to svelte) And when I make a request Svelte -&gt; FastAPI Token -&gt; FastAPI",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I crafted some Python code for fastAPI with keycloak integration, it may be helpful to share it. Declare auth functions #/auth.py from fastapi.security import OAuth2AuthorizationCodeBearer from keycloak import KeycloakOpenID # pip require python-keycloak from config import settings from fastapi import Security, HTTPException, status,Depends from pydantic import Json from models import User # This is used for fastapi docs authentification oauth2_scheme = OAuth2AuthorizationCodeBearer( authorizationUrl=settings.authorization_url, # https://sso.example.com/auth/ tokenUrl=settings.token_url, # https://sso.example.com/auth/realms/example-realm/protocol/openid-connect/token ) # This actually does the auth checks # client_secret_key is not mandatory if the client is public on keycloak keycloak_openid = KeycloakOpenID( server_url=settings.server_url, # https://sso.example.com/auth/ client_id=settings.client_id, # backend-client-id realm_name=settings.realm, # example-realm client_secret_key=settings.client_secret, # your backend client secret verify=True ) async def get_idp_public_key(): return ( &quot;-----BEGIN PUBLIC KEY-----\\n&quot; f&quot;{keycloak_openid.public_key()}&quot; &quot;\\n-----END PUBLIC KEY-----&quot; ) # Get the payload/token from keycloak async def get_payload(token: str = Security(oauth2_scheme)) -&gt; dict: try: return keycloak_openid.decode_token( token, key= await get_idp_public_key(), options={ &quot;verify_signature&quot;: True, &quot;verify_aud&quot;: False, &quot;exp&quot;: True } ) except Exception as e: raise HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=str(e), # &quot;Invalid authentication credentials&quot;, headers={&quot;WWW-Authenticate&quot;: &quot;Bearer&quot;}, ) # Get user infos from the payload async def get_user_info(payload: dict = Depends(get_payload)) -&gt; User: try: return User( id=payload.get(&quot;sub&quot;), username=payload.get(&quot;preferred_username&quot;), email=payload.get(&quot;email&quot;), first_name=payload.get(&quot;given_name&quot;), last_name=payload.get(&quot;family_name&quot;), realm_roles=payload.get(&quot;realm_access&quot;, {}).get(&quot;roles&quot;, []), client_roles=payload.get(&quot;realm_access&quot;, {}).get(&quot;roles&quot;, []) ) except Exception as e: raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail=str(e), # &quot;Invalid authentication credentials&quot;, headers={&quot;WWW-Authenticate&quot;: &quot;Bearer&quot;}, ) Define your models , you can use what ever model you need : #/models.py from pydantic import BaseModel, EmailStr class User(BaseModel): id: str username: str email: str first_name: str last_name: str realm_roles: list client_roles: list class authConfiguration(BaseModel): server_url: str realm: str client_id: str client_secret: str authorization_url: str token_url: str Define your config: #/config.py from models import authConfiguration settings = authConfiguration( server_url=&quot;http://localhost:8080/&quot;, realm=&quot;roc&quot;, client_id=&quot;rns:roc:portal&quot;, client_secret=&quot;&quot;, authorization_url=&quot;http://localhost:8080/realms/roc/protocol/openid-connect/auth&quot;, token_url=&quot;http://localhost:8080/realms/roc/protocol/openid-connect/token&quot;, ) and finally define your routes and secure them: #/main.py import uvicorn from fastapi import FastAPI,Depends from models import User from auth import get_user_info app = FastAPI() @app.get(&quot;/&quot;) async def root(): return {&quot;message&quot;: &quot;Hello World&quot;} @app.get(&quot;/secure&quot;) async def root(user: User = Depends(get_user_info)): return {&quot;message&quot;: f&quot;Hello {user.username} you have the following service: {user.realm_roles}&quot;} if __name__ == '__main__': uvicorn.run('main:app', host=&quot;127.0.0.1&quot;, port=8081) On the keycloak side create a realm and within this real create a client. Create a test user to use it for authentification and that's it! References: https://github.com/tiangolo/fastapi/discussions/9066 https://pypi.org/project/python-keycloak/",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The easiest way to add Keycloak authentication to an app is oauth2-proxy. oauth2-proxy stands in front of your application like a reverse proxy and it'll handle all the OAuth2 bits for you. oauth2-proxy will then generate the sign-in URL to Keycloak (/realms/{realm-name}/protocol/openid-connect/auth) with the appropriate parameters to start an OAuth2 sign in process and set it to redirect back to the application page, Keycloak will display sign-in page, then after the user completed their sign in, it'll redirect back to the application, at which point oauth2-proxy will check the JWT token are valid, set a cookie, and then pass along the username, access token, and ID token to the server as HTTP headers to the proxied request. When using a oauth2-proxy kind of approach, neither your Svelte frontend app or FastAPI app needs to know anything about OIDC/OAuth2, they don't even need access to the client secret, which reduces the chance of your application leaking those secrets. The application may need to be able to decode a JWT token to read Access token and ID token (which contains the auth claims), but if all you need is just a Keycloak validated username/email, then your FastAPI app can just read the HTTP headers with Header() to get the username. Decoding JWT without signature verification (because we assume that oauth2-proxy already validated the signature) is pretty simple to implement with just standard library, it's basically just a couple string manipulation, base64, and json parsing. Alternatively, you can implement OAuth2 flow directly in your application. Direct integration may be necessary if the application needs to do things that are more complicated than just basic authentication and authorisation with the JWT tokens or if you want finer control over the auth process. Keycloak supports OAuth2 via OIDC, so you can use any OIDC library like pyoidc or with a Keycloak specific integration like fastapi-keycloak-middleware. Implementing OAuth2 without OIDC is also possible but it will be much more involved. Keycloak, OIDC, and OAuth2 all has a lot of configuration involved to support various different use cases and security requirements, you'll need to look into these configurations in more detail to figure out which suits your requirement. Implementing all this without using an OIDC and/or OAuth2 libraries is possible, but I wouldn't really recommend them. You'll just end up reading a lot of boring OAuth2 and OIDC specs.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "oauth-2.0",
        "fastapi",
        "openid-connect"
      ],
      "question_score": 9,
      "answer_score": 12,
      "created": "2023-08-17T12:56:00",
      "question_id": 76921747,
      "answer_id": 77186511
    }
  },
  {
    "question": "Why do I get the error &quot;Unrecognized request argument supplied: functions&quot; when using `functions` when calling Azure OpenAI GPT?",
    "expected_answer": "Function support with the Azure API was added in 2023-07-01-preview. The API version needs to be updated in your example: openai.api_version = &quot;2023-07-01-preview&quot;",
    "context_chunks": [
      {
        "text": "I'm trying to use functions when calling Azure OpenAI GPT, as documented in https://platform.openai.com/docs/api-reference/chat/create#chat/create-functions I use: import openai openai.api_type = &quot;azure&quot; openai.api_base = &quot;https://XXXXXXXX.openai.azure.com/&quot; openai.api_version = &quot;2023-06-01-preview&quot; openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;) response = openai.ChatCompletion.create( engine=&quot;gpt-35-turbo-XXX&quot;, model=&quot;gpt-35-turbo-0613-XXXX&quot; messages=messages, functions=functions, function_call=&quot;auto&quot;, ) but I get the error: openai.error.InvalidRequestError: Unrecognized request argument supplied: functions Why? Data to run the example code above (messages and functions need to be defined): messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}] functions = [ { &quot;name&quot;: &quot;fetch_pages&quot;, &quot;description&quot;: &quot;Fetch the content of specified pages from the document.&quot;, &quot;parameters&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;pages&quot;: { &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;description&quot;: &quot;The list of pages to fetch.&quot; } }, &quot;required&quot;: [&quot;pages&quot;] } }, { &quot;name&quot;: &quot;fetch_section&quot;, &quot;description&quot;: &quot;Fetch the content of a specified section.&quot;, &quot;parameters&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;section_title&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The title of the section to fetch.&quot; } }, &quot;required&quot;: [&quot;section_title&quot;] } }, { &quot;name&quot;: &quot;search&quot;, &quot;description&quot;: &quot;Search the document for a string query.&quot;, &quot;parameters&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;query&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The search term.&quot; } }, &quot;required&quot;: [&quot;query&quot;] } } ]",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Function support with the Azure API was added in 2023-07-01-preview. The API version needs to be updated in your example: openai.api_version = &quot;2023-07-01-preview&quot;",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As a complement to Krista's answer, it seems that one must use latest 0613 versions of gpt-35-turbo and gpt-4. Example code: import openai openai.api_type = &quot;azure&quot; openai.api_base = &quot;https://XXXX.openai.azure.com/&quot; openai.api_version = &quot;2023-07-01-preview&quot; openai.api_key = &quot;XXXX&quot; messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}] functions = [ { &quot;name&quot;: &quot;fetch_pages&quot;, &quot;description&quot;: &quot;Fetch the content of specified pages from the document.&quot;, &quot;parameters&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;pages&quot;: { &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;description&quot;: &quot;The list of pages to fetch.&quot; } }, &quot;required&quot;: [&quot;pages&quot;] } }, { &quot;name&quot;: &quot;fetch_section&quot;, &quot;description&quot;: &quot;Fetch the content of a specified section.&quot;, &quot;parameters&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;section_title&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The title of the section to fetch.&quot; } }, &quot;required&quot;: [&quot;section_title&quot;] } }, { &quot;name&quot;: &quot;search&quot;, &quot;description&quot;: &quot;Search the document for a string query.&quot;, &quot;parameters&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;query&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The search term.&quot; } }, &quot;required&quot;: [&quot;query&quot;] } } ] response = openai.ChatCompletion.create( engine=&quot;gpt-35-turbo-XXXX&quot;, model=&quot;gpt-35-turbo-0613-XXXX&quot; messages=messages, functions=functions, function_call=&quot;auto&quot;, ) print(response) Output: { &quot;choices&quot;: [ { &quot;content_filter_results&quot;: { &quot;hate&quot;: { &quot;filtered&quot;: false, &quot;severity&quot;: &quot;safe&quot; }, &quot;self_harm&quot;: { &quot;filtered&quot;: false, &quot;severity&quot;: &quot;safe&quot; }, &quot;sexual&quot;: { &quot;filtered&quot;: false, &quot;severity&quot;: &quot;safe&quot; }, &quot;violence&quot;: { &quot;filtered&quot;: false, &quot;severity&quot;: &quot;safe&quot; } }, &quot;finish_reason&quot;: &quot;stop&quot;, &quot;index&quot;: 0, &quot;message&quot;: { &quot;content&quot;: &quot;Hi there! How can I assist you today?&quot;, &quot;role&quot;: &quot;assistant&quot; } } ], &quot;created&quot;: 1690229943, &quot;id&quot;: &quot;chatcmpl-sdkopsdvomsdvpomi156sdv&quot;, &quot;model&quot;: &quot;gpt-35-turbo&quot;, &quot;object&quot;: &quot;chat.completion&quot;, &quot;prompt_annotations&quot;: [ { &quot;content_filter_results&quot;: { &quot;hate&quot;: { &quot;filtered&quot;: false, &quot;severity&quot;: &quot;safe&quot; }, &quot;self_harm&quot;: { &quot;filtered&quot;: false, &quot;severity&quot;: &quot;safe&quot; }, &quot;sexual&quot;: { &quot;filtered&quot;: false, &quot;severity&quot;: &quot;safe&quot; }, &quot;violence&quot;: { &quot;filtered&quot;: false, &quot;severity&quot;: &quot;safe&quot; } }, &quot;prompt_index&quot;: 0 } ], &quot;usage&quot;: { &quot;completion_tokens&quot;: 11, &quot;prompt_tokens&quot;: 127, &quot;total_tokens&quot;: 138 } }",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "azure",
        "gpt-3",
        "azure-openai",
        "large-language-model"
      ],
      "question_score": 9,
      "answer_score": 15,
      "created": "2023-07-24T18:35:33",
      "question_id": 76757194,
      "answer_id": 76757275
    }
  },
  {
    "question": "The difference between `poetry add` and `poetry install`",
    "expected_answer": "poetry add library_name installs the library and adds it to the pyproject.toml file. Note - both installs the library and adds it to the file. poetry install is used when you've directly edited the pyproject.toml file and added the dependency names manually. In that case, they aren't installed yet, so, poetry install takes care of that.",
    "context_chunks": [
      {
        "text": "I've thought that poetry add package would simply add the package to pyproject.toml but it seems it doesn't just add but also installs it in a virtual environment. But what does poetry install do? When I run it after I added the deps with add, I am getting the following message: Installing dependencies from lock file No dependencies to install or update Note that I started a project from scratch with mkdir new_dir; cd new_dir; poetry init.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "poetry add library_name installs the library and adds it to the pyproject.toml file. Note - both installs the library and adds it to the file. poetry install is used when you've directly edited the pyproject.toml file and added the dependency names manually. In that case, they aren't installed yet, so, poetry install takes care of that.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "the add command adds packages to your pyproject.toml and installs them. The install command reads the pyproject.toml file from the current project, resolves the dependencies, and installs them. Refer the docs",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "dependencies",
        "python-poetry"
      ],
      "question_score": 9,
      "answer_score": 17,
      "created": "2023-06-20T14:29:50",
      "question_id": 76515800,
      "answer_id": 76544402
    }
  },
  {
    "question": "-bash: cannot execute: required file not found",
    "expected_answer": "head -n 3 dartsapp.py | cat -e outputs: #!/usr/bin/python3^M$ ^M$ # Python-App fM-CM-&lt;r das PunktezM-CM-$hlen beim Darts^M$ and so those ^Ms at the end of each line tells us that dartsapp.py has DOS line endings, see linux-bash-shell-script-error-cannot-execute-required-file-not-found for more information on this specific error and Why does my tool output overwrite itself and how do I fix it? for more information on DOS line endings in general. Both of those questions contain various fixes for this problem, starting with running dos2unix dartsapp.py.",
    "context_chunks": [
      {
        "text": "I am having multiple problems with making my pythonscript dartsapp.py executable for my pi. At first i tried to use chmod +x dartsapp.py and ./dartsapp.py but i got the error: -bash: ./dartsapp.py: cannot execute: required file not found I also added the dir to PATH but still no luck. Afterwards i tried to use pyinstaller but when i try to call pyinstaller --version i get the same -bash: pyinstaller: command not found even though my PATH looks like this: maxdu@LEDpi:~/0306 $ echo $PATH /home/maxdu/0306:/home/maxdu/0306/dartsapp.py:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games What am i missing / doing wrong here? Update: ls -l /usr/bin/python3 gives me lrwxrwxrwx 1 root root 10 Apr 9 2023 /usr/bin/python3 -&gt; python3.11 and head -n 3 dartsapp.py | cat -e gives me #!/usr/bin/python3^M$ ^M$ # Python-App fM-CM-&lt;r das PunktezM-CM-$hlen beim Darts^M$ and type python3 gives me python3 is hashed (/usr/bin/python3)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "head -n 3 dartsapp.py | cat -e outputs: #!/usr/bin/python3^M$ ^M$ # Python-App fM-CM-&lt;r das PunktezM-CM-$hlen beim Darts^M$ and so those ^Ms at the end of each line tells us that dartsapp.py has DOS line endings, see linux-bash-shell-script-error-cannot-execute-required-file-not-found for more information on this specific error and Why does my tool output overwrite itself and how do I fix it? for more information on DOS line endings in general. Both of those questions contain various fixes for this problem, starting with running dos2unix dartsapp.py.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "To run dartsapp.py with the python interpreter: You might find some success with this command: python3 ./dartsapp.py Unlike shell scripts (typically ending in .sh or no extension), which can be executed so long as you have permission to do so, you'll need to use the python command to execute a python script. python3 comes as standard with most modern linux installations, so this should work on a raspberry pi as well as your dev machine. To turn dartsapp.py into a self-contained executable using pyinstaller: If you're dead-set on having a self-contained executable, you'll need to install pyinstaller using pip: python3 -m pip install pyinstaller And then call pyinstaller on the file: python3 -m pyinstaller dartsapp.py (There's a lot more than just that to pyinstaller - see the manual - but you shouldn't really need it to get your program running)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "bash",
        "pyinstaller"
      ],
      "question_score": 9,
      "answer_score": 10,
      "created": "2024-06-03T11:31:33",
      "question_id": 78570018,
      "answer_id": 78570143
    }
  },
  {
    "question": "Encountering 503 Error When Calling Gemini API from Google Colab",
    "expected_answer": "Error 503 corresponds to &quot;service unavailable&quot;. If you read the full error message on the response, you might get &quot;The model is overloaded. Please try again later.&quot; This is definitely something going on on Google's end - not yours. There is very little you can do about it. However, you should certainly account for it. Retrying attempts in a loop with progressive back-off is a standard approach to server unavailable messages. You indicate you're using the &quot;google.generativeai&quot; package, which corresponds with the AI Studio offering (aistudio.google.com) - not Google Cloud. This is currently a free-tier only service that is not Generally Available. So it seems likely they are still tuning things to meet expected user demand and scale things out as they get ready for production and for the introduction of an additional paid tier.",
    "context_chunks": [
      {
        "text": "I'm working on a project using Google Colab to run Python code that interacts with the Gemini API (a part of Google's Cloud AI tools). The goal is to automate call transcript categorization into predefined categories using Gemini's AI. Here's a brief overview of what I'm doing: I read an Excel file of call transcripts, send these transcripts to Gemini for categorization, and then update the Excel file based on the categories identified by the AI (marking them with 0s and 1s). Below is a snippet of my code for setting up the API and sending a request to Gemini: import google.generativeai as genai GOOGLE_API_KEY = &quot;your_api_key_here&quot; genai.configure(api_key=GOOGLE_API_KEY) model = genai.GenerativeModel('gemini-pro') def send_to_gemini(transcript): prompt = f&quot;Categorize the following transcript: {transcript}&quot; try: response = model.generate_content(prompt) return response.text except Exception as e: print(f&quot;Failed to send request to Gemini: {e}&quot;) However, I keep getting an ERROR:tornado.access:503 suggesting a server-side issue: ERROR:tornado.access:503 POST /v1beta/models/gemini-pro:generateContent (127.0.0.1) 4039.47ms Any advice or insights would be greatly appreciated.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Error 503 corresponds to &quot;service unavailable&quot;. If you read the full error message on the response, you might get &quot;The model is overloaded. Please try again later.&quot; This is definitely something going on on Google's end - not yours. There is very little you can do about it. However, you should certainly account for it. Retrying attempts in a loop with progressive back-off is a standard approach to server unavailable messages. You indicate you're using the &quot;google.generativeai&quot; package, which corresponds with the AI Studio offering (aistudio.google.com) - not Google Cloud. This is currently a free-tier only service that is not Generally Available. So it seems likely they are still tuning things to meet expected user demand and scale things out as they get ready for production and for the introduction of an additional paid tier.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This isn't exactly answering the question but will be helpful for those who, like me, found this page because they were getting a ton of 503 errors when using the google.generativeai package I had this error when caching pdfs. Switching to vertexai helped and removed most of the errors instantly. Instead of using google.generativeai's CachedContent, use vertexai's implementation, for example. There will be a bit of fiddling but not a lot. Plus, it uses the same funds from your google account, so no more billing to set up.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "google-api",
        "google-api-python-client",
        "google-gemini"
      ],
      "question_score": 9,
      "answer_score": 13,
      "created": "2024-03-13T13:07:49",
      "question_id": 78154047,
      "answer_id": 78154465
    }
  },
  {
    "question": "AttributeError: &#39;FigureCanvasInterAgg&#39; object has no attribute &#39;tostring_rgb&#39;. Did you mean: &#39;tostring_argb&#39;?",
    "expected_answer": "This seems to be an issue with matplotlib 3.10.0 as can be seen in this GitHub issue. Downgrading to matplotlib&lt;3.10 solved it for a couple of users. See whether installing version 3.9 fixes your issue pip install matplotlib==3.9.0 or try other lower versions mentioned in the issue. Be aware that some older versions may not be compatible with recent releases of Python. See Release history.",
    "context_chunks": [
      {
        "text": "#AttributeError: 'FigureCanvasInterAgg' object has no attribute 'tostring_rgb'. Did you mean: 'tostring_argb'? #import matplotlib.pyplot as plt #======================== # This can be work # import matplotlib # matplotlib.use('TkAgg') # import matplotlib.pyplot as plt #========================= with open('notebook.txt', encoding='utf-8') as file: # contents = file.read() # print(contents) # for line in file: # print('line:', line) contents = file.readlines() print(contents) newList = [] for content in contents: newContent = content.replace('\\n', '') money = newContent.split(':')[-1] newList.append(int(money)) # 6月: 9000 # contents = content.replace('\\n', '') print(newList) x = [1, 2, 3, 4, 5, 6] y = newList plt.plot(x, y, 'r') plt.xlabel('month') plt.ylabel('money') plt.legend() plt.show() 1月: 7000 2月: 10000 3月: 15000 4月: 12000 5月: 13000 6月: 9000 I am learning to draw graphs with matplotlib, but import matplolib.plylot as plt does not recognize the data. I have pip installed matplotlib, but I suspect it is not installed in the right path. Is there any way to solve this problem?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This seems to be an issue with matplotlib 3.10.0 as can be seen in this GitHub issue. Downgrading to matplotlib&lt;3.10 solved it for a couple of users. See whether installing version 3.9 fixes your issue pip install matplotlib==3.9.0 or try other lower versions mentioned in the issue. Be aware that some older versions may not be compatible with recent releases of Python. See Release history.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The following code runs successfully on my computer, and my maplotlib verson is 3.7.1 if you don't know your matplotlib verson,you can press &quot;ctrl&quot; and 'r',then input &quot;cmd&quot; to open the terminal,and input &quot;pip list&quot;,then you can find your matlotlib version import matplotlib.pyplot as plt from matplotlib import rcParams # 设置支持中文字体 rcParams['font.sans-serif'] = ['SimHei'] # 使用黑体 rcParams['axes.unicode_minus'] = False # 正常显示负号 with open('notebook.txt', encoding='utf-8') as file: contents = file.readlines() # 按行读取文件内容 newList = [] for content in contents: newContent = content.replace('\\n', '') # 去掉换行符 money = newContent.split(':')[-1].strip() # 提取“:”后面的金额部分并去掉空格 newList.append(int(money)) x = [1, 2, 3, 4, 5, 6] plt.plot(x, newList, 'r', label='收入') # 绘制红色折线图，并设置图例标签 plt.xlabel('月份') # 设置 x 轴标签 plt.ylabel('金额') # 设置 y 轴标签 plt.legend() # 显示图例 plt.show() # 展示图形",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 9,
      "answer_score": 10,
      "created": "2025-01-18T06:59:22",
      "question_id": 79366678,
      "answer_id": 79366679
    }
  },
  {
    "question": "Find maximum length of consecutive zeros in each row",
    "expected_answer": "For a torch implementation, you can use scatter_reduce in place of np.maximum.at def test_np(x): padded_matrix = np.pad(x, ((0, 0), (1, 1)), constant_values=1) diffs = np.diff(padded_matrix, axis=1) start_indices = np.where(diffs == -1) end_indices = np.where(diffs == 1) run_lengths = end_indices[1] - start_indices[1] max_zeros = np.zeros(x.shape[0], dtype=int) np.maximum.at(max_zeros, start_indices[0], run_lengths) return max_zeros def test_torch(x): padded_matrix = F.pad(x, (1,1), value=1) diffs = torch.diff(padded_matrix, axis=1) start_indices = torch.where(diffs==-1) end_indices = torch.where(diffs == 1) run_lengths = end_indices[1] - start_indices[1] max_zeros = torch.zeros(x.shape[0]).long().scatter_reduce( dim=0, index=start_indices[0], src=run_lengths, reduce='amax' ) return max_zeros rows = 12 cols = 32 thresh = 0.5 x_torch = (torch.rand(rows, cols)&gt;thresh).long() x_np = x_torch.numpy() result_torch = test_torch(x_torch) result_np = test_np(x_np) Benchmarking on my system both versions give equivalent performance on CPU.",
    "context_chunks": [
      {
        "text": "My goal is to find the maximum length of consecutive zeros in each row. So for instance, if I have a tensor like input = torch.tensor([[0, 1, 0, 0, 0, 1],[0, 0, 1, 0, 1, 0],[1, 0, 0, 0, 0, 0]]) I expect to get the result tensor([3, 2, 5]) I’ve done this using numpy (listed below assuming “input” is a numpy binary matrix) and it tends to be very efficient, but I cannot seem to find a way using tensors that is at least as efficient. I’ve tried to follow a similar logic using torch operations but the performance is always worse. Thanks! import numpy as np input = np.array([[0, 1, 0, 0, 0, 1],[0, 0, 1, 0, 1, 0],[1, 0, 0, 0, 0, 0]]) # Pad the matrix with ones padded_matrix = np.pad(input, ((0, 0), (1, 1)), constant_values=1) # Compute differences diffs = np.diff(padded_matrix, axis=1) # Identify start and end of zero runs start_indices = np.where(diffs == -1) end_indices = np.where(diffs == 1) #Compute lengths of zero runs run_lengths = end_indices[1] - start_indices[1] # Create a result array initialized with zeros max_zeros = np.zeros(binaryGrid.shape[0], dtype=int) # Use np.maximum.at to find the maximum run length for each row np.maximum.at(max_zeros, start_indices[0], run_lengths) print(max_zeros)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "For a torch implementation, you can use scatter_reduce in place of np.maximum.at def test_np(x): padded_matrix = np.pad(x, ((0, 0), (1, 1)), constant_values=1) diffs = np.diff(padded_matrix, axis=1) start_indices = np.where(diffs == -1) end_indices = np.where(diffs == 1) run_lengths = end_indices[1] - start_indices[1] max_zeros = np.zeros(x.shape[0], dtype=int) np.maximum.at(max_zeros, start_indices[0], run_lengths) return max_zeros def test_torch(x): padded_matrix = F.pad(x, (1,1), value=1) diffs = torch.diff(padded_matrix, axis=1) start_indices = torch.where(diffs==-1) end_indices = torch.where(diffs == 1) run_lengths = end_indices[1] - start_indices[1] max_zeros = torch.zeros(x.shape[0]).long().scatter_reduce( dim=0, index=start_indices[0], src=run_lengths, reduce='amax' ) return max_zeros rows = 12 cols = 32 thresh = 0.5 x_torch = (torch.rand(rows, cols)&gt;thresh).long() x_np = x_torch.numpy() result_torch = test_torch(x_torch) result_np = test_np(x_np) Benchmarking on my system both versions give equivalent performance on CPU.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I don't have torch installed, but I guess you can borrow the idea below by using numpy and string.split np.array( [ max(np.vectorize(len)(s.split(&quot;1&quot;))) for s in np.apply_along_axis(lambda x: &quot;&quot;.join(x), 1, input.astype(str)) ] ) which should give array([3, 2, 5])",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "pytorch"
      ],
      "question_score": 9,
      "answer_score": 3,
      "created": "2024-12-05T17:00:00",
      "question_id": 79255608,
      "answer_id": 79255887
    }
  },
  {
    "question": "How to group dataframe rows into list in polars group_by",
    "expected_answer": "You could do this. import polars as pl df = pl.DataFrame( { 'Letter': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'D','D','E'], 'Value': [1, 2, 3, 4, 5, 6, 7, 8, 9,10] } ) g = df.group_by('Letter', maintain_order=True).agg(pl.col('Value')) print(g) This will print ┌────────┬───────────┐ │ Letter ┆ Value │ │ --- ┆ --- │ │ str ┆ list[i64] │ ╞════════╪═══════════╡ │ A ┆ [1, 2] │ │ B ┆ [3, 4, 5] │ │ C ┆ [6, 7] │ │ D ┆ [8, 9] │ │ E ┆ [10] │ └────────┴───────────┘ maintain_order=True is required if you want to order of the groups to be consistent with the input data.",
    "context_chunks": [
      {
        "text": "import polars as pl df = pl.DataFrame({ &quot;Letter&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;D&quot;, &quot;E&quot;], &quot;Value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] }) I want to group Letter and collect their corresponding Value in a List. Related Pandas question: How to group dataframe rows into list in pandas groupby I know pandas code will not work here: df.group_by(&quot;a&quot;)[&quot;b&quot;].apply(list) TypeError: 'GroupBy' object is not subscriptable Output will be: ┌────────┬───────────┐ │ Letter ┆ Value │ │ --- ┆ --- │ │ str ┆ list[i64] │ ╞════════╪═══════════╡ │ A ┆ [1, 2] │ │ B ┆ [3, 4, 5] │ │ C ┆ [6, 7] │ │ D ┆ [8, 9] │ │ E ┆ [10] │ └────────┴───────────┘",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You could do this. import polars as pl df = pl.DataFrame( { 'Letter': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'D','D','E'], 'Value': [1, 2, 3, 4, 5, 6, 7, 8, 9,10] } ) g = df.group_by('Letter', maintain_order=True).agg(pl.col('Value')) print(g) This will print ┌────────┬───────────┐ │ Letter ┆ Value │ │ --- ┆ --- │ │ str ┆ list[i64] │ ╞════════╪═══════════╡ │ A ┆ [1, 2] │ │ B ┆ [3, 4, 5] │ │ C ┆ [6, 7] │ │ D ┆ [8, 9] │ │ E ┆ [10] │ └────────┴───────────┘ maintain_order=True is required if you want to order of the groups to be consistent with the input data.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Very simple: If you don't specify an aggregation function in a pl.DataFrame.group_by().agg() construct, all values in a group will be aggregated into a list. df.group_by(&quot;Letter&quot;).agg(&quot;Value&quot;) shape: (5, 2) ┌────────┬───────────┐ │ Letter ┆ Value │ │ --- ┆ --- │ │ str ┆ list[i64] │ ╞════════╪═══════════╡ │ A ┆ [1, 2] │ │ B ┆ [3, 4, 5] │ │ C ┆ [6, 7] │ │ D ┆ [8, 9] │ │ E ┆ [10] │ └────────┴───────────┘",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-polars"
      ],
      "question_score": 9,
      "answer_score": 9,
      "created": "2024-03-04T15:17:10",
      "question_id": 78102296,
      "answer_id": 78102338
    }
  },
  {
    "question": "Is it possible use VS Code to pass multiple command line arguments to Python script?",
    "expected_answer": "Answering my own question after further testing. I will leave it unaccepted for a while to see if other answers come in. TL;DR This is possible as long as there are no spaces in the path to the Python interpreter. The fact that it breaks if there are spaces in the path is a currently open bug. The question arises because of a previously attempted bug fix. Workaround Make sure your launch.json does not have brackets around ${command:pickArgs}. Make sure the path to Python interpreter has no spaces. If it does, create a link to the environment containing the interpreter (the target) from a path that does not have spaces. On Windows in PowerShell, this looks like: New-Item -ItemType Junction -Path C:any\\path\\without\\spaces -Target 'C:\\path to Python\\environment which may\\have spaces in it' Note that the ItemType has to be Junction, not SymbolicLink. If you use a SymbolicLink, the path will be followed (it will find the interpreter) but then replaced with the target path, and the embedded spaces will still create problems. Long-Term Wait for currently open issue Spaces in python interpreter path or program result in incorrectly quoted debugging commands with arguments #233 to be fixed. Explanation As far as I can tell, this is what happened. In a previous version of VS Code, launch.json would be created with the following configuration: &quot;args&quot;: &quot;${command:pickArgs}&quot; Notice there are no brackets around ${command:pickArgs}. This works and allows the user to submit multiple command line arguments via dialog box when debugging. However, due to a bug, there was an error if the path to the Python interpreter had spaces in it: Spaces in python interpreter path or program result in incorrectly quoted debugging commands with arguments #233. An attempted fix inserted brackets into launch.json: Fix error in args with default config #385. This fixed the problem with spaces in the Python path, but created the problem that the question asks about, that multiple command line arguments are treated as part of the same string. This fix appears to have made it into v1.92.2, the version I am using, breaking the ability to parse command line arguments. The solution therefore is to remove the brackets around ${command:pickArgs} introduced by the fix in issue #385, but since the Python environment I was testing had spaces in the path, this triggered issue #233. After additional information, issue #233 was reopened. The workaround is to make sure to use an environment without spaces in the path. The long-term solution is to wait for the fix to issue #233.",
    "context_chunks": [
      {
        "text": "According to the official documentation &quot;Python debugging in VS Code&quot;, launch.json can be configured to run with specific command line arguments, or you can use ${command:pickArgs} to input arguments at run time. Examples of putting arguments in launch.json: Specifying arguments in launch.json for Python Visual Studio Code: How debug Python script with arguments However, I would rather use but I would rather use ${command:pickArgs} because it makes it easier to test multiple times with different values. The first time I tried this, I allowed VS Code to create launch.json. By default it contained the following: &quot;args&quot;: [ &quot;${command:pickArgs}&quot; ] When I run the file, I get a dialog for inputting arguments: However, if I put in multiple arguments, they get wrapped in quotes and treated as a single string argument. In a case where, e.g. the arguments are supposed to be numeric, an error is generated. For example, if I pass in 4 7, which both need to be cast to int, sys.argv[1] gets the value '4 7' rather than '4', yielding the error invalid literal for int() with base 10: '4 7' I have tried comma-separating the arguments, and putting quotes around them, and what I get is sys.argv[1] with values like '4, 7' or '&quot;4&quot;, &quot;7&quot;'. Needless to say, these don't work either. I've seen examples online of a launch.json configuration as follows: &quot;args&quot;: &quot;${command:pickArgs}&quot; That is, there are no brackets around ${command:pickArgs}. However, this generates a problem in that if there are spaces in the path to the Python interpreter, the path gets broken apart at the spaces. See, for example: Spaces in python interpreter path or program result in incorrectly quoted debugging commands with arguments #233 The solution seems to be to put the brackets in, which is what I started with in the first place. Since that's what VS Code did automatically, I'm not sure where the varying examples are coming from (with or without brackets) and can't find documentation on this other than the very short mention of ${command:pickArgs} in the official documentation I linked at the very beginning. So, I have not been able to figure out a way to pass in multiple arguments using ${command:pickArgs} (as opposed to hard-coding directly in launch.json), and the only promising solution (remove the brackets) is poorly documented, generates other errors, and the solution seems to be to put the brackets back in. Is this possible to do at all?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Answering my own question after further testing. I will leave it unaccepted for a while to see if other answers come in. TL;DR This is possible as long as there are no spaces in the path to the Python interpreter. The fact that it breaks if there are spaces in the path is a currently open bug. The question arises because of a previously attempted bug fix. Workaround Make sure your launch.json does not have brackets around ${command:pickArgs}. Make sure the path to Python interpreter has no spaces. If it does, create a link to the environment containing the interpreter (the target) from a path that does not have spaces. On Windows in PowerShell, this looks like: New-Item -ItemType Junction -Path C:any\\path\\without\\spaces -Target 'C:\\path to Python\\environment which may\\have spaces in it' Note that the ItemType has to be Junction, not SymbolicLink. If you use a SymbolicLink, the path will be followed (it will find the interpreter) but then replaced with the target path, and the embedded spaces will still create problems. Long-Term Wait for currently open issue Spaces in python interpreter path or program result in incorrectly quoted debugging commands with arguments #233 to be fixed. Explanation As far as I can tell, this is what happened. In a previous version of VS Code, launch.json would be created with the following configuration: &quot;args&quot;: &quot;${command:pickArgs}&quot; Notice there are no brackets around ${command:pickArgs}. This works and allows the user to submit multiple command line arguments via dialog box when debugging. However, due to a bug, there was an error if the path to the Python interpreter had spaces in it: Spaces in python interpreter path or program result in incorrectly quoted debugging commands with arguments #233. An attempted fix inserted brackets into launch.json: Fix error in args with default config #385. This fixed the problem with spaces in the Python path, but created the problem that the question asks about, that multiple command line arguments are treated as part of the same string. This fix appears to have made it into v1.92.2, the version I am using, breaking the ability to parse command line arguments. The solution therefore is to remove the brackets around ${command:pickArgs} introduced by the fix in issue #385, but since the Python environment I was testing had spaces in the path, this triggered issue #233. After additional information, issue #233 was reopened. The workaround is to make sure to use an environment without spaces in the path. The long-term solution is to wait for the fix to issue #233.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I have been fighting this for a while. I finally got fed up and remembered I'm using python so why fight vscode when it can be easily fixed in python: launch.json &quot;configurations&quot;: [ { &quot;name&quot;: &quot;Run ...&quot;, &quot;type&quot;: &quot;debugpy&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/program.py&quot;, &quot;console&quot;: &quot;integratedTerminal&quot;, &quot;env&quot;: { &quot;USED_VSCODE_COMMAND_PICKARGS&quot;: &quot;1&quot; }, &quot;args&quot;: [&quot;${command:pickArgs}&quot;] } ] program.py import argparse, os, shlex, sys ... argv = ( # VSCode's ${command:pickArgs} is passed as one string, split it up shlex.split(&quot; &quot;.join(sys.argv[1:])) if &quot;USED_VSCODE_COMMAND_PICKARGS&quot; in os.environ else sys.argv[1:] ) args = parser(argv) Note: the args: [...] in launch.json can have whatever args you would normally put there including more ${command:pickArgs} and/or ${input:*} args example VSCode &gt; Run and Debug &gt; Run ... In the &quot;Command Line Arguments&quot; popup enter: are these &quot;the args&quot; --you -R 'looking for' -? In program.py, argv becomes: ['are', 'these', 'the args', '--you', '-R', 'looking for', '-?'] and is handled perfectly fine by the argparse parser",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code",
        "vscode-debugger"
      ],
      "question_score": 9,
      "answer_score": 8,
      "created": "2024-08-23T20:13:41",
      "question_id": 78907444,
      "answer_id": 78912781
    }
  },
  {
    "question": "Use polars when-then-otherwise on multiple output columns at once",
    "expected_answer": "Apply the condition to all columns at once. Also replace alias by name.keep: cols = ['value', 'value_other', 'value_other2'] (df.with_columns( pl.when(pl.col('value').is_null() &amp; (pl.col('quantity') == 0)) .then(0) .otherwise(pl.col(cols)) .name.keep() )) Output: ┌────────┬──────────┬───────┬─────────────┬──────────────┐ │ item ┆ quantity ┆ value ┆ value_other ┆ value_other2 │ │ --- ┆ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ i64 ┆ i64 ┆ i64 ┆ i64 │ ╞════════╪══════════╪═══════╪═════════════╪══════════════╡ │ CASH ┆ 100 ┆ 99 ┆ 97 ┆ 94 │ │ CHECK ┆ -20 ┆ 47 ┆ 57 ┆ 37 │ │ DEBT ┆ 0 ┆ 0 ┆ 0 ┆ 0 │ │ CHECK ┆ 10 ┆ 90 ┆ 91 ┆ 93 │ │ CREDIT ┆ 0 ┆ 0 ┆ 0 ┆ 0 │ │ CASH ┆ 0 ┆ 120 ┆ 110 ┆ 115 │ └────────┴──────────┴───────┴─────────────┴──────────────┘",
    "context_chunks": [
      {
        "text": "Assume I have this dataframe import polars as pl df = pl.DataFrame({ 'item': ['CASH', 'CHECK', 'DEBT', 'CHECK', 'CREDIT', 'CASH'], 'quantity': [100, -20, 0, 10, 0, 0], 'value': [99, 47, None, 90, None, 120], 'value_other': [97, 57, None, 91, None, 110], 'value_other2': [94, 37, None, 93, None, 115], }) ┌────────┬──────────┬───────┬─────────────┬──────────────┐ │ item ┆ quantity ┆ value ┆ value_other ┆ value_other2 │ │ --- ┆ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ i64 ┆ i64 ┆ i64 ┆ i64 │ ╞════════╪══════════╪═══════╪═════════════╪══════════════╡ │ CASH ┆ 100 ┆ 99 ┆ 97 ┆ 94 │ │ CHECK ┆ -20 ┆ 47 ┆ 57 ┆ 37 │ │ DEBT ┆ 0 ┆ null ┆ null ┆ null │ │ CHECK ┆ 10 ┆ 90 ┆ 91 ┆ 93 │ │ CREDIT ┆ 0 ┆ null ┆ null ┆ null │ │ CASH ┆ 0 ┆ 120 ┆ 110 ┆ 115 │ └────────┴──────────┴───────┴─────────────┴──────────────┘ Now I want to set all value columns to 0 for all rows where value is null and quantity == 0. Right now I have this solution cols = ['value', 'value_other', 'value_other2'] df = df.with_columns([ pl.when(pl.col('value').is_null() &amp; (pl.col('quantity') == 0)) .then(0) .otherwise(pl.col(col)) .alias(col) for col in cols ]) which correctly gives ┌────────┬──────────┬───────┬─────────────┬──────────────┐ │ item ┆ quantity ┆ value ┆ value_other ┆ value_other2 │ │ --- ┆ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ i64 ┆ i64 ┆ i64 ┆ i64 │ ╞════════╪══════════╪═══════╪═════════════╪══════════════╡ │ CASH ┆ 100 ┆ 99 ┆ 97 ┆ 94 │ │ CHECK ┆ -20 ┆ 47 ┆ 57 ┆ 37 │ │ DEBT ┆ 0 ┆ 0 ┆ 0 ┆ 0 │ │ CHECK ┆ 10 ┆ 90 ┆ 91 ┆ 93 │ │ CREDIT ┆ 0 ┆ 0 ┆ 0 ┆ 0 │ │ CASH ┆ 0 ┆ 120 ┆ 110 ┆ 115 │ └────────┴──────────┴───────┴─────────────┴──────────────┘ However, I feel this is very inefficient as my when condition is executed for every value column. Is there a way to achieve this using only polar internal functions &amp; without the native for-loop?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Apply the condition to all columns at once. Also replace alias by name.keep: cols = ['value', 'value_other', 'value_other2'] (df.with_columns( pl.when(pl.col('value').is_null() &amp; (pl.col('quantity') == 0)) .then(0) .otherwise(pl.col(cols)) .name.keep() )) Output: ┌────────┬──────────┬───────┬─────────────┬──────────────┐ │ item ┆ quantity ┆ value ┆ value_other ┆ value_other2 │ │ --- ┆ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ i64 ┆ i64 ┆ i64 ┆ i64 │ ╞════════╪══════════╪═══════╪═════════════╪══════════════╡ │ CASH ┆ 100 ┆ 99 ┆ 97 ┆ 94 │ │ CHECK ┆ -20 ┆ 47 ┆ 57 ┆ 37 │ │ DEBT ┆ 0 ┆ 0 ┆ 0 ┆ 0 │ │ CHECK ┆ 10 ┆ 90 ┆ 91 ┆ 93 │ │ CREDIT ┆ 0 ┆ 0 ┆ 0 ┆ 0 │ │ CASH ┆ 0 ┆ 120 ┆ 110 ┆ 115 │ └────────┴──────────┴───────┴─────────────┴──────────────┘",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can pass list of column names into pl.col() and when\\then\\otherwise accepts Expr which can contain multiple columns. cols = ['value', 'value_other', 'value_other2'] df.with_columns( pl .when((pl.col.quantity != 0) | pl.col.value.is_not_null()) .then(pl.col(cols)) .otherwise(0) ) # or df.with_columns( pl .when(pl.col.quantity != 0).then(pl.col(cols)) .when(pl.col.value.is_not_null()).then(pl.col(cols)) .otherwise(0) ) shape: (6, 5) ┌────────┬──────────┬───────┬─────────────┬──────────────┐ │ item ┆ quantity ┆ value ┆ value_other ┆ value_other2 │ │ --- ┆ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ i64 ┆ i64 ┆ i64 ┆ i64 │ ╞════════╪══════════╪═══════╪═════════════╪══════════════╡ │ CASH ┆ 100 ┆ 99 ┆ 97 ┆ 94 │ │ CHECK ┆ -20 ┆ 47 ┆ 57 ┆ 37 │ │ DEBT ┆ 0 ┆ 0 ┆ 0 ┆ 0 │ │ CHECK ┆ 10 ┆ 90 ┆ 91 ┆ 93 │ │ CREDIT ┆ 0 ┆ 0 ┆ 0 ┆ 0 │ │ CASH ┆ 0 ┆ 120 ┆ 110 ┆ 115 │ └────────┴──────────┴───────┴─────────────┴──────────────┘",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "dataframe",
        "python-polars"
      ],
      "question_score": 9,
      "answer_score": 8,
      "created": "2024-08-01T16:56:20",
      "question_id": 78822168,
      "answer_id": 78822193
    }
  },
  {
    "question": "How do I download all files from a Google Drive folder with more than 50 files?",
    "expected_answer": "You can use the Google Drive API: https://developers.google.com/drive/api/quickstart/python Here is a script I have used in the past: from googleapiclient.discovery import build from googleapiclient.http import MediaIoBaseDownload from google_auth_oauthlib.flow import InstalledAppFlow import io import os # Define the scopes SCOPES = ['https://www.googleapis.com/auth/drive.readonly'] # Obtain your Google credentials def get_credentials(): flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES) creds = flow.run_local_server(port=0) return creds # Build the downloader creds = get_credentials() drive_downloader = build('drive', 'v3', credentials=creds) # Replace 'FOLDER_ID' with your actual Google Drive folder ID folder_id = 'FOLDER_ID' # query = f&quot;Folder ID '{folder_id}'&quot; # you may get error for this line query = f&quot;'{folder_id}' in parents&quot; # this works ref https://stackoverflow.com/q/73119251/248616 results = drive_downloader.files().list(q=query, pageSize=1000).execute() items = results.get('files', []) # Download the files for item in items: request = drive_downloader.files().get_media(fileId=item['id']) f = io.FileIO(item['name'], 'wb') downloader = MediaIoBaseDownload(f, request) done = False while done is False: status, done = downloader.next_chunk() print(f&quot;Download {int(status.progress() * 100)}.&quot;) print(f&quot;Downloaded {len(items)} files from the folder.&quot;)",
    "context_chunks": [
      {
        "text": "I cannot figure out how to write a program to download all files from a publicly accessible Google Drive folder, which has more than 1,000 of them. This is what I've tried so far: import gdown url = 'https://drive.google.com/drive/folders/MY-PUBLICLY-ACCESSIBLE-FOLDER-ID?usp=drive_link' gdown.download_folder(url, quiet=True, remaining_ok=True, use_cookies=False) But it only downloads 50 of the files.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can use the Google Drive API: https://developers.google.com/drive/api/quickstart/python Here is a script I have used in the past: from googleapiclient.discovery import build from googleapiclient.http import MediaIoBaseDownload from google_auth_oauthlib.flow import InstalledAppFlow import io import os # Define the scopes SCOPES = ['https://www.googleapis.com/auth/drive.readonly'] # Obtain your Google credentials def get_credentials(): flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES) creds = flow.run_local_server(port=0) return creds # Build the downloader creds = get_credentials() drive_downloader = build('drive', 'v3', credentials=creds) # Replace 'FOLDER_ID' with your actual Google Drive folder ID folder_id = 'FOLDER_ID' # query = f&quot;Folder ID '{folder_id}'&quot; # you may get error for this line query = f&quot;'{folder_id}' in parents&quot; # this works ref https://stackoverflow.com/q/73119251/248616 results = drive_downloader.files().list(q=query, pageSize=1000).execute() items = results.get('files', []) # Download the files for item in items: request = drive_downloader.files().get_media(fileId=item['id']) f = io.FileIO(item['name'], 'wb') downloader = MediaIoBaseDownload(f, request) done = False while done is False: status, done = downloader.next_chunk() print(f&quot;Download {int(status.progress() * 100)}.&quot;) print(f&quot;Downloaded {len(items)} files from the folder.&quot;)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Work Around: Since gdown's download_folder function doesn't care how many subfolders of 50 files or fewer it has to download we can use the following function to organize the files in the folder you want to save in a new path by creating a temp_folder where all the files are moved into subfolders comprising of 50 files or fewer and then running the gdown's download_folder function: import os def organize_folder_into_subfolders(path_to_original_folder, max_number_of_files_per_subfolder=50): '''Moves all files in a folder into newly created subfolders comprising of the max_number_of_files_per_subfolder or fewer''' files_in_folder = os.listdir(path_to_original_folder) if not path_to_original_folder.endswith('/'): path_to_original_folder += '/' temp_path_to_original_folder = path_to_original_folder + 'temp_folder' os.makedirs(temp_path_to_original_folder) subfolders_dict = {'temp_subfolder_0': []} os.makedirs(temp_path_to_original_folder + '/' + 'temp_subfolder_0') for _file_name in files_in_folder: if len(subfolders_dict['temp_subfolder_' + str(len(subfolders_dict) - 1)]) == max_number_of_files_per_subfolder: subfolders_dict['temp_subfolder_' + str(len(subfolders_dict))] = [] os.makedirs(temp_path_to_original_folder + '/' + 'temp_subfolder_' + str(len(subfolders_dict) - 1)) subfolders_dict['temp_subfolder_' + str(len(subfolders_dict) - 1)].append(_file_name) for _file_subfolder_path, _file_names in subfolders_dict.items(): for _file_name in _file_names: os.rename(path_to_original_folder + _file_name, temp_path_to_original_folder + '/' + _file_subfolder_path + '/' + _file_name) return subfolders_dict And then run the download_folder function: import gdown url = 'https://drive.google.com/drive/folders/1OXV4qhFF_qJ8VqyrXpR7CzHDsToaqY_W?usp=drive_link' gdown.download_folder(url, quiet=True, use_cookies=False, remaining_ok=True) And then if you want your original and new folders not organized as subfolders, we can use this function to &quot;undo&quot; or put the files back into the original and new folders and delete the temp subfolders: import os def undo_organize_folder_into_subfolders(path_to_original_folder, path_to_new_folder, subfolders_dict): '''Moves the files organized as subfolders back to the original &amp; new folders and deletes subfolders''' if not path_to_original_folder.endswith('/'): path_to_original_folder += '/' if not path_to_new_folder.endswith('/'): path_to_new_folder += '/' temp_path_to_original_folder = path_to_original_folder + 'temp_folder' temp_path_to_new_folder = path_to_new_folder + 'temp_folder' for _file_subfolder_path, _file_names in subfolders_dict.items(): for _file_name in _file_names: os.rename(temp_path_to_original_folder + '/' + _file_subfolder_path + '/' + _file_name, path_to_original_folder + _file_name) os.rename(temp_path_to_new_folder + '/' + _file_subfolder_path + '/' + _file_name, path_to_new_folder + _file_name) os.rmdir(temp_path_to_original_folder + '/' + _file_subfolder_path) os.rmdir(temp_path_to_new_folder + '/' + _file_subfolder_path) os.rmdir(temp_path_to_original_folder) os.rmdir(temp_path_to_new_folder) And just make sure you have your current working directory set: from google.colab import drive drive.mount('/content/drive', force_remount=True) %cd '/content/drive/My Drive/Colab Notebooks/'",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "google-drive-api"
      ],
      "question_score": 9,
      "answer_score": 11,
      "created": "2023-06-15T19:34:13",
      "question_id": 76485003,
      "answer_id": 76736234
    }
  },
  {
    "question": "NotImplementedError: Cannot copy out of meta tensor; no data",
    "expected_answer": "By searching I found that: GPU is the problem This error is caused by Accelerate auto-offloading weights to either the CPU or disk because of insufficient memory on the GPU. So on the g3.4xlarge (8GB VRAM, 122 GB memory) you'd run: python inference/bot.py --model togethercomputer/Pythia-Chat-Base-7B -g 0:6 -r 120. This will load up to 6 GB of the model onto the gpu and the rest into memory.",
    "context_chunks": [
      {
        "text": "base_model = AutoModelForCausalLM.from_pretrained( 'meta-llama/Llama-2-7b-chat-hf', token=access_token, trust_remote_code=True, device_map=&quot;auto&quot;, torch_dtype=torch.float16, offload_folder=&quot;offload/&quot; ) model = PeftModel.from_pretrained( base_model, 'FinGPT/fingpt-forecaster_dow30_llama2-7b_lora', token=access_token, offload_folder=&quot;offload/&quot; ) model = model.eval() After running this model in google colab, I am encountering the error: NotImplementedError: Cannot copy out of meta tensor; no data! Please try to resolve the error I tried to change float16 to bfloat16 and float32 but still encountered the same error.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "By searching I found that: GPU is the problem This error is caused by Accelerate auto-offloading weights to either the CPU or disk because of insufficient memory on the GPU. So on the g3.4xlarge (8GB VRAM, 122 GB memory) you'd run: python inference/bot.py --model togethercomputer/Pythia-Chat-Base-7B -g 0:6 -r 120. This will load up to 6 GB of the model onto the gpu and the rest into memory.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In my case, passing argument device_map='auto' wasn't reliable and causing the same error. Solution, dynamically check if there's a GPU. def get_device_map() -&gt; str: return 'cuda' if torch.cuda.is_available() else 'cpu' device = get_device_map() # 'cpu' base_model = LlamaForCausalLM.from_pretrained( pretrained_model_name_or_path=huggingface_params.pretrained_model_name, load_in_8bit=False, device_map=device, # HERE offload_folder=huggingface_params.offload_folder ) base_model.config.save_pretrained(huggingface_params.offload_folder) base_model.save_pretrained(huggingface_params.offload_folder) Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&lt;00:00, 3.56it/s] This allowed me to successfully instantiate and offload the model: base_model = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path=huggingface_params.offload_folder) Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:30&lt;00:00, 15.08s/it]",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 9,
      "answer_score": 7,
      "created": "2023-11-25T09:51:59",
      "question_id": 77547377,
      "answer_id": 77547603
    }
  },
  {
    "question": "Segmentation Fault when Using SentenceTransformer Inside Docker Container",
    "expected_answer": "Couldn't find out why newer python versions are failing, but python:3.6.15 tag works just fine on mac arm64 apple silicon M1. Edit (better alternative based in @jackson-bierfeldt comment): Downgrading PyTorch to 2.0.1 solved it. So, put something like torch==2.0.* to requirements.txt, no need to downgrade Python itself.",
    "context_chunks": [
      {
        "text": "Edit: After test on different machines it is Apple M1 M2 specific bug. I am trying to run a Flask application inside a Docker container on Apple Silicon M2 (could be an issue), where I use the SentenceTransformer model to encode sentences. However, when I call the encode method on the model, the application crashes with a segmentation fault. Here's the relevant code: from sentence_transformers import SentenceTransformer model = SentenceTransformer('all-MiniLM-L6-v2') #Our sentences we like to encode sentences = ['This framework generates embeddings for each input sentence', 'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.'] #Sentences are encoded by calling model.encode() sentence_embeddings = model.encode(sentences) Source: https://www.sbert.net/docs/quickstart.html after using import faulthandler and faulthandler.enable() The error traceback is: Fatal Python error: Segmentation fault Thread 0x0000ffff640ff1a0 (most recent call first): File &quot;/usr/local/lib/python3.11/threading.py&quot;, line 331 in wait File &quot;/usr/local/lib/python3.11/threading.py&quot;, line 629 in wait File &quot;/usr/local/lib/python3.11/site-packages/tqdm/_monitor.py&quot;, line 60 in run File &quot;/usr/local/lib/python3.11/threading.py&quot;, line 1045 in _bootstrap_inner File &quot;/usr/local/lib/python3.11/threading.py&quot;, line 1002 in _bootstrap Current thread 0x0000ffffa6814020 (most recent call first): File &quot;/usr/local/lib/python3.11/site-packages/transformers/activations.py&quot;, line 78 in forward File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1527 in _call_impl File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1518 in _wrapped_call_impl File &quot;/usr/local/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py&quot;, line 452 in forward File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1527 in _call_impl File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1518 in _wrapped_call_impl File &quot;/usr/local/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py&quot;, line 551 in feed_forward_chunk File &quot;/usr/local/lib/python3.11/site-packages/transformers/pytorch_utils.py&quot;, line 240 in apply_chunking_to_forward File &quot;/usr/local/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py&quot;, line 539 in forward File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1527 in _call_impl File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1518 in _wrapped_call_impl File &quot;/usr/local/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py&quot;, line 612 in forward File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1527 in _call_impl File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1518 in _wrapped_call_impl File &quot;/usr/local/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py&quot;, line 1022 in forward File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1527 in _call_impl File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1518 in _wrapped_call_impl File &quot;/usr/local/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py&quot;, line 66 in forward File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1527 in _call_impl File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1518 in _wrapped_call_impl File &quot;/usr/local/lib/python3.11/site-packages/torch/nn/modules/container.py&quot;, line 215 in forward File &quot;/usr/local/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py&quot;, line 165 in encode File &quot;&lt;stdin&gt;&quot;, line 1 in &lt;module&gt; Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.linalg._flinalg, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._statlib, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, regex._regex, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.utils._logistic_sigmoid, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.utils._vector_sentinel, sklearn.feature_extraction._hashing_fast, sklearn.utils._random, sklearn.utils._seq_dataset, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_fast, sklearn.linear_model._cd_fast, sklearn._loss._loss, sklearn.utils.arrayfuncs, sklearn.svm._liblinear, sklearn.svm._libsvm, sklearn.svm._libsvm_sparse, sklearn.utils._weight_vector, sklearn.linear_model._sgd_fast, sklearn.linear_model._sag_fast, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, sklearn.datasets._svmlight_format_fast, charset_normalizer.md, yaml._yaml, sentencepiece._sentencepiece, PIL._imaging (total: 163) Segmentation fault Some points: The Docker container has ample memory allocated. I've tried updating the libraries (torch, transformers, and sentence-transformers). The same code works perfectly outside the Docker environment. How can I resolve this segmentation fault when running the code inside Docker? Here is a pip list of actual versions. Package Version --------------------- --------- blinker 1.6.3 certifi 2023.7.22 charset-normalizer 3.3.0 click 8.1.7 filelock 3.12.4 Flask 3.0.0 fsspec 2023.9.2 huggingface-hub 0.17.3 idna 3.4 itsdangerous 2.1.2 Jinja2 3.1.2 joblib 1.3.2 MarkupSafe 2.1.3 mpmath 1.3.0 networkx 3.1 nltk 3.8.1 numpy 1.26.0 packaging 23.2 Pillow 10.0.1 pip 23.2.1 PyYAML 6.0.1 regex 2023.10.3 requests 2.31.0 safetensors 0.4.0 scikit-learn 1.3.1 scipy 1.11.3 sentence-transformers 2.2.2 sentencepiece 0.1.99 setuptools 65.5.1 sympy 1.12 threadpoolctl 3.2.0 tokenizers 0.14.1 torch 2.1.0 torchvision 0.16.0 tqdm 4.66.1 transformers 4.34.0 typing_extensions 4.8.0 urllib3 2.0.6 Werkzeug 3.0.0 wheel 0.41.2 For more clarification, here is the most important part of my Dockerfile. FROM python:3.11 RUN pip install --upgrade pip RUN pip install Flask==3.0.0 sentence-transformers==2.2.2",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Couldn't find out why newer python versions are failing, but python:3.6.15 tag works just fine on mac arm64 apple silicon M1. Edit (better alternative based in @jackson-bierfeldt comment): Downgrading PyTorch to 2.0.1 solved it. So, put something like torch==2.0.* to requirements.txt, no need to downgrade Python itself.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Encountered the same issue on an M2 Pro 13.4.1 developing inside the official Python 3.11.4 Docker image. I was encountering the issue on both PyTorch versions 2.1.0 and 2.0.1+cpu, so the other fixes stated did not apply in my situation. I was able to resolve the segmentation faults by switching to the official PyTorch Docker image. I was able to use latest, which at the time of this post is the following image. While not super ideal, it did the trick in my case",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "docker",
        "apple-m1",
        "sentence-transformers"
      ],
      "question_score": 9,
      "answer_score": 8,
      "created": "2023-10-13T18:36:32",
      "question_id": 77290003,
      "answer_id": 77293545
    }
  },
  {
    "question": "Pydantic v2 custom type validators with info",
    "expected_answer": "A simpler approach would be to perform validation via an Annotated type. However, there are cases where you may need a fully customized type. Annotated Field (The easy way) from datetime import datetime, date from functools import partial from typing import Any, List from typing_extensions import Annotated from pydantic import TypeAdapter from pydantic.functional_validators import BeforeValidator def try_parse_date(v: Any, allowed_formats: List[str]) -&gt; Any: if isinstance(v, str): for fmt in allowed_formats: try: return datetime.strptime(v, fmt).date() except ValueError: continue else: return v CustomDate = Annotated[ date, BeforeValidator( partial( try_parse_date, allowed_formats=['%Y-%m-%d', '%Y/%m/%d', '%Y_%m_%d'] ) ) ] Here's a test which ensures expected behavior: def test_custom_type(): values = [ &quot;2023-06-01&quot;, &quot;2023/06/01&quot;, &quot;2023_06_01&quot;, date(2023, 6, 1) ] expected = date(2023, 6, 1) ta = TypeAdapter(CustomDate) result = [ta.validate_python(x) for x in values] assert all(x==expected for x in result) Fully Customized Type The issue you are experiencing relates to the order of which pydantic executes validation. Given that date format has its own core schema (ex: will validate a timestamp or similar conversion), you will want to execute your validation prior to the core validation. Related Answer (with simpler code): Defining custom types in Pydantic v2 To solve this, you will need to define __get_pydantic_core_schema__ in your custom type. I've chained the schema validation below, which allows multiple types to be merged into one (ex: say you want to convert a datetime into a date, you can do so in the chain). I've also used general_plain_validator_function which doesn't need a specific schema to operate (the most vanilla option). Code for creating custom Date type in Pydantic V2: from datetime import datetime, date from typing import Any, List from pydantic import BaseModel, GetCoreSchemaHandler from pydantic_core import CoreSchema, core_schema class CustomDate(date): &quot;&quot;&quot;Custom date&quot;&quot;&quot; allowed_formats: List[str] = ['%Y-%m-%d', '%Y/%m/%d'] @classmethod def try_parse_date(cls, v: Any, info: core_schema.ValidationInfo) -&gt; Any: if isinstance(v, str): for fmt in cls.allowed_formats: try: return datetime.strptime(v, fmt).date() except ValueError: continue else: return v @classmethod def truncate_datetime(cls, v: Any, info: core_schema.ValidationInfo) -&gt; Any: &quot;&quot;&quot;If a datetime value is provided, truncate to a date&quot;&quot;&quot; if isinstance(v, datetime): return v.date() else: return v @classmethod def __get_pydantic_core_schema__( cls, source_type: Any, handler: GetCoreSchemaHandler ) -&gt; CoreSchema: return core_schema.chain_schema( [ core_schema.general_plain_validator_function( function=cls.truncate_datetime, ), core_schema.general_plain_validator_function( function=cls.try_parse_date, ) ] ) You can use the defined type, or customize allow_formats by subclassing the type: class ExampleModel(BaseModel): class MyDate(CustomDate): allowed_formats = ['%Y-%m-%d', '%Y/%m/%d', '%Y_%m_%d'] dt: MyDate Here's a quick test which shows things are working: def test_model(): values = [ &quot;2023-06-01&quot;, &quot;2023/06/01&quot;, &quot;2023_06_01&quot;, date(2023, 6, 1), datetime(2023, 6, 1, 1) ] expected = date(2023, 6, 1) data = [ExampleModel(dt=v) for v in values] assert all(x.dt == expected for x in data)",
    "context_chunks": [
      {
        "text": "I'm trying to update my code to pydantic v2 and having trouble finding a good way to replicate the custom types I had in version 1. I'll use my custom date type as an example. The original implementation and usage looked something like this: from datetime import date from pydantic import BaseModel class CustomDate(date): # Override POTENTIAL_FORMATS and fill it with date format strings to match your data POTENTIAL_FORMATS = [] @classmethod def __get_validators__(cls): yield cls.validate_date @classmethod def validate_date(cls, field_value, values, field, config) -&gt; date: if type(field_value) is date: return field_value return to_date(field.name, field_value, cls.POTENTIAL_FORMATS, return_str=False) class ExampleModel(BaseModel): class MyDate(CustomDate): POTENTIAL_FORMATS = ['%Y-%m-%d', '%Y/%m/%d'] dt: MyDate I tried to follow the official docs and the examples laid out here below and it mostly worked, but the info parameter does not have the fields I need (data and field_name). Attempting to access them gives me an AttributeError. info.field_name *** AttributeError: No attribute named 'field_name' Both the Annotated and __get_pydantic_core_schema__ approaches have this issue from datetime import date from typing import Annotated from pydantic import BaseModel, BeforeValidator from pydantic_core import core_schema class CustomDate: POTENTIAL_FORMATS = [] @classmethod def validate(cls, field_value, info): if type(field_value) is date: return field_value return to_date(info.field_name, field_value, potential_formats, return_str=False) @classmethod def __get_pydantic_core_schema__(cls, source, handler) -&gt; core_schema.CoreSchema: return core_schema.general_plain_validator_function(cls.validate) def custom_date(potential_formats): &quot;&quot;&quot; :param potential_formats: A list of datetime format strings &quot;&quot;&quot; def validate_date(field_value, info) -&gt; date: if type(field_value) is date: return field_value return to_date(info.field_name, field_value, potential_formats, return_str=False) CustomDate = Annotated[date, BeforeValidator(validate_date)] return CustomDate class ExampleModel(BaseModel): class MyDate(CustomDate): POTENTIAL_FORMATS = ['%Y-%m-%d', '%Y/%m/%d'] dt: MyDate dt2: custom_date(['%Y-%m-%d', '%Y/%m/%d']) If I just include the validate_date function as a regular field_validator I get info with all the fields I need, it's only when using it with custom types that I see this issue. How do I write a custom type that has access to previously validated fields and the name of the field being validated?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "A simpler approach would be to perform validation via an Annotated type. However, there are cases where you may need a fully customized type. Annotated Field (The easy way) from datetime import datetime, date from functools import partial from typing import Any, List from typing_extensions import Annotated from pydantic import TypeAdapter from pydantic.functional_validators import BeforeValidator def try_parse_date(v: Any, allowed_formats: List[str]) -&gt; Any: if isinstance(v, str): for fmt in allowed_formats: try: return datetime.strptime(v, fmt).date() except ValueError: continue else: return v CustomDate = Annotated[ date, BeforeValidator( partial( try_parse_date, allowed_formats=['%Y-%m-%d', '%Y/%m/%d', '%Y_%m_%d'] ) ) ] Here's a test which ensures expected behavior: def test_custom_type(): values = [ &quot;2023-06-01&quot;, &quot;2023/06/01&quot;, &quot;2023_06_01&quot;, date(2023, 6, 1) ] expected = date(2023, 6, 1) ta = TypeAdapter(CustomDate) result = [ta.validate_python(x) for x in values] assert all(x==expected for x in result) Fully Customized Type The issue you are experiencing relates to the order of which pydantic executes validation. Given that date format has its own core schema (ex: will validate a timestamp or similar conversion), you will want to execute your validation prior to the core validation. Related Answer (with simpler code): Defining custom types in Pydantic v2 To solve this, you will need to define __get_pydantic_core_schema__ in your custom type. I've chained the schema validation below, which allows multiple types to be merged into one (ex: say you want to convert a datetime into a date, you can do so in the chain). I've also used general_plain_validator_function which doesn't need a specific schema to operate (the most vanilla option). Code for creating custom Date type in Pydantic V2: from datetime import datetime, date from typing import Any, List from pydantic import BaseModel, GetCoreSchemaHandler from pydantic_core import CoreSchema, core_schema class CustomDate(date): &quot;&quot;&quot;Custom date&quot;&quot;&quot; allowed_formats: List[str] = ['%Y-%m-%d', '%Y/%m/%d'] @classmethod def try_parse_date(cls, v: Any, info: core_schema.ValidationInfo) -&gt; Any: if isinstance(v, str): for fmt in cls.allowed_formats: try: return datetime.strptime(v, fmt).date() except ValueError: continue else: return v @classmethod def truncate_datetime(cls, v: Any, info: core_schema.ValidationInfo) -&gt; Any: &quot;&quot;&quot;If a datetime value is provided, truncate to a date&quot;&quot;&quot; if isinstance(v, datetime): return v.date() else: return v @classmethod def __get_pydantic_core_schema__( cls, source_type: Any, handler: GetCoreSchemaHandler ) -&gt; CoreSchema: return core_schema.chain_schema( [ core_schema.general_plain_validator_function( function=cls.truncate_datetime, ), core_schema.general_plain_validator_function( function=cls.try_parse_date, ) ] ) You can use the defined type, or customize allow_formats by subclassing the type: class ExampleModel(BaseModel): class MyDate(CustomDate): allowed_formats = ['%Y-%m-%d', '%Y/%m/%d', '%Y_%m_%d'] dt: MyDate Here's a quick test which shows things are working: def test_model(): values = [ &quot;2023-06-01&quot;, &quot;2023/06/01&quot;, &quot;2023_06_01&quot;, date(2023, 6, 1), datetime(2023, 6, 1, 1) ] expected = date(2023, 6, 1) data = [ExampleModel(dt=v) for v in values] assert all(x.dt == expected for x in data)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As of version 2.4 you can get the field_name and data together. See the updated docs here. Now the first version of my custom data type looks like: class CustomDate: POTENTIAL_FORMATS = [] @classmethod def validate(cls, field_value, info): if type(field_value) is date: return field_value return to_date(info.field_name, field_value, cls.POTENTIAL_FORMATS, return_str=False) @classmethod def __get_pydantic_core_schema__(cls, source, handler) -&gt; core_schema.CoreSchema: return core_schema.with_info_before_validator_function( cls.validate, handler(date), field_name=handler.field_name ) Where all I needed to change was which core_schema validator function I was using. The second version of my custom data type (the one using Annotated) now works as is with no changes. Before Pydantic 2.4 It looks like accessing info.data and info.field_name inside a custom type validator is not currently possible in v2 according to this feature request. If all you need is info.data, then it looks like you can define your validator with core_schema.field_before_validator_function (I'd guess all the field_* validators work), although you will need to make up a field name: from dataclasses import dataclass from typing import Annotated, List, Any, Callable from pydantic import ValidationError, BaseModel, Field, BeforeValidator, field_validator, GetCoreSchemaHandler from pydantic_core import core_schema, CoreSchema def fn(v: str, info: core_schema.ValidationInfo, *args, **kwargs) -&gt; str: try: print(f'Validating {info.field_name}') return info.data['use_this'] except AttributeError as err: return 'No data' class AsFieldB4Method(str): @classmethod def __get_pydantic_core_schema__( cls, source_type: Any, handler: GetCoreSchemaHandler, *args, **kwargs ) -&gt; CoreSchema: return core_schema.field_before_validator_function(fn, 'not_the_real_field_name', core_schema.str_schema()) class MyModel(BaseModel): use_this: str core_schema_field_b4_method: AsFieldB4Method # Partially works From the comments, it sounds like the pydantic team want to make it work with non-field validators and to make accessing info.field_name possible, so hopefully that happens. I'll update this answer when the change happens, but check that link in case I missed it.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pydantic"
      ],
      "question_score": 9,
      "answer_score": 7,
      "created": "2023-09-13T22:43:56",
      "question_id": 77100890,
      "answer_id": 77108459
    }
  },
  {
    "question": "AttributeError: &#39;NoneType&#39; object has no attribute &#39;to_capabilities&#39;. Getting this error when running a python Script on Appium",
    "expected_answer": "import unittest from appium import webdriver from appium.webdriver.common.appiumby import AppiumBy # Import Appium UiAutomator2 driver for Android platforms (AppiumOptions) from appium.options.android import UiAutomator2Options capabilities = dict( platformName='Android', automationName='uiautomator2', deviceName='Samsung S9', appPackage='com.android.settings', appActivity='.Settings', language='en', locale='US' ) appium_server_url = 'http://localhost:4723' # Converts capabilities to AppiumOptions instance capabilities_options = UiAutomator2Options().load_capabilities(capabilities) class TestAppium(unittest.TestCase): def setUp(self) -&gt; None: self.driver = webdriver.Remote(command_executor=appium_server_url,options=capabilities_options) def tearDown(self) -&gt; None: if self.driver: self.driver.quit() def test_find_battery(self) -&gt; None: el = self.driver.find_element(by=AppiumBy.XPATH, value='//*[@text=&quot;Battery&quot;]') el.click() if __name__ == '__main__': unittest.main()",
    "context_chunks": [
      {
        "text": "import unittest from appium import webdriver from appium.webdriver.common.appiumby import AppiumBy capabilities = dict( platformName='Android', automationName='uiautomator2', deviceName='Samsung S9', appPackage='com.android.settings', appActivity='.Settings', language='en', locale='US' ) appium_server_url = 'http://localhost:4723' class TestAppium(unittest.TestCase): def setUp(self) -&gt; None: self.driver = webdriver.Remote(appium_server_url, capabilities) def tearDown(self) -&gt; None: if self.driver: self.driver.quit() def test_find_battery(self) -&gt; None: el = self.driver.find_element(by=AppiumBy.XPATH, value='//*[@text=&quot;Battery&quot;]') el.click() if __name__ == '__main__': unittest.main() The above is the example code from official Appium website (http://appium.io/docs/en/2.1/quickstart/test-py/), I have installed all the prequisites requred but still I'm getting the below error when I run the python file: C:\\Users\\syeda\\Desktop&gt;python test.py E ====================================================================== ERROR: test_find_battery (__main__.TestAppium.test_find_battery) ---------------------------------------------------------------------- Traceback (most recent call last): File &quot;C:\\Users\\syeda\\Desktop\\test.py&quot;, line 19, in setUp self.driver = webdriver.Remote(appium_server_url, capabilities) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\syeda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\appium\\webdriver\\webdriver.py&quot;, line 229, in __init__ super().__init__( File &quot;C:\\Users\\syeda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py&quot;, line 185, in __init__ capabilities = options.to_capabilities() ^^^^^^^^^^^^^^^^^^^^^^^ AttributeError: 'NoneType' object has no attribute 'to_capabilities' ---------------------------------------------------------------------- Ran 1 test in 0.001s FAILED (errors=1) I made sure the Appium server is running. I'm not sure why this eroor is occuring. I tried searching on the web but no luck.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "import unittest from appium import webdriver from appium.webdriver.common.appiumby import AppiumBy # Import Appium UiAutomator2 driver for Android platforms (AppiumOptions) from appium.options.android import UiAutomator2Options capabilities = dict( platformName='Android', automationName='uiautomator2', deviceName='Samsung S9', appPackage='com.android.settings', appActivity='.Settings', language='en', locale='US' ) appium_server_url = 'http://localhost:4723' # Converts capabilities to AppiumOptions instance capabilities_options = UiAutomator2Options().load_capabilities(capabilities) class TestAppium(unittest.TestCase): def setUp(self) -&gt; None: self.driver = webdriver.Remote(command_executor=appium_server_url,options=capabilities_options) def tearDown(self) -&gt; None: if self.driver: self.driver.quit() def test_find_battery(self) -&gt; None: el = self.driver.find_element(by=AppiumBy.XPATH, value='//*[@text=&quot;Battery&quot;]') el.click() if __name__ == '__main__': unittest.main()",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I was able to get around this error with Appium-Python-Client 3.1.1 by adding the following code from appium.options.common import AppiumOptions driver = webdriver.Remote(appium_server_url, options=AppiumOptions().load_capabilities(capabilities)) The AppiumOptions instance provides the load_capabilities() method that the original code is complaining about on line 185, in __init__",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver",
        "appium"
      ],
      "question_score": 9,
      "answer_score": 6,
      "created": "2023-09-16T16:26:22",
      "question_id": 77118636,
      "answer_id": 77121798
    }
  },
  {
    "question": "Where can I find an exhaustive list of actions for spark?",
    "expected_answer": "All the methods annotated in the with @group action are actions. They can be found as a list here in scaladocs. They can also be found in the source where each method is defined, looking like this: * @group action * @since 1.6.0 */ def show(numRows: Int): Unit = show(numRows, truncate = true) Additionally, some other methods do not have that annotation, but also perform an eager evaluation: Those that call withAction. Checkpoint, for example, actually performs an action but isn't grouped as such in the docs: private[sql] def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) &quot;checkpoint&quot; else &quot;localCheckpoint&quot; withAction(actionName, queryExecution) { physicalPlan =&gt; val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { To find all of them Go to the source Use control + F Search for private def withAction Click on withAction On the right you should see a list of methods that use them. This is how that list currently looks:",
    "context_chunks": [
      {
        "text": "I want to know exactly what I can do in spark without triggering the computation of the spark RDD/DataFrame. It's my understanding that only actions trigger the execution of the transformations in order to produce a DataFrame. The problem is that I'm unable to find a comprehensive list of spark actions. Spark documentation lists some actions, but it's not exhaustive. For example show is not there, but it is considered an action. Where can I find a full list of actions? Can I assume that all methods listed here are also actions?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "All the methods annotated in the with @group action are actions. They can be found as a list here in scaladocs. They can also be found in the source where each method is defined, looking like this: * @group action * @since 1.6.0 */ def show(numRows: Int): Unit = show(numRows, truncate = true) Additionally, some other methods do not have that annotation, but also perform an eager evaluation: Those that call withAction. Checkpoint, for example, actually performs an action but isn't grouped as such in the docs: private[sql] def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) &quot;checkpoint&quot; else &quot;localCheckpoint&quot; withAction(actionName, queryExecution) { physicalPlan =&gt; val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { To find all of them Go to the source Use control + F Search for private def withAction Click on withAction On the right you should see a list of methods that use them. This is how that list currently looks:",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I don't think there exists an exhaustive list of all Spark actions out there. But I think it is helpful to build up a mental model on the difference and refer to the documentation when needed. For transformation there is no expected output from calling the function alone. It is only when you call an action that Spark starts to compute the results. There are three kinds of actions as follows (Excerpt from Spark: The Definitive Guide) The link you provided lists some actions, but includes transformations in there as well",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "dataframe",
        "apache-spark",
        "pyspark"
      ],
      "question_score": 9,
      "answer_score": 5,
      "created": "2024-07-08T21:20:53",
      "question_id": 78722890,
      "answer_id": 78724425
    }
  },
  {
    "question": "Python Selenium: &#39;unexpected keyword argument &#39;executable_path&#39;",
    "expected_answer": "As mentioned in my earlier solution, https://stackoverflow.com/a/76550727/7058266, this is due to changes in selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that executable_path was removed. If you want to pass in an executable_path, you'll have to use the service arg now. from selenium import webdriver from selenium.webdriver.firefox.service import Service service = Service(executable_path=&quot;PATH_TO_GECKODRIVER&quot;) options = webdriver.FirefoxOptions() driver = webdriver.Firefox(service=service, options=options) # ... driver.quit() But you no longer need to specify an executable_path due to a fully operational Selenium Manager in 4.10.0, so this is all you need: from selenium import webdriver from selenium.webdriver.firefox.service import Service service = Service() options = webdriver.FirefoxOptions() driver = webdriver.Firefox(service=service, options=options) # ... driver.quit()",
    "context_chunks": [
      {
        "text": "I just started using selenium with Python and I keep getting the following error code: TypeError: WebDriver.__init__() got an unexpected keyword argument 'executable_path' Here's the code for the context: from selenium.webdriver import Firefox from selenium.webdriver.common.keys import Keys url = 'https://example' driver_path = r&quot;D:\\path\\to\\geckodriver.exe&quot; browser = Firefox(executable_path=driver_path) browser.get(url) Thanks in advance! I checked the path, the version of the selenium package and made sure that I have the right geckodriver.exe but still get the error.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "As mentioned in my earlier solution, https://stackoverflow.com/a/76550727/7058266, this is due to changes in selenium 4.10.0: https://github.com/SeleniumHQ/selenium/commit/9f5801c82fb3be3d5850707c46c3f8176e3ccd8e Note that executable_path was removed. If you want to pass in an executable_path, you'll have to use the service arg now. from selenium import webdriver from selenium.webdriver.firefox.service import Service service = Service(executable_path=&quot;PATH_TO_GECKODRIVER&quot;) options = webdriver.FirefoxOptions() driver = webdriver.Firefox(service=service, options=options) # ... driver.quit() But you no longer need to specify an executable_path due to a fully operational Selenium Manager in 4.10.0, so this is all you need: from selenium import webdriver from selenium.webdriver.firefox.service import Service service = Service() options = webdriver.FirefoxOptions() driver = webdriver.Firefox(service=service, options=options) # ... driver.quit()",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "unexpected keyword argument means exactly that, you're trying to pass a keyword argument or named argument to a function that does not have that argument defined. Remember there are 2 types of arguments in python, based on how you supply them to a function, keyword/named and positional. def greet(name, age): return f&quot;Hello, {name}! You are {age} years old.&quot; # Using positional arguments, where order matters result = greet(&quot;Leo&quot;, 30) print(result) # Output: Hello, Leo! You are 30 years old. result = greet(30, &quot;Leo&quot;) print(result) # Output: Hello, 30! You are Leo years old. # Using keyword/named arguments, where order does not matter (unless you're mixing positional and keyword, then positionals go first, always) result = greet(age=30, name=&quot;Leo&quot;) print(result) # Output: Hello, Leo! You are 30 years old. Check the documentation for your version of selenium and check if, for the Firefox class, the init has an argument called executable_path.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver",
        "firefox"
      ],
      "question_score": 9,
      "answer_score": 8,
      "created": "2023-07-31T10:09:19",
      "question_id": 76802588,
      "answer_id": 76803790
    }
  },
  {
    "question": "Why doesn&#39;t langchain ConversationalRetrievalChain remember the chat history, even though I added it to the chat_history parameter?",
    "expected_answer": "You don't need to explicitly append question and answer to the history ConversationalRetrievalChain model will automatically take of it Your are creating ConversationalRetrievalChain object inside the ask method and passing question's to it. What happens is each time you are asking a question to it, a new chat oject is created from ConversationalRetrievalChain which will overwrite the previous memory and start's fresh. To resolve this create chat object ConversationalRetrievalChain outside the ask function and pass it as argument to it. like chat = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory) ask(&quot;Who is Bound by his Agreement?&quot;, chat) #Answers correctly ask(&quot;What did I ask in previous question?&quot;, chat) #Doesn't remember def ask(question: str, chat: Object): answer = chat({&quot;question&quot;: question)[&quot;answer&quot;] print(answer) return answer",
    "context_chunks": [
      {
        "text": "Studying AI and LangChain, and I was trying to make a conversational chatbot. So far so good, I managed to get feed it custom texts and it answers questions based on the text, but for some reason it doesn't remember the previous answers. From this question, it appears that ConversationalRetrievalChain needs to take the chat_history parameter to retain memories, but even though I supply it, it still can't remember anything. Here is my code: history = [] def ask(question: str): chat = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory) answer = chat({&quot;question&quot;: question, &quot;chat_history&quot;: history})[&quot;answer&quot;] history.append((question, answer)) print(answer) return answer ask(&quot;Who is Bound by this Agreement?&quot;) #Answers correctly ask(&quot;What did I ask in previous question?&quot;) #Doesn't remember I have verified that the chat history is indeed recorded into the history list. So why doesn't model remember what was before?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You don't need to explicitly append question and answer to the history ConversationalRetrievalChain model will automatically take of it Your are creating ConversationalRetrievalChain object inside the ask method and passing question's to it. What happens is each time you are asking a question to it, a new chat oject is created from ConversationalRetrievalChain which will overwrite the previous memory and start's fresh. To resolve this create chat object ConversationalRetrievalChain outside the ask function and pass it as argument to it. like chat = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory) ask(&quot;Who is Bound by his Agreement?&quot;, chat) #Answers correctly ask(&quot;What did I ask in previous question?&quot;, chat) #Doesn't remember def ask(question: str, chat: Object): answer = chat({&quot;question&quot;: question)[&quot;answer&quot;] print(answer) return answer",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "ConversationalRetrievalChain are performing few steps: Rephrasing input to standalone question Retrieving documents Asking question with provided context if you pass memory to config it will also update it with questions and answers. As i didn't find anything about used prompts in docs I was looking for them in repo and there are two crucial ones: const question_generator_template = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History: {chat_history} Follow Up Input: {question} Standalone question:`; and export const DEFAULT_QA_PROMPT = /*#__PURE__*/ new PromptTemplate({ template: &quot;Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:, inputVariables: [&quot;context&quot;, &quot;question&quot;], }); As you can see, only question_generator_template has chat_history context. I run into same issue as you and I changed prompt for qaChain, as in chains every part of it has access to all input variables you can just modify prompt and add chat_history input like this: const QA_PROMPT = new PromptTemplate({ template: &quot;Use the following pieces of context and chat history to answer the question at the end.\\n&quot; + &quot;If you don't know the answer, just say that you don't know, &quot; + &quot;don't try to make up an answer.\\n\\n&quot; + &quot;{context}\\n\\nChat history: {chat_history}\\n\\nQuestion: {question} \\nHelpful Answer:&quot;, inputVariables: [&quot;context&quot;, &quot;question&quot;, &quot;chat_history&quot;], }); and then pass it to fromLLM() function: chat = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory, qaChainOptions: {type: &quot;stuff&quot;, prompt: QA_PROMPT}) Now final prompt which actually asks question has chat_history available, and should work as you expect. You can also pass verbose: true to config so it will log all calls with prompts, so it's easier do debug. Let me know if it helped you.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "artificial-intelligence",
        "chatbot",
        "langchain"
      ],
      "question_score": 9,
      "answer_score": 5,
      "created": "2023-07-19T13:54:31",
      "question_id": 76722077,
      "answer_id": 77263321
    }
  },
  {
    "question": "How to Efficiently Convert a Markdown Table to a DataFrame in Python?",
    "expected_answer": "Like this import re import pandas as pd text = &quot;&quot;&quot; | Some Title | Some Description | Some Number | |------------|------------------------------|-------------| | Dark Souls | This is a fun game | 5 | | Bloodborne | This one is even better | 2 | | Sekiro | This one is also pretty good | 110101 | &quot;&quot;&quot; pattern = r&quot;\\| ([\\w\\s]+) \\| ([\\w\\s]+) \\| ([\\w\\s]+) \\|&quot; # Use the findall function to extract all rows that match the pattern matches = re.findall(pattern, text) # Extract the header and data rows header = matches[0] data = matches[1:] # Create a pandas DataFrame using the extracted header and data rows df = pd.DataFrame(data, columns=header) # Optionally, convert numerical columns to appropriate types df['Some Number'] = df['Some Number'].astype(int) print(df)",
    "context_chunks": [
      {
        "text": "I need to convert a markdown table into a pandas DataFrame. I've managed to do this using the pd.read_csv function with '|' as the separator, but it seems like there's some additional cleanup required. Specifically, I need to remove the row containing '-----', which is used for table separation, and I also want to get rid of the last column. Here's a simplified example of what I'm doing: import pandas as pd from io import StringIO # The text containing the table text = &quot;&quot;&quot; | Some Title | Some Description | Some Number | |------------|------------------------------|-------------| | Dark Souls | This is a fun game | 5 | | Bloodborne | This one is even better | 2 | | Sekiro | This one is also pretty good | 110101 | &quot;&quot;&quot; # Use StringIO to create a file-like object from the text text_file = StringIO(text) # Read the table using pandas read_csv with '|' as the separator df = pd.read_csv(text_file, sep='|', skipinitialspace=True) # Remove leading/trailing whitespace from column names df.columns = df.columns.str.strip() # Remove the index column df = df.iloc[:, 1:] Is there a more elegant and efficient way to convert a markdown table into a DataFrame without needing to perform these additional cleanup steps? I'd appreciate any suggestions or insights on improving this process.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Like this import re import pandas as pd text = &quot;&quot;&quot; | Some Title | Some Description | Some Number | |------------|------------------------------|-------------| | Dark Souls | This is a fun game | 5 | | Bloodborne | This one is even better | 2 | | Sekiro | This one is also pretty good | 110101 | &quot;&quot;&quot; pattern = r&quot;\\| ([\\w\\s]+) \\| ([\\w\\s]+) \\| ([\\w\\s]+) \\|&quot; # Use the findall function to extract all rows that match the pattern matches = re.findall(pattern, text) # Extract the header and data rows header = matches[0] data = matches[1:] # Create a pandas DataFrame using the extracted header and data rows df = pd.DataFrame(data, columns=header) # Optionally, convert numerical columns to appropriate types df['Some Number'] = df['Some Number'].astype(int) print(df)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "this will work. Also it does not require the import io line. Another method was proposed which uses the re module which is a nice alternative too. So you have 2 methods. import pandas as pd text = &quot;&quot;&quot; | Some Title | Some Description | Some Number | |------------|------------------------------|-------------| | Dark Souls | This is a fun game | 5 | | Bloodborne | This one is even better | 2 | | Sekiro | This one is also pretty good | 110101 | &quot;&quot;&quot; lines = text.split(&quot;\\n&quot;) header = lines[1].strip(&quot;|&quot;).split(&quot;|&quot;) data = [] # Loop through lines starting from 2 for line in lines[2:]: # Break once we hit an empty line if not line.strip(): break cols = line.strip(&quot;|&quot;).split(&quot;|&quot;) row = dict(zip(header, cols)) data.append(row) df = pd.DataFrame(data) print(df) which will produce this: Some Title Some Description Some Number 0 ------------ ------------------------------ ------------- 1 Dark Souls This is a fun game 5 2 Bloodborne This one is even better 2 3 Sekiro This one is also pretty good 110101",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "nlp",
        "markdown"
      ],
      "question_score": 9,
      "answer_score": 4,
      "created": "2023-09-08T16:21:24",
      "question_id": 77068488,
      "answer_id": 77068592
    }
  },
  {
    "question": "Installing torch with GPU support but without downloading 3 GB of duplicated siloed CUDA libraries?",
    "expected_answer": "I'm also looking for a (better) answer to this question. In the meantime I have a solution that seems to work. If you use PDM for your project's dependency management, you can tell its dependency resolver to specifically ignore certain packages. For torch 2.2.1 w/ cuda12, you can add the following to your pyproject.toml: [tool.pdm.resolution] # Don't let PDM install all these runtime libraries -- they add GB's of bloat! excludes = [ &quot;nvidia-cublas-cu12&quot;, &quot;nvidia-cuda-cupti-cu12&quot;, &quot;nvidia-cuda-nvrtc-cu12&quot;, &quot;nvidia-cuda-runtime-cu12&quot;, &quot;nvidia-cudnn-cu12&quot;, &quot;nvidia-cufft-cu12&quot;, &quot;nvidia-curand-cu12&quot;, &quot;nvidia-cusolver-cu12&quot;, &quot;nvidia-cusparse-cu12&quot;, &quot;nvidia-nccl-cu12&quot;, &quot;nvidia-nvtx-cu12&quot;, &quot;triton&quot;, ] Then if you run pdm add torch==2.2.1, it will install the cuda version of pytorch but without installing the several GB of drivers. This is especially useful when installing your software into the official pytorch/cuda docker image, which already has all these libraries present. Unfortunately, this makes it somewhat awkward if you actually do want to install these deps in certain environments that don't already have nvidia libraries installed. A workaround is to directly add an optional dependency group that forces these each to be installed. The exclude list above applies only for &quot;implied&quot; dependencies, not top-level dependencies of your project. So you can do: pdm add -G cuda nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 ... then to force the nvidia libraries to install, you run pdm install -G cuda.",
    "context_chunks": [
      {
        "text": "I'm trying to make CUDA containers for DNN less heavy. PyTorch goes against my efforts, as it seems to come bundled with its own siloed copy of a large subset of CUDA libraries. But what if we have them already (possibly newer) and want to install just torch? And bonus question: why cannot PyTorch detect or accept your system CUDA libraries which match its own desired (and bundled in) major and minor CUDA version (e.g. 11.8)? Why does it have to force pip to download its own hard-coded CUDA 11.8.x when your system already has 11.8.y and y&gt;x (i.e. slightly newer build of the same version)? After all, tensorflow can accept such minor differences in CUDA builds, and avoid unwanted duplication of these heavy dependencies (measured in gigabytes). A more concrete illustration of the problem (run under the reasonably new GPU driver - supporting the latest CUDA 12.2 - and the official nvidia/cuda-11.8-cudnn8-devel-ubuntu22.04:latest container with CUDA 11.8.0 installed inside): !nvidia-smi +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.104.05 Driver Version: 535.104.05 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ [..] !env | grep CU | sort CUDA_VERSION=11.8.0 [..] NV_CUDA_LIB_VERSION=11.8.0-1 [..] NV_CUDNN_PACKAGE=libcudnn8=8.9.0.131-1+cuda11.8 NV_CUDNN_PACKAGE_DEV=libcudnn8-dev=8.9.0.131-1+cuda11.8 [..] NV_LIBCUBLAS_DEV_VERSION=11.11.3.6-1 [..] NV_LIBCUBLAS_VERSION=11.11.3.6-1 NV_LIBCUSPARSE_DEV_VERSION=11.7.5.86-1 NV_LIBCUSPARSE_VERSION=11.7.5.86-1 If you just intuitively try to install pip install torch, it will not download CUDA itself, but it will download the remaining NVIDIA libraries: its own (older) cuDNN (0.5 GB) and (older) NCCL, as well as various cu11* packages, including CUDA runtime (for an older version of CUDA - 11.7 instead of 11.8): !pip install torch Collecting torch Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 5.3 MB/s eta 0:00:0000:0100:02 [..] Collecting nvidia-cudnn-cu11==8.5.0.96 Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 5.1 MB/s eta 0:00:0000:0100:02 Collecting nvidia-cufft-cu11==10.9.0.58 Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 8.7 MB/s eta 0:00:0000:0100:01 Collecting nvidia-cusolver-cu11==11.4.0.1 Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 MB 9.7 MB/s eta 0:00:0000:0100:01 [..] Collecting nvidia-cublas-cu11==11.10.3.66 Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 7.2 MB/s eta 0:00:0000:0100:01 Collecting nvidia-curand-cu11==10.2.10.91 Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 MB 10.2 MB/s eta 0:00:0000:0100:01 [..] Collecting nvidia-nccl-cu11==2.14.3 Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.1/177.1 MB 8.7 MB/s eta 0:00:0000:0100:01 Collecting nvidia-cuda-cupti-cu11==11.7.101 Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 11.4 MB/s eta 0:00:0000:0100:01 [..] Collecting nvidia-nvtx-cu11==11.7.91 Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 KB 10.7 MB/s eta 0:00:00 Collecting nvidia-cuda-runtime-cu11==11.7.99 Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 KB 11.0 MB/s eta 0:00:00a 0:00:01 [..] Successfully installed cmake-3.27.5 filelock-3.12.4 lit-17.0.1 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0 But if you try to install torch for CUDA 11.8 (specifying --index-url https://download.pytorch.org/whl/cu118), then torch will make pip download it's own CUDA 11.8 bundle (2.3 GB, nearly 4 times larger than the torch wheel alone), probably not even checking if one is already installed (and available e.g. for tensorflow): !pip install torch --index-url https://download.pytorch.org/whl/cu118 Looking in indexes: https://download.pytorch.org/whl/cu118 Collecting torch Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 GB 2.3 MB/s eta 0:00:00:00:0100:06 [..] Installing collected packages: mpmath, lit, cmake, sympy, networkx, filelock, triton, torch Successfully installed cmake-3.25.0 filelock-3.9.0 lit-15.0.7 mpmath-1.3.0 networkx-3.0 sympy-1.12 torch-2.0.1+cu118 triton-2.0.0 So what was inside torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl? We have some 3 GB (unpacked) of goodies there: cuDNN and several dynamic libraries with &quot;cu&quot; in their names, including the biggest one called libtorch_cuda.so, which - judging by its name and the sheer size - is PyTorch's own distribution of CUDA (or just a subset?): 148K libcudnn.so.8 680K libcudart-d0da41ae.so.11.0 1.3M libc10_cuda.so 72M libcudnn_ops_train.so.8 91M libcublas.so.11 94M libcudnn_ops_infer.so.8 98M libcudnn_cnn_train.so.8 116M libcudnn_adv_train.so.8 125M libcudnn_adv_infer.so.8 241M libtorch_cuda_linalg.so 548M libcublasLt.so.11 621M libcudnn_cnn_infer.so.8 1.3G libtorch_cuda.so So it seems we don't need to install CUDA if it's PyTorch alone we are after, as it will download its own. Unless we make torch notice the pre-installed CUDA and cuDNN, the duplication of these libraries is unavoidable in other scenarios, because we will need the official NVIDIA version for the remaining DNN packages, such as Tensorflow or Hugging Face transformers. Note: adding --no-deps argument to pip install would miss even the required C++ dynamic libraries (not just the bundled CUDA).",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I'm also looking for a (better) answer to this question. In the meantime I have a solution that seems to work. If you use PDM for your project's dependency management, you can tell its dependency resolver to specifically ignore certain packages. For torch 2.2.1 w/ cuda12, you can add the following to your pyproject.toml: [tool.pdm.resolution] # Don't let PDM install all these runtime libraries -- they add GB's of bloat! excludes = [ &quot;nvidia-cublas-cu12&quot;, &quot;nvidia-cuda-cupti-cu12&quot;, &quot;nvidia-cuda-nvrtc-cu12&quot;, &quot;nvidia-cuda-runtime-cu12&quot;, &quot;nvidia-cudnn-cu12&quot;, &quot;nvidia-cufft-cu12&quot;, &quot;nvidia-curand-cu12&quot;, &quot;nvidia-cusolver-cu12&quot;, &quot;nvidia-cusparse-cu12&quot;, &quot;nvidia-nccl-cu12&quot;, &quot;nvidia-nvtx-cu12&quot;, &quot;triton&quot;, ] Then if you run pdm add torch==2.2.1, it will install the cuda version of pytorch but without installing the several GB of drivers. This is especially useful when installing your software into the official pytorch/cuda docker image, which already has all these libraries present. Unfortunately, this makes it somewhat awkward if you actually do want to install these deps in certain environments that don't already have nvidia libraries installed. A workaround is to directly add an optional dependency group that forces these each to be installed. The exclude list above applies only for &quot;implied&quot; dependencies, not top-level dependencies of your project. So you can do: pdm add -G cuda nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 ... then to force the nvidia libraries to install, you run pdm install -G cuda.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "By default, the gpu version of pytorch installs its own bundled cuda libraries. To instead use the system wide cuda installation, you can build pytorch from source. Note that my OS is Linux and I was using venv only for managing virtual environments. For a different OS, you would have to execute different commands and if you're using conda, you might have to execute some extra commands as given in the official instructions. First, you have to have installed nvidia driver, CUDA and CUDNN with compatible versions manually on your system. Refer to CUDA installation guide and CUDNN installation guide. Install some required tools. sudo apt-get update sudo apt-get install -y build-essential cmake git libopenblas-dev libblas-dev libjpeg-dev libpng-dev python3-dev Clone the git repo of pytorch. git clone https://github.com/pytorch/pytorch cd pytorch git submodule sync git submodule update --init --recursive Activate your virtual environment with source /path/to/env/bin/activate and then, run the following bash script from inside the pytorch directory. pip install -r requirements.txt export USE_CUDA=1 #replace 12.8 with your cuda version export CUDA_HOME=/usr/local/cuda-12.8 export USE_SYSTEM_NCCL=0 export USE_SYSTEM_CUDNN=1 export BUILD_CAFFE2=0 #replace 8.9 with the compute capability of your gpu for minimal install. Can also be omitted export TORCH_CUDA_ARCH_LIST=&quot;8.9&quot; #Without this the build breaks due to some error. export USE_FLASH_ATTENTION=0 #I had to limit the number of threads building uses since building was using 100% cpu for quite a long time and it crashed at one point. Replace 16 with a logical number or you can omit it export MAX_JOBS=16 python setup.py install You can also create a build wheel if you want to install it repeatedly. For this, replace python setup.py install with python setup.py bdist_wheel . The wheel will be found in pytorch/dist/ . Save the wheel and the requirements.txt file to a desired location (without changing the file name). Then to install pytorch on any virtual environment, simply run: pip install -r requirements.txt pip install &lt;build_wheel_name&gt;.whl You can then delete the git repo. You can also install other libraries like mkl or magma as per your need as given in the official instructions",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pip",
        "pytorch",
        "gpu"
      ],
      "question_score": 9,
      "answer_score": 3,
      "created": "2023-09-26T20:52:33",
      "question_id": 77183339,
      "answer_id": 78194925
    }
  },
  {
    "question": "(trapped) error reading bcrypt version v.4.1.2",
    "expected_answer": "The issue comes from an outdated Passlib installation (last public release was ~4 years ago, no recent signs of active development). If you don’t specifically need Passlib, the easiest fix is to drop it and use bcrypt directly, eg: import bcrypt def verify_password(plain_password: str, hashed_password: str) -&gt; bool: return bcrypt.checkpw(plain_password.encode(&quot;utf-8&quot;), hashed_password.encode(&quot;utf-8&quot;)) def get_password_hash(password: str) -&gt; str: return bcrypt.hashpw(password.encode(&quot;utf-8&quot;), bcrypt.gensalt()).decode(&quot;utf-8&quot;) This avoids any dependency issues and keeps things simple. You can still use Passlib for other reasons, although, due to maintenance issues [1], consider alternatives [2]. https://foss.heptapod.net/python-libs/passlib/-/issues/187 https://pypi.org/project/pwdlib/",
    "context_chunks": [
      {
        "text": "I am programming an API with FastAPI and also using the module bcrypt==4.1.2 When running the app I get the following error: ... INFO: Finished server process [24084] INFO: Started server process [3256] INFO: Waiting for application startup. INFO: Application startup complete. (trapped) error reading bcrypt version Traceback (most recent call last): File &quot;D:\\xnet_api\\.venv\\Lib\\site-packages\\passlib\\handlers\\bcrypt.py&quot;, line 620, in _load_backend_mixin version = _bcrypt.__about__.__version__ ^^^^^^^^^^^^^^^^^ AttributeError: module 'bcrypt' has no attribute '__about__' INFO: 127.0.0.1:53814 - &quot;POST /user/login HTTP/1.1&quot; 200 OK In an old Github post I read that this is a bug with bycrypt: https://github.com/langflow-ai/langflow/issues/1173#issuecomment-1839591335 Hey. This is a bug on passlib. I'll try to push an update on that ASAP. The issue is bcrypt's version has to be pinned at 4.0.1 until they fix it in passlib. However, this was a long time ago and the error still exists. When I use version 4.0.1, the error no longer occurs. However, version 4.1.2 is already available and I would like to use the latest version. My question is, is there a way to fix the error? Can I simply ignore the error?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The issue comes from an outdated Passlib installation (last public release was ~4 years ago, no recent signs of active development). If you don’t specifically need Passlib, the easiest fix is to drop it and use bcrypt directly, eg: import bcrypt def verify_password(plain_password: str, hashed_password: str) -&gt; bool: return bcrypt.checkpw(plain_password.encode(&quot;utf-8&quot;), hashed_password.encode(&quot;utf-8&quot;)) def get_password_hash(password: str) -&gt; str: return bcrypt.hashpw(password.encode(&quot;utf-8&quot;), bcrypt.gensalt()).decode(&quot;utf-8&quot;) This avoids any dependency issues and keeps things simple. You can still use Passlib for other reasons, although, due to maintenance issues [1], consider alternatives [2]. https://foss.heptapod.net/python-libs/passlib/-/issues/187 https://pypi.org/project/pwdlib/",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In the .venv\\Lib\\site-packages\\passlib\\handlers\\bcrypt.py file change the below line version = _bcrypt.__about__.__version__ to version = _bcrypt.__version__",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "bcrypt"
      ],
      "question_score": 9,
      "answer_score": 3,
      "created": "2024-06-16T11:11:26",
      "question_id": 78628938,
      "answer_id": 79547644
    }
  },
  {
    "question": "Handling complex parentheses structures to get the expected data",
    "expected_answer": "Any time you're considering using a lengthy and/or complicated regexp to try to solve a problem, keep in mind the quote: Some people, when confronted with a problem, think &quot;I know, I'll use regular expressions.&quot; Now they have two problems. Using any awk: $ cat tst.awk { rec = $0 while ( match(rec, /\\([^()]*)/) ) { tgt = substr($0,RSTART+1,RLENGTH-2) rec = substr(rec,1,RSTART-1) RS substr(rec,RSTART+1,RLENGTH-2) RS substr(rec,RSTART+RLENGTH) } gsub(/ *\\([^()]*) */, &quot;&quot;, tgt) print tgt } $ awk -f tst.awk file bla bla1 Rinku Singh Rohit Sharma Virat kohli Ranbir kapoor, Milkha Singh I'm saving a copy of $0 in rec and then in the loop I'm converting every (foo) inside rec to \\nfoo\\n (assuming the default RS and that the RS cannot be present in a RS-separated record) and also saving the foo from $0 (to retain the possibly nested original ( and ) pairs) in the variable tgt. So when the loop ends tgt contains the last foo substring that was present in this input record, e.g. Ranbir kapoor (Lagaan), Milkha Singh (On chutti) (Lagaan). Then with the final gsub() I remove all (...) substrings from tgt, including any surrounding blanks, leaving just the desired output. If you can ever have more levels of parenthesised strings remaining in tgt than just 1 level deep, just change gsub(/ *\\([^()]*) */, &quot;&quot;, tgt) to while ( gsub(/ *\\([^()]*) */, &quot;&quot;, tgt) );.",
    "context_chunks": [
      {
        "text": "We have data from a REST API call stored in an output file that looks as follows: Sample Input File: test test123 - test (bla bla1 (On chutti)) test test123 bla12 teeee (Rinku Singh) balle balle (testagain) (Rohit Sharma) test test123 test1111 test45345 (Surya) (Virat kohli (Lagaan)) testagain blae kaun hai ye banda (Ranbir kapoor (Lagaan), Milkha Singh (On chutti) (Lagaan)) Expected Output: bla bla1 Rinku Singh Rohit Sharma Virat kohli Ranbir kapoor, Milkha Singh Conditions to Derive the Expected Output: Always consider the last occurrence of parentheses () in each line. We need to extract the values within this last, outermost pair of parentheses. Inside the last occurrence of (), extract all values that appear before each occurrence of nested parentheses (). Eg: test test123 - test (bla bla1 (On chutti)) last parenthesis starts from (bla to till chutti)) so I need bla bla1 since its before inner (On chutti). So look for the last parenthesis and then inside how many pair of parenthesis comes we need to get data before them, eg: in line testagain blae kaun hai ye banda (Ranbir kapoor (Lagaan), Milkha Singh (On chutti) (Lagaan)) needed is Ranbir kapoor and Milkha Singh. Attempted Regex: I tried using the following regular expression on Working Demo of regex: Regex: ^(?:^[^(]+\\([^)]+\\) \\(([^(]+)\\([^)]+\\)\\))|[^(]+\\(([^(]+)\\([^)]+\\),\\s([^\\(]+)\\([^)]+\\)\\s\\([^\\)]+\\)\\)|(?:(?:.*?)\\((.*?)\\(.*?\\)\\))|(?:[^(]+\\(([^)]+)\\))$ The Regex that I have tried is working fine but I want to improve it with the advice of experts here. Preferred Languages: Looking to improve this regex OR a Python, or an awk answer is also ok. I myself will also try to add an awk answer.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Any time you're considering using a lengthy and/or complicated regexp to try to solve a problem, keep in mind the quote: Some people, when confronted with a problem, think &quot;I know, I'll use regular expressions.&quot; Now they have two problems. Using any awk: $ cat tst.awk { rec = $0 while ( match(rec, /\\([^()]*)/) ) { tgt = substr($0,RSTART+1,RLENGTH-2) rec = substr(rec,1,RSTART-1) RS substr(rec,RSTART+1,RLENGTH-2) RS substr(rec,RSTART+RLENGTH) } gsub(/ *\\([^()]*) */, &quot;&quot;, tgt) print tgt } $ awk -f tst.awk file bla bla1 Rinku Singh Rohit Sharma Virat kohli Ranbir kapoor, Milkha Singh I'm saving a copy of $0 in rec and then in the loop I'm converting every (foo) inside rec to \\nfoo\\n (assuming the default RS and that the RS cannot be present in a RS-separated record) and also saving the foo from $0 (to retain the possibly nested original ( and ) pairs) in the variable tgt. So when the loop ends tgt contains the last foo substring that was present in this input record, e.g. Ranbir kapoor (Lagaan), Milkha Singh (On chutti) (Lagaan). Then with the final gsub() I remove all (...) substrings from tgt, including any surrounding blanks, leaving just the desired output. If you can ever have more levels of parenthesised strings remaining in tgt than just 1 level deep, just change gsub(/ *\\([^()]*) */, &quot;&quot;, tgt) to while ( gsub(/ *\\([^()]*) */, &quot;&quot;, tgt) );.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Regex is generally not appropriate for parsing nested sets of parentheses. Here is a short Python script that does what you asked: import fileinput for line in fileinput.input(): line_result = &quot;&quot; parenthesis_level = 0 # Keeps track of how deep we are inside the parenthesis for char in line: if char == &quot;)&quot;: parenthesis_level -= 1 if parenthesis_level == 1 and char not in &quot;()&quot;: line_result += char if char == &quot;(&quot;: if parenthesis_level == 0: # Only keep the last outermost parenthesis line_result = &quot;&quot; # Discard any result from previous top-level parenthesis parenthesis_level += 1 print(line_result) I used fileinput for this PoC, but it should be trivial to replace it with whatever your data source is. I tried it with: echo &quot;test test123 - test (bla bla1 (On chutti)) test test123 bla12 teeee (Rinku Singh) balle balle (testagain) (Rohit Sharma) test test123 test1111 test45345 (Surya) (Virat kohli (Lagaan)) testagain blae kaun hai ye banda (Ranbir kapoor (Lagaan), Milkha Singh (On chutti)&quot; | python test.py and got the following result: bla bla1 Rinku Singh Rohit Sharma Virat kohli Ranbir kapoor , Milkha Singh Bonus : As a way to make a point about not using regex for this kind of purpose, I did some light code-golfing and shortened the script above to the following : import fileinput as i for l in i.input(): p=0 for c in l: if c==&quot;)&quot;:p-=1 if p==1 and c not in &quot;()&quot;:r+=c if c==&quot;(&quot;: if p==0:r=&quot;&quot; p+= 1 print(r) Ignoring the import line (but still counting every character below including indents and line returns), this is 136 characters long, 14 characters shorter than the regular expression shown in the question. This shortened Python code is (in my opinion) still more readable/maintainable/extendable than any regex anyone can come up with.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "regex",
        "awk"
      ],
      "question_score": 8,
      "answer_score": 8,
      "created": "2024-11-16T11:20:41",
      "question_id": 79195042,
      "answer_id": 79195124
    }
  },
  {
    "question": "What is this Python list containing all tokens from my code?",
    "expected_answer": "First off, that doesn't look like Python 3.10 output. That output should only happen on Python 3.8 or lower, due to parser changes. 3.10 produces a very different list. The list you're seeing is the list of objects associated with the arena used to produce your code's abstract syntax tree. Quoting Python/pyarena.c: /* The arena manages two kinds of memory, blocks of raw memory and a list of PyObject* pointers. PyObjects are decrefed when the arena is freed. */ Most of the memory used by Python/ast.c is handled by an arena. In particular, every time it needs a new identifier: static identifier new_identifier(const char *n, struct compiling *c) { ... if (PyArena_AddPyObject(c-&gt;c_arena, id) &lt; 0) { numeric constant: case NUMBER: { ... if (PyArena_AddPyObject(c-&gt;c_arena, pynum) &lt; 0) { or string literal: /* Make a Constant node, but decref the PyUnicode object being added. */ static expr_ty make_str_node_and_del(PyObject **str, struct compiling *c, const node* n) { ... if (PyArena_AddPyObject(c-&gt;c_arena, s) &lt; 0) { it calls PyArena_AddPyObject to add the identifier, number, or string literal to the arena's list of objects. When you run your file, execution goes through the pyrun_file function, which looks like this: static PyObject * pyrun_file(FILE *fp, PyObject *filename, int start, PyObject *globals, PyObject *locals, int closeit, PyCompilerFlags *flags) { PyArena *arena = PyArena_New(); if (arena == NULL) { return NULL; } mod_ty mod; mod = PyParser_ASTFromFileObject(fp, filename, NULL, start, 0, 0, flags, NULL, arena); if (closeit) { fclose(fp); } PyObject *ret; if (mod != NULL) { ret = run_mod(mod, filename, globals, locals, flags, arena); } else { ret = NULL; } PyArena_Free(arena); return ret; } First, it parses your code into an AST, populating the list you see as a side effect: mod = PyParser_ASTFromFileObject(fp, filename, NULL, start, 0, 0, flags, NULL, arena); Then it calls run_mod to compile the AST and execute the resulting code object: if (mod != NULL) { ret = run_mod(mod, filename, globals, locals, flags, arena); } else { ret = NULL; } And only after your code has been executed, the arena is freed, freeing the associated list: PyArena_Free(arena);",
    "context_chunks": [
      {
        "text": "If I run this code in Python 3.10: import gc def main(): a = 23764723 ref = gc.get_referrers(a)[0] print(ref) if __name__ == &quot;__main__&quot;: main() I get the following output: ['gc', 'main', 'a', 23764723, 'ref', 'gc', 'get_referrers', 'a', 0, 'print', 'ref', '__main__', '__name__', 'main'] What is this list, that seems to contain all of the literals(?) from my code? Is there an explanation in the Python docs anywhere?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "First off, that doesn't look like Python 3.10 output. That output should only happen on Python 3.8 or lower, due to parser changes. 3.10 produces a very different list. The list you're seeing is the list of objects associated with the arena used to produce your code's abstract syntax tree. Quoting Python/pyarena.c: /* The arena manages two kinds of memory, blocks of raw memory and a list of PyObject* pointers. PyObjects are decrefed when the arena is freed. */ Most of the memory used by Python/ast.c is handled by an arena. In particular, every time it needs a new identifier: static identifier new_identifier(const char *n, struct compiling *c) { ... if (PyArena_AddPyObject(c-&gt;c_arena, id) &lt; 0) { numeric constant: case NUMBER: { ... if (PyArena_AddPyObject(c-&gt;c_arena, pynum) &lt; 0) { or string literal: /* Make a Constant node, but decref the PyUnicode object being added. */ static expr_ty make_str_node_and_del(PyObject **str, struct compiling *c, const node* n) { ... if (PyArena_AddPyObject(c-&gt;c_arena, s) &lt; 0) { it calls PyArena_AddPyObject to add the identifier, number, or string literal to the arena's list of objects. When you run your file, execution goes through the pyrun_file function, which looks like this: static PyObject * pyrun_file(FILE *fp, PyObject *filename, int start, PyObject *globals, PyObject *locals, int closeit, PyCompilerFlags *flags) { PyArena *arena = PyArena_New(); if (arena == NULL) { return NULL; } mod_ty mod; mod = PyParser_ASTFromFileObject(fp, filename, NULL, start, 0, 0, flags, NULL, arena); if (closeit) { fclose(fp); } PyObject *ret; if (mod != NULL) { ret = run_mod(mod, filename, globals, locals, flags, arena); } else { ret = NULL; } PyArena_Free(arena); return ret; } First, it parses your code into an AST, populating the list you see as a side effect: mod = PyParser_ASTFromFileObject(fp, filename, NULL, start, 0, 0, flags, NULL, arena); Then it calls run_mod to compile the AST and execute the resulting code object: if (mod != NULL) { ret = run_mod(mod, filename, globals, locals, flags, arena); } else { ret = NULL; } And only after your code has been executed, the arena is freed, freeing the associated list: PyArena_Free(arena);",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "if i recall correctly get_referrers gives you a list of all object that directly refer to the value (not variable) you try to ask for, in your case that would be your main file referring to the value 23764723 in addition you would list all other things your main file refers to as well as whatever other objects refer to that number you can try something like a = {'x':[],'y':0} ref = gc.get_referrers(a['x'])[0] which prints the entire value of a (trying this with a['y'] would be checking what refers to the value 0 which besides a additionally returns a whole host of other objects) in this case you only get a as only this value refers to the unique instance of []",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "list"
      ],
      "question_score": 9,
      "answer_score": 9,
      "created": "2024-04-26T16:55:07",
      "question_id": 78391980,
      "answer_id": 78436180
    }
  },
  {
    "question": "OpenAI API error: &quot;The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY env variable&quot;",
    "expected_answer": "You need to set the OpenAI API key. There are two options if you're using the OpenAI Python SDK &gt;=v1.0.0: Option 1 (recommended): Set the OpenAI API key as an environment variable import os from openai import OpenAI client = OpenAI( api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;), ) Option 2: Set the OpenAI API key directly from openai import OpenAI client = OpenAI( api_key = &quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;, )",
    "context_chunks": [
      {
        "text": "I'm a little confused about using OpenAI in Python and need a little help to make this code work. I tried several solutions found on StackOverflow, none of them are working. My goal is to make a Python code that asks two questions to the user, and then the gpt-3.5-turbo-instruct model finds the answer and exports a questions_reponses.csv with it. I will then convert this to XML to import it into a moodle environment. Error message: OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable Environnement: MacOS/Thonny My code: import pandas as pd # Configure OpenAI API key from openai import OpenAI client = OpenAI() openai.api_key = 'my secret key openai' # Prompt user for 2 questions questions = [] for i in range(2): question = input(&quot;Posez une question: &quot;) questions.append(question) # Use OpenAI to answer questions answers = [] for question in questions: response = client.Completions.create( engine=&quot;gpt-3.5-turbo-instruct&quot;, prompt=f&quot;Question: {question}\\nRéponse:&quot;, max_tokens=1024, n=1, stop=None, temperature=0.7, ) answer = response.choices[0].text.strip() answers.append(answer) # Create a pandas dataframe with questions and answers df = pd.DataFrame({&quot;Question&quot;: questions, &quot;Réponse&quot;: answers}) # Export dataframe to CSV file df.to_csv(&quot;questions_reponses.csv&quot;, index=False) print(&quot;Le fichier CSV a été créé avec succès.&quot;)` I've tried to set the environment variable OPENAI_API_KEY in my os, but it didn't work (I always get the same error message). So I keep trying to set the key inside the code below. I don't know if my syntax is okay. Remark on the answer I am reaching out to the answerer here. As to the rules of Stack Exchange, I must do this in the question. I've just tried option 1, same error message: import pandas as pd # Configure OpenAI API key import os from openai import OpenAI client = OpenAI() OpenAI.api_key = os.getenv('OPENAI_API_KEY') # Prompt user for 5 questions questions = [] for i in range(1): question = input(&quot;Posez une question: &quot;) questions.append(question) # Use OpenAI to answer questions answers = [] for question in questions: response = client.Completions.create( engine=&quot;gpt-3.5-turbo-instruct&quot;, prompt=f&quot;Question: {question}\\nRéponse:&quot;, max_tokens=1024, n=1, stop=None, temperature=0.7, ) answer = response.choices[0].text.strip() answers.append(answer) # Create a pandas dataframe with questions and answers df = pd.DataFrame({&quot;Question&quot;: questions, &quot;Réponse&quot;: answers}) # Export dataframe to CSV file df.to_csv(&quot;questions_reponses.csv&quot;, index=False) print(&quot;Le fichier CSV a été créé avec succès.&quot;) I've just tried option 2, same error message: import pandas as pd # Configure OpenAI API key from openai import OpenAI client = OpenAI() OpenAI.api_key = &quot;sk-xxxxxxxxxxxxxx&quot; # Prompt user for 5 questions questions = [] for i in range(1): question = input(&quot;Posez une question: &quot;) questions.append(question) # Use OpenAI to answer questions answers = [] for question in questions: response = client.Completions.create( engine=&quot;gpt-3.5-turbo-instruct&quot;, prompt=f&quot;Question: {question}\\nRéponse:&quot;, max_tokens=1024, n=1, stop=None, temperature=0.7, ) answer = response.choices[0].text.strip() answers.append(answer) # Create a pandas dataframe with questions and answers df = pd.DataFrame({&quot;Question&quot;: questions, &quot;Réponse&quot;: answers}) # Export dataframe to CSV file df.to_csv(&quot;questions_reponses.csv&quot;, index=False) print(&quot;Le fichier CSV a été créé avec succès.&quot;) I don't understand what is going on. MIND: the answer works, it was just not checked in full This is a remark from an outsider. As to the check of the heading above: the questioner has only tried the approach 1. With approach 2, it would work, that is why the questioner could already accept the answer.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You need to set the OpenAI API key. There are two options if you're using the OpenAI Python SDK &gt;=v1.0.0: Option 1 (recommended): Set the OpenAI API key as an environment variable import os from openai import OpenAI client = OpenAI( api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;), ) Option 2: Set the OpenAI API key directly from openai import OpenAI client = OpenAI( api_key = &quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;, )",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "First pip install python-dotenv Then set your .env file OPENAI_API_KEY=&quot;*****&quot; NOW import os from openai import OpenAI from dotenv import load_dotenv load_dotenv() client = OpenAI( api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;), ) //CODE HERE",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "environment-variables",
        "openai-api",
        "api-key",
        "gpt-3"
      ],
      "question_score": 8,
      "answer_score": 13,
      "created": "2024-01-27T10:52:08",
      "question_id": 77890911,
      "answer_id": 77890927
    }
  },
  {
    "question": "Generating binary arrays with alternating values based on change indices in NumPy",
    "expected_answer": "One way among many others a=np.array([2,5,9,10]) x=np.zeros((a.max()+1,), dtype=np.uint8) x[a]=1 b=x.cumsum()%2 Some explanation (but I guess code, in this rare case, is its own explanation, since it is quite easy, once you see it) x (after x[a]=1) contains 1 at each given position in a. So x.cumsum() contains a value that increments for each of those values: 0 for the 2 first, then 1 for 3 next then 2, then 3, then 4... So x.cumsum()%2 alternates between 1 and 0. Note that I use np.uint8 type because I am cheap, and I can't help thinking &quot;why shoud I pay 32 bits when 8 are enough for a size 11 array&quot;. But in reality, since 256 is even, it wouldn't really matter even if a had billions of values. Just x.cumsum() would roll back from 255 to 0 because of overflow. And then x.cumsum()%2 would have the same value.",
    "context_chunks": [
      {
        "text": "I have an array a of increasing indexes, e.g. [2 5 9 10], which indicates positions of value change. Assuming the output values are 0 and 1, I want to get array b: [0 0 1 1 1 0 0 0 0 1 0] Is there a NumPy magic to transform a into b?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "One way among many others a=np.array([2,5,9,10]) x=np.zeros((a.max()+1,), dtype=np.uint8) x[a]=1 b=x.cumsum()%2 Some explanation (but I guess code, in this rare case, is its own explanation, since it is quite easy, once you see it) x (after x[a]=1) contains 1 at each given position in a. So x.cumsum() contains a value that increments for each of those values: 0 for the 2 first, then 1 for 3 next then 2, then 3, then 4... So x.cumsum()%2 alternates between 1 and 0. Note that I use np.uint8 type because I am cheap, and I can't help thinking &quot;why shoud I pay 32 bits when 8 are enough for a size 11 array&quot;. But in reality, since 256 is even, it wouldn't really matter even if a had billions of values. Just x.cumsum() would roll back from 255 to 0 because of overflow. And then x.cumsum()%2 would have the same value.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Probably you can try np.diff + np.repeat like below d = np.diff([0] + a.tolist() + [max(a).tolist()+1]) np.repeat((np.arange(d.size)) % 2, d) which gives array([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0])",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy"
      ],
      "question_score": 8,
      "answer_score": 14,
      "created": "2024-12-20T21:22:12",
      "question_id": 79298424,
      "answer_id": 79298434
    }
  },
  {
    "question": "How to determine if a large integer is a power of 3 in Python?",
    "expected_answer": "Any approach that involves repeated division is going to be slow with numbers that large. Instead, consider using the bit length of the number (effectively the ceiling of its base 2 logarithm) to approximate the corresponding power of three, then check to see if it is indeed equal: import math def what_power_of_3(n): if n &lt;= 0: return -1 k = math.floor(n.bit_length() * (math.log(2) / math.log(3))) if n == 3**k: return k else: return -1 This is fast: the test requires only a single exponentiation, so for e.g. 3**10000000, it requires only a few seconds on my smartphone. A brief sketch to show why this is correct: Because of the equality check, if n is not a power of 3, the answer will always be correct (because n == 3**k cannot be true). So it suffices to prove that this answer always finds the correct k when n == 3 ** k. Let k &gt; 0, n = 3 ** k, and t = n.bit_length(). Then, 3 ** k &lt; 2 ** t &lt; 3 ** (k + 1) by definition of bit_length(). Thus k &lt; t * log(2) / log(3) &lt; k + 1, and so n.bit_length() * (math.log(2) / math.log(3)) is going to be a fractional value that lies strictly between k and k+1; thus, the floor of that value will be exactly k.",
    "context_chunks": [
      {
        "text": "I'm trying to determine if a given positive integer ( N ) is a power of 3, i.e., if there exists a nonnegative integer ( x ) such that ( 3^x = N ). The challenge is that ( N ) can be extremely large, with up to ( 10^7 ) digits. Here's the logic I want to implement: If ( N ) is less than or equal to 0, return -1. Use logarithmic calculations to determine if ( N ) is a power of 3. If it is a power of 3, print the value of ( x ); otherwise, print -1. I've tried the following code, but I'm concerned about precision issues with large integers: import math def is_power_of_three(N): if N &lt;= 0: return -1 log3_N = math.log10(N) / math.log10(3) if abs(log3_N - round(log3_N)) &lt; 1e-10: return int(round(log3_N)) else: return -1 # Example usage: N = int(input(&quot;Enter a number: &quot;)) print(is_power_of_three(N)) Question: Is there a more efficient way to check if ( N ) is a power of 3, especially for very large values? How can I handle precision issues in Python when calculating logarithms for such large integers? Are there alternative methods to achieve the same goal?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Any approach that involves repeated division is going to be slow with numbers that large. Instead, consider using the bit length of the number (effectively the ceiling of its base 2 logarithm) to approximate the corresponding power of three, then check to see if it is indeed equal: import math def what_power_of_3(n): if n &lt;= 0: return -1 k = math.floor(n.bit_length() * (math.log(2) / math.log(3))) if n == 3**k: return k else: return -1 This is fast: the test requires only a single exponentiation, so for e.g. 3**10000000, it requires only a few seconds on my smartphone. A brief sketch to show why this is correct: Because of the equality check, if n is not a power of 3, the answer will always be correct (because n == 3**k cannot be true). So it suffices to prove that this answer always finds the correct k when n == 3 ** k. Let k &gt; 0, n = 3 ** k, and t = n.bit_length(). Then, 3 ** k &lt; 2 ** t &lt; 3 ** (k + 1) by definition of bit_length(). Thus k &lt; t * log(2) / log(3) &lt; k + 1, and so n.bit_length() * (math.log(2) / math.log(3)) is going to be a fractional value that lies strictly between k and k+1; thus, the floor of that value will be exactly k.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As the comments in the existing answers highlight, things get a bit iffy once you bring floating point precision into a problem with large integers. Even if the bounds are safe in practice, I'd personally rather avoid floats, as demonstrated below. I compute the base 3 logarithm of the integer from its last O(log(log(N)) bits using some number theory and abstract algebra. The basic observation is that 3 is a generator of the cyclic group &lt;3&gt; modulo 2^(K+2) with order 2^K. This means we pick K such that 2^K is equal to or greater than log3(N), then compute the discrete logarithm of N mod 2^(K+2), and we get the answer to log3(N). I compute the discrete logarithm bit by bit, in O(b^2) operations (assuming multiplication is constant time, due to the small size of integers it operates on), or O(log(log(N))^2) time (if we really want to factor in the time of multiplications it comes out to O(log(log(N))^3 log(log(log(N))) log(log(log(log(N)))))). First, we have a helper function for computing the countth squaring of a base base modulo m. Here I pass a bit mask mask = m - 1, since the modulus is a power of two, so we can do the reduction in constant time (or linear in log(log(N)), if you want to be precise). def nsqr(base, count, mask): for _ in range(count): base &amp;= mask base *= base return base &amp; mask Next is the computation of the discrete logarithm itself. The function takes the power of three itself, pow3, and the log2 of the desired group order, ord_exp. def dlog3m2x(pow3, ord_exp): mod = 1 &lt;&lt; (ord_exp + 2) mod_mask = mod - 1 root1 = nsqr(3, ord_exp - 1, mod_mask) inv_3_pow = pow(3, -1, mod) result = 0 for i in range(ord_exp): if nsqr(pow3, ord_exp - 1 - i, mod_mask) == root1: pow3 *= inv_3_pow pow3 &amp;= mod_mask result |= 1 &lt;&lt; i inv_3_pow *= inv_3_pow inv_3_pow &amp;= mod_mask return result Lastly, we have the high level wrapper around this, which takes just a power of three, and computes log3(N) using the discrete logarithm helper. def ilog3(pow3): exp_max = pow3.bit_length() ord_exp = exp_max.bit_length() return dlog3m2x(pow3, ord_exp) I've verified the correctness of this up to 3^20000, and also benchmarked individual larger values up to 3^20000000. Even for N=3^20000000, it returns the correct log3(N) (without using floats) in less than a thousandth of a second on my machine. It took far longer to just pre-compute N itself to hand off to function to benchmark. Regarding the original question, the solution would be similar to the top answer currently (which uses floating point arithmetic), and just check pow(3, ilog3(N)) == N. I'd recommend either implementing an early-exit into the power function, or checking that the computed ilog3(N) (which is only correct for N=3^x exact) is not larger than a safe estimate ilog3(N) &lt; N.bit_length(), otherwise you may end up computing a ridiculously large bogus value to find it does not in fact equal N. def is_pow3(n): x = ilog3(n) if x &gt;= n.bit_length(): return False return 3 ** x == n You can also speedup the rejections by reducing N modulo 2^M for some smallish M such that ceil(log2(log2(N))) &lt; M &lt; floor(log2(N)), and verifying 3^ilog3(N) mod 2^M (computed with modular exponentiation by squaring) against that. This will help dramatically with speeding up rejection of random non-powers-of-three, but validating actual perfect powers of 3 will still require a full exponentiation. def is_pow3_ff(n): x = ilog3(n) if x &gt;= n.bit_length(): return False exp_max = n.bit_length() ord_exp = exp_max.bit_length() mod = 1 &lt;&lt; min(exp_max, ord_exp * 8) mod_mask = mod - 1 if pow(3, x, mod) != n &amp; mod_mask: return False return 3 ** x == n You can of course choose a different threshold, balancing the expense of computing O(log2(x)) multiplications of M bit numbers with the chance of M-K bits just so happening to match the corresponding bits of N.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "math",
        "precision"
      ],
      "question_score": 8,
      "answer_score": 13,
      "created": "2024-10-26T22:32:45",
      "question_id": 79129581,
      "answer_id": 79129710
    }
  },
  {
    "question": "How do I get out of a venv in VSCODE?",
    "expected_answer": "Accidentally deleted the virtual environment folder before deactivating it properly in VS Code. Since the files are gone, VS Code still thinks the environment is active even though it no longer exists. Here are a few things you can try to reset it: Close VS Code entirely and reopen it. This sometimes resets the cached virtual env. Delete the .vscode folder in your project directory. This contains VS Code config files that may still reference the deleted venv. Open a terminal/command prompt outside of VS Code and run deactivate. This deactivates any active virtual envs in that shell session. Lookup where VS Code keeps its global virtual env settings and delete any references to the deleted venv there. For example, on Linux it's in ~/.config/Code/User/settings.json. Use shortcuts &quot;ctrl+shift+P&quot; and type &quot;Python: Clear Workspace Interpreter Settings&quot; AND &quot;Python: Select Interpreter&quot; to change the environment. As a last resort, uninstall and reinstall VS Code. This will reset all its settings. The key is fully closing VS Code and clearing any cached/config files that still think the venv exists. Doing this should allow VS Code to pick up that the virtual environment is gone and reset itself. If these steps don't work then Deleting the VS Code cache/config files can potentially help resolve these issues. The .vscode folder in your project directory. This contains workspace/project specific settings and cache for VS Code. The global VS Code config folders, which are located here: Windows: %APPDATA%\\Code\\User Mac: ~/Library/Application Support/Code/User Linux: ~/.config/Code/User Deleting these folders will remove any cached/configured settings related to the virtual environment that no longer exists. It will reset VS Code's state and should allow it to detect that the virtualenv is gone.",
    "context_chunks": [
      {
        "text": "I made a venv in vscode using python shell to try it out and accidentally deleted all the venv files before deactivating the venv. Now I can't deactivate it. Again, the og files are GONE. I've done some research and simply typing &quot;deactivate&quot; didn't even work the first time. Exiting the shell doesn't work. Clearing caches doesn't work. Uninstalling and reintstalling vscode doesn't work. Every time I open the program im in a venv. Someone please help, I'm at my wits end. I've also tried sudo commands. The only thing I haven't tried is commands from my OS cmd prompt. Frankly Im too scared I'll mess something up in there if I am just going around deactivating stuff.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Accidentally deleted the virtual environment folder before deactivating it properly in VS Code. Since the files are gone, VS Code still thinks the environment is active even though it no longer exists. Here are a few things you can try to reset it: Close VS Code entirely and reopen it. This sometimes resets the cached virtual env. Delete the .vscode folder in your project directory. This contains VS Code config files that may still reference the deleted venv. Open a terminal/command prompt outside of VS Code and run deactivate. This deactivates any active virtual envs in that shell session. Lookup where VS Code keeps its global virtual env settings and delete any references to the deleted venv there. For example, on Linux it's in ~/.config/Code/User/settings.json. Use shortcuts &quot;ctrl+shift+P&quot; and type &quot;Python: Clear Workspace Interpreter Settings&quot; AND &quot;Python: Select Interpreter&quot; to change the environment. As a last resort, uninstall and reinstall VS Code. This will reset all its settings. The key is fully closing VS Code and clearing any cached/config files that still think the venv exists. Doing this should allow VS Code to pick up that the virtual environment is gone and reset itself. If these steps don't work then Deleting the VS Code cache/config files can potentially help resolve these issues. The .vscode folder in your project directory. This contains workspace/project specific settings and cache for VS Code. The global VS Code config folders, which are located here: Windows: %APPDATA%\\Code\\User Mac: ~/Library/Application Support/Code/User Linux: ~/.config/Code/User Deleting these folders will remove any cached/configured settings related to the virtual environment that no longer exists. It will reset VS Code's state and should allow it to detect that the virtualenv is gone.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "VS Code automatically detects the Python interpreter for a project and activates the corresponding virtual environment in the shell, unless the project lacks one. Although I saw that even when the virtual environment was deleted and I deactivated the env, when I reopened the Project, the Virtual Environment was already activated. To resolve this, I used Command + Shift + P, selected Python: Clear Workspace Interpreter Settings, and then chose Python: Clear Cache and Reload Window.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code",
        "virtual-environment"
      ],
      "question_score": 8,
      "answer_score": 16,
      "created": "2023-11-08T13:26:29",
      "question_id": 77445913,
      "answer_id": 77446435
    }
  },
  {
    "question": "Is there a scenario where `foo in list(bar)` cannot be replaced by `foo in bar`?",
    "expected_answer": "I've sometimes done/seen that when bar got modified in the loop, e.g.: bar = {1, 2, 3} for foo in list(bar): bar.add(foo + 1) With your replacement, that raises RuntimeError: Set changed size during iteration. Attempt This Online! An example from Python's standard library for k in list(_config_vars): if k.startswith(_INITPRE): del _config_vars[k] Dozens more (many done for the above reason, though not all).",
    "context_chunks": [
      {
        "text": "I'm digging into a codebase containing thousands of occurrences of foo in list(bar), e.g.: as a boolean expression: if foo in list(bar) or ...: ... in a for loop: for foo in list(bar): ... in a generator expression: &quot;,&quot;.join(str(foo) for foo in list(bar)) Is there a scenario (like a given version of Python, a known behavior with a type checker, etc.) where foo in list(bar) is not just a memory-expensive version of foo in bar? What am I missing here?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I've sometimes done/seen that when bar got modified in the loop, e.g.: bar = {1, 2, 3} for foo in list(bar): bar.add(foo + 1) With your replacement, that raises RuntimeError: Set changed size during iteration. Attempt This Online! An example from Python's standard library for k in list(_config_vars): if k.startswith(_INITPRE): del _config_vars[k] Dozens more (many done for the above reason, though not all).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Here it makes a difference: bar = iter('bar') foo = 'b' if foo in list(bar): print(*bar) That prints an empty line. With your replacement, it prints a r instead. Attempt This Online!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 8,
      "answer_score": 11,
      "created": "2024-09-04T20:18:09",
      "question_id": 78950432,
      "answer_id": 78950486
    }
  },
  {
    "question": "ImportError: cannot import name &#39;packaging&#39; from &#39;pkg_resources&#39; when trying to install causal_conv1d",
    "expected_answer": "This seems to be a problem with the latest version of setuptools. Similar reportings have been done in github issues and the solution that worked for me was to specify the setuptools version to 69.5.1, so for example: pip install setuptools==69.5.1 In your case, you should run this command before trying to install causal_conv1d. I had the same issue in a docker container and similarly, I added a RUN command to install that particular version of setuptools.",
    "context_chunks": [
      {
        "text": "I was trying to install &quot;causal_conv1d&quot; using: pip install --no-cache-dir -t /scratch/ahmed/lib causal_conv1d==1.0.0 The error I got is: Collecting causal_conv1d==1.0.0 Downloading causal_conv1d-1.0.0.tar.gz (6.4 kB) Preparing metadata (setup.py) ... error error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─&gt; [9 lines of output] Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt; File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 34, in &lt;module&gt; File &quot;/tmp/pip-install-9i0wsv2k/causal-conv1d_fc0a21267f664102adca1aa336c93106/setup.py&quot;, line 19, in &lt;module&gt; from torch.utils.cpp_extension import ( File &quot;/scratch/ahmed/lib/torch/utils/cpp_extension.py&quot;, line 28, in &lt;module&gt; from pkg_resources import packaging # type: ignore[attr-defined] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ImportError: cannot import name 'packaging' from 'pkg_resources' (/scratch/ahmed/lib/pkg_resources/__init__.py) [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed × Encountered error while generating package metadata. ╰─&gt; See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This seems to be a problem with the latest version of setuptools. Similar reportings have been done in github issues and the solution that worked for me was to specify the setuptools version to 69.5.1, so for example: pip install setuptools==69.5.1 In your case, you should run this command before trying to install causal_conv1d. I had the same issue in a docker container and similarly, I added a RUN command to install that particular version of setuptools.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I don't know the exact problem, but it seems this problem happened when I used two directories for Python &quot;lib&quot;: one was the default Anaconda lib, and I had another separate one. The problem disappeared when I used only the default Anaconda lib. It works fine now.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pytorch",
        "pip",
        "anaconda",
        "mamba-ssm"
      ],
      "question_score": 8,
      "answer_score": 24,
      "created": "2024-06-10T18:39:14",
      "question_id": 78604018,
      "answer_id": 78606253
    }
  },
  {
    "question": "SQLAlchemy 2.0 ORM filter show wrong type in Pycharm",
    "expected_answer": "PyCharm assumes that expressions of the form a &gt; b evaluate to bool when no other type information is available. Most likely, SQLAlchemy isn't providing rich enough type hints and/or stubfiles for PyCharm to correctly infer the type of that expression. To resolve that warning, you can inform PyCharm about the true type of the expression using typing.cast: .where( cast(&quot;ColumnElement[bool]&quot;, Albums.Id &gt; user_last_sync_id) ) Alternatively, you can suppress the warning by adding # type: ignore to the end of the line.",
    "context_chunks": [
      {
        "text": "I'm using Pycharm to develop an app with SQLAlchemy 2.0. When I attempt to query some table using ORM approach. Pycharm always display type error in the filter query. For example, in the code snippet below: with Session(engine) as session: session.scalars(select(Albums.AlbumId).where(Albums.Id &gt; user_last_sync_id)) ^^^ Show wrong type Get the following message Expected type 'ColumnElement[bool] | _HasClauseElement | SQLCoreOperations[bool] | ExpressionElementRole[bool] | () -&gt; ColumnElement[bool] | LambdaElement'，, got 'bool' instead Even though it indicates a type error, but scripts still be executed (And get correct data) without showing any error messages. What could potentially be causing this issue in the code? Is there a way to make code more &quot;correct&quot; to let Pycharm not to display type error?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "PyCharm assumes that expressions of the form a &gt; b evaluate to bool when no other type information is available. Most likely, SQLAlchemy isn't providing rich enough type hints and/or stubfiles for PyCharm to correctly infer the type of that expression. To resolve that warning, you can inform PyCharm about the true type of the expression using typing.cast: .where( cast(&quot;ColumnElement[bool]&quot;, Albums.Id &gt; user_last_sync_id) ) Alternatively, you can suppress the warning by adding # type: ignore to the end of the line.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I was looking to this exact problem and i came across this issue. Thanks for reporting it. @Brian61354270 is right, pycharm and mypy (if you have the plugin) will complain about type of the equal expression. I just tried to install the stubs and the warning is just gone away pip install sqlalchemy-stubs",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "sqlalchemy",
        "pycharm"
      ],
      "question_score": 8,
      "answer_score": 18,
      "created": "2023-08-08T19:33:57",
      "question_id": 76862713,
      "answer_id": 76862768
    }
  },
  {
    "question": "Python sibling relative import error: &#39;no known parent package&#39;",
    "expected_answer": "You should be running your project from the parent of project dir as: $ python -m project.main # note no .py This tells python that there is a package named project and inside it a module named main - then relative and absolute imports work correctly - once you change the import in main in either of from .bar import Bar # relative from project.bar import Bar # absolute",
    "context_chunks": [
      {
        "text": "I want to import a module from a subpackage so I went here Relative importing modules from parent folder subfolder and since it was not working I read all the literature here on stack and found a smaller problem that I can reproduce but cannot solve. I want to use relative imports because I don't wanna deal with sys.path etc and I don't wanna install every module of my project to be imported everywhere. I wanna make it work with relative imports. My project structure: project/ __init__.py bar.py foo.py main.py bar.py: from .foo import Foo class Bar(): @staticmethod def get_foo(): return Foo() foo.py: class Foo(): pass main.py: from bar import Bar def main(): f = Bar.get_foo() if __name__ == '__main__': main() I am running the project code from terminal with python main.py and I get the following: Traceback (most recent call last): File &quot;**omitted** project/main.py&quot;, line 1, in &lt;module&gt; from bar import Bar File &quot;**omitted** project/bar.py&quot;, line 1, in &lt;module&gt; from .foo import Foo ImportError: attempted relative import with no known parent package Why am I getting this error? It seems that bar doesn't recognize project as the parent package but: __init__.py is in place bar.py is being run as a module not a script since it is called from main and not from the command line (so __package__ and __name__ should be in place to help solve the relative import Relative imports for the billionth time) Why am I getting this error? What am I getting wrong? I have worked for a while just adding the parent of cwd to the PYTHONPATH but I wanna fix this once and for all.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You should be running your project from the parent of project dir as: $ python -m project.main # note no .py This tells python that there is a package named project and inside it a module named main - then relative and absolute imports work correctly - once you change the import in main in either of from .bar import Bar # relative from project.bar import Bar # absolute",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "@kevin41's answer is solid. I wanted to add some additional info that seemed a bit much for comments. As a general rule, your top level script, the one being run by Python and which has __name__==&quot;__main__&quot;, should NOT be inside any package. By running main.py inside of package, the package itself is never imported. Relative imports are package relative, not path relative. Python knows it is working with a package import and should use relative imports based on the __package__ special variable being set, and __package__ gets set based on using an import with a ., such as import package.bar, in which case __package__==&quot;package&quot; The __package__ special variable is never set for the top level script, which is why main.py must use absolute imports. When main.py imports bar using an absolute import, the value of the __package__ special variable while bar is being processed is also None, because bar was not imported as part of a package, therefore it can't use relative imports either. With some small changes we can observe how bar imports foo based on how main is importing bar. bar.py try: from .foo import Foo print(&quot;Using relative import&quot;) except ImportError: from foo import Foo print(&quot;Using absoloute import&quot;) print(f&quot;{__package__=} {__name__=} {__file__=}&quot;) class Bar(): @staticmethod def get_foo(): return Foo() Output when running main.py as you have it configured: Using absoloute import __package__='' __name__='bar' __file__='c:\\\\Users\\\\...\\\\package\\\\bar.py' Moving main.py outside of the package directory and changing the import to from package.bar import Bar, gives us the output Using relative import __package__='package' __name__='package.bar' __file__='c:\\\\Users\\\\...\\\\package\\\\bar.py' The correct solution is to move your top level script outside of the package folder. If your application needs a primary script within the package, then that script should use relative imports and it should be imported by another top-level script outside of the package. Example structure: project/ __init__.py bar.py foo.py main.py app.py main.py first line: from .bar import Bar app.py from package.main import main main() Additional relevant posts: Import from another file inside the same module and running from a main.py outside the module throws an import error relative python imports. What is the difference between single dot and no dot",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-import",
        "relative-import"
      ],
      "question_score": 8,
      "answer_score": 15,
      "created": "2023-10-18T13:34:26",
      "question_id": 77316817,
      "answer_id": 77321590
    }
  },
  {
    "question": "How can I bump the Python package version using uv?",
    "expected_answer": "I do this: # Replace with your version VERSION=&quot;0.5.0&quot; uvx --from=toml-cli toml set --toml-path=pyproject.toml project.version $VERSION If you need to retrieve: uvx --from=toml-cli toml get --toml-path=pyproject.toml project.version Works in a Dockerfile, makefile, etc. If you need to bump the version by using specifiers like patch, minor, or major: v=$(uvx --from=toml-cli toml get --toml-path=pyproject.toml project.version) # Bump patch version part=&quot;patch&quot; uvx --from bump2version bumpversion --allow-dirty --current-version &quot;$v&quot; &quot;$part&quot; pyproject.toml",
    "context_chunks": [
      {
        "text": "Poetry has the version command to increment a package version. Does the uv package manager have anything similar?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I do this: # Replace with your version VERSION=&quot;0.5.0&quot; uvx --from=toml-cli toml set --toml-path=pyproject.toml project.version $VERSION If you need to retrieve: uvx --from=toml-cli toml get --toml-path=pyproject.toml project.version Works in a Dockerfile, makefile, etc. If you need to bump the version by using specifiers like patch, minor, or major: v=$(uvx --from=toml-cli toml get --toml-path=pyproject.toml project.version) # Bump patch version part=&quot;patch&quot; uvx --from bump2version bumpversion --allow-dirty --current-version &quot;$v&quot; &quot;$part&quot; pyproject.toml",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This feature has been introduced with the release of uv 0.7.0. Especially, uv version now can be used to inspect and bump a project's version. Show current version. $ uv version stack-example 1.2.3 Bump patch / minor / major version. $ uv version --bump patch stack-example 1.2.3 =&gt; 1.2.4 $ uv version --bump minor stack-example 1.2.4 =&gt; 1.3.0 $ uv version --bump major stack-example 1.3.0 =&gt; 2.0.0 Show version of uv itself (previously by running uv version). $ uv self version uv 0.7.1 (90f46f89a 2025-04-30)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-packaging",
        "uv"
      ],
      "question_score": 8,
      "answer_score": 8,
      "created": "2024-11-11T01:31:29",
      "question_id": 79176155,
      "answer_id": 79276213
    }
  },
  {
    "question": "How to load a huggingface dataset from local path?",
    "expected_answer": "I solved this question by myself, it is easy to use: data_files = {“train”:“train-00000-of-00001-2a1df75c6bce91ab.parquet”,“test”:“test-00000-of-00001-8c7c51afc6d45980.parquet”} raw_datasets = load_dataset(“parquet”, data_dir=‘/Your/Path/Dahoas/rm-static/data’, data_files=data_files)",
    "context_chunks": [
      {
        "text": "Take a simple example in this website, https://huggingface.co/datasets/Dahoas/rm-static: if I want to load this dataset online, I just directly use, from datasets import load_dataset dataset = load_dataset(&quot;Dahoas/rm-static&quot;) What if I want to load dataset from local path, so I download the files and keep the same folder structure from web Files and versions fristly, -data |-test-00000-of-00001-bf4c733542e35fcb.parquet |-train-00000-of-00001-2a1df75c6bce91ab.parquet -.gitattributes -README.md -dataset_infos.json Then, put them into my folder, but shows error when loading: dataset_path =&quot;/data/coco/dataset/Dahoas/rm-static&quot; tmp_dataset = load_dataset(dataset_path) It shows FileNotFoundError: No (supported) data files or dataset script found in /data/coco/dataset/Dahoas/rm-static.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I solved this question by myself, it is easy to use: data_files = {“train”:“train-00000-of-00001-2a1df75c6bce91ab.parquet”,“test”:“test-00000-of-00001-8c7c51afc6d45980.parquet”} raw_datasets = load_dataset(“parquet”, data_dir=‘/Your/Path/Dahoas/rm-static/data’, data_files=data_files)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Save the data with save_to_disk then load it with load_from_disk. For example: import datasets ds = datasets.load_dataset(&quot;Dahoas/rm-static&quot;) ds.save_to_disk(&quot;Path/to/save&quot;) and later if you wanna re-utilize it just normal load_dataset will work ds = datasets.load_from_disk(&quot;Path/to/save&quot;) you can verify the same by printing the dataset you will be getting same result for both. This is the easier way out. The file format it is generally saved in is arrow. For the second method where you are downloading the parquet file. Would require you to explicitly declaring the dataset and it config, might be included in json and then you can load it.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "huggingface",
        "huggingface-datasets",
        "huggingface-hub"
      ],
      "question_score": 8,
      "answer_score": 8,
      "created": "2023-09-01T03:03:06",
      "question_id": 77020278,
      "answer_id": 77021081
    }
  },
  {
    "question": "ImportError: cannot import name &#39;JSONEncoder&#39; from &#39;flask.json&#39;",
    "expected_answer": "It appears the repo is not maintained recently. Instead of degrading flask, you should install flask-mongoengine from this fork https://github.com/idoshr/flask-mongoengine/tree/1.0.1 uninstall current version by pip uninstall flask-mongoengine And install using pip install git+https://github.com/idoshr/flask-mongoengine.git@1.0.1 This worked for me. I got this from here https://github.com/MongoEngine/flask-mongoengine/issues/525#issuecomment-1717066429",
    "context_chunks": [
      {
        "text": "I'm following a course on full-stack with Flask. My init.py looks like: from flask import Flask from config import Config from flask_mongoengine import MongoEngine app = Flask(__name__) app.config.from_object(Config) db = MongoEngine() db.init_app(app) from application import routes However, when importing from flask_mongoengine import MongoEngine, I'm getting an ImportError: ImportError: cannot import name 'JSONEncoder' from 'flask.json' My venv looks like: blinker==1.6.2 click==8.1.3 colorama==0.4.6 dnspython==2.3.0 email-validator==2.0.0.post2 Flask==2.3.2 flask-mongoengine==1.0.0 Flask-WTF==1.1.1 idna==3.4 itsdangerous==2.1.2 Jinja2==3.1.2 MarkupSafe==2.1.3 mongoengine==0.27.0 pymongo==4.4.0 python-dotenv==1.0.0 Werkzeug==2.3.6 WTForms==3.0.1 Is there anything I can do here to avoid this conflict? Thanks!",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "It appears the repo is not maintained recently. Instead of degrading flask, you should install flask-mongoengine from this fork https://github.com/idoshr/flask-mongoengine/tree/1.0.1 uninstall current version by pip uninstall flask-mongoengine And install using pip install git+https://github.com/idoshr/flask-mongoengine.git@1.0.1 This worked for me. I got this from here https://github.com/MongoEngine/flask-mongoengine/issues/525#issuecomment-1717066429",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I downgrade my flask version and issue solved. pip install flask==2.2.5",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "mongodb",
        "flask",
        "python-venv"
      ],
      "question_score": 8,
      "answer_score": 9,
      "created": "2023-06-28T07:23:31",
      "question_id": 76570896,
      "answer_id": 77260223
    }
  },
  {
    "question": "How do I do calculations with a sliding window while being memory-efficient?",
    "expected_answer": "calling reshape here creates a (L-2) x (L-2) x 9 copy instead of a view This is because the two last axis of the target array cannot be reshaped together here in Numpy. Indeed, it would mean the stride of the last dimension would be variable between items which is not supported (and will certainly never be because it would make many operations very slow and far more complicated). Strides can only be constant for a given axis. When a view cannot be created, Numpy performs an expensive copy. Is there a way to do this operation in a vectorized fashion, but with a smaller memory footprint? The key in such a case is to perform the operation chunk by chunk. If the input is big, it can be faster than computing the final array in one unique operation because of CPU cache and page faults. Indeed, the temporary array can be recycled in memory. That being said, the chunks need not to be too small or otherwise the overhead of calling Numpy function and the one of the CPython loop will become expensive. If L is pretty big, you can just iterate line by line. Otherwise, you need to compute chunks of lines. Here is an example: swv = np.lib.stride_tricks.sliding_window_view(a, (3, 3)) # (L-2) x (L-2) x 3 x 3 out = np.empty((L-2, L-2), dtype=np.uint8) k = 8 for i in range(0, L-2, k): out[i:i+k] = swv[i:i+k,:,:,:].reshape(-1, L-2, 9)[:,:,1::2].argmax(axis = 2).astype(np.uint8) Benchmark Here are performance results for a = np.random.rand(L, L).astype(np.float32) with L = 1000 on my machine with a i5-9600KF and Numpy 1.24.3: Initial implementation: - time: 51 ms - memory overhead: O(L**2) Proposed implementation: - time: 43 ms - memory overhead: O(L * k) The computation is faster here for all k&lt;30. For k&gt;=30, the arrays are too big for the computation to be efficient and it takes roughly the same time than your computation (in fact, the proposed implementation is slightly faster even in this case). We can also conclude that CPython loops are not slow as long as the chunks are big enough for the overheads to be small compared to the computation time. The computation also takes less memory. The only downside is that the code is bigger. There is no free lunch. Notes and faster implementations Note that a reasonable value for k can be max(int(512*1024 / (3*3*a.itemsize*(L-2)) + 0.5), 1). With this formula, the computation should take no more than few MiB of RAM if possible. If this is not possible because k=1, then it should take C*a.itemsize*(L-2)*3*3/1024**2 MiB where C is a small constant (typically 2). Here is an extended benchmark with other competitive implementations: nocomment's first implementation (&quot;mine&quot;): - time: 19 ms - memory overhead: O(L**2) nocomment's second implementation (&quot;mine4&quot;): - time: 13 ms - memory overhead: O(L**2) Native scalar code: - time: 1.5 ms - memory overhead: O(1) ken's best implementation (&quot;neighbor_argmax&quot;): - time: 0.38 ms - memory overhead: O(1) Optimized native SIMD code: - time: 0.25 ms - memory overhead: O(1) The nocomment's first implementation is faster but it requires significantly more memory (though less than the initial code). Indeed, at least 3 temporary boolean arrays needs to be allocated simultaneously. The size of each boolean array is (L-2)**2 bytes. This means at least 3 * (L-2)**2 bytes need to be allocated. This is significantly more than C * k * L (where C is a constant which should be 30~50) as long as k stay small and L is relatively big. The second implementation is faster on my machine, but it should also require more memory. The ken's (best sequential) implementation is great since Numba generates an assembly code using SIMD instructions and its memory usage is very tiny (like native codes). It is not as good as an optimized native code but pretty close to. I think the main downside is the significant compilation time (800 ms only paid once during the very-first call). On top of that, one can note that Numpy is far slower that what can be implemented in a native code (e.g. in C/C++). The gap is even bigger when he native code is optimized to benefit from SIMD units available on all mainstream CPUs. A native code requires &lt;1KiB of additional memory. There is no chance for a Numpy code to be even close to that performance. An optimized SIMD-friendly native code is about 34 times faster than the proposed Numpy implementation and 200 times faster than the initial code while also using even less memory! Overall, we can see that there is a huge gap between native/jitted codes and ones using only Numpy both in term of memory usage and speed. Note about using GPUs Regarding performance, a GPU code is not going to be faster (than the native and ken solutions) if the input array is stored in (host) RAM. Indeed, on my machine the two fastest CPU implementation are already close to be memory-bound while they only use a single CPU core! Once multithreaded, they are completely memory-bound. This should be true on all mainstream machine. There is no way to make a code faster than that since the input needs at least to be read from the RAM and the output needs at least to be written (and this is already a bottleneck for a fast multithreaded implementation). To perform the computation on a GPU, data must be moved from the host RAM to the device RAM (i.e. GPU RAM). This data transfer is generally done using the PCI interconnect which is always slower than the host RAM (often actually significantly slower). Some high-end servers benefits from a generally faster NvLink interconnect but the idea is the same: the interconnect is slower than the host memory. Thus, all GPU code will be slower than the fastest multithreaded CPU codes for this problem. That being said, a GPU code can be faster than the fastest CPU code if the input and output arrays are stored on the GPU device (and not moved from/to the host one). One way to write a GPU implementation is simply to use Cupy. To do that you need to first replace all np namespace with cp, then import cupy as cp, and then use cupy.lib.stride_tricks.as_strided instead of the current sliding_window_view. This is simple, but also pretty inefficient. I expect the version of nocomment to be more GPU-friendly and still simple to port on GPU using the same method (i.e. using Cupy). That being said, it is still far from being efficient because it creates big temporary arrays in GPU RAM which are not needed. VRAM accesses are expensive. To get a fast code, you need to write a GPU kernel (eg. using OpenCL or CUDA -- the later only fully works on Nvidia GPUs). This is lower-level, cumbersome and it requires to understand a bit how GPUs works, but the resulting speed-up is generally pretty big, especially for compute-bound operations. If done correctly, this computation should be memory-bound. This means the speed-up of the fastest GPU code compared to the fastest CPU code is approximately GPU_VRAM_bandwidth / CPU_RAM_bandwidth. On a mainstream PC with a CPU/GPU having a similar price and power consumption, I think you can expect a 4~8x speed up. For example, on my machine with a Nvidia 1660 Super and 2x3200MHz DDR4 DIMM, the speed up should be 8 (though my GPU was more expensive and also consume a bit more power in practice). There is a catch though, there are some additional overhead which makes a GPU implementation not so great in this specific case: the GPU runtime (e.g. CUDA) needs to be initialized (which takes about 300 ms on my machine) allocating data on GPU and starting kernel tend to be expensive (e.g. about dozens of µs per allocation and per kernel) not to mention synchronizations the amount of memory on GPU is often smaller than on CPU Thus, for L=1000, the speed up should be actually not so good, even if data is stored on the GPU. This overhead can be mitigated if you run multiple iteration of the same kernel or other operations. Here is a simple untested CUDA kernel: __global__ void compute_simd_kernel(float* arr, int size, uint8_t* out) { const int i = blockIdx.y + 1; const int j = blockIdx.x * blockDim.x + threadIdx.x + 1; if (i &lt; size - 1 &amp;&amp; j &lt; size - 1) { const float upVal = arr[(i-1)*size+j]; const float leftVal = arr[i*size+j-1]; const float rightVal = arr[i*size+j+1]; const float downVal = arr[(i+1)*size+j]; uint8_t maxPos = 0; float maxVal = upVal; if (leftVal &gt; maxVal) { maxPos = 1; maxVal = leftVal; } if (rightVal &gt; maxVal) { maxPos = 2; maxVal = rightVal; } if (downVal &gt; maxVal) { maxPos = 3; } out[(i-1)*(size-2)+(j-1)] = maxPos; } }",
    "context_chunks": [
      {
        "text": "I am working with very large (several GB) 2-dimensional square NumPy arrays. Given an input array a, for each element, I would like to find the direction of its largest adjacent neighbor. I am using the provided sliding window view to try to avoid creating unnecessary copies: # a is an L x L array of type np.float32 swv = sliding_window_view(a, (3, 3)) # (L-2) x (L-2) x 3 x 3 directions = swv.reshape(L-2, L-2, 9)[:,:,1::2].argmax(axis = 2).astype(np.uint8) However, calling reshape here creates a (L-2) x (L-2) x 9 copy instead of a view, which consumes an undesirably large chunk of memory. Is there a way to do this operation in a vectorized fashion, but with a smaller memory footprint? EDIT: Many of the responses are geared towards NumPy, which uses CPU (since that's what I initially asked, to simplify the problem). Would the optimal strategy be different for using CuPy, which is NumPy for GPU? As far as I know, it makes using Numba much less straightforward.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "calling reshape here creates a (L-2) x (L-2) x 9 copy instead of a view This is because the two last axis of the target array cannot be reshaped together here in Numpy. Indeed, it would mean the stride of the last dimension would be variable between items which is not supported (and will certainly never be because it would make many operations very slow and far more complicated). Strides can only be constant for a given axis. When a view cannot be created, Numpy performs an expensive copy. Is there a way to do this operation in a vectorized fashion, but with a smaller memory footprint? The key in such a case is to perform the operation chunk by chunk. If the input is big, it can be faster than computing the final array in one unique operation because of CPU cache and page faults. Indeed, the temporary array can be recycled in memory. That being said, the chunks need not to be too small or otherwise the overhead of calling Numpy function and the one of the CPython loop will become expensive. If L is pretty big, you can just iterate line by line. Otherwise, you need to compute chunks of lines. Here is an example: swv = np.lib.stride_tricks.sliding_window_view(a, (3, 3)) # (L-2) x (L-2) x 3 x 3 out = np.empty((L-2, L-2), dtype=np.uint8) k = 8 for i in range(0, L-2, k): out[i:i+k] = swv[i:i+k,:,:,:].reshape(-1, L-2, 9)[:,:,1::2].argmax(axis = 2).astype(np.uint8) Benchmark Here are performance results for a = np.random.rand(L, L).astype(np.float32) with L = 1000 on my machine with a i5-9600KF and Numpy 1.24.3: Initial implementation: - time: 51 ms - memory overhead: O(L**2) Proposed implementation: - time: 43 ms - memory overhead: O(L * k) The computation is faster here for all k&lt;30. For k&gt;=30, the arrays are too big for the computation to be efficient and it takes roughly the same time than your computation (in fact, the proposed implementation is slightly faster even in this case). We can also conclude that CPython loops are not slow as long as the chunks are big enough for the overheads to be small compared to the computation time. The computation also takes less memory. The only downside is that the code is bigger. There is no free lunch. Notes and faster implementations Note that a reasonable value for k can be max(int(512*1024 / (3*3*a.itemsize*(L-2)) + 0.5), 1). With this formula, the computation should take no more than few MiB of RAM if possible. If this is not possible because k=1, then it should take C*a.itemsize*(L-2)*3*3/1024**2 MiB where C is a small constant (typically 2). Here is an extended benchmark with other competitive implementations: nocomment's first implementation (&quot;mine&quot;): - time: 19 ms - memory overhead: O(L**2) nocomment's second implementation (&quot;mine4&quot;): - time: 13 ms - memory overhead: O(L**2) Native scalar code: - time: 1.5 ms - memory overhead: O(1) ken's best implementation (&quot;neighbor_argmax&quot;): - time: 0.38 ms - memory overhead: O(1) Optimized native SIMD code: - time: 0.25 ms - memory overhead: O(1) The nocomment's first implementation is faster but it requires significantly more memory (though less than the initial code). Indeed, at least 3 temporary boolean arrays needs to be allocated simultaneously. The size of each boolean array is (L-2)**2 bytes. This means at least 3 * (L-2)**2 bytes need to be allocated. This is significantly more than C * k * L (where C is a constant which should be 30~50) as long as k stay small and L is relatively big. The second implementation is faster on my machine, but it should also require more memory. The ken's (best sequential) implementation is great since Numba generates an assembly code using SIMD instructions and its memory usage is very tiny (like native codes). It is not as good as an optimized native code but pretty close to. I think the main downside is the significant compilation time (800 ms only paid once during the very-first call). On top of that, one can note that Numpy is far slower that what can be implemented in a native code (e.g. in C/C++). The gap is even bigger when he native code is optimized to benefit from SIMD units available on all mainstream CPUs. A native code requires &lt;1KiB of additional memory. There is no chance for a Numpy code to be even close to that performance. An optimized SIMD-friendly native code is about 34 times faster than the proposed Numpy implementation and 200 times faster than the initial code while also using even less memory! Overall, we can see that there is a huge gap between native/jitted codes and ones using only Numpy both in term of memory usage and speed. Note about using GPUs Regarding performance, a GPU code is not going to be faster (than the native and ken solutions) if the input array is stored in (host) RAM. Indeed, on my machine the two fastest CPU implementation are already close to be memory-bound while they only use a single CPU core! Once multithreaded, they are completely memory-bound. This should be true on all mainstream machine. There is no way to make a code faster than that since the input needs at least to be read from the RAM and the output needs at least to be written (and this is already a bottleneck for a fast multithreaded implementation). To perform the computation on a GPU, data must be moved from the host RAM to the device RAM (i.e. GPU RAM). This data transfer is generally done using the PCI interconnect which is always slower than the host RAM (often actually significantly slower). Some high-end servers benefits from a generally faster NvLink interconnect but the idea is the same: the interconnect is slower than the host memory. Thus, all GPU code will be slower than the fastest multithreaded CPU codes for this problem. That being said, a GPU code can be faster than the fastest CPU code if the input and output arrays are stored on the GPU device (and not moved from/to the host one). One way to write a GPU implementation is simply to use Cupy. To do that you need to first replace all np namespace with cp, then import cupy as cp, and then use cupy.lib.stride_tricks.as_strided instead of the current sliding_window_view. This is simple, but also pretty inefficient. I expect the version of nocomment to be more GPU-friendly and still simple to port on GPU using the same method (i.e. using Cupy). That being said, it is still far from being efficient because it creates big temporary arrays in GPU RAM which are not needed. VRAM accesses are expensive. To get a fast code, you need to write a GPU kernel (eg. using OpenCL or CUDA -- the later only fully works on Nvidia GPUs). This is lower-level, cumbersome and it requires to understand a bit how GPUs works, but the resulting speed-up is generally pretty big, especially for compute-bound operations. If done correctly, this computation should be memory-bound. This means the speed-up of the fastest GPU code compared to the fastest CPU code is approximately GPU_VRAM_bandwidth / CPU_RAM_bandwidth. On a mainstream PC with a CPU/GPU having a similar price and power consumption, I think you can expect a 4~8x speed up. For example, on my machine with a Nvidia 1660 Super and 2x3200MHz DDR4 DIMM, the speed up should be 8 (though my GPU was more expensive and also consume a bit more power in practice). There is a catch though, there are some additional overhead which makes a GPU implementation not so great in this specific case: the GPU runtime (e.g. CUDA) needs to be initialized (which takes about 300 ms on my machine) allocating data on GPU and starting kernel tend to be expensive (e.g. about dozens of µs per allocation and per kernel) not to mention synchronizations the amount of memory on GPU is often smaller than on CPU Thus, for L=1000, the speed up should be actually not so good, even if data is stored on the GPU. This overhead can be mitigated if you run multiple iteration of the same kernel or other operations. Here is a simple untested CUDA kernel: __global__ void compute_simd_kernel(float* arr, int size, uint8_t* out) { const int i = blockIdx.y + 1; const int j = blockIdx.x * blockDim.x + threadIdx.x + 1; if (i &lt; size - 1 &amp;&amp; j &lt; size - 1) { const float upVal = arr[(i-1)*size+j]; const float leftVal = arr[i*size+j-1]; const float rightVal = arr[i*size+j+1]; const float downVal = arr[(i+1)*size+j]; uint8_t maxPos = 0; float maxVal = upVal; if (leftVal &gt; maxVal) { maxPos = 1; maxVal = leftVal; } if (rightVal &gt; maxVal) { maxPos = 2; maxVal = rightVal; } if (downVal &gt; maxVal) { maxPos = 3; } out[(i-1)*(size-2)+(j-1)] = maxPos; } }",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Since using sliding_window_view is not efficient for your use case, I will provide an alternative using Numba. First, to simplify the implementation, define the following argmax alternative. from numba import njit @njit def argmax(*values): &quot;&quot;&quot;argmax alternative that can take an arbitrary number of arguments. Usage: argmax(0, 1, 3, 2) # 2 &quot;&quot;&quot; max_arg = 0 max_value = values[0] for i in range(1, len(values)): value = values[i] if value &gt; max_value: max_value = value max_arg = i return max_arg This is a standard argmax function, except it takes multiple scalar arguments instead of a single numpy array. Using this argmax alternative, your operation can be easily re-implemented. @njit(cache=True) def neighbor_argmax(a): height, width = a.shape[0] - 2, a.shape[1] - 2 out = np.empty((height, width), dtype=np.uint8) for y in range(height): for x in range(width): # window: a[y:y + 3, x:x + 3] # center: a[y + 1, x + 1] out[y, x] = argmax( a[y, x + 1], # up a[y + 1, x], # left a[y + 1, x + 2], # right a[y + 2, x + 1], # down ) return out This function requires only a few variables to operate, excluding the input and output buffers. So we don't need to worry about memory footprint. Alternatively, you can use stencil, a sliding window utility for Numba. With stencil, you only need to define the kernel. Numba will take care of the rest. from numba import njit, stencil @stencil def kernel(window): # window: window[-1:2, -1:2] # center: window[0, 0] return np.uint8( # Don't forget to cast to np.uint8. argmax( window[-1, 0], # up window[0, -1], # left window[0, 1], # right window[1, 0], # down ) ) @njit(cache=True) def neighbor_argmax_stencil(a): return kernel(a)[1:-1, 1:-1] # Slicing is not mandatory. It can also be inlined, if you like. @njit(cache=True) def neighbor_argmax_stencil_inlined(a): f = stencil(lambda w: np.uint8(argmax(w[-1, 0], w[0, -1], w[0, 1], w[1, 0]))) return f(a)[1:-1, 1:-1] # Slicing is not mandatory. However, stencil is very limited in functionality and cannot completely replace sliding_window_view. One difference is that there is no option to skip the edges. It is always padded with a constant value (0 by default). That is, if you put (L, L) matrix, you will get (L, L) output, not (L-2, L-2). This is why I am slicing the output in the code above to match your implementation. However, this may not be the desired behavior, as it breaks memory contiguity. You can copy after slicing, but be aware that it will increase the peak memory usage. In addition, it should be noted that these functions can also be easily adapted for multi-threading. For details, please refer to the benchmark code below. Here is the benchmark. import math import timeit import numpy as np from numba import njit, prange, stencil from numpy.lib.stride_tricks import sliding_window_view def baseline(a): L = a.shape[0] swv = sliding_window_view(a, (3, 3)) # (L-2) x (L-2) x 3 x 3 directions = swv.reshape(L - 2, L - 2, 9)[:, :, 1::2].argmax(axis=2).astype(np.uint8) return directions @njit def argmax(*values): &quot;&quot;&quot;argmax alternative that can accept an arbitrary number of arguments. Usage: argmax(0, 1, 3, 2) # 2 &quot;&quot;&quot; max_arg = 0 max_value = values[0] for i in range(1, len(values)): value = values[i] if value &gt; max_value: max_value = value max_arg = i return max_arg @njit(cache=True) def neighbor_argmax(a): height, width = a.shape[0] - 2, a.shape[1] - 2 out = np.empty((height, width), dtype=np.uint8) for y in range(height): for x in range(width): # window: a[y:y + 3, x:x + 3] # center: a[y + 1, x + 1] out[y, x] = argmax( a[y, x + 1], # up a[y + 1, x], # left a[y + 1, x + 2], # right a[y + 2, x + 1], # down ) return out @njit(cache=True, parallel=True) # Add parallel=True. def neighbor_argmax_mt(a): height, width = a.shape[0] - 2, a.shape[1] - 2 out = np.empty((height, width), dtype=np.uint8) for y in prange(height): # Change this to prange. for x in range(width): # window: a[y:y + 3, x:x + 3] # center: a[y + 1, x + 1] out[y, x] = argmax( a[y, x + 1], # up a[y + 1, x], # left a[y + 1, x + 2], # right a[y + 2, x + 1], # down ) return out @stencil def kernel(window): # window: window[-1:2, -1:2] # center: window[0, 0] return np.uint8( # Don't forget to cast to np.uint8. argmax( window[-1, 0], # up window[0, -1], # left window[0, 1], # right window[1, 0], # down ) ) @njit(cache=True) def neighbor_argmax_stencil(a): return kernel(a)[1:-1, 1:-1] # Slicing is not mandatory. @njit(cache=True) def neighbor_argmax_stencil_with_copy(a): return kernel(a)[1:-1, 1:-1].copy() # Slicing is not mandatory. @njit(cache=True, parallel=True) def neighbor_argmax_stencil_mt(a): return kernel(a)[1:-1, 1:-1] # Slicing is not mandatory. @njit(cache=True) def neighbor_argmax_stencil_inlined(a): f = stencil(lambda w: np.uint8(argmax(w[-1, 0], w[0, -1], w[0, 1], w[1, 0]))) return f(a)[1:-1, 1:-1] # Slicing is not mandatory. def benchmark(): size = 2000 # Total nbytes (in MB) for a. n = math.ceil(math.sqrt(size * (10 ** 6) / 4)) rng = np.random.default_rng(0) a = rng.random(size=(n, n), dtype=np.float32) print(f&quot;{a.shape=}, {a.nbytes=:,}&quot;) expected = baseline(a) # expected = neighbor_argmax_mt(a) assert expected.shape == (n - 2, n - 2) and expected.dtype == np.uint8 candidates = [ baseline, neighbor_argmax, neighbor_argmax_mt, neighbor_argmax_stencil, neighbor_argmax_stencil_mt, neighbor_argmax_stencil_with_copy, neighbor_argmax_stencil_inlined, ] name_len = max(len(f.__name__) for f in candidates) for f in candidates: assert np.array_equal(expected, f(a)), f.__name__ t = timeit.repeat(lambda: f(a), repeat=3, number=1) print(f&quot;{f.__name__:{name_len}} : {min(t)}&quot;) if __name__ == &quot;__main__&quot;: benchmark() Result: a.shape=(22361, 22361), a.nbytes=2,000,057,284 baseline : 24.971996600041166 neighbor_argmax : 0.1917789001017809 neighbor_argmax_mt : 0.11929619999136776 neighbor_argmax_stencil : 0.2940085999434814 neighbor_argmax_stencil_mt : 0.17756330000702292 neighbor_argmax_stencil_with_copy : 0.46573049994185567 neighbor_argmax_stencil_inlined : 0.29338629997801036 I think these results are enough to make you consider giving Numba a try :) The following section was added after this answer was accepted. Here is the CUDA version. (I'm using numba 0.60.0) from numba import cuda @cuda.jit(device=True) def argmax_cuda(values): # cuda.jit cannot handle an arbitrary number of arguments. max_arg = 0 max_value = values[0] for i in range(1, len(values)): value = values[i] if value &gt; max_value: max_value = value max_arg = i return max_arg @cuda.jit def neighbor_argmax_cuda_impl(a, out): y, x = cuda.grid(2) if y &lt; out.shape[0] and x &lt; out.shape[1]: out[y, x] = argmax_cuda( # Make sure to use a tuple, not a list. ( a[y, x + 1], # up a[y + 1, x], # left a[y + 1, x + 2], # right a[y + 2, x + 1], # down ) ) def neighbor_argmax_cuda(a, out): # If the input/output array is not on the GPU, you can transfer it like this. # However, note that this operation alone takes longer than neighbor_argmax_mt. # a = cuda.to_device(a) # out = cuda.to_device(out) # Block settings. I'm not sure if this is the optimal one. threadsperblock = (16, 16) blockspergrid_x = int(np.ceil(out.shape[1] / threadsperblock[1])) blockspergrid_y = int(np.ceil(out.shape[0] / threadsperblock[0])) blockspergrid = (blockspergrid_x, blockspergrid_y) neighbor_argmax_cuda_impl[blockspergrid, threadsperblock](a, out) # Back to CPU, if necessary. # out = out.copy_to_host() return out As Jérôme explained in detail, the time taken to transfer the input/output arrays from the host to the device cannot be ignored. a.shape=(22361, 22361), a.nbytes=2,000,057,284 neighbor_argmax : 0.47917880106251687 neighbor_argmax_mt : 0.08353979291860014 neighbor_argmax_cuda (with transfer) : 0.5072600540006533 neighbor_argmax_cuda (without transfer) : 9.134004358202219e-05 (I had to use another machine to use CUDA. For that reason, the results for the CPU are different from the ones I put above.)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "sliding-window"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2024-08-24T00:46:19",
      "question_id": 78907902,
      "answer_id": 78908076
    }
  },
  {
    "question": "Llama-2 7B-hf repeats context of question directly from input prompt, cuts off with newlines",
    "expected_answer": "This is a common issue with pre-trained base models like Llama. My first thought would be to select a model that has some sort of instruction tuning done to it i.e https://huggingface.co/meta-llama/Llama-2-7b-chat. Instruction tuning impacts the model's ability to solve tasks reliably, as opposed to the base model, which is often just trained to predict the next token (which is often why the cutoff happens). The second thing, in my experience, I have seen that has helped is using the same prompt format that was used during training. You can see in the source code the prompt format used in training and generation by Meta. Here is a thread about it. Finally, for repetition, using a Logits Processor at generation-time has been helpful to reduce repetition.",
    "context_chunks": [
      {
        "text": "Context: I am trying to query Llama-2 7B, taken from HuggingFace (meta-llama/Llama-2-7b-hf). I give it a question and context (I would guess anywhere from 200-1000 tokens), and ask it to answer the question based on the context (context is retrieved from a vectorstore using similarity search). Here are my two problems: The answer ends, and the rest of the tokens until it reaches max_new_tokens are all newlines. Or it just doesn't generate any text and the entire response is newlines. Adding a repetition_penalty of 1.1 or greater has solved infinite newline generation, but does not get me full answers. For answers that do generate, they are copied word for word from the given context. This remains the same with repetition_penalty=1.1, and making the repetition penalty too high makes the answer nonsense. I have only tried using temperature=0.4 and temperature=0.8, but from what I have done, tuning temperature and repetition_penalty both result in either the context being copied or a nonsensical answer. Note about the &quot;context&quot;: I am using a document stored in a Chroma vector store, and similarity search retrieves the relevant information before I pass it to Llama. Example Problem: My query is to summarize a certain Topic X. query = &quot;Summarize Topic X&quot; The retrieved context from the vectorstore has 3 sources that looks something like this (I format the sources in my query to the LLM separated by newlines): context = &quot;&quot;&quot;When talking about Topic X, Scenario Y is always referred to. This is due to the relation of Topic X is a broad topic which covers many aspects of life. No one knows when Topic X became a thing, its origin is unknown even to this day.&quot;&quot;&quot; Then the response from Llama-2 directly mirrors one piece of context, and includes no information from the others. Furthermore, it produces many newlines after the answer. If the answer is 100 tokens, and max_new_tokens is 150, I have 50 newlines. response = &quot;When talking about Topic X, Scenario Y is always referred to. This is due to the relation of \\n\\n\\n\\n&quot; One of my biggest issues is that in addition to copying one piece of context, if the context ends mid-sentence, so does the LLM response. Is anyone else experiencing anything like this (newline issue or copying part of your input prompt)? Has anyone found a solution?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is a common issue with pre-trained base models like Llama. My first thought would be to select a model that has some sort of instruction tuning done to it i.e https://huggingface.co/meta-llama/Llama-2-7b-chat. Instruction tuning impacts the model's ability to solve tasks reliably, as opposed to the base model, which is often just trained to predict the next token (which is often why the cutoff happens). The second thing, in my experience, I have seen that has helped is using the same prompt format that was used during training. You can see in the source code the prompt format used in training and generation by Meta. Here is a thread about it. Finally, for repetition, using a Logits Processor at generation-time has been helpful to reduce repetition.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "If you are using HuggingFacePipeline for creating llm, then in pipeline you just mention return_full_text=False, it exclude your context and query and only return actual answer. pipe = transformers.pipeline( &quot;text-generation&quot;, model=model, tokenizer= tokenizer, device_map=&quot;auto&quot;, max_new_tokens = 512, do_sample=True, return_full_text=False, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id ) from langchain.llms import HuggingFacePipeline llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.1})",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "artificial-intelligence",
        "huggingface-transformers",
        "large-language-model"
      ],
      "question_score": 8,
      "answer_score": 13,
      "created": "2023-07-26T14:46:45",
      "question_id": 76772509,
      "answer_id": 76813045
    }
  },
  {
    "question": "Can&#39;t use Jupyter Notebook because of &quot;missing or misshapen translation settings schema&quot;",
    "expected_answer": "I was getting this exact same error when I updated Jupyter Notebook through pip in MacOS recently. I am not sure really what exactly happened but it seems like something got corrupted in the installation and env. I found this GitHub issue with a solution that worked for me to solve this error (and apparently also worked for a lot more other people) Basically reinstalling jupyterlab (regardless if you use Jupyter Lab or Jupyter Notebook Classic) pip3 uninstall jupyterlab pip3 install jupyterlab jupyter lab build jupyter lab jupyter notebook In the Github thread comment they install an specific version of jupyterlab (3.6.3), this was not needed in my case as I installed the latest.",
    "context_chunks": [
      {
        "text": "I have assignments in school where I'm supposed to use Jupyter Notebook by running it from the terminal, hosting it locally. The first time I did it, it went fine until I tried quitting it on the browser, where nothing happened, so I just closed the browser and the terminal window. Since then, I'm getting the error as written in the title &quot;Missing or misshapen translation settings schema&quot;. I've googled and looked at solutions, tried them without any luck. Most talk about Anaconda and other stuff that they've had to uninstall and update, but I don't use those anyway. I tried this which I thought might work, but it didn't unfortunately. The error is: [W 2023-08-30 20:55:30.861 JupyterNotebookApp] Missing or misshapen translation settings schema: HTTP 404: Not Found (Schema not found: /opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/share/jupyter/lab/schemas/@jupyterlab/translation-extension/plugin.json) [W 2023-08-30 20:55:30.861 JupyterNotebookApp] Settings directory does not exist at /opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/share/jupyter/lab/schemas [W 2023-08-30 20:55:30.908 JupyterNotebookApp] Missing or misshapen translation settings schema: HTTP 404: Not Found (Schema not found: /opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/share/jupyter/lab/schemas/@jupyterlab/translation-extension/plugin.json) [W 2023-08-30 20:55:30.908 ServerApp] 404 GET /lab/api/settings/@jupyter-notebook/application-extension:shell?1693421730904 (::1): Schema not found: /opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/share/jupyter/lab/schemas/@jupyter-notebook/application-extension/shell.json",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I was getting this exact same error when I updated Jupyter Notebook through pip in MacOS recently. I am not sure really what exactly happened but it seems like something got corrupted in the installation and env. I found this GitHub issue with a solution that worked for me to solve this error (and apparently also worked for a lot more other people) Basically reinstalling jupyterlab (regardless if you use Jupyter Lab or Jupyter Notebook Classic) pip3 uninstall jupyterlab pip3 install jupyterlab jupyter lab build jupyter lab jupyter notebook In the Github thread comment they install an specific version of jupyterlab (3.6.3), this was not needed in my case as I installed the latest.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Symlink jupyter in the following way: ln -s /opt/homebrew/share/jupyter/ /opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/share/jupyter (kudos to the same GitHub issue)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "jupyter-notebook",
        "terminal",
        "jupyter",
        "homebrew"
      ],
      "question_score": 8,
      "answer_score": 17,
      "created": "2023-08-30T18:59:47",
      "question_id": 77010850,
      "answer_id": 77304436
    }
  },
  {
    "question": "Get href link using python playwright",
    "expected_answer": "Using get_attribute: link = page.locator('.item-info-container ').get_by_role('link').get_attribute('href') More than one locator: link_locators = page.locator('.item-info-container ').get_by_role('link').all() for _ in link_locators: print(_.get_attribute('href'))",
    "context_chunks": [
      {
        "text": "I am trying to extract the link inside a href but all I am finding it is the text inside the element The website code is the following: &lt;div class=&quot;item-info-container &quot;&gt; &lt;a href=&quot;/imovel/32600863/&quot; role=&quot;heading&quot; aria-level=&quot;2&quot; class=&quot;item-link xh-highlight&quot; title=&quot;Apartamento T3 na avenida da Liberdade, São José de São Lázaro e São João do Souto, Braga&quot;&gt; Apartamento T3 na avenida da Liberdade, São José de São Lázaro e São João do Souto, Braga &lt;/a&gt; And the code I am using is: element_handle = page.locator('//div[@class=&quot;item-info-container &quot;]//a').all_inner_texts() No matter if I specify //a[@href] or not, my output is always the title text: Apartamento T3 na avenida da Liberdade, São José de São Lázaro e São João do Souto, Braga When what I really want to achieve is: /imovel/32600863/ Any ideas of where my logic is failing me?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Using get_attribute: link = page.locator('.item-info-container ').get_by_role('link').get_attribute('href') More than one locator: link_locators = page.locator('.item-info-container ').get_by_role('link').all() for _ in link_locators: print(_.get_attribute('href'))",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Managed to do it by finding all elements and then getting the attribute after handling all elements. handleLinks = page.locator('//div[@class=&quot;item-info-container &quot;]/a') for links in handleLinks.element_handles(): linkF = links.get_attribute('href') print(linkF) and the outcome would be: /imovel/32611494/ /imovel/32642523/ /imovel/32633771/ /imovel/32527162/ /imovel/30344934/ /imovel/31221488/ /imovel/32477875/ /imovel/31221480/ /imovel/32450120/ /imovel/32515628/ /imovel/32299064/",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "web-scraping",
        "xpath",
        "playwright",
        "playwright-python"
      ],
      "question_score": 8,
      "answer_score": 15,
      "created": "2023-07-06T01:10:31",
      "question_id": 76624911,
      "answer_id": 76626222
    }
  },
  {
    "question": "Having trouble to import the library pandas",
    "expected_answer": "I just run pip install Pyarrow and then it worked. The warning clearly specified that the Pyarrow will be a major component in the next release. Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),",
    "context_chunks": [
      {
        "text": "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0) -everytime I execute on PyCharm &quot;import pandas as pd&quot; I got this error Give me solution what should I install or upgrade.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I just run pip install Pyarrow and then it worked. The warning clearly specified that the Pyarrow will be a major component in the next release. Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "DeprecationWarning: Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0), (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries) but was not found to be installed on your system. If this would cause problems for you, please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466 I believe this to be a warning message as the title states: 'DeprecationWarning' Basic test example: import pandas as pd # create pandas data set function name() using pandas def name(): name = 'Jack' surname = 'Crowley' name_dataset = { 'Name': [name], 'Surname': [surname], } name_dataframe = pd.DataFrame(name_dataset, ) return name_dataframe # call and print the name function print(name()) The above basic code outputs as below: import pandas Name Surname 0 Jack Crowley NB: for more info on this visit pyarrow and pandas integration or just: pip install pyarrow",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "import",
        "pyarrow"
      ],
      "question_score": 8,
      "answer_score": 15,
      "created": "2024-01-29T18:00:23",
      "question_id": 77901731,
      "answer_id": 77909427
    }
  },
  {
    "question": "Benchmarking a simple hydrology module in R, Python, and Julia",
    "expected_answer": "An &quot;R&quot; version using RCPP. Rcpp::sourceCpp(code=r&quot;( #include &lt;Rcpp.h&gt; #include &lt;algorithm&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector rcpp(NumericVector infiltration, NumericVector PET) { double rcScl = 0.1; double rcExp = 1.3; double PETexp = 2; double Zr = 1000.0; double n = 0.5; double smhp = 0.00; double smwp = 0.10; double smfc = 0.25; double s0 = 0.5; size_t nt = infiltration.size(); NumericVector SM_store(no_init(nt+1)); SM_store[0] = s0; double smhp_stor = Zr * smhp; double smwp_stor = Zr * smwp; double smfc_stor = Zr * smfc; double max_stor = Zr * n; for(size_t i=0; i&lt;nt; ++i) { double thisSMstor = SM_store[i]; if(thisSMstor &gt; smhp_stor) thisSMstor -= std::min(thisSMstor - smhp_stor, PET[i] * pow(thisSMstor / max_stor, PETexp)); double deepDrainage = (thisSMstor &gt; smfc_stor) ? std::min(thisSMstor - smfc_stor, rcScl * thisSMstor * pow(thisSMstor / smfc_stor, rcExp)) : 0; SM_store[i+1] = std::min(max_stor, thisSMstor - std::min(thisSMstor, deepDrainage) + infiltration[i]); } return SM_store; } )&quot;) RNGkind('Mersenne-Twister', 'Inversion', 'Rejection') set.seed(42) exInfil = rnorm(365*20, mean=1, sd=1) exPET = rnorm(365*20, mean=2, sd=1) bench::mark(rcpp(exInfil, exPET)) # expression min median `itr/sec` mem_alloc `gc/sec` n_itr n_gc total_time # &lt;bch:expr&gt; &lt;bch&gt; &lt;bch:&gt; &lt;dbl&gt; &lt;bch:byt&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;bch:tm&gt; #1 rcpp(exInfil… 284µs 305µs 3318. 59.6KB 2.01 1649 1 497ms On my PC it takes 0.305ms but in fact this is then in C++. A slightly changed Julia version: function SM_routine_fB(infiltration::Vector{Float64}, PET::Vector{Float64}) rcScl = 0.1 rcExp = 1.3 PETexp = 2 Zr = 1000.0 n = 0.5 smhp = 0.00 smwp = 0.10 smfc = 0.25 s0 = 0.5 nt = length(infiltration) SM_store = Vector{Float64}(undef, nt+1) SM_store[1] = s0 smhp_stor = Zr * smhp smwp_stor = Zr * smwp smfc_stor = Zr * smfc max_stor = Zr * n @fastmath for i in 1:nt thisSMstor = SM_store[i] if thisSMstor &gt; smhp_stor thisSMstor -= min(thisSMstor - smhp_stor, PET[i] * (thisSMstor / max_stor) ^ PETexp) end deepDrainage = (thisSMstor &gt; smfc_stor) ? min(thisSMstor - smfc_stor, rcScl * thisSMstor * (thisSMstor / smfc_stor)^rcExp) : 0 SM_store[i+1] = min(max_stor, thisSMstor - min(thisSMstor, deepDrainage) + infiltration[i]) end return(SM_store / n) end using Distributions, BenchmarkTools exInfiltration = rand(Normal(1, 1), 365*20); exPET = rand(Normal(1, 2), 365*20); @benchmark SM_routine_fB($exInfiltration, $exPET) #BenchmarkTools.Trial: 10000 samples with 1 evaluation. # Range (min … max): 348.894 μs … 1.257 ms ┊ GC (min … max): 0.00% … 60.58% # Time (median): 369.202 μs ┊ GC (median): 0.00% # Time (mean ± σ): 379.450 μs ± 41.742 μs ┊ GC (mean ± σ): 0.39% ± 2.93% # # █ ▆▇▂▁▂█▇▃▃▃▃▆▃▃▄▃▄▃▃▂▂▂▂▁▁▁▂▂▁▁▁▁▁▂▁▁▂▂▁▁▁▁ ▂ # █▇▇▅███████████████████████████████████████████████▇█▇▆▆▆▆▆▆ █ # 349 μs Histogram: log(frequency) by time 470 μs &lt; # # Memory estimate: 114.22 KiB, allocs estimate: 4. What is 0.369ms in Julia.",
    "context_chunks": [
      {
        "text": "I started working recently with a team whose members variably prefer Python, R, and Julia (in order of popularity). We are translating a set of hydrology modules built by various team members into a common library and I want to do some basic benchmarking before coming to a consensus on which language we should choose for this purpose. The problem is that I'm pretty bad at Python and new to Julia, and I believe I may be biasing results due to poor coding practices. The results for these modules (code below) are: Python (93 ms), R (55 ms), and Julia (0.5 ms). The code can't be vectorized (to my understanding), so I assumed Julia would be fastest but not by 100x, and I also assumed that Python would be faster than R. Can anyone point out some basic errors in efficiency? Python: import numpy as np def SM_routine_f(infiltration=None, PET=None, rcScl=0.1, rcExp=1.3, PETexp=2, Zr=1000, n=0.5, smhp=0.00, smwp=0.10, smfc=0.25, s0=0.5): nt = len(infiltration) SM_store = np.empty(nt+1) SM_store[0] = s0 smhp_stor = Zr * smhp smwp_stor = Zr * smwp smfc_stor = Zr * smfc max_stor = Zr * n for i in range(nt): thisSMstor = SM_store[i] AET = np.where(thisSMstor &gt; smhp_stor, (thisSMstor - smhp_stor) * PET[i] * (thisSMstor / max_stor) ** PETexp / max_stor, 0) thisSMstor -= AET deepDrainage = np.where(thisSMstor &gt; smfc_stor, rcScl * thisSMstor * (thisSMstor / smfc_stor) ** rcExp - (thisSMstor - smfc_stor), 0) SM_store[i+1] = min(max_stor, thisSMstor + infiltration[i] - deepDrainage) return SM_store / n exInfil = np.random.normal(loc=1, scale=1, size=365*20) exPET = np.random.normal(loc=2, scale=1, size=365*20) import timeit timeit.timeit(lambda: SM_routine_f(infiltration=exInfil, PET=exPET), number=1) R: SM_routine_f = function(infiltration = NA, PET = NA, rcScl = 0.1, # water retention curve scalar rcExp = 1.3, # water retention curve exponent PETexp = 2, # exponent of PET decay Zr = 1000, # root depth n = 0.5, # soil porosity smhp = 0.00, # soil moisture at hydroscopic point smwp = 0.10, # soil moisture at wilting point smfc = 0.25, # soil moisture field capacity s0 = .5) # initial soil moisture { nt = length(infiltration) SM_store = c(s0, rep(NA, nt)) smhp_stor = Zr * smhp smwp_stor = Zr * smwp smfc_stor = Zr * smfc max_stor = Zr * n for(i in 1:length(infiltration)) { thisSMstor = SM_store[i] AET = ifelse(thisSMstor &gt; smhp_stor, min(thisSMstor - smhp_stor, PET[i] * (thisSMstor / max_stor) ^ PETexp), 0) thisSMstor = thisSMstor - AET deepDrainage = ifelse(thisSMstor &gt; smfc_stor, min(thisSMstor - smfc_stor, rcScl * thisSMstor * (thisSMstor / smfc_stor)^rcExp), 0) SM_store[i+1] = min(max_stor, thisSMstor - min(c(thisSMstor, deepDrainage)) + infiltration[i]) } return(SM_store / n) } # generating dummy data for testing, e.g. 20 years of climate data exInfil = rnorm(365*20, mean=1, sd=1) exPET = rnorm(365*20, mean=2, sd=1) # benchmarking microbenchmark::microbenchmark(SM_routine_f(infiltration=exInfil, PET = exPET)) Julia: function SM_routine_f(infiltration::Vector{Float64}, PET::Vector{Float64}) rcScl = 0.1 rcExp = 1.3 PETexp = 2 Zr = 1000.0 n = 0.5 smhp = 0.00 smwp = 0.10 smfc = 0.25 s0 = 0.5 nt = length(infiltration) SM_store = [s0; fill(missing,nt)] smhp_stor = Zr * smhp smwp_stor = Zr * smwp smfc_stor = Zr * smfc max_stor = Zr * n for i in 1:length(infiltration) thisSMstor = SM_store[i] AET = (thisSMstor &gt; smhp_stor) ? min(thisSMstor - smhp_stor, PET[i] * (thisSMstor / max_stor) ^ PETexp) : 0 thisSMstor = thisSMstor - AET deepDrainage = (thisSMstor &gt; smfc_stor) ? min(thisSMstor - smfc_stor, rcScl * thisSMstor * (thisSMstor / smfc_stor)^rcExp) : 0 SM_store[i+1] = min(max_stor, thisSMstor - min(thisSMstor, deepDrainage) + infiltration[i]) end return(SM_store / n) end using Distributions, BenchmarkTools exInfiltration = rand(Normal(1, 1), 365*20); exPET = rand(Normal(1, 2), 365*20); @benchmark SM_routine_f($exInfiltration, $exPET)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "An &quot;R&quot; version using RCPP. Rcpp::sourceCpp(code=r&quot;( #include &lt;Rcpp.h&gt; #include &lt;algorithm&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector rcpp(NumericVector infiltration, NumericVector PET) { double rcScl = 0.1; double rcExp = 1.3; double PETexp = 2; double Zr = 1000.0; double n = 0.5; double smhp = 0.00; double smwp = 0.10; double smfc = 0.25; double s0 = 0.5; size_t nt = infiltration.size(); NumericVector SM_store(no_init(nt+1)); SM_store[0] = s0; double smhp_stor = Zr * smhp; double smwp_stor = Zr * smwp; double smfc_stor = Zr * smfc; double max_stor = Zr * n; for(size_t i=0; i&lt;nt; ++i) { double thisSMstor = SM_store[i]; if(thisSMstor &gt; smhp_stor) thisSMstor -= std::min(thisSMstor - smhp_stor, PET[i] * pow(thisSMstor / max_stor, PETexp)); double deepDrainage = (thisSMstor &gt; smfc_stor) ? std::min(thisSMstor - smfc_stor, rcScl * thisSMstor * pow(thisSMstor / smfc_stor, rcExp)) : 0; SM_store[i+1] = std::min(max_stor, thisSMstor - std::min(thisSMstor, deepDrainage) + infiltration[i]); } return SM_store; } )&quot;) RNGkind('Mersenne-Twister', 'Inversion', 'Rejection') set.seed(42) exInfil = rnorm(365*20, mean=1, sd=1) exPET = rnorm(365*20, mean=2, sd=1) bench::mark(rcpp(exInfil, exPET)) # expression min median `itr/sec` mem_alloc `gc/sec` n_itr n_gc total_time # &lt;bch:expr&gt; &lt;bch&gt; &lt;bch:&gt; &lt;dbl&gt; &lt;bch:byt&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;bch:tm&gt; #1 rcpp(exInfil… 284µs 305µs 3318. 59.6KB 2.01 1649 1 497ms On my PC it takes 0.305ms but in fact this is then in C++. A slightly changed Julia version: function SM_routine_fB(infiltration::Vector{Float64}, PET::Vector{Float64}) rcScl = 0.1 rcExp = 1.3 PETexp = 2 Zr = 1000.0 n = 0.5 smhp = 0.00 smwp = 0.10 smfc = 0.25 s0 = 0.5 nt = length(infiltration) SM_store = Vector{Float64}(undef, nt+1) SM_store[1] = s0 smhp_stor = Zr * smhp smwp_stor = Zr * smwp smfc_stor = Zr * smfc max_stor = Zr * n @fastmath for i in 1:nt thisSMstor = SM_store[i] if thisSMstor &gt; smhp_stor thisSMstor -= min(thisSMstor - smhp_stor, PET[i] * (thisSMstor / max_stor) ^ PETexp) end deepDrainage = (thisSMstor &gt; smfc_stor) ? min(thisSMstor - smfc_stor, rcScl * thisSMstor * (thisSMstor / smfc_stor)^rcExp) : 0 SM_store[i+1] = min(max_stor, thisSMstor - min(thisSMstor, deepDrainage) + infiltration[i]) end return(SM_store / n) end using Distributions, BenchmarkTools exInfiltration = rand(Normal(1, 1), 365*20); exPET = rand(Normal(1, 2), 365*20); @benchmark SM_routine_fB($exInfiltration, $exPET) #BenchmarkTools.Trial: 10000 samples with 1 evaluation. # Range (min … max): 348.894 μs … 1.257 ms ┊ GC (min … max): 0.00% … 60.58% # Time (median): 369.202 μs ┊ GC (median): 0.00% # Time (mean ± σ): 379.450 μs ± 41.742 μs ┊ GC (mean ± σ): 0.39% ± 2.93% # # █ ▆▇▂▁▂█▇▃▃▃▃▆▃▃▄▃▄▃▃▂▂▂▂▁▁▁▂▂▁▁▁▁▁▂▁▁▂▂▁▁▁▁ ▂ # █▇▇▅███████████████████████████████████████████████▇█▇▆▆▆▆▆▆ █ # 349 μs Histogram: log(frequency) by time 470 μs &lt; # # Memory estimate: 114.22 KiB, allocs estimate: 4. What is 0.369ms in Julia.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It's bad to work with NumPy data types like that. If I convert the input arrays to Python lists of Python floats, do the work without NumPy, and convert back to a NumPy array at the end, it's about 20 times faster. Three attempts: original 94.9 ms improved 4.4 ms original 95.1 ms improved 4.4 ms original 93.2 ms improved 4.5 ms Further improvement by avoiding Python's slow min function makes it about 30 times faster: original 91.4 ms improved 3.3 ms original 99.4 ms improved 3.3 ms original 96.7 ms improved 3.2 ms Some micro-optimizations can speed it up some more: 3.11 ± 0.03 ms improved 2.58 ± 0.02 ms optimized Code for original, improved, comparison and benchmark: import numpy as np def original(infiltration=None, PET=None, rcScl=0.1, rcExp=1.3, PETexp=2, Zr=1000, n=0.5, smhp=0.00, smwp=0.10, smfc=0.25, s0=0.5): nt = len(infiltration) SM_store = np.empty(nt+1) SM_store[0] = s0 smhp_stor = Zr * smhp smwp_stor = Zr * smwp smfc_stor = Zr * smfc max_stor = Zr * n for i in range(nt): thisSMstor = SM_store[i] AET = np.where(thisSMstor &gt; smhp_stor, (thisSMstor - smhp_stor) * PET[i] * (thisSMstor / max_stor) ** PETexp / max_stor, 0) thisSMstor -= AET deepDrainage = np.where(thisSMstor &gt; smfc_stor, rcScl * thisSMstor * (thisSMstor / smfc_stor) ** rcExp - (thisSMstor - smfc_stor), 0) SM_store[i+1] = min(max_stor, thisSMstor + infiltration[i] - deepDrainage) return SM_store / n exInfil = np.random.normal(loc=1, scale=1, size=365*20) exPET = np.random.normal(loc=2, scale=1, size=365*20) def improved(infiltration=None, PET=None, rcScl=0.1, rcExp=1.3, PETexp=2, Zr=1000, n=0.5, smhp=0.00, smwp=0.10, smfc=0.25, s0=0.5): infiltration = infiltration.tolist() PET = PET.tolist() nt = len(infiltration) SM_store = [None] * (nt+1) SM_store[0] = s0 smhp_stor = Zr * smhp smwp_stor = Zr * smwp smfc_stor = Zr * smfc max_stor = Zr * n for i in range(nt): thisSMstor = SM_store[i] AET = (thisSMstor - smhp_stor) * PET[i] * (thisSMstor / max_stor) ** PETexp / max_stor if thisSMstor &gt; smhp_stor else 0 thisSMstor -= AET deepDrainage = rcScl * thisSMstor * (thisSMstor / smfc_stor) ** rcExp - (thisSMstor - smfc_stor) if thisSMstor &gt; smfc_stor else 0 tmp = thisSMstor + infiltration[i] - deepDrainage SM_store[i+1] = max_stor if max_stor &lt; tmp else tmp SM_store = np.array(SM_store) return SM_store / n exInfil = np.random.normal(loc=1, scale=1, size=365*20) exPET = np.random.normal(loc=2, scale=1, size=365*20) expect = original(infiltration=exInfil, PET=exPET) result = improved(infiltration=exInfil, PET=exPET) print((result == expect).all()) import timeit for f in [original, improved] * 3: t = min(timeit.repeat(lambda: f(infiltration=exInfil, PET=exPET), number=1)) print(f.__name__, f'{t * 1e3: 5.1f} ms') Attempt This Online!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "r",
        "julia",
        "benchmarking"
      ],
      "question_score": 8,
      "answer_score": 7,
      "created": "2023-10-16T12:27:35",
      "question_id": 77301961,
      "answer_id": 77309447
    }
  },
  {
    "question": "SSL: CERTIFICATE_VERIFY_FAILED certificate verify failed : self signed certificate in certificate chain (_ssl.c:992)",
    "expected_answer": "self signed certificate in certificate chain means that certificate chain validation has failed. Your script does not trust the certificate or one of its issuers. For more information see Beginning with SSL for a Platform Engineer. The answer from Tzane had most of what you need. But it looks like you also might want to know WHAT certificate to add. So, first get the CA certificate, and any intermediate certs by running the following on a command line: openssl s_client -connect your-name.atlassian.net:443 -showcerts In the output there is a block that starts with Certificate chain. The output I got from Atlassian.net had only a server cert and CA cert. There are blocks of output that start with -----BEGIN CERTIFICATE----- and end with -----END CERTIFICATE----- These blocks, including the lines I just showed, are a certificate. Copy the last certificate and create a pem file e.g. ca-root.pem. Place this in the same directory as your python file and then update your requests block to be: verify = &quot;ca-root.pem&quot; response = requests.request( &quot;GET&quot;, url, headers=headers, verify=verify ) Hope this helps. ----- UPDATE ----- Using the domain you provided, msci.atlassian.net, I have the CA cert provided at this time by Digital Cert. -----BEGIN CERTIFICATE----- MIIEvjCCA6agAwIBAgIQBtjZBNVYQ0b2ii+nVCJ+xDANBgkqhkiG9w0BAQsFADBh MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3 d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBD QTAeFw0yMTA0MTQwMDAwMDBaFw0zMTA0MTMyMzU5NTlaME8xCzAJBgNVBAYTAlVT MRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxKTAnBgNVBAMTIERpZ2lDZXJ0IFRMUyBS U0EgU0hBMjU2IDIwMjAgQ0ExMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEAwUuzZUdwvN1PWNvsnO3DZuUfMRNUrUpmRh8sCuxkB+Uu3Ny5CiDt3+PE0J6a qXodgojlEVbbHp9YwlHnLDQNLtKS4VbL8Xlfs7uHyiUDe5pSQWYQYE9XE0nw6Ddn g9/n00tnTCJRpt8OmRDtV1F0JuJ9x8piLhMbfyOIJVNvwTRYAIuE//i+p1hJInuW raKImxW8oHzf6VGo1bDtN+I2tIJLYrVJmuzHZ9bjPvXj1hJeRPG/cUJ9WIQDgLGB Afr5yjK7tI4nhyfFK3TUqNaX3sNk+crOU6JWvHgXjkkDKa77SU+kFbnO8lwZV21r eacroicgE7XQPUDTITAHk+qZ9QIDAQABo4IBgjCCAX4wEgYDVR0TAQH/BAgwBgEB /wIBADAdBgNVHQ4EFgQUt2ui6qiqhIx56rTaD5iyxZV2ufQwHwYDVR0jBBgwFoAU A95QNVbRTLtm8KPiGxvDl7I90VUwDgYDVR0PAQH/BAQDAgGGMB0GA1UdJQQWMBQG CCsGAQUFBwMBBggrBgEFBQcDAjB2BggrBgEFBQcBAQRqMGgwJAYIKwYBBQUHMAGG GGh0dHA6Ly9vY3NwLmRpZ2ljZXJ0LmNvbTBABggrBgEFBQcwAoY0aHR0cDovL2Nh Y2VydHMuZGlnaWNlcnQuY29tL0RpZ2lDZXJ0R2xvYmFsUm9vdENBLmNydDBCBgNV HR8EOzA5MDegNaAzhjFodHRwOi8vY3JsMy5kaWdpY2VydC5jb20vRGlnaUNlcnRH bG9iYWxSb290Q0EuY3JsMD0GA1UdIAQ2MDQwCwYJYIZIAYb9bAIBMAcGBWeBDAEB MAgGBmeBDAECATAIBgZngQwBAgIwCAYGZ4EMAQIDMA0GCSqGSIb3DQEBCwUAA4IB AQCAMs5eC91uWg0Kr+HWhMvAjvqFcO3aXbMM9yt1QP6FCvrzMXi3cEsaiVi6gL3z ax3pfs8LulicWdSQ0/1s/dCYbbdxglvPbQtaCdB73sRD2Cqk3p5BJl+7j5nL3a7h qG+fh/50tx8bIKuxT8b1Z11dmzzp/2n3YWzW2fP9NsarA4h20ksudYbj/NhVfSbC EXffPgK2fPOre3qGNm+499iTcc+G33Mw+nur7SpZyEKEOxEXGlLzyQ4UfaJbcme6 ce1XR2bFuAJKZTRei9AqPCCcUZlM51Ke92sRKw2Sfh3oius2FkOH6ipjv3U/697E A7sKPPcw7+uvTPyLNhBzPvOk -----END CERTIFICATE-----",
    "context_chunks": [
      {
        "text": "I am using python to make a get request to jira cloud rest api to get details of an issue, but getting this SSL verification failed error message, I am using this script import requests import json url = &quot;https://your-domain.atlassian.net/rest/agile/1.0/issue/{issueIdOrKey}&quot; headers = { &quot;Accept&quot;: &quot;application/json&quot;, &quot;Authorization&quot;: &quot;Bearer &lt;access_token&gt;&quot; } response = requests.request( &quot;GET&quot;, url, headers=headers ) print(json.dumps(json.loads(response.text), sort_keys=True, indent=4, separators=(&quot;,&quot;, &quot;: &quot;))) error message- requests.exceptions.SSLError: HTTPSConnectionPool('host=your-domain.atlasian.net', port=443): Max retries exceeded with url: /rest/agile/1.0/issue/{issueIdOrKey} (caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed : self signed certificate in certificate chain (_ssl.c:992)'))) Suggest me possible ways to resolve this issue. Thank you!",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "self signed certificate in certificate chain means that certificate chain validation has failed. Your script does not trust the certificate or one of its issuers. For more information see Beginning with SSL for a Platform Engineer. The answer from Tzane had most of what you need. But it looks like you also might want to know WHAT certificate to add. So, first get the CA certificate, and any intermediate certs by running the following on a command line: openssl s_client -connect your-name.atlassian.net:443 -showcerts In the output there is a block that starts with Certificate chain. The output I got from Atlassian.net had only a server cert and CA cert. There are blocks of output that start with -----BEGIN CERTIFICATE----- and end with -----END CERTIFICATE----- These blocks, including the lines I just showed, are a certificate. Copy the last certificate and create a pem file e.g. ca-root.pem. Place this in the same directory as your python file and then update your requests block to be: verify = &quot;ca-root.pem&quot; response = requests.request( &quot;GET&quot;, url, headers=headers, verify=verify ) Hope this helps. ----- UPDATE ----- Using the domain you provided, msci.atlassian.net, I have the CA cert provided at this time by Digital Cert. -----BEGIN CERTIFICATE----- MIIEvjCCA6agAwIBAgIQBtjZBNVYQ0b2ii+nVCJ+xDANBgkqhkiG9w0BAQsFADBh MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3 d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBD QTAeFw0yMTA0MTQwMDAwMDBaFw0zMTA0MTMyMzU5NTlaME8xCzAJBgNVBAYTAlVT MRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxKTAnBgNVBAMTIERpZ2lDZXJ0IFRMUyBS U0EgU0hBMjU2IDIwMjAgQ0ExMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEAwUuzZUdwvN1PWNvsnO3DZuUfMRNUrUpmRh8sCuxkB+Uu3Ny5CiDt3+PE0J6a qXodgojlEVbbHp9YwlHnLDQNLtKS4VbL8Xlfs7uHyiUDe5pSQWYQYE9XE0nw6Ddn g9/n00tnTCJRpt8OmRDtV1F0JuJ9x8piLhMbfyOIJVNvwTRYAIuE//i+p1hJInuW raKImxW8oHzf6VGo1bDtN+I2tIJLYrVJmuzHZ9bjPvXj1hJeRPG/cUJ9WIQDgLGB Afr5yjK7tI4nhyfFK3TUqNaX3sNk+crOU6JWvHgXjkkDKa77SU+kFbnO8lwZV21r eacroicgE7XQPUDTITAHk+qZ9QIDAQABo4IBgjCCAX4wEgYDVR0TAQH/BAgwBgEB /wIBADAdBgNVHQ4EFgQUt2ui6qiqhIx56rTaD5iyxZV2ufQwHwYDVR0jBBgwFoAU A95QNVbRTLtm8KPiGxvDl7I90VUwDgYDVR0PAQH/BAQDAgGGMB0GA1UdJQQWMBQG CCsGAQUFBwMBBggrBgEFBQcDAjB2BggrBgEFBQcBAQRqMGgwJAYIKwYBBQUHMAGG GGh0dHA6Ly9vY3NwLmRpZ2ljZXJ0LmNvbTBABggrBgEFBQcwAoY0aHR0cDovL2Nh Y2VydHMuZGlnaWNlcnQuY29tL0RpZ2lDZXJ0R2xvYmFsUm9vdENBLmNydDBCBgNV HR8EOzA5MDegNaAzhjFodHRwOi8vY3JsMy5kaWdpY2VydC5jb20vRGlnaUNlcnRH bG9iYWxSb290Q0EuY3JsMD0GA1UdIAQ2MDQwCwYJYIZIAYb9bAIBMAcGBWeBDAEB MAgGBmeBDAECATAIBgZngQwBAgIwCAYGZ4EMAQIDMA0GCSqGSIb3DQEBCwUAA4IB AQCAMs5eC91uWg0Kr+HWhMvAjvqFcO3aXbMM9yt1QP6FCvrzMXi3cEsaiVi6gL3z ax3pfs8LulicWdSQ0/1s/dCYbbdxglvPbQtaCdB73sRD2Cqk3p5BJl+7j5nL3a7h qG+fh/50tx8bIKuxT8b1Z11dmzzp/2n3YWzW2fP9NsarA4h20ksudYbj/NhVfSbC EXffPgK2fPOre3qGNm+499iTcc+G33Mw+nur7SpZyEKEOxEXGlLzyQ4UfaJbcme6 ce1XR2bFuAJKZTRei9AqPCCcUZlM51Ke92sRKw2Sfh3oius2FkOH6ipjv3U/697E A7sKPPcw7+uvTPyLNhBzPvOk -----END CERTIFICATE-----",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "We had a similar cert issue with another web API that came up randomly on some machines. What we ended up doing is getting the ISRG Root X1 from Let's Encrypt and passing it manually to the request verify = &quot;isrgrootx1.pem&quot; response = requests.request( &quot;GET&quot;, url, headers=headers, verify=verify ) Update If you are using &gt;= Python 3.10, using truststore is also a good option. It allows you to use OS native &quot;cerfiticate store&quot; instead of the certifi package.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "ssl",
        "ssl-certificate",
        "jira-rest-api"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2023-06-29T07:32:27",
      "question_id": 76578915,
      "answer_id": 76647227
    }
  },
  {
    "question": "Uninstall uv Python package installer",
    "expected_answer": "The official documentation now has complete uninstall instructions: # Remove all data that uv has stored uv cache clean rm -r &quot;$(uv python dir)&quot; rm -r &quot;$(uv tool dir)&quot; # Remove binaries rm ~/.local/bin/uv ~/.local/bin/uvx If your original installation was a version earlier than 0.5.2 (November 2024), then replace the last line with: rm ~/.cargo/bin/uv ~/.cargo/bin/uvx These instructions cover macOS and Linux. See the documentation for Windows instructions.",
    "context_chunks": [
      {
        "text": "I recently installed uv on Linux using the command line provided in the documentation: curl -LsSf https://astral.sh/uv/install.sh | sh It created an executable in /home/currentuser/.cargo/bin/uv. Now, just out of curiosity, I would like to know how to remove it properly. Is it as simple as deleting a file? Or is there a script or command line which is cleaner? Please note that I also tried with pip install/uninstall uv, and it worked perfectly, but the installation path was different.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The official documentation now has complete uninstall instructions: # Remove all data that uv has stored uv cache clean rm -r &quot;$(uv python dir)&quot; rm -r &quot;$(uv tool dir)&quot; # Remove binaries rm ~/.local/bin/uv ~/.local/bin/uvx If your original installation was a version earlier than 0.5.2 (November 2024), then replace the last line with: rm ~/.cargo/bin/uv ~/.cargo/bin/uvx These instructions cover macOS and Linux. See the documentation for Windows instructions.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In the installation script, they say This script .. unpacks the binaries and installs them to \\$CARGO_HOME/bin (\\$HOME/.cargo/bin) It will then add that dir to PATH by adding the appropriate line to your shell profiles. So you can just remove the file, and that's all. Also, you can remove the . &quot;$HOME/.cargo/env&quot;. They added the rc file to your shell, but this is not necessary. BTW, they do have a task to add uninstall documentation, Add uninstallation steps for the tool in the documentation #1696, so later we will see the official instruction...",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "linux",
        "python-packaging",
        "uv"
      ],
      "question_score": 8,
      "answer_score": 12,
      "created": "2024-02-28T17:12:25",
      "question_id": 78076401,
      "answer_id": 79020762
    }
  },
  {
    "question": "How to return plain text in FastAPI",
    "expected_answer": "When you return a string from a FastAPI endpoint, FastAPI will automatically convert it to a JSON response, which is why the quotes are being escaped in your example. JSON strings must escape double quotes with a backslash. If you want to return unformatted text (plain text) and not JSON, you can do so by using FastAPI's PlainTextResponse object (docs here). I'm using FastApi version 0.104 here: import uvicorn from fastapi import FastAPI from fastapi.responses import PlainTextResponse app = FastAPI() @app.get( &quot;/&quot;, response_class=PlainTextResponse, ) def read_root(): return 'Hello &quot;World&quot;!' uvicorn.run(app)",
    "context_chunks": [
      {
        "text": "In this example, the entrypoint http://127.0.0.1:8000/ returns formatted text: &quot;Hello \\&quot;World\\&quot;!&quot; The quotes are masked by a slash, and quotes are added both at the beginning and at the end. How to return unformatted text, identical to my string Hello &quot;World&quot;!. import uvicorn from fastapi import FastAPI app = FastAPI() @app.get(&quot;/&quot;,) def read_root(): return 'Hello &quot;World&quot;!' uvicorn.run(app)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "When you return a string from a FastAPI endpoint, FastAPI will automatically convert it to a JSON response, which is why the quotes are being escaped in your example. JSON strings must escape double quotes with a backslash. If you want to return unformatted text (plain text) and not JSON, you can do so by using FastAPI's PlainTextResponse object (docs here). I'm using FastApi version 0.104 here: import uvicorn from fastapi import FastAPI from fastapi.responses import PlainTextResponse app = FastAPI() @app.get( &quot;/&quot;, response_class=PlainTextResponse, ) def read_root(): return 'Hello &quot;World&quot;!' uvicorn.run(app)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can also explicitly return a PlainTextResponse object: import uvicorn from fastapi import FastAPI from fastapi.responses import PlainTextResponse app = FastAPI() @app.get(&quot;/&quot;) def read_root(): return PlainTextResponse('Hello &quot;World&quot;!') uvicorn.run(app) Since media type is immaterial here, you can also return a Response object (PlainTextResponse is a child class of Response where media_type=&quot;text/plain&quot;): import uvicorn from fastapi import FastAPI, Response app = FastAPI() @app.get(&quot;/&quot;) def read_root(): return Response('Hello &quot;World&quot;!') uvicorn.run(app)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "fastapi"
      ],
      "question_score": 8,
      "answer_score": 13,
      "created": "2023-12-20T09:12:58",
      "question_id": 77690364,
      "answer_id": 77690392
    }
  },
  {
    "question": "How do I choose default python homebrew version?",
    "expected_answer": "The issue seems to be in the difference between python@3.11 and python@3.12 Homebrew Ruby scripts. 3.11 has link_overwrite for python3 which sets a bunch of symlinks in /usr/local/bin/ (which is usually in your $PATH) to use 3.11. 3.12 lacks this and so your shell is not seeing them. You can rectify this either by modifying your $PATH as in the other reply, or replacing the symlink manually: cd /usr/local/bin &amp;&amp; rm python3 &amp;&amp; ln -s ../Cellar/python@3.12/3.12.1/bin/python3.12 python3",
    "context_chunks": [
      {
        "text": "I have all three versions of python 3.10, 3.11 and 3.12 using homebrew. but somehow homebrew defaults to using 3.11 as the default. when i type which python3, it shows 3.11.6 as the version instead of 3.12. why did it default to 3.11 and how do i change this to 3.12? I was expecting the latest version 3.12 to be the default.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The issue seems to be in the difference between python@3.11 and python@3.12 Homebrew Ruby scripts. 3.11 has link_overwrite for python3 which sets a bunch of symlinks in /usr/local/bin/ (which is usually in your $PATH) to use 3.11. 3.12 lacks this and so your shell is not seeing them. You can rectify this either by modifying your $PATH as in the other reply, or replacing the symlink manually: cd /usr/local/bin &amp;&amp; rm python3 &amp;&amp; ln -s ../Cellar/python@3.12/3.12.1/bin/python3.12 python3",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Edit 2024-02-25: About four days ago, the below PR was merged. I believe brew install python should now install Python 3.12. python3 in Homebrew won't point at 3.12 until this PR is merged. AFAIK Homebrew don't switch the default python3 to the latest version until all the Python software that Homebrew packages are confirmed to work with the new version. If you really need python3 to point at 3.12 globally right now, I personally like @Iskander14yo's suggestion to use pyenv (above).",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "homebrew",
        "python-3.11",
        "python-3.12"
      ],
      "question_score": 8,
      "answer_score": 7,
      "created": "2023-10-20T14:06:30",
      "question_id": 77331589,
      "answer_id": 77655631
    }
  },
  {
    "question": "Split array of integers into subarrays with the biggest sum of difference between min and max",
    "expected_answer": "First let's solve a simpler problem. Let's run through an array, and give mins and maxes for all windows of fixed size. def window_mins_maxes (size, array): min_values = deque() min_positions = deque() max_values = deque() max_positions = deque() for i, value in enumerate(array): if size &lt;= i: yield (i, min_values[0], max_values[0]) if min_positions[0] &lt;= i - size: min_values.popleft() min_positions.popleft() if max_positions[0] &lt;= i - size: max_values.popleft() max_positions.popleft() while 0 &lt; len(min_values) and value &lt;= min_values[-1]: min_values.pop() min_positions.pop() min_values.append(value) min_positions.append(i) while 0 &lt; len(max_values) and max_values[-1] &lt;= value: max_values.pop() max_positions.pop() max_values.append(value) max_positions.append(i) yield (len(array), min_values[0], max_values[0]) This clearly takes memory O(size). What's less obvious is that it takes time O(n) to process an array of length n. But we can see that with amortized analysis. To each element we'll attribute the cost of checking the possible value that is smaller than it, the cost of some later element checking that it should be removed, and the cost of being added. That accounts for all operations (though this isn't the order that they happen) and is a fixed amount of work per element. Also note that the memory needed for this part of the solution fits within O(n). So far I'd consider this a well-known dynamic programming problem. Now let's make it more challenging. We will tackle the partition problem as a traditional dynamic programming problem. We'll build up an array best_weight of the best partition to that point, and prev_index of the start of the previous partition ending just before that point. To build it up, we'll use the above algorithm to take a previous partition and add one of min_len to it. If it is better than the previous, we'll save its information in those arrays. We'll then scan forward from that partition and do that up to max_len. Then we move on to the next possible start of a partition. When we're done we'll find the answer from that code. Here is what that looks like: def partition_array(numbers, min_len, max_len): if max_len &lt; min_len or len(numbers) &lt; min_len: return (None, None) best_weight = [None for _ in numbers] prev_index = [None for _ in numbers] # Need an extra entry for off of the end of the array. best_weight.append(None) prev_index.append(None) best_weight[0] = 0 for i, min_value, max_value in window_mins_maxes(min_len, numbers): window_start_weight = best_weight[i - min_len] if window_start_weight is not None: j = i while j - i &lt; max_len - min_len and j &lt; len(numbers): new_weight = window_start_weight + max_value - min_value if best_weight[j] is None or best_weight[j] &lt; new_weight: best_weight[j] = new_weight prev_index[j] = i - min_len if numbers[j] &lt; min_value: min_value = numbers[j] if max_value &lt; numbers[j]: max_value = numbers[j] j += 1 # And fill in the longest value. new_weight = window_start_weight + max_value - min_value if best_weight[j] is None or best_weight[j] &lt; new_weight: best_weight[j] = new_weight prev_index[j] = i - min_len if best_weight[-1] is None: return (None, None) else: path = [len(numbers)] while prev_index[path[-1]] is not None: path.append(prev_index[path[-1]]) path = list(reversed(path)) partitioned = [numbers[path[i]:path[i+1]] for i in range(len(path)-1)] return (best_weight[-1], partitioned) Note that we do O(1) work for each possible start and length. And so that is time O((max_len + 1 - min_len)*n). And the data structures we used are all bounded above by O(n) in size. Giving the overall efficiency that I promised in the comments. Now let's test it. print(partition_array([5, 8, 4, 5, 1, 3, 5, 1, 3, 1], 3, 7)) print(partition_array([1, 6, 2, 2, 5, 2, 8, 1, 5, 6], 3, 4)) print(partition_array([5, 8, 4, 5, 1, 3, 5, 1, 3, 1, 2], 4, 5)) And the output is: (12, [[5, 8, 4], [5, 1, 3], [5, 1, 3, 1]]) (16, [[1, 6, 2], [2, 5, 2, 8], [1, 5, 6]]) (None, None)",
    "context_chunks": [
      {
        "text": "I'm trying to find the algorithm efficiently solving this problem: Given an unsorted array of numbers, you need to divide it into several subarrays of length from a to b, so that the sum of differences between the minimum and maximum numbers in each of the subarrays is the greatest. The order of the numbers must be preserved. Examples: a = 3, b = 7 input: [5, 8, 4, 5, 1, 3, 5, 1, 3, 1] answer: [[5, 8, 4], [5, 1, 3], [5, 1, 3, 1]] (diff sum is 12) a = 3, b = 4 input: [1, 6, 2, 2, 5, 2, 8, 1, 5, 6] answer: [[1, 6, 2], [2, 5, 2, 8], [1, 5, 6]] (diff sum is 16) a = 4, b = 5 input: [5, 8, 4, 5, 1, 3, 5, 1, 3, 1, 2] answer: splitting is impossible The only solution I've come up with so far is trying all of the possible subarray combinations. from collections import deque def partition_array(numbers, min_len, max_len): max_diff_subarray = None queue = deque() for end in range(min_len - 1, max_len): if end &lt; len(numbers): diff = max(numbers[0:end + 1]) - min(numbers[0:end + 1]) queue.append(Subarray(previous=None, start=0, end=end, diff_sum=diff)) while queue: subarray = queue.popleft() if subarray.end == len(numbers) - 1: if max_diff_subarray is None: max_diff_subarray = subarray elif max_diff_subarray.diff_sum &lt; subarray.diff_sum: max_diff_subarray = subarray continue start = subarray.end + 1 for end in range(start + min_len - 1, start + max_len): if end &lt; len(numbers): diff = max(numbers[start:end + 1]) - min(numbers[start:end + 1]) queue.append(Subarray(previous=subarray, start=start, end=end, diff_sum=subarray.diff_sum + diff)) else: break return max_diff_subarray class Subarray: def __init__(self, previous=None, start=0, end=0, diff_sum=0): self.previous = previous self.start = start self.end = end self.diff_sum = diff_sum numbers = [5, 8, 4, 5, 1, 3, 5, 1, 3, 1] a = 3 b = 7 result = partition_array(numbers, a, b) print(result.diff_sum) Are there any more time efficient solutions?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "First let's solve a simpler problem. Let's run through an array, and give mins and maxes for all windows of fixed size. def window_mins_maxes (size, array): min_values = deque() min_positions = deque() max_values = deque() max_positions = deque() for i, value in enumerate(array): if size &lt;= i: yield (i, min_values[0], max_values[0]) if min_positions[0] &lt;= i - size: min_values.popleft() min_positions.popleft() if max_positions[0] &lt;= i - size: max_values.popleft() max_positions.popleft() while 0 &lt; len(min_values) and value &lt;= min_values[-1]: min_values.pop() min_positions.pop() min_values.append(value) min_positions.append(i) while 0 &lt; len(max_values) and max_values[-1] &lt;= value: max_values.pop() max_positions.pop() max_values.append(value) max_positions.append(i) yield (len(array), min_values[0], max_values[0]) This clearly takes memory O(size). What's less obvious is that it takes time O(n) to process an array of length n. But we can see that with amortized analysis. To each element we'll attribute the cost of checking the possible value that is smaller than it, the cost of some later element checking that it should be removed, and the cost of being added. That accounts for all operations (though this isn't the order that they happen) and is a fixed amount of work per element. Also note that the memory needed for this part of the solution fits within O(n). So far I'd consider this a well-known dynamic programming problem. Now let's make it more challenging. We will tackle the partition problem as a traditional dynamic programming problem. We'll build up an array best_weight of the best partition to that point, and prev_index of the start of the previous partition ending just before that point. To build it up, we'll use the above algorithm to take a previous partition and add one of min_len to it. If it is better than the previous, we'll save its information in those arrays. We'll then scan forward from that partition and do that up to max_len. Then we move on to the next possible start of a partition. When we're done we'll find the answer from that code. Here is what that looks like: def partition_array(numbers, min_len, max_len): if max_len &lt; min_len or len(numbers) &lt; min_len: return (None, None) best_weight = [None for _ in numbers] prev_index = [None for _ in numbers] # Need an extra entry for off of the end of the array. best_weight.append(None) prev_index.append(None) best_weight[0] = 0 for i, min_value, max_value in window_mins_maxes(min_len, numbers): window_start_weight = best_weight[i - min_len] if window_start_weight is not None: j = i while j - i &lt; max_len - min_len and j &lt; len(numbers): new_weight = window_start_weight + max_value - min_value if best_weight[j] is None or best_weight[j] &lt; new_weight: best_weight[j] = new_weight prev_index[j] = i - min_len if numbers[j] &lt; min_value: min_value = numbers[j] if max_value &lt; numbers[j]: max_value = numbers[j] j += 1 # And fill in the longest value. new_weight = window_start_weight + max_value - min_value if best_weight[j] is None or best_weight[j] &lt; new_weight: best_weight[j] = new_weight prev_index[j] = i - min_len if best_weight[-1] is None: return (None, None) else: path = [len(numbers)] while prev_index[path[-1]] is not None: path.append(prev_index[path[-1]]) path = list(reversed(path)) partitioned = [numbers[path[i]:path[i+1]] for i in range(len(path)-1)] return (best_weight[-1], partitioned) Note that we do O(1) work for each possible start and length. And so that is time O((max_len + 1 - min_len)*n). And the data structures we used are all bounded above by O(n) in size. Giving the overall efficiency that I promised in the comments. Now let's test it. print(partition_array([5, 8, 4, 5, 1, 3, 5, 1, 3, 1], 3, 7)) print(partition_array([1, 6, 2, 2, 5, 2, 8, 1, 5, 6], 3, 4)) print(partition_array([5, 8, 4, 5, 1, 3, 5, 1, 3, 1, 2], 4, 5)) And the output is: (12, [[5, 8, 4], [5, 1, 3], [5, 1, 3, 1]]) (16, [[1, 6, 2], [2, 5, 2, 8], [1, 5, 6]]) (None, None)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Just an alternative for btilly's window_mins_maxes, also O(len(array)) time and O(size) space: from itertools import accumulate def window_mins_maxes(size, array): leftmins = None for stop in range(size, len(array) + 1): if not leftmins: left = array[stop-size : stop][::-1] right = array[stop-1 : stop+size] leftmins = list(accumulate(left, min)) leftmaxs = list(accumulate(left, max)) rightmins = accumulate(right, min) rightmaxs = accumulate(right, max) yield ( stop, min(leftmins.pop(), next(rightmins)), max(leftmaxs.pop(), next(rightmaxs)) ) The idea: If you want the minimum of some array range, split the range into a left part and a right part, and take the minimum of the left's minimum and the right's minimum. For example for 10 elements and window size 4: 0 1 2 3 4 5 6 7 8 9 L L L L R L L L R R L L R R R L R R R R L L L L R ... The Ls mark the left part and the Rs mark the right part of the window. The right part is growing, so its minima are just its cumulative minima. The left part is shrinking, so its minima are it's reversed cumulative suffix minima. Whenever I don't have left-part minima, I compute a chunk of them and reset the right part, as shown in the last step above. I made L and R overlap so they're never empty, not even when size=1. Attempt This Online!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm"
      ],
      "question_score": 8,
      "answer_score": 9,
      "created": "2024-04-20T20:22:26",
      "question_id": 78359610,
      "answer_id": 78360359
    }
  },
  {
    "question": "OpenAI API: How do I enable JSON mode using the gpt-4-vision-preview model?",
    "expected_answer": "You can get the JSON response back only if using gpt-4-1106-preview or gpt-3.5-turbo-1106, as stated in the official OpenAI documentation: A common way to use Chat Completions is to instruct the model to always return JSON in some format that makes sense for your use case, by providing a system message. This works well, but occasionally the models may generate output that does not parse to valid JSON. To prevent these errors and improve model performance, when calling gpt-4-1106-preview or gpt-3.5-turbo-1106, you can set response_format to { type: &quot;json_object&quot; } to enable JSON mode. When JSON mode is enabled, the model is constrained to only generate strings that parse into valid JSON. Also, I've made a YouTube tutorial on how to get the response in JSON format and posted the code on my GitHub profile. Working example in Python If you run test.py, you'll get the following response: { &quot;response&quot;: &quot;Hello! How can I assist you today?&quot; } test.py import os from openai import OpenAI client = OpenAI() OpenAI.api_key = os.getenv('OPENAI_API_KEY') completion = client.chat.completions.create( model=&quot;gpt-4-1106-preview&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant. Your response should be in JSON format.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;} ], response_format={&quot;type&quot;: &quot;json_object&quot;} ) print(completion.choices[0].message.content) Working example in Node.js If you run test.js, you'll get the following response: { &quot;response&quot;: &quot;Hello! How can I assist you today?&quot; } test.js const OpenAI = require(&quot;openai&quot;); const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY, }); async function main() { const completion = await openai.chat.completions.create({ model: &quot;gpt-4-1106-preview&quot;, messages: [ { role: &quot;system&quot;, content: &quot;You are a helpful assistant. Your response should be in JSON format.&quot;, }, { role: &quot;user&quot;, content: &quot;Hello!&quot; }, ], response_format: { type: &quot;json_object&quot; }, }); console.log(completion.choices[0].message.content); } main();",
    "context_chunks": [
      {
        "text": "Update: It seems like they made a mistake in the API docs, and fixed it now. Earlier, it said &quot;when calling gpt-4-vision-preview or gpt-3.5-turbo,&quot; but now reads &quot;when calling gpt-4-1106-preview or gpt-3.5-turbo-1106.&quot; According to Text generation - OpenAI API, &quot;when calling gpt-4-vision-preview or gpt-3.5-turbo, you can set response_format to { type: &quot;json_object&quot; } to enable JSON mode.&quot; However, the following code throws an error: {'error': {'message': '1 validation error for Request\\nbody -&gt; response_format\\n extra fields not permitted (type=value_error.extra)', 'type': 'invalid_request_error', 'param': None, 'code': None}} If I comment &quot;response_format&quot;: {&quot;type&quot;: &quot;json_object&quot;}, it works fine. headers = { &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot; } payload = { &quot;model&quot;: &quot;gpt-4-vision-preview&quot;, &quot;response_format&quot;: {&quot;type&quot;: &quot;json_object&quot;}, &quot;messages&quot;: [ { &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant. Your response should be in JSON format.&quot; }, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [ { &quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt }, { &quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: { &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot; } } ] } ], &quot;max_tokens&quot;: 1000, } response = requests.post(&quot;https://api.openai.com/v1/chat/completions&quot;, headers=headers, json=payload) print(response.json())",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can get the JSON response back only if using gpt-4-1106-preview or gpt-3.5-turbo-1106, as stated in the official OpenAI documentation: A common way to use Chat Completions is to instruct the model to always return JSON in some format that makes sense for your use case, by providing a system message. This works well, but occasionally the models may generate output that does not parse to valid JSON. To prevent these errors and improve model performance, when calling gpt-4-1106-preview or gpt-3.5-turbo-1106, you can set response_format to { type: &quot;json_object&quot; } to enable JSON mode. When JSON mode is enabled, the model is constrained to only generate strings that parse into valid JSON. Also, I've made a YouTube tutorial on how to get the response in JSON format and posted the code on my GitHub profile. Working example in Python If you run test.py, you'll get the following response: { &quot;response&quot;: &quot;Hello! How can I assist you today?&quot; } test.py import os from openai import OpenAI client = OpenAI() OpenAI.api_key = os.getenv('OPENAI_API_KEY') completion = client.chat.completions.create( model=&quot;gpt-4-1106-preview&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant. Your response should be in JSON format.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;} ], response_format={&quot;type&quot;: &quot;json_object&quot;} ) print(completion.choices[0].message.content) Working example in Node.js If you run test.js, you'll get the following response: { &quot;response&quot;: &quot;Hello! How can I assist you today?&quot; } test.js const OpenAI = require(&quot;openai&quot;); const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY, }); async function main() { const completion = await openai.chat.completions.create({ model: &quot;gpt-4-1106-preview&quot;, messages: [ { role: &quot;system&quot;, content: &quot;You are a helpful assistant. Your response should be in JSON format.&quot;, }, { role: &quot;user&quot;, content: &quot;Hello!&quot; }, ], response_format: { type: &quot;json_object&quot; }, }); console.log(completion.choices[0].message.content); } main();",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Unfortunately at the moment, the gpt-4-vision-preview and gpt-3.5-turbo models don't support the JSON output format. In the official documentation from OpenAI, you can read about the JSON mode. There are mentioned only two models: gpt-4-1106-preview and gpt-3.5-turbo-1106. Therefore, the solution for you is to choose one of these models.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "openai-api",
        "gpt-4"
      ],
      "question_score": 8,
      "answer_score": 9,
      "created": "2023-11-06T23:46:14",
      "question_id": 77434808,
      "answer_id": 77436958
    }
  },
  {
    "question": "Why does an operation on a large integer silently overflow?",
    "expected_answer": "Why does an operation on a large integer silently overflow? As a short answer, that's because of how numpy deals with overflows. On my platform (with the same versions of Python/Packages as yours) : from platform import * import numpy as np; import pandas as pd system(), version(), machine() python_version(), pd.__version__, np.__version__ ('Linux', '#34~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep 7 13:12:03 UTC 2', 'x86_64') ('3.12.0', '2.1.1', '1.26.0') I can reproduce your issue but with a larger integer than the one you choose as an example : pd.Series([2**63], dtype=&quot;int32&quot;) raises this : OverflowError: Python int too large to convert to C long While pd.Series([2**31], dtype=&quot;int32&quot;) raises this : ValueError: Values are too large to be losslessly converted to int32. To cast anyway, use pd.Series(values).astype(int32) Details Let's agree that you're using two different type of objects, which potentially means two different scenarios, i.e, 1) error raised or 2) no error raised : pd.Series : the Series constructor pd.Series.add : a method of the former The construction : pd.Series([2**31], dtype=&quot;int32&quot;) It is handled behind the scenes by sanitize_array which receives your input (the list [2**31], i.e [2147483648]) and call in this case maybe_cast_to_integer_array. The latter will make a classical NumPy construction using np.array : casted = np.array([2147483648], dtype=&quot;int32&quot;) DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays. The conversion of 2147483648 to int32 will fail in the future. For the old behavior, usually: np.array(value).astype(dtype) will give the desired result (the cast overflows). np.array([2147483648], dtype='int32') You may ask yourself why the warning above doesn't show off while constructing your Series, well that's because pandas silents it. Now, right after casting, pandas calls np.asarray without specifying a dtype to let NumPy infer a dtype (which is int64 here) : arr = np.asarray(arr). And since, casted.dtype &lt; arr.dtype, the ValueError is triggered. The addition : pd.Series([2**31-1], dtype=&quot;int32&quot;) + 1 This one is delegated to _na_arithmetic_op that receives array([2147483647], dtype=int32) and 1 and try to add them together with the help of _evaluate_standard to make a classical operator.add operation that is equivalent to np.array([2147483647]) + 1 and since the fixed size of NumPy numeric types may cause overflow errors when a value requires more memory than available in the data type, the result is the array([-2147483648], dtype=int32) which is passed to sanitize_array to construct back a Series : pd.Series([2**31-1], dtype=&quot;int32&quot;) + 1 0 -2147483648 dtype: int32 NB : When you go beyond the limit of the int32, NumPy wraps around to the minimum value : a = np.array([2**31-1], dtype=&quot;int32&quot;); b = 1 a+b # this gives array([-2147483648], dtype=int32) Here is some other examples : def wrap_int32(i, N, l=2**31): return ((i+N) % l) - l wrap_int32(2**31, 0) # -2147483648 wrap_int32(2**31, 1) # -2147483647 wrap_int32(2**31, 2) # -2147483646 wrap_int32(2**31, 3) # -2147483645 # ... I have a list that contains very large integers and I want to cast it into a pandas column with a specific dtype. As an example, if the list contains 2**31, which is outside the limit of int32 dtype, casting it into dtype int32 throws an OverflowError, which lets me know to use another dtype or handle the number in some other way beforehand. Maybe you should consider opening an issue so that the arithmetic operations made by pandas raise an error in case of an overflow. And as a workaround (or maybe a solution?) for your usecase, you can try catching upstream the integers that doesn't fall within the int32 range : iint32 = np.iinfo(np.int32) lst = [100, 1234567890000, -1e19, 2**31, 2**31-1, -350] out = [i for i in lst if iint32.min &lt;= i and i &lt;= iint32.max] # [100, 2147483647, -350]",
    "context_chunks": [
      {
        "text": "I have a list that contains very large integers and I want to cast it into a pandas column with a specific dtype. As an example, if the list contains 2**31, which is outside the limit of int32 dtype, casting it into dtype int32 throws an Overflow Error, which lets me know to use another dtype or handle the number in some other way beforehand. import pandas as pd pd.Series([2**31], dtype='int32') # OverflowError: Python int too large to convert to C long But if a number is large but inside the dtype limits (i.e. 2**31-1), and some number is added to it which results in a value that is outside the dtype limits, then instead of an OverflowError, the operation is executed without any errors, yet the value is now inverted, becoming a completely wrong number for the column. pd.Series([2**31-1], dtype='int32') + 1 0 -2147483648 dtype: int32 Why is it happening? Why doesn’t it raise an error like the first case? PS. I'm using pandas 2.1.1 and numpy 1.26.0 on Python 3.12.0.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Why does an operation on a large integer silently overflow? As a short answer, that's because of how numpy deals with overflows. On my platform (with the same versions of Python/Packages as yours) : from platform import * import numpy as np; import pandas as pd system(), version(), machine() python_version(), pd.__version__, np.__version__ ('Linux', '#34~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep 7 13:12:03 UTC 2', 'x86_64') ('3.12.0', '2.1.1', '1.26.0') I can reproduce your issue but with a larger integer than the one you choose as an example : pd.Series([2**63], dtype=&quot;int32&quot;) raises this : OverflowError: Python int too large to convert to C long While pd.Series([2**31], dtype=&quot;int32&quot;) raises this : ValueError: Values are too large to be losslessly converted to int32. To cast anyway, use pd.Series(values).astype(int32) Details Let's agree that you're using two different type of objects, which potentially means two different scenarios, i.e, 1) error raised or 2) no error raised : pd.Series : the Series constructor pd.Series.add : a method of the former The construction : pd.Series([2**31], dtype=&quot;int32&quot;) It is handled behind the scenes by sanitize_array which receives your input (the list [2**31], i.e [2147483648]) and call in this case maybe_cast_to_integer_array. The latter will make a classical NumPy construction using np.array : casted = np.array([2147483648], dtype=&quot;int32&quot;) DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays. The conversion of 2147483648 to int32 will fail in the future. For the old behavior, usually: np.array(value).astype(dtype) will give the desired result (the cast overflows). np.array([2147483648], dtype='int32') You may ask yourself why the warning above doesn't show off while constructing your Series, well that's because pandas silents it. Now, right after casting, pandas calls np.asarray without specifying a dtype to let NumPy infer a dtype (which is int64 here) : arr = np.asarray(arr). And since, casted.dtype &lt; arr.dtype, the ValueError is triggered. The addition : pd.Series([2**31-1], dtype=&quot;int32&quot;) + 1 This one is delegated to _na_arithmetic_op that receives array([2147483647], dtype=int32) and 1 and try to add them together with the help of _evaluate_standard to make a classical operator.add operation that is equivalent to np.array([2147483647]) + 1 and since the fixed size of NumPy numeric types may cause overflow errors when a value requires more memory than available in the data type, the result is the array([-2147483648], dtype=int32) which is passed to sanitize_array to construct back a Series : pd.Series([2**31-1], dtype=&quot;int32&quot;) + 1 0 -2147483648 dtype: int32 NB : When you go beyond the limit of the int32, NumPy wraps around to the minimum value : a = np.array([2**31-1], dtype=&quot;int32&quot;); b = 1 a+b # this gives array([-2147483648], dtype=int32) Here is some other examples : def wrap_int32(i, N, l=2**31): return ((i+N) % l) - l wrap_int32(2**31, 0) # -2147483648 wrap_int32(2**31, 1) # -2147483647 wrap_int32(2**31, 2) # -2147483646 wrap_int32(2**31, 3) # -2147483645 # ... I have a list that contains very large integers and I want to cast it into a pandas column with a specific dtype. As an example, if the list contains 2**31, which is outside the limit of int32 dtype, casting it into dtype int32 throws an OverflowError, which lets me know to use another dtype or handle the number in some other way beforehand. Maybe you should consider opening an issue so that the arithmetic operations made by pandas raise an error in case of an overflow. And as a workaround (or maybe a solution?) for your usecase, you can try catching upstream the integers that doesn't fall within the int32 range : iint32 = np.iinfo(np.int32) lst = [100, 1234567890000, -1e19, 2**31, 2**31-1, -350] out = [i for i in lst if iint32.min &lt;= i and i &lt;= iint32.max] # [100, 2147483647, -350]",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Not 100% sure, but as an educated guess: The first overflow happens on the border between Python and C, and the overflow is being detected during the conversion. However, the second overflow is happening entirely within C, where no integer overflow checks exist.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "long-integer"
      ],
      "question_score": 8,
      "answer_score": 9,
      "created": "2023-10-11T00:35:19",
      "question_id": 77269618,
      "answer_id": 77296321
    }
  },
  {
    "question": "How can I mock my environment variables for my pytest?",
    "expected_answer": "To update the environment, and restore it after the test: import os from unittest import mock @pytest.fixture() def setenvvar(monkeypatch): with mock.patch.dict(os.environ, clear=True): envvars = { &quot;API_AUDIENCE&quot;: &quot;https://mock.com&quot;, } for k, v in envvars.items(): monkeypatch.setenv(k, v) yield # This is the magical bit which restore the environment after This is just a fixture, you can use it (or not) in any test you want, at the scope you want, no extra dependencies.",
    "context_chunks": [
      {
        "text": "I've looked at some examples to mock environment variables. Here In my case, I have to mock a variable in my config.py file that is setup as follows: class ServiceConfig(BaseSettings): ... API_AUDIENCE: str = os.environ.get(&quot;API_AUDIENCE&quot;) ... ... I have that called here: class Config(BaseSettings): auth: ClassVar[ServiceConfig] = ServiceConfig() I've attempted to patch that value as follows: @mock.patch.dict(os.environ, {&quot;API_AUDIENCE&quot;: &quot;https://mock.com&quot;}) class Test(TestCase): Another way I've tried is: @patch('config.Config.API_AUDIENCE', &quot;https://mock.com&quot;) def ....: The setup is incorrect both ways, getting this error: E pydantic_core._pydantic_core.ValidationError: 1 validation error for ServiceConfig E API_AUDIENCE What is the correct way to setup mocking an environment variable?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "To update the environment, and restore it after the test: import os from unittest import mock @pytest.fixture() def setenvvar(monkeypatch): with mock.patch.dict(os.environ, clear=True): envvars = { &quot;API_AUDIENCE&quot;: &quot;https://mock.com&quot;, } for k, v in envvars.items(): monkeypatch.setenv(k, v) yield # This is the magical bit which restore the environment after This is just a fixture, you can use it (or not) in any test you want, at the scope you want, no extra dependencies.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can use setenv: def test_something(self, monkeypatch): monkeypatch.setenv(&quot;API_AUDIENCE&quot;, &quot;https://mock.com&quot;) importlib.reload(conf) assert &quot;mock&quot; in Config.auth",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pytest",
        "pydantic",
        "pytest-mock-patch"
      ],
      "question_score": 8,
      "answer_score": 9,
      "created": "2023-10-09T00:15:49",
      "question_id": 77255758,
      "answer_id": 77256931
    }
  },
  {
    "question": "Pandas 2.1.0 warning &#39;&#39;The &#39;method&#39; keyword in Series.replace is deprecated and will be removed in a future version&#39;&#39;",
    "expected_answer": "You can do this instead : df[&quot;temp_open&quot;] = df[&quot;temp_open&quot;].replace(&quot;&quot;, None).ffill() And if you want to keep the nulls (if any) untouched, you can use : df[&quot;temp_open&quot;] = ( df[&quot;temp_open&quot;].replace(&quot;&quot;, None).ffill().where(df[&quot;temp_open&quot;].notnull()) ) Output : print(df) temp_open 0 A 1 A 2 NaN 3 B 4 C 5 C Used input : df = pd.DataFrame({&quot;temp_open&quot;: [&quot;A&quot;, &quot;&quot;, None, &quot;B&quot;, &quot;C&quot;, &quot;&quot;]})",
    "context_chunks": [
      {
        "text": "I have a pandas line of code that gives me a future deprecation warning as stated in the title and I can't find in the pandas documentation how to modify it in order to remove the warning. The line of code is the following: df['temp_open']=df['temp_open'].replace('',method='ffill') Any help would be greatly appreciated. I tried to fill blanks and it works, but I would like to get rid of the warning.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can do this instead : df[&quot;temp_open&quot;] = df[&quot;temp_open&quot;].replace(&quot;&quot;, None).ffill() And if you want to keep the nulls (if any) untouched, you can use : df[&quot;temp_open&quot;] = ( df[&quot;temp_open&quot;].replace(&quot;&quot;, None).ffill().where(df[&quot;temp_open&quot;].notnull()) ) Output : print(df) temp_open 0 A 1 A 2 NaN 3 B 4 C 5 C Used input : df = pd.DataFrame({&quot;temp_open&quot;: [&quot;A&quot;, &quot;&quot;, None, &quot;B&quot;, &quot;C&quot;, &quot;&quot;]})",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Argument method from function replace is being depreciated in favour of a function ffill. Docs: replace-docs ffill-docs What you have to do is to refactor your code in a way @Timeless have answered your question.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "replace",
        "deprecation-warning"
      ],
      "question_score": 8,
      "answer_score": 11,
      "created": "2023-09-25T07:35:11",
      "question_id": 77170645,
      "answer_id": 77170722
    }
  },
  {
    "question": "After `uv init`, adding script to `[project.scripts]` does not work",
    "expected_answer": "[project.scripts] is not uv-specific; as documented: In this example, after installing your project, a spam-cli command will be available. You'd need to uv pip install -e . your package, so the wrapper scripts are generated and placed in your (virtual)env's bin/ directory.",
    "context_chunks": [
      {
        "text": "I created a new project with uv 0.5.7: uv init myproject cd myproject uv sync And my project looks like this: ├── hello.py ├── pyproject.toml ├── README.md └── uv.lock At this point, the following works: uv run python hello.py uv run python -m hello I would like to make a script name um by adding the following to pyproject.toml: [project.scripts] um = &quot;hello:main&quot; However, that script is not found when I run it: $ uv run um [umbala:main:] error: Failed to spawn: `um` Caused by: No such file or directory (os error 2) I need help to create this script, um. What did I miss?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "[project.scripts] is not uv-specific; as documented: In this example, after installing your project, a spam-cli command will be available. You'd need to uv pip install -e . your package, so the wrapper scripts are generated and placed in your (virtual)env's bin/ directory.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You might be missing a build system. uv requires a defined build system to determine whether to install the current project in the virtualenv. Something like this should work: [build-system] requires = [&quot;hatchling&quot;] build-backend = &quot;hatchling.build&quot; Using the entry point tables requires a build system to be defined. https://docs.astral.sh/uv/concepts/projects/config/#entry-points",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "uv"
      ],
      "question_score": 8,
      "answer_score": 7,
      "created": "2024-12-14T14:53:24",
      "question_id": 79280838,
      "answer_id": 79280880
    }
  },
  {
    "question": "How do I get the generic class property in python",
    "expected_answer": "You can define a metaclass: from typing import Generic, TypeVar T = TypeVar(&quot;T&quot;) class AbstractBaseClass: pass class BaseClassMeta(type): def __init__(cls, name, bases, attrs): assert &quot;target_type&quot; not in attrs if len(attrs[&quot;__orig_bases__&quot;]) == 1: base, = attrs[&quot;__orig_bases__&quot;] if args := getattr(base, &quot;__args__&quot;, None): cls.target_type = args return super().__init__(name, bases, attrs) class BaseClass(AbstractBaseClass, Generic[T], metaclass=BaseClassMeta): pass class TargetClass(): pass class ExampleClass(BaseClass[TargetClass]): pass print(ExampleClass().target_type) # TargetClass This will make all subclasses of BaseClass that have a single superclass which has a single generic parameter have an attribute target_type, set to that parameter. This will be available to instances of the class too, and subclasses.",
    "context_chunks": [
      {
        "text": "I have a class like: class ExampleClass(BaseClass[Someclass]): pass class BaseClass(AbstractBaseClass, Generic[T]): pass I want to be able to do something like ExampleClass.targetType where I will return Someclass.__name__ how can I do this inside BaseClass I cannot seem to do T.__name__ I can workaround by defining a method like class ExampleClass(BaseClass[Something]): version = 1 def get_target_class_name() -&gt; str: return Something.__name__ But I will need to copy this for every class",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can define a metaclass: from typing import Generic, TypeVar T = TypeVar(&quot;T&quot;) class AbstractBaseClass: pass class BaseClassMeta(type): def __init__(cls, name, bases, attrs): assert &quot;target_type&quot; not in attrs if len(attrs[&quot;__orig_bases__&quot;]) == 1: base, = attrs[&quot;__orig_bases__&quot;] if args := getattr(base, &quot;__args__&quot;, None): cls.target_type = args return super().__init__(name, bases, attrs) class BaseClass(AbstractBaseClass, Generic[T], metaclass=BaseClassMeta): pass class TargetClass(): pass class ExampleClass(BaseClass[TargetClass]): pass print(ExampleClass().target_type) # TargetClass This will make all subclasses of BaseClass that have a single superclass which has a single generic parameter have an attribute target_type, set to that parameter. This will be available to instances of the class too, and subclasses.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can make use of typing.get_args and __orig_bases__. get_args will give you the arguments of a type: assert get_args(Dict[int, str]) == (int, str) and __orig_bases__ stores the original bases as a tuple. In the context of the example code below assert Derived.__orig_bases__ == (Base[int],) so the resulting code would be something like: from abc import ABC from typing import Generic, TypeVar, get_args T = TypeVar(&quot;T&quot;) class Base(ABC, Generic[T]): def target_type(self) -&gt; str: return get_args(self.__orig_bases__[0])[0].__name__ class Derived(Base[int]): pass d = Derived() print(d.target_type()) # prints 'int'",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "generics"
      ],
      "question_score": 8,
      "answer_score": 9,
      "created": "2023-06-14T08:34:38",
      "question_id": 76471591,
      "answer_id": 76501155
    }
  },
  {
    "question": "Speed up numpy looking for best indices",
    "expected_answer": "IMPROVEMENT 1 The following approach completes the task in approximately 0.4 seconds on my machine. The key idea is to avoid using np.argmin and instead leverage binary search (np.searchsorted) on a pre-sorted array of z-coordinates. This approach avoids the inefficiency of repeatedly using np.argmin by pre-sorting the z-coordinates for each x and then employing np.searchsorted to efficiently find the closest z-values. By sorting the z-coordinates with a complexity of O(N log N) and using binary search for each integer z, which has a complexity of O(log N), the method significantly reduces computational load compared to the original approach. This strategy involves finding the insertion point for each integer z in the sorted z-coordinates and comparing it with its neighbors to determine the closest z-value. This eliminates the need for redundant calculations and ensures that only necessary comparisons are made, thereby improving performance. x_size = 2000 y_size = 2500 z_size = 400 rng = np.random.default_rng(123) z_coordinates = np.linspace(0, z_size, y_size) + rng.laplace(0, 1, (x_size, y_size)) # Sort the z_coordinates for each x and get sorted indices idx_sorted = np.argsort(z_coordinates, axis=1) z_sorted = np.take_along_axis(z_coordinates, idx_sorted, axis=1) zs = np.arange(z_size, dtype=np.float64) y_coordinates = np.empty((x_size, z_size), dtype=np.uint16) for x in range(x_size): positions = np.searchsorted(z_sorted[x, :], zs) positions = np.clip(positions, 1, y_size - 1) z_prev = z_sorted[x, positions - 1] z_curr = z_sorted[x, positions] diffs_prev = np.abs(z_prev - zs) diffs_curr = np.abs(z_curr - zs) closest = np.where(diffs_prev &lt; diffs_curr, positions - 1, positions) y_coordinates[x, :] = idx_sorted[x, closest] IMPROVEMENT 2 One way of improving performance is to add the following line of code: z_coordinates = z_coordinates.astype(np.float32).copy(order='C') Ensuring a C-contiguous memory layout and using float32 instead of float64 significantly speeds up computation by optimizing memory access patterns and reducing data size. C-contiguous arrays store elements in a row-major order, improving cache efficiency and enabling vectorized operations by minimizing cache misses and leveraging CPU optimizations. Switching to float32 halves memory usage, reducing memory bandwidth demands and allowing more data to fit into the CPU cache, which is especially beneficial for large arrays. These changes align the data with the CPU's expectations, enabling faster and more efficient processing, which in your case approximately halves the execution time. The primary improvement comes from using float32 instead of float64, significantly reducing memory usage and enhancing performance.",
    "context_chunks": [
      {
        "text": "I have a numpy array that maps x-y-coordinates to the appropriate z-coordinates. For this I use a 2D array that represents x and y as its axes and contains the corresponding z values: import numpy as np x_size = 2000 y_size = 2500 z_size = 400 rng = np.random.default_rng(123) z_coordinates = np.linspace(0, z_size, y_size) + rng.laplace(0, 1, (x_size, y_size)) So each of the 2000*2500 x-y-points is assigned a z-value (float between 0 and 400). Now I want to look up for each integer z and integer x which is the closest y-value, essentially creating a map that is of shape (x_size, z_size) and holds the best y-values. The simplest approach is creating an empty array of target shape and iterating over each z value: y_coordinates = np.empty((x_size, z_size), dtype=np.uint16) for i in range(z_size): y_coordinates[:, i] = np.argmin( np.abs(z_coordinates - i), axis=1, ) however this takes about 11 s on my machine, which unfortunately is way to slow. Surely using a more vectorised approach would be faster, such as: y_coordinates = np.argmin( np.abs( z_coordinates[..., np.newaxis] - np.arange(z_size) ), axis=1, ) Surprisingly this runs about 60% slower than the version above (tested at 1/10th size, since at full size this uses excessive memory). Also wrapping the code blocks in functions and decorating them with numba's @jit(nopython=True) doesn't help. How can I speed up the calculation?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "IMPROVEMENT 1 The following approach completes the task in approximately 0.4 seconds on my machine. The key idea is to avoid using np.argmin and instead leverage binary search (np.searchsorted) on a pre-sorted array of z-coordinates. This approach avoids the inefficiency of repeatedly using np.argmin by pre-sorting the z-coordinates for each x and then employing np.searchsorted to efficiently find the closest z-values. By sorting the z-coordinates with a complexity of O(N log N) and using binary search for each integer z, which has a complexity of O(log N), the method significantly reduces computational load compared to the original approach. This strategy involves finding the insertion point for each integer z in the sorted z-coordinates and comparing it with its neighbors to determine the closest z-value. This eliminates the need for redundant calculations and ensures that only necessary comparisons are made, thereby improving performance. x_size = 2000 y_size = 2500 z_size = 400 rng = np.random.default_rng(123) z_coordinates = np.linspace(0, z_size, y_size) + rng.laplace(0, 1, (x_size, y_size)) # Sort the z_coordinates for each x and get sorted indices idx_sorted = np.argsort(z_coordinates, axis=1) z_sorted = np.take_along_axis(z_coordinates, idx_sorted, axis=1) zs = np.arange(z_size, dtype=np.float64) y_coordinates = np.empty((x_size, z_size), dtype=np.uint16) for x in range(x_size): positions = np.searchsorted(z_sorted[x, :], zs) positions = np.clip(positions, 1, y_size - 1) z_prev = z_sorted[x, positions - 1] z_curr = z_sorted[x, positions] diffs_prev = np.abs(z_prev - zs) diffs_curr = np.abs(z_curr - zs) closest = np.where(diffs_prev &lt; diffs_curr, positions - 1, positions) y_coordinates[x, :] = idx_sorted[x, closest] IMPROVEMENT 2 One way of improving performance is to add the following line of code: z_coordinates = z_coordinates.astype(np.float32).copy(order='C') Ensuring a C-contiguous memory layout and using float32 instead of float64 significantly speeds up computation by optimizing memory access patterns and reducing data size. C-contiguous arrays store elements in a row-major order, improving cache efficiency and enabling vectorized operations by minimizing cache misses and leveraging CPU optimizations. Switching to float32 halves memory usage, reducing memory bandwidth demands and allowing more data to fit into the CPU cache, which is especially beneficial for large arrays. These changes align the data with the CPU's expectations, enabling faster and more efficient processing, which in your case approximately halves the execution time. The primary improvement comes from using float32 instead of float64, significantly reducing memory usage and enhancing performance.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "for some reason on my machine using a for loop is faster than vectorization: for j in range(x_size): y_coordinates[j, i] = np.argmin(np.abs(z_coordinates[j, :] - i)) Not sure what is the reason behind this. However, if this is how you actually generate the z_coordinates, I believe you can speed it up by eliminating unlikely choices of y as for each y, z is likely to be distributed within a small range: y_coordinates_new = np.empty((x_size, z_size), dtype=np.uint16) m_min = np.min(z_coordinates, axis = 0)[:, np.newaxis] - np.arange(z_size) m_max = np.max(z_coordinates, axis = 0)[:, np.newaxis] - np.arange(z_size) m_range_min = (m_min*m_max &gt; 0) * np.minimum(np.abs(m_min), np.abs(m_max)) m_range_max = np.maximum(np.abs(m_min), np.abs(m_max)) for i in range(z_size): candidates = np.where(m_range_min[:, i] &lt;= np.min(m_range_max[:, i]))[0] y_coordinates_new[:, i] = candidates[np.argmin( np.abs(z_coordinates[:, candidates] - i), axis=1, )] On my machine this is 16x faster for your example. Obviously it would be slower if you increase the variance of your random part. Explained: The idea is that your samples (z) are sparse by y, meaning that z values are distributed within 2500 relatively small &quot;buckets&quot;. So to find the point closest to i, we don't need to check all the buckets as the point we are looking for would likely only exist in a few candidate buckets. First we calculate what is the minimum distance and maximum distance to each i, within each bucket: # min and max (z-i) in each bucket m_min = np.min(z_coordinates, axis = 0)[:, np.newaxis] - np.arange(z_size) m_max = np.max(z_coordinates, axis = 0)[:, np.newaxis] - np.arange(z_size) # min and max possible distance to i in each bucket m_range_min = (m_min*m_max &gt; 0) * np.minimum(np.abs(m_min), np.abs(m_max)) m_range_max = np.maximum(np.abs(m_min), np.abs(m_max)) The minimum distance is a rough estimate for speed, if i is between the min z value and max z value in each bucket, I just assume minimum possible distance to i is 0. You could probably do better here by finding the closest value to 0. Now, for each of these buckets, we know what is the minimum/maximum possible distance to i. We can say that given two buckets a, b, and respectively min and max distance [d1_a, d2_a], [d1_b, d2_b], we can rule out b in favor of a if d1_b &gt; d2_a. This is the case where all points in bucket b would be further away from i compared to any points in a. With this idea in mind, our candidates, defined as buckets where it is not strictly &quot;worse&quot; than any of our buckets, can be find via: np.where(m_range_min[:, i] &lt;= np.min(m_range_max[:, i]))[0] Think of it this way, if d1 (the min distance) is larger than any d2 (the max distance) of any other bucket we have, there is no reason to keep this bucket. This method is somewhat a math hack, what is nice about it is that you can combine with what other people mentioned here, like numba, presort, etc. Presort is a good idea, as there is really no point to search multiple times through the same array. We could do it further by maintaining some search index to only increase it when doing the loop, but I am afraid that might be slower than numpy's vectorization without some careful implementation.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "arrays",
        "numpy",
        "coordinates",
        "coordinate-transformation"
      ],
      "question_score": 8,
      "answer_score": 4,
      "created": "2024-12-12T16:34:55",
      "question_id": 79275886,
      "answer_id": 79276417
    }
  },
  {
    "question": "How to toggle torch cpu and gpu installation using poetry",
    "expected_answer": "The GitHub issue #6409 underscores the challenge of managing CPU and GPU dependencies in Poetry due to its lack of support for mutually exclusive groups. The proposed workarounds include using environment markers, maintaining separate configuration files, or automating the process with scripts. These methods, while not perfect, provide viable solutions to toggle between CPU and GPU installations of PyTorch and torchvision. I will provide a summary of those: 1. Environment Markers: One way to manage CPU and GPU installations is to use environment markers. This method involves setting custom environment variables to indicate which version of the libraries to install. Here is an example pyproject.toml: [tool.poetry.dependencies] python = &quot;^3.8&quot; # Other dependencies torch = [ {version = &quot;^2.2.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin' and extra == 'gpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'linux' and extra == 'gpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'win32' and extra == 'gpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin' and extra == 'cpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'linux' and extra == 'cpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'win32' and extra == 'cpu'&quot;}, ] torchvision = [ {version = &quot;^0.17.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin' and extra == 'gpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'linux' and extra == 'gpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'win32' and extra == 'gpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin' and extra == 'cpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'linux' and extra == 'cpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'win32' and extra == 'cpu'&quot;}, ] [tool.poetry.extras] cpu = [&quot;torch&quot;, &quot;torchvision&quot;] gpu = [&quot;torch&quot;, &quot;torchvision&quot;] [[tool.poetry.source]] name = &quot;torchcpu&quot; url = &quot;https://download.pytorch.org/whl/cpu&quot; priority = &quot;explicit&quot; [[tool.poetry.source]] name = &quot;torch_cuda121&quot; url = &quot;https://download.pytorch.org/whl/cu121&quot; priority = &quot;explicit&quot; 2. Separate pyproject.toml Files: Another approach is to maintain separate pyproject.toml files for CPU and GPU configurations. This method requires manual switching but ensures that only the necessary dependencies are installed. CPU Configuration (pyproject_cpu.toml): [tool.poetry.dependencies] python = &quot;^3.8&quot; torch = {version = &quot;^2.2.1&quot;, source=&quot;torchcpu&quot;} torchvision = {version = &quot;^0.17.1&quot;, source=&quot;torchcpu&quot;} [[tool.poetry.source]] name = &quot;torchcpu&quot; url = &quot;https://download.pytorch.org/whl/cpu&quot; priority = &quot;explicit&quot; GPU Configuration (pyproject_gpu.toml): [tool.poetry.dependencies] python = &quot;^3.8&quot; torch = {version = &quot;^2.2.1&quot;, source=&quot;torch_cuda121&quot;} torchvision = {version = &quot;^0.17.1&quot;, source=&quot;torch_cuda121&quot;} [[tool.poetry.source]] name = &quot;torch_cuda121&quot; url = &quot;https://download.pytorch.org/whl/cu121&quot; priority = &quot;explicit&quot; You can switch between these configurations by renaming the appropriate file to pyproject.toml before running poetry install. 3. Custom Script: A more automated solution involves using a script to toggle between CPU and GPU configurations. Example Python Script: import os import sys def switch_dependencies(version): if version == 'cpu': os.rename('pyproject_cpu.toml', 'pyproject.toml') elif version == 'gpu': os.rename('pyproject_gpu.toml', 'pyproject.toml') else: print(&quot;Invalid version. Choose 'cpu' or 'gpu'.&quot;) sys.exit(1) os.system('poetry install') if __name__ == &quot;__main__&quot;: if len(sys.argv) != 2: print(&quot;Usage: python switch_dependencies.py [cpu|gpu]&quot;) sys.exit(1) version = sys.argv[1] switch_dependencies(version) Run the script with: python switch_dependencies.py cpu or python switch_dependencies.py gpu",
    "context_chunks": [
      {
        "text": "I am trying to enable the installation of cpu and gpu versions of torch and torchvision, using poetry install --with cpu and poetry install --with gpu, respectively. I have the following in my pyproject.toml: [tool.poetry.dependencies] python = &quot;^3.8&quot; filterpy = &quot;^1.4.5&quot; gdown = &quot;^5.1.0&quot; lapx = &quot;^0.5.5&quot; loguru = &quot;^0.7.2&quot; numpy = &quot;1.24.4&quot; pyyaml = &quot;^6.0.1&quot; regex = &quot;^2024.0.0&quot; yacs = &quot;^0.1.8&quot; scikit-learn = &quot;^1.3.0&quot; pandas = &quot;^2.0.0&quot; opencv-python = &quot;^4.7.0&quot; ftfy = &quot;^6.1.3&quot; gitpython = &quot;^3.1.42&quot; [tool.poetry.group.gpu] optional = true [tool.poetry.group.gpu.dependencies] torch = [ {version = &quot;^2.2.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'linux'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'win32'&quot;}, ] torchvision = [ {version = &quot;^0.17.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'linux'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'win32'&quot;}, ] [tool.poetry.group.cpu] optional = true [tool.poetry.group.cpu.dependencies] torch = [ {version = &quot;^2.2.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'linux'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'win32'&quot;}, ] torchvision = [ {version = &quot;^0.17.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'linux'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'win32'&quot;}, ] [[tool.poetry.source]] name = &quot;torchcpu&quot; url = &quot;https://download.pytorch.org/whl/cpu&quot; priority = &quot;explicit&quot; [[tool.poetry.source]] name = &quot;torch_cuda121&quot; url = &quot;https://download.pytorch.org/whl/cu121&quot; priority = &quot;explicit&quot; When running poetry lock I get: Incompatible constraints in requirements of boxmot (10.0.71): torch (&gt;=2.2.1,&lt;3.0.0) ; sys_platform == &quot;linux&quot; or sys_platform == &quot;win32&quot; ; source=torchcpu torch (&gt;=2.2.1,&lt;3.0.0) ; sys_platform == &quot;linux&quot; or sys_platform == &quot;win32&quot; ; source=torch_cuda121 So it seems that my way of approaching this problem does not seem to be the right one, but I have no idea how to proceed either. What am I missing? How can this be achieved with poetry?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The GitHub issue #6409 underscores the challenge of managing CPU and GPU dependencies in Poetry due to its lack of support for mutually exclusive groups. The proposed workarounds include using environment markers, maintaining separate configuration files, or automating the process with scripts. These methods, while not perfect, provide viable solutions to toggle between CPU and GPU installations of PyTorch and torchvision. I will provide a summary of those: 1. Environment Markers: One way to manage CPU and GPU installations is to use environment markers. This method involves setting custom environment variables to indicate which version of the libraries to install. Here is an example pyproject.toml: [tool.poetry.dependencies] python = &quot;^3.8&quot; # Other dependencies torch = [ {version = &quot;^2.2.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin' and extra == 'gpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'linux' and extra == 'gpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'win32' and extra == 'gpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin' and extra == 'cpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'linux' and extra == 'cpu'&quot;}, {version = &quot;^2.2.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'win32' and extra == 'cpu'&quot;}, ] torchvision = [ {version = &quot;^0.17.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin' and extra == 'gpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'linux' and extra == 'gpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torch_cuda121&quot;, markers = &quot;sys_platform == 'win32' and extra == 'gpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;pypi&quot;, markers = &quot;sys_platform == 'darwin' and extra == 'cpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'linux' and extra == 'cpu'&quot;}, {version = &quot;^0.17.1&quot;, source=&quot;torchcpu&quot;, markers = &quot;sys_platform == 'win32' and extra == 'cpu'&quot;}, ] [tool.poetry.extras] cpu = [&quot;torch&quot;, &quot;torchvision&quot;] gpu = [&quot;torch&quot;, &quot;torchvision&quot;] [[tool.poetry.source]] name = &quot;torchcpu&quot; url = &quot;https://download.pytorch.org/whl/cpu&quot; priority = &quot;explicit&quot; [[tool.poetry.source]] name = &quot;torch_cuda121&quot; url = &quot;https://download.pytorch.org/whl/cu121&quot; priority = &quot;explicit&quot; 2. Separate pyproject.toml Files: Another approach is to maintain separate pyproject.toml files for CPU and GPU configurations. This method requires manual switching but ensures that only the necessary dependencies are installed. CPU Configuration (pyproject_cpu.toml): [tool.poetry.dependencies] python = &quot;^3.8&quot; torch = {version = &quot;^2.2.1&quot;, source=&quot;torchcpu&quot;} torchvision = {version = &quot;^0.17.1&quot;, source=&quot;torchcpu&quot;} [[tool.poetry.source]] name = &quot;torchcpu&quot; url = &quot;https://download.pytorch.org/whl/cpu&quot; priority = &quot;explicit&quot; GPU Configuration (pyproject_gpu.toml): [tool.poetry.dependencies] python = &quot;^3.8&quot; torch = {version = &quot;^2.2.1&quot;, source=&quot;torch_cuda121&quot;} torchvision = {version = &quot;^0.17.1&quot;, source=&quot;torch_cuda121&quot;} [[tool.poetry.source]] name = &quot;torch_cuda121&quot; url = &quot;https://download.pytorch.org/whl/cu121&quot; priority = &quot;explicit&quot; You can switch between these configurations by renaming the appropriate file to pyproject.toml before running poetry install. 3. Custom Script: A more automated solution involves using a script to toggle between CPU and GPU configurations. Example Python Script: import os import sys def switch_dependencies(version): if version == 'cpu': os.rename('pyproject_cpu.toml', 'pyproject.toml') elif version == 'gpu': os.rename('pyproject_gpu.toml', 'pyproject.toml') else: print(&quot;Invalid version. Choose 'cpu' or 'gpu'.&quot;) sys.exit(1) os.system('poetry install') if __name__ == &quot;__main__&quot;: if len(sys.argv) != 2: print(&quot;Usage: python switch_dependencies.py [cpu|gpu]&quot;) sys.exit(1) version = sys.argv[1] switch_dependencies(version) Run the script with: python switch_dependencies.py cpu or python switch_dependencies.py gpu",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "poetry does not support mutually exclusive groups. An endless discussion on how to solve this is taking place here. Anyways, I ended up solving it by changing the pyproject.toml in the ci.yml as the goal was to enable cpu torch in the ci pipeline. While keeping the cuda-enabled version for the local installation: if [[ &quot;$OSTYPE&quot; == &quot;darwin&quot;* ]]; then # macOS sed -i '' 's/source=&quot;torch_cuda121&quot;/source=&quot;torchcpu&quot;/g' pyproject.toml elif [[ &quot;$OSTYPE&quot; == &quot;linux-gnu&quot;* ]]; then # Linux sed -i 's/source=&quot;torch_cuda121&quot;/source=&quot;torchcpu&quot;/g' pyproject.toml fi",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-poetry"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2024-05-12T10:01:44",
      "question_id": 78467412,
      "answer_id": 78511183
    }
  },
  {
    "question": "How can I make VS Code format my Python code?",
    "expected_answer": "Or simply install Black Formatter from VSC extension menu: https://marketplace.visualstudio.com/items?itemName=ms-python.black-formatter :)",
    "context_chunks": [
      {
        "text": "I have following Python code: check_files(original_file = original_file_name, used_file = used_file_name, unused_file = unused_file_name) I want to make it instead to look like: check_files(original_file = original_file_name, used_file = used_file_name, unused_file = unused_file_name) Also I want to correct formatting not only for function calls but also that way dictionary key/value pairs and etc. For Example, in RStudio, if I select the code and press CTRL + I RStudio will correct formating as I have described above. Is there any similar way to correct formating in VSCode?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Or simply install Black Formatter from VSC extension menu: https://marketplace.visualstudio.com/items?itemName=ms-python.black-formatter :)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Based on the comments by @starball, @jarmod and additional googling I found that you need to follow those steps: Step 1. Install Python extension from marketplace: https://marketplace.visualstudio.com/items?itemName=ms-python.python Step 2. Install one of the formatter packages for Python. The Python extension supports source code formatting using either autopep8 (the default), black, or yapf. from and More about it here: https://code.visualstudio.com/docs/python/editing#_formatting Step 3. Select which code formatter you want to use in python.formatting.provider which is in settings&gt;Extensions&gt;Python (this maybe automatically set after step 1 and step 2). Also, in settings&gt;Extensions&gt;Python there are more options to select. How to use formatting: The code formatting is available in Visual Studio Code (VSCode) through the following shortcuts or key combinations: On Windows Shift + Alt + F On macOS Shift + Option + F On Linux Ctrl + Shift + I Format Selection (Ctrl+K Ctrl+F) - Format the selected text. Or you can use right click menu: from: https://mkyong.com/vscode/how-to-format-source-code-in-visual-studio-code-vscode/ and from: https://code.visualstudio.com/docs/editor/codebasics#_formatting",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2023-06-27T23:14:54",
      "question_id": 76569147,
      "answer_id": 78628780
    }
  },
  {
    "question": "How to run computations on other rows efficiently",
    "expected_answer": "You could try non-equi joins: .join_where() df_sum = ( df.unique(&quot;value&quot;) # summing requires unique LHS .join_where(df, pl.col.value &gt; pl.col.value_right) .group_by(&quot;value&quot;) .sum() ) df_max = ( df.join_where(df, pl.col.value != pl.col.value_right) .group_by(&quot;value&quot;) .max() ) (df.select(&quot;value&quot;) .join(df_sum, on=&quot;value&quot;, how=&quot;left&quot;) .join(df_max, on=&quot;value&quot;, how=&quot;left&quot;) ) shape: (5, 3) ┌───────┬─────────────┬───────────────────┐ │ value ┆ value_right ┆ value_right_right │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ i64 │ ╞═══════╪═════════════╪═══════════════════╡ │ 3 ┆ 1 ┆ 9 │ │ 7 ┆ 8 ┆ 9 │ │ 1 ┆ null ┆ 9 │ │ 9 ┆ 15 ┆ 7 │ │ 4 ┆ 4 ┆ 9 │ └───────┴─────────────┴───────────────────┘ Although, for these specific tasks - it doesn't really scale to larger data. Alternatively, if you can sort your data, the problem essentially becomes .cum_sum() + .max() operations (with a couple of &quot;shifts&quot;) - which are extremely fast operations. (df .sort(&quot;value&quot;) .with_columns( pl.when(pl.col.value.is_last_distinct()) .then(pl.col.value.cum_sum()) .shift() .forward_fill() .fill_null(0) .alias(&quot;sum_lower&quot;) ) .with_columns(max_other = pl.col.value.max()) .with_columns( pl.when(pl.col.value == pl.col.max_other) .then(pl.col.value.rle().get(-2).struct.field(&quot;value&quot;)) .otherwise(pl.col.max_other) .alias(&quot;max_other&quot;) ) ) shape: (5, 3) ┌───────┬───────────┬───────────┐ │ value ┆ sum_lower ┆ max_other │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ i64 │ ╞═══════╪═══════════╪═══════════╡ │ 1 ┆ 0 ┆ 9 │ │ 3 ┆ 1 ┆ 9 │ │ 4 ┆ 4 ┆ 9 │ │ 7 ┆ 8 ┆ 9 │ │ 9 ┆ 15 ┆ 7 │ └───────┴───────────┴───────────┘ What is happening? If we add some duplicate values to your example. df = pl.DataFrame({&quot;value&quot;: [3, 7, 1, 9, 4, 4, 3, 3]}) (df.sort(&quot;value&quot;) .with_columns(is_last = pl.col.value.is_last_distinct()) .with_columns(pl.when(pl.col.value.is_last_distinct()).then(pl.col.value.cum_sum()).alias(&quot;sum0&quot;)) .with_columns(pl.col(&quot;sum0&quot;).shift().alias(&quot;sum1&quot;)) .with_columns(pl.col(&quot;sum1&quot;).forward_fill().alias(&quot;sum2&quot;)) ) shape: (8, 5) ┌───────┬─────────┬──────┬──────┬──────┐ │ value ┆ is_last ┆ sum0 ┆ sum1 ┆ sum2 │ │ --- ┆ --- ┆ --- ┆ --- ┆ --- │ │ i64 ┆ bool ┆ i64 ┆ i64 ┆ i64 │ ╞═══════╪═════════╪══════╪══════╪══════╡ │ 1 ┆ true ┆ 1 ┆ null ┆ null │ │ 3 ┆ false ┆ null ┆ 1 ┆ 1 │ # from value=1 │ 3 ┆ false ┆ null ┆ null ┆ 1 │ │ 3 ┆ true ┆ 10 ┆ null ┆ 1 │ │ 4 ┆ false ┆ null ┆ 10 ┆ 10 │ # from value=3 │ 4 ┆ true ┆ 18 ┆ null ┆ 10 │ │ 7 ┆ true ┆ 25 ┆ 18 ┆ 18 │ # from value=4 │ 9 ┆ true ┆ 34 ┆ 25 ┆ 25 │ # from value=7 └───────┴─────────┴──────┴──────┴──────┘ With the data sorted, .is_last_distinct() gives you the last value in each run/streak and we shift it 1 step forward so that the first value in each run/streak now has the &quot;sum_lower&quot; (i.e. the cum_sum from the &quot;previous run&quot;). .forward_fill() is used so that each value in the run is assigned the sum_lower (in the possible case of duplicates). The &quot;max_other&quot; logic seems to be that you then want the .max() value in all cases except for the last, i.e. when value == max. With sorted data, .rle() is a fast way to give you a unique set of values and retain their sorted order. df.sort(&quot;value&quot;).select(pl.col.value.rle()) shape: (5, 1) ┌───────────┐ │ value │ │ --- │ │ struct[2] │ ╞═══════════╡ │ {1,1} │ │ {3,3} │ │ {2,4} │ │ {1,7} │ # &lt;- │ {1,9} │ └───────────┘",
    "context_chunks": [
      {
        "text": "I am working with a Polars DataFrame and need to perform computations on each row using values from other rows. Currently, I am using the map_elements method, but it is not efficient. In the following example, I add two new columns to a DataFrame: sum_lower: The sum of all elements that are smaller than the current element. max_other: The maximum value from the DataFrame, excluding the current element. Here is my current implementation: import polars as pl COL_VALUE = &quot;value&quot; def fun_sum_lower(current_row, df): tmp_df = df.filter(pl.col(COL_VALUE) &lt; current_row[COL_VALUE]) sum_lower = tmp_df.select(pl.sum(COL_VALUE)).item() return sum_lower def fun_max_other(current_row, df): tmp_df = df.filter(pl.col(COL_VALUE) != current_row[COL_VALUE]) max_other = tmp_df.select(pl.col(COL_VALUE)).max().item() return max_other if __name__ == '__main__': df = pl.DataFrame({COL_VALUE: [3, 7, 1, 9, 4]}) df = df.with_columns( pl.struct([COL_VALUE]) .map_elements(lambda row: fun_sum_lower(row, df), return_dtype=pl.Int64) .alias(&quot;sum_lower&quot;) ) df = df.with_columns( pl.struct([COL_VALUE]) .map_elements(lambda row: fun_max_other(row, df), return_dtype=pl.Int64) .alias(&quot;max_other&quot;) ) print(df) The output of the above code is: shape: (5, 3) ┌───────┬───────────┬───────────┐ │ value ┆ sum_lower ┆ max_other │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ i64 │ ╞═══════╪═══════════╪═══════════╡ │ 3 ┆ 1 ┆ 9 │ │ 7 ┆ 8 ┆ 9 │ │ 1 ┆ 0 ┆ 9 │ │ 9 ┆ 15 ┆ 7 │ │ 4 ┆ 4 ┆ 9 │ └───────┴───────────┴───────────┘ While this code works, it is not efficient due to the use of lambdas and row-wise operations. Is there a more efficient way to achieve this in Polars, without using lambdas, iterating over rows, or running Python code? I also tried using Polars methods: cum_sum, group_by_dynamic, and rolling, but I don't think those can be used for this task.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You could try non-equi joins: .join_where() df_sum = ( df.unique(&quot;value&quot;) # summing requires unique LHS .join_where(df, pl.col.value &gt; pl.col.value_right) .group_by(&quot;value&quot;) .sum() ) df_max = ( df.join_where(df, pl.col.value != pl.col.value_right) .group_by(&quot;value&quot;) .max() ) (df.select(&quot;value&quot;) .join(df_sum, on=&quot;value&quot;, how=&quot;left&quot;) .join(df_max, on=&quot;value&quot;, how=&quot;left&quot;) ) shape: (5, 3) ┌───────┬─────────────┬───────────────────┐ │ value ┆ value_right ┆ value_right_right │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ i64 │ ╞═══════╪═════════════╪═══════════════════╡ │ 3 ┆ 1 ┆ 9 │ │ 7 ┆ 8 ┆ 9 │ │ 1 ┆ null ┆ 9 │ │ 9 ┆ 15 ┆ 7 │ │ 4 ┆ 4 ┆ 9 │ └───────┴─────────────┴───────────────────┘ Although, for these specific tasks - it doesn't really scale to larger data. Alternatively, if you can sort your data, the problem essentially becomes .cum_sum() + .max() operations (with a couple of &quot;shifts&quot;) - which are extremely fast operations. (df .sort(&quot;value&quot;) .with_columns( pl.when(pl.col.value.is_last_distinct()) .then(pl.col.value.cum_sum()) .shift() .forward_fill() .fill_null(0) .alias(&quot;sum_lower&quot;) ) .with_columns(max_other = pl.col.value.max()) .with_columns( pl.when(pl.col.value == pl.col.max_other) .then(pl.col.value.rle().get(-2).struct.field(&quot;value&quot;)) .otherwise(pl.col.max_other) .alias(&quot;max_other&quot;) ) ) shape: (5, 3) ┌───────┬───────────┬───────────┐ │ value ┆ sum_lower ┆ max_other │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ i64 │ ╞═══════╪═══════════╪═══════════╡ │ 1 ┆ 0 ┆ 9 │ │ 3 ┆ 1 ┆ 9 │ │ 4 ┆ 4 ┆ 9 │ │ 7 ┆ 8 ┆ 9 │ │ 9 ┆ 15 ┆ 7 │ └───────┴───────────┴───────────┘ What is happening? If we add some duplicate values to your example. df = pl.DataFrame({&quot;value&quot;: [3, 7, 1, 9, 4, 4, 3, 3]}) (df.sort(&quot;value&quot;) .with_columns(is_last = pl.col.value.is_last_distinct()) .with_columns(pl.when(pl.col.value.is_last_distinct()).then(pl.col.value.cum_sum()).alias(&quot;sum0&quot;)) .with_columns(pl.col(&quot;sum0&quot;).shift().alias(&quot;sum1&quot;)) .with_columns(pl.col(&quot;sum1&quot;).forward_fill().alias(&quot;sum2&quot;)) ) shape: (8, 5) ┌───────┬─────────┬──────┬──────┬──────┐ │ value ┆ is_last ┆ sum0 ┆ sum1 ┆ sum2 │ │ --- ┆ --- ┆ --- ┆ --- ┆ --- │ │ i64 ┆ bool ┆ i64 ┆ i64 ┆ i64 │ ╞═══════╪═════════╪══════╪══════╪══════╡ │ 1 ┆ true ┆ 1 ┆ null ┆ null │ │ 3 ┆ false ┆ null ┆ 1 ┆ 1 │ # from value=1 │ 3 ┆ false ┆ null ┆ null ┆ 1 │ │ 3 ┆ true ┆ 10 ┆ null ┆ 1 │ │ 4 ┆ false ┆ null ┆ 10 ┆ 10 │ # from value=3 │ 4 ┆ true ┆ 18 ┆ null ┆ 10 │ │ 7 ┆ true ┆ 25 ┆ 18 ┆ 18 │ # from value=4 │ 9 ┆ true ┆ 34 ┆ 25 ┆ 25 │ # from value=7 └───────┴─────────┴──────┴──────┴──────┘ With the data sorted, .is_last_distinct() gives you the last value in each run/streak and we shift it 1 step forward so that the first value in each run/streak now has the &quot;sum_lower&quot; (i.e. the cum_sum from the &quot;previous run&quot;). .forward_fill() is used so that each value in the run is assigned the sum_lower (in the possible case of duplicates). The &quot;max_other&quot; logic seems to be that you then want the .max() value in all cases except for the last, i.e. when value == max. With sorted data, .rle() is a fast way to give you a unique set of values and retain their sorted order. df.sort(&quot;value&quot;).select(pl.col.value.rle()) shape: (5, 1) ┌───────────┐ │ value │ │ --- │ │ struct[2] │ ╞═══════════╡ │ {1,1} │ │ {3,3} │ │ {2,4} │ │ {1,7} │ # &lt;- │ {1,9} │ └───────────┘",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For your specific use case you don't really need join, you can calculate values with window functions. pl.Expr.shift() to exclude current row. pl.Expr.cum_sum() to calculate sum of all elements up to the current row. pl.Expr.max() to calculate max. pl.Expr.bottom_k() to calculate 2 largest elements so then we can take pl.Expr.min() as second largest. ( df .sort(&quot;value&quot;) .with_columns( sum_lower = pl.col.value.shift(1).cum_sum().fill_null(0), max_other = pl.when(pl.col.value.max() != pl.col.value) .then(pl.col.value.max()) .otherwise(pl.col.value.bottom_k(2).min()) ) ) shape: (5, 3) ┌───────┬───────────┬───────────┐ │ value ┆ sum_lower ┆ max_other │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ i64 │ ╞═══════╪═══════════╪═══════════╡ │ 1 ┆ 0 ┆ 9 │ │ 3 ┆ 1 ┆ 9 │ │ 4 ┆ 4 ┆ 9 │ │ 7 ┆ 8 ┆ 9 │ │ 9 ┆ 15 ┆ 7 │ └───────┴───────────┴───────────┘ You can also use pl.DataFrame.with_row_index() to keep current order so you can revert to it at the end with pl.DataFrame.sort(). ( df.with_row_index() .sort(&quot;value&quot;) .with_columns( sum_lower = pl.col.value.shift(1).cum_sum().fill_null(0), max_other = pl.when(pl.col.value.max() != pl.col.value) .then(pl.col.value.max()) .otherwise(pl.col.value.bottom_k(2).min()) ) .sort(&quot;index&quot;) .drop(&quot;index&quot;) ) Another possible solution would be to use DuckDB integration with Polars. Using window functions, getting advantage of excellent DuckDB windows framing options. max(arg, n) to calculate top 2 largest elements. import duckdb duckdb.sql(&quot;&quot;&quot; select d.value, coalesce(sum(d.value) over( order by d.value rows unbounded preceding exclude current row ), 0) as sum_lower, max(d.value) over( rows between unbounded preceding and unbounded following exclude current row ) as max_other from df as d &quot;&quot;&quot;).pl() shape: (5, 3) ┌───────┬───────────────┬───────────┐ │ value ┆ sum_lower ┆ max_other │ │ --- ┆ --- ┆ --- │ │ i64 ┆ decimal[38,0] ┆ i64 │ ╞═══════╪═══════════════╪═══════════╡ │ 1 ┆ 0 ┆ 9 │ │ 3 ┆ 1 ┆ 9 │ │ 4 ┆ 4 ┆ 9 │ │ 7 ┆ 8 ┆ 9 │ │ 9 ┆ 15 ┆ 7 │ └───────┴───────────────┴───────────┘ Or using lateral join: import duckdb duckdb.sql(&quot;&quot;&quot; select d.value, coalesce(s.value, 0) as sum_lower, m.value as max_other from df as d, lateral (select sum(t.value) as value from df as t where t.value &lt; d.value) as s, lateral (select max(t.value) as value from df as t where t.value != d.value) as m &quot;&quot;&quot;).pl() shape: (5, 3) ┌───────┬───────────┬───────────┐ │ value ┆ sum_lower ┆ max_other │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ i64 │ ╞═══════╪═══════════╪═══════════╡ │ 3 ┆ 1 ┆ 9 │ │ 7 ┆ 8 ┆ 9 │ │ 1 ┆ 0 ┆ 9 │ │ 9 ┆ 15 ┆ 7 │ │ 4 ┆ 4 ┆ 9 │ └───────┴───────────┴───────────┘ duplicate values pure polars solution above works well if there're no duplicate values, but if there are, you can also work around it. Here're 2 examples depending on whether you want to keep original order or not: # not keeping original order ( df .select(pl.col.value.value_counts()).unnest(&quot;value&quot;) .sort(&quot;value&quot;) .with_columns( sum_lower = pl.col.value.shift(1).cum_sum().fill_null(0), max_other = pl.when(pl.col.value.max() != pl.col.value) .then(pl.col.value.max()) .otherwise(pl.col.value.bottom_k(2).min()), value = pl.col.value.repeat_by(&quot;count&quot;) ).drop(&quot;count&quot;).explode(&quot;value&quot;) ) # keeping original order ( df.with_row_index() .group_by(&quot;value&quot;).agg(&quot;index&quot;) .sort(&quot;value&quot;) .with_columns( sum_lower = pl.col.value.shift(1).cum_sum().fill_null(0), max_other = pl.when(pl.col.value.max() != pl.col.value) .then(pl.col.value.max()) .otherwise(pl.col.value.bottom_k(2).min()) ) .explode(&quot;index&quot;) .sort(&quot;index&quot;) .drop(&quot;index&quot;) )",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "dataframe",
        "python-polars"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2024-10-24T11:29:33",
      "question_id": 79121678,
      "answer_id": 79121706
    }
  },
  {
    "question": "Diffusers SDXL &quot;TypeError: argument of type &#39;NoneType&#39; is not iterable&quot;",
    "expected_answer": "for version diffusers==0.20.0 using StableDiffusionXLPipeline instead StableDiffusion fix the problem import torch from diffusers import StableDiffusionXLPipeline pipe = StableDiffusionXLPipeline.from_pretrained(&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.float16, added_cond_kwargs={}) pipe = pipe.to(&quot;cuda&quot;) prompt = &quot;an illustration truck, wide shot, in frame, centered, hd, 8k, white solid background&quot; image = pipe(prompt).images[0] image.save(&quot;geeks.jpg&quot;)",
    "context_chunks": [
      {
        "text": "Just got access to SDXL model, looking to test it for its upcoming release... Unfortunately, the code we currently use for our service seems to not work with stabilityai/stable-diffusion-xl-base-0.9, and I'm not entirely sure what is different with SDXL and what I need to change. We are using a different pipeline so we can generate previews of the images, so its not the typical template that is provided on the SDXL model readme. The error seems to be happening in unet_2d_condition.py (in diffusers lib) Traceback (most recent call last): File &quot;C:\\Users\\myalt\\Desktop\\testing image grid\\main.py&quot;, line 159, in &lt;module&gt; socker_listener.generate_image() File &quot;C:\\Users\\myalt\\Desktop\\testing image grid\\main.py&quot;, line 154, in generate_image foo = self.blocking_code() File &quot;C:\\Users\\myalt\\Desktop\\testing image grid\\main.py&quot;, line 109, in blocking_code noise_pred = self.unet(latent_model_input, t, File &quot;C:\\Users\\myalt\\Desktop\\testing image grid\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 1501, in _call_impl return forward_call(*args, **kwargs) File &quot;C:\\Users\\myalt\\Desktop\\testing image grid\\venv\\lib\\site-packages\\diffusers\\models\\unet_2d_condition.py&quot;, line 839, in forward if &quot;text_embeds&quot; not in added_cond_kwargs: TypeError: argument of type 'NoneType' is not iterable I've updated to diffusers==0.18.2. Here is a example code which makes a bunch of images and puts them into a grid, using this a custom pipeline https://hatebin.com/tqppqfsehk",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "for version diffusers==0.20.0 using StableDiffusionXLPipeline instead StableDiffusion fix the problem import torch from diffusers import StableDiffusionXLPipeline pipe = StableDiffusionXLPipeline.from_pretrained(&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.float16, added_cond_kwargs={}) pipe = pipe.to(&quot;cuda&quot;) prompt = &quot;an illustration truck, wide shot, in frame, centered, hd, 8k, white solid background&quot; image = pipe(prompt).images[0] image.save(&quot;geeks.jpg&quot;)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Interesting, this looks like a bug in diffusers. added_cond_kwargs is specified as being optional, and the function signature explicitly defaults it to None. However, the subsequent code does not check for the None case, treating it as if it is a dictionary. It's worth filing an issue with diffusers if it hasn't already been reported. They can probably fix it by simply setting added_cond_kwargs to {} if it's passed as None. In the meantime, you should be able to work around this by explicitly passing added_cond_kwargs={} to the unet function.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "torch",
        "stable-diffusion"
      ],
      "question_score": 8,
      "answer_score": 5,
      "created": "2023-07-17T18:23:17",
      "question_id": 76707159,
      "answer_id": 77519070
    }
  },
  {
    "question": "Finding the last unique string in a list",
    "expected_answer": "I believe CodeSignal AI is correct in identifying an issue with your solution. Consider this test case: [&quot;apple&quot;, &quot;banana&quot;, &quot;banana&quot;]. Your code currently returns &quot;&quot;, but the answer should be &quot;apple&quot;.",
    "context_chunks": [
      {
        "text": "I was doing some coding on CodeSignal to refresh my knowledge. I was asked this question: Suppose you've got a list of words, let's say ['apple', 'banana', 'apple', 'mango', 'banana']. Each word could be repeated an arbitrary number of times. Think of this list as a conveyor belt in a space-age fruit factory. Now, your task is to identify the last unique fruit on the belt, i.e., the one that didn't repeat. If all the fruits are repeating, then there ain't any unique fruit, and your function should return an empty string (''). Your function should take a list of strings (the conveyor belt of fruits) as input. Now, a string can be any word, not just a fruit name, and the list can have any number of strings. There could also be an edge case where the list has no strings at all (Empty conveyor belt, eh?). For output, your function should return the last unique string in the list or an empty string if there are not any of them. To solve this task, you are expected to use sets. Sets are efficient for tracking seen and duplicate elements due to their fast membership testing capability. and my solution is: def find_unique_string(words): last_uniq = '' seen = set() for w in words: if last_uniq == w: last_uniq = '' if w not in seen: last_uniq = w seen.add(w) return last_uniq and the test cases are: print(find_unique_string(['apple', 'banana', 'apple', 'mango', 'banana'])) # It should print: 'mango' print(find_unique_string(['hello', 'world', 'hello'])) # It should print: 'world' print(find_unique_string(['hello', 'world', 'hello', 'world'])) # It should print: '' print(find_unique_string([])) # It should print: '' print(find_unique_string(['apple', 'banana', 'apple', 'kiwi', 'banana', 'kiwi'])) # it should print '' The CodeSignal AI doesn't accept my code as a solution and reports that it is not correct, but it can not provide any cases in which the result of my code is wrong. Now my question: Who is right? Me or CodeSignal AI? If I am right, how can I prove to CodeSignal AI that I am right so I can pass the test and go to the next one? If AI is right, can you give me a sample test case that breaks my code and doesn't generate the expected output? Note: I do not want you to write a solution which would have two sets, one for seen and one for duplicate as this is the result that AI is expecting (I think), but I want to know if my code is correct and if it is, how to prove that it is, or you have a test case to prove that my code is wrong.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I believe CodeSignal AI is correct in identifying an issue with your solution. Consider this test case: [&quot;apple&quot;, &quot;banana&quot;, &quot;banana&quot;]. Your code currently returns &quot;&quot;, but the answer should be &quot;apple&quot;.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Your code fails to find the correct last unique string. The reason is that you overwrite last_uniq each time when it matches the last unique string, consider the test case ['hello', 'world', 'world'], when your loop iterating on the first world, it will consider that word be your last unique string. Then when it matches the second world, it will miss the first unique string hello. More easy way to solve this problem is to use a map to record the element and its occurrence times. With python, you can use Counter to do that.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm"
      ],
      "question_score": 8,
      "answer_score": 5,
      "created": "2024-12-14T02:39:02",
      "question_id": 79279969,
      "answer_id": 79279980
    }
  },
  {
    "question": "How to check if a python package is installed using poetry",
    "expected_answer": "Assuming you have activated the virtual environment in the .venv folder (using, for example source .venv/bin/activate), you can use pip list to list all of the installed packages in that virtual environment. There is a poetry show command to list all available packages as well that you might want to look into: https://python-poetry.org/docs/cli/#show Update: You can parse the output of the poetry install --dry-run command to see what packages are installed, without actually installing any packages. Look for the lines with both &quot;Installing&quot; and &quot;Skipped&quot; and &quot;reason: Already installed&quot;, to see what packages are already installed.",
    "context_chunks": [
      {
        "text": "I'm using Poetry to manage my Python project, and I would like to check if a package is installed. This means the package is actually fetched from the remote repository and is located in the .venv/ folder. I went through the official documentation but didn't find any command that can do that. Any ideas? Thanks. Update: I ended up with a solution by running the following command and parsing its output. Thanks all for the help here! poetry install --dry-run --sync --no-ansi",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Assuming you have activated the virtual environment in the .venv folder (using, for example source .venv/bin/activate), you can use pip list to list all of the installed packages in that virtual environment. There is a poetry show command to list all available packages as well that you might want to look into: https://python-poetry.org/docs/cli/#show Update: You can parse the output of the poetry install --dry-run command to see what packages are installed, without actually installing any packages. Look for the lines with both &quot;Installing&quot; and &quot;Skipped&quot; and &quot;reason: Already installed&quot;, to see what packages are already installed.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I guess you want to check whether a particular package is installed in your poetry environment. You can use the show command: poetry show &lt;packagename&gt; which will show package information. niraj@HOST-ANALYTICS:~/Work/Code/apitutorialsnew$ poetry show pandas name : pandas version : 1.5.3 description : Powerful data structures for data analysis, time series, and statistics dependencies - numpy &gt;=1.20.3 - numpy &gt;=1.21.0 - python-dateutil &gt;=2.8.1 - pytz &gt;=2020.1 required by - seaborn &gt;=0.25 - statsmodels &gt;=0.25",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-poetry"
      ],
      "question_score": 8,
      "answer_score": 8,
      "created": "2023-09-13T06:08:53",
      "question_id": 77094430,
      "answer_id": 77094540
    }
  },
  {
    "question": "Speed-up for finding an optimal partition line",
    "expected_answer": "TL;DR: This answer provides a much faster solution than the one of @AndrejKesely. Is also makes use of Numba, but the final code is more optimized despite being also more complex. First simple implementation The initial code is not efficient because it calls Numpy function in a loop. In such a case, Numpy functions are very expensive. Numba and Cython are the way to go to make the code significantly faster. Still, operations on Nx2 arrays are not efficient, neither in Numpy nor in Numba. Moreover, generating temporary arrays (like Q) tends not to be optimal. The solution is typically to compute the array D on the fly without generating a temporary array Q. Here is a naive relatively-fast implementation we can write based on this: import numpy as np import numba as nb # Naive implementation based on the initial code: see below def generate_D(n): import fractions def slope(d): di, dj = d if dj == 0: return float('inf') if di &lt;= 0 else float('-inf'), -di return fractions.Fraction(di, dj), fractions.Fraction(-1, dj) D = [(di, dj) for di in range(-(n - 1), n) for dj in range(-(n - 1), n)] D.sort(key=slope) return np.array(D, dtype=np.int64) # Naive Numba implementation: see below @nb.njit('(int64[:,::1], int64[:,::1], int64)') def best_line_main_loop_seq(grid, D, s_max_so_far): n, m = grid.shape s_max = s_max_so_far left_sum = 0 for j in range(m): left_sum += grid[:,j].sum() for i in range(n): s = left_sum for k in range(D.shape[0]): qi = D[k, 0] + i qj = D[k, 1] + j if 0 &lt;= qi and qi &lt; n and 0 &lt;= qj and qj &lt; m: if qi == 0 and qj == 0: break val = grid[qi,qj] s += -val if qj &lt;= j else val s_max = max(s_max, s) return s_max # Main computing function def best_line(grid): n, m = grid.shape D = generate_D(n) s_max = grid.sum() s_max = max(s_max, best_line_main_loop_par_unroll_transposed(grid.T.copy(), D, s_max)) s_max = max(s_max, best_line_main_loop_par_unroll_transposed(grid, D, s_max)) return s_max # Benchmark N = 30 np.random.seed(42) big_square = np.random.randint(-3, 4, size=(N, N)) grid = np.array(big_square).astype(np.int64) assert best_line(grid) == 57 %time best_line(grid) The performance of this sequential code is not far from the one of the parallel implementation of @AndrejKesely (a bit slower on my machine, especially with a large N). Faster generation of the array D The above code is limited by the slow sort in generate_D(n) due to the rather inefficient fractions module, especially for small N values. The code can be made faster by comparing fractions in Numba directly. However, Numba unfortunately do not support keywords for the sort function so this feature needs to be reimplemented. This is a bit cumbersome to do manually, but the speed up worth the effort. Here is the resulting implementation: @nb.njit('(UniTuple(int64,2), UniTuple(int64,2))') def compare_fraction(a, b): a_top, a_bot = a b_top, b_bot = b if a_bot &lt; 0: a_top = -a_top a_bot = -a_bot if b_bot &lt; 0: b_top = -b_top b_bot = -b_bot fixed_a = a_top * b_bot fixed_b = b_top * a_bot if fixed_a &lt; fixed_b: return -1 if fixed_a == fixed_b: return 0 return 1 @nb.njit('(int64[::1], int64[::1])') def compare_slope(a, b): ai, aj = a bi, bj = b if aj == 0: # slope_a is special if ai &lt;= 0: # slope_a = (INF,-ai) if bj == 0 and bi &lt;= 0: if -ai &lt; -bi: return -1 elif -ai == -bi: return 0 else: return 1 else: return 1 else: # slope_a = (-INF,-ai) if bj == 0 and bi &gt; 0: # slope_b = (-INF,-bi) if -ai &lt; -bi: return -1 elif -ai == -bi: return 0 else: return 1 else: return -1 else: if bj == 0: # slope_b is special if bi &lt;= 0: # slope_b = (INF,-bi) return -1 else: # slope_b = (-INF,-bi) return 1 slope_a = ((ai,aj), (-1,aj)) slope_b = ((bi,bj), (-1,bj)) res = compare_fraction(slope_a[0], slope_b[0]) if res == 0: return compare_fraction(slope_a[1], slope_b[1]) return res # Quite naive quick-sort, but simple one @nb.njit('(int64[:,::1],)') def sort_D(arr): if len(arr) &lt;= 1: return else: pivot = arr[0].copy() left = 1 right = len(arr) - 1 while True: while left &lt;= right and compare_slope(arr[left], pivot) &lt;= 0: left = left + 1 while compare_slope(arr[right], pivot) &gt;= 0 and right &gt;= left: right = right - 1 if right &lt; left: break else: arr[left], arr[right] = arr[right].copy(), arr[left].copy() arr[0], arr[right] = arr[right].copy(), arr[0].copy() sort_D(arr[:right]) sort_D(arr[right+1:]) @nb.njit('(int64,)') def generate_D(n): D_list = [(di, dj) for di in range(-(n - 1), n) for dj in range(-(n - 1), n)] D_arr = np.array(D_list, dtype=np.int64) sort_D(D_arr) return D_arr The above generate_D function is more than 10 times faster on my machine. Faster main loop The naive main loop code provided above can be improved. First of all, it can be parallelized though the code does not scale well (certainly due to the work imbalance caused by the break in the loop). This can be done efficiently with prange on the outer loop (using prange on the inner loop is not optimal because the amount of work is small and creating/joining threads is expensive). Moreover, the loop can be unrolled so to reduce the number of conditional checks, especially on j. While the performance improvement can be significant (e.g. 2 times faster), this resulting code is also clearly less readable/maintainable (actually pretty ugly). This this a trade-off to pay since the operation is too complex for the Numba JIT to do it (and more generally most compilers). Finally, the array access can be made more cache-friendly by virtually transposing the array so to improve the locality of memory accesses (i.e. accessing many items of a same row in the target grid rather than accessing different rows). This optimization is especially useful for large N values (e.g. &gt;200). Here is the resulting optimized main code: @nb.njit('(int64[:,::1], int64[:,::1], int64)', parallel=True, cache=True) def best_line_main_loop_par_unroll_transposed(grid, D, s_max_so_far): m, n = grid.shape s_max = s_max_so_far all_left_sum = np.empty(m, dtype=np.int64) left_sum = 0 for j in range(m): left_sum += grid[j,:].sum() all_left_sum[j] = left_sum for j in nb.prange(m): left_sum = all_left_sum[j] for i in range(0, n//4*4, 4): i1 = i i2 = i + 1 i3 = i + 2 i4 = i + 3 s1 = left_sum s2 = left_sum s3 = left_sum s4 = left_sum continue_loop_i1 = True continue_loop_i2 = True continue_loop_i3 = True continue_loop_i4 = True for k in range(D.shape[0]): qj = D[k, 1] + j if 0 &lt;= qj and qj &lt; m: qi1 = D[k, 0] + i1 qi2 = D[k, 0] + i2 qi3 = D[k, 0] + i3 qi4 = D[k, 0] + i4 mult = np.int64(-1 if qj &lt;= j else 1) if qj != 0: if continue_loop_i1 and 0 &lt;= qi1 and qi1 &lt; n: s1 += mult * grid[qj,qi1] if continue_loop_i2 and 0 &lt;= qi2 and qi2 &lt; n: s2 += mult * grid[qj,qi2] if continue_loop_i3 and 0 &lt;= qi3 and qi3 &lt; n: s3 += mult * grid[qj,qi3] if continue_loop_i4 and 0 &lt;= qi4 and qi4 &lt; n: s4 += mult * grid[qj,qi4] s_max = max(s_max, max(max(s1, s2), max(s3, s4))) else: if continue_loop_i1 and 0 &lt;= qi1 and qi1 &lt; n: if qi1 == 0 and qj == 0: continue_loop_i1 = False else: s1 += mult * grid[qj,qi1] s_max = max(s_max, s1) if continue_loop_i2 and 0 &lt;= qi2 and qi2 &lt; n: if qi2 == 0 and qj == 0: continue_loop_i2 = False else: s2 += mult * grid[qj,qi2] s_max = max(s_max, s2) if continue_loop_i3 and 0 &lt;= qi3 and qi3 &lt; n: if qi3 == 0 and qj == 0: continue_loop_i3 = False else: s3 += mult * grid[qj,qi3] s_max = max(s_max, s3) if continue_loop_i4 and 0 &lt;= qi4 and qi4 &lt; n: if qi4 == 0 and qj == 0: continue_loop_i4 = False else: s4 += mult * grid[qj,qi4] s_max = max(s_max, s4) if not continue_loop_i1 and not continue_loop_i2 and not continue_loop_i3 and not continue_loop_i4: break for i in range(n//4*4, n): s = left_sum for k in range(D.shape[0]): qi = D[k, 0] + i qj = D[k, 1] + j mult = np.int64(-1 if qj &lt;= j else 1) if 0 &lt;= qi and qi &lt; n and 0 &lt;= qj and qj &lt; m: if qi == 0 and qj == 0: break s += mult * grid[qj,qi] s_max = max(s_max, s) return s_max def best_line(grid): n, m = grid.shape D = generate_D(n) s_max = grid.sum() s_max = max(s_max, best_line_main_loop_par_unroll_transposed(grid.T.copy(), D, s_max)) s_max = max(s_max, best_line_main_loop_par_unroll_transposed(grid, D, s_max)) return s_max Note that Numba takes a significant time to compile this function, hence the cache=True flag to avoid recompiling it over and over. Performance results Here are performance results on my machine (with a i5-9600KF CPU, CPython 3.8.1 on Windows, and Numba 0.58.1): With N = 30: - Initial code: 6461 ms - AndrejKesely's code: 54 ms - This code: 4 ms &lt;----- With N = 300: - Initial code: TOO LONG - AndrejKesely's code: 109 s - This code: 12 s &lt;----- Thus, this implementation is more than 1600 times faster than the initial implementation, and also 9 times faster than the one of @AndrejKesely. This is the fastest one by a large margin. Notes The provided implementation can theoretically be optimized a bit further thanks to SIMD instructions. However, this is AFAIK not possible to easily do that with Numba. A native language need to be used to do that (e.g. C, C++, Rust). SIMD-friendly native languages (e.g. CUDA, ISPC) are certainly the way to go to do that quite easily. Indeed, doing that manually with native SIMD intrinsics or SIMD library is cumbersome and it will likely make the code completely unreadable. I expect this to be 2x-4x faster but certainly not much more. On CPU, this requires a hardware supporting fast SIMD masked-load instructions and blending/masking ones (e.g. quite-recent mainstream AMD/Intel x86-64 CPUs). On GPU, one need to care about maintaining SIMD lanes mainly active (not so simple since GPUs have very wide SIMD registers and warp divergence tends to increase due to the break and conditionals), not to mention memory accesses should be as contiguous as possible without bank conflits to get a fast implementation (this is probably far from being easy to do though here).",
    "context_chunks": [
      {
        "text": "This coding question derived from this question. Consider an n by n grid of integers. The task is to draw a straight line across the grid so that the part that includes the top left corner sums to the largest number possible. Here is a picture of an optimal solution with score 45: We include a square in the part that is to be summed if its middle is above or on the line. Above means in the part including the top left corner of the grid. (To make this definition clear, no line can start exactly in the top left corner of the grid.) The task is to choose the line that maximizes the sum of the part that includes the top left square. The line must go straight from one side to another. The line can start or end anywhere on a side and not just at integer points. The Python code given is: import numpy as np import fractions def best_line(grid): n, m = grid.shape D = [(di, dj) for di in range(-(n - 1), n) for dj in range(-(n - 1), n)] def slope(d): di, dj = d if dj == 0: return float('inf') if di &lt;= 0 else float('-inf'), -di else: return fractions.Fraction(di, dj), fractions.Fraction(-1, dj) D.sort(key=slope) D = np.array(D, dtype=np.int64) s_max = grid.sum() for grid in (grid, grid.T): left_sum = 0 for j in range(grid.shape[1]): left_sum += grid[:,j].sum() for i in range(grid.shape[0]): p = np.array([i, j], dtype=np.int64) Q = p + D Q = Q[np.all((0 &lt;= Q) &amp; (Q &lt; np.array(grid.shape)), axis=1)] s = left_sum for q in Q: if not np.any(q): break if q[1] &lt;= j: s -= grid[q[0],q[1]] else: s += grid[q[0],q[1]] s_max = max(s_max, s) return s_max This code is already slow for n=30. Is there any way to speed it up in practice? Test cases As the problem is quite complicated, I have given some example inputs and outputs. The easiest test cases are when the input matrix is made of positive (or negative) integers only. In that case a line that makes the part to sum the whole matrix (or the empty matrix if all the integers are negative) wins. Only slightly less simple is if there is a line that clearly separates the negative integers from the non negative integers in the matrix. Here is a slightly more difficult example with an optimal line shown. The optimal value is 14. The grid in machine readable form is: [[ 3 -1 -2 -1] [ 0 1 -1 1] [ 1 1 3 0] [ 3 3 -1 -1]] Here is an example with optimal value 0. [[-3 -3 2 -3] [ 0 -2 -1 0] [ 1 0 2 0] [-1 -2 1 -1]] This matrix has optimal score 31: [[ 3 0 1 3 -1 1 1 3 -2 -1] [ 3 -1 -1 1 0 -1 2 1 -2 0] [ 2 2 -2 0 1 -3 0 -2 2 1] [ 0 -3 -3 -1 -1 3 -2 0 0 3] [ 2 2 3 2 -1 0 3 0 -3 -1] [ 1 -1 3 1 -3 3 -2 0 -3 0] [ 2 -2 -2 -3 -2 1 -2 0 0 3] [ 0 3 0 1 3 -1 2 -3 0 -2] [ 0 -2 2 2 2 -2 0 2 1 3] [-2 -2 0 -2 -2 2 0 2 3 3]] In Python/numpy, an easy way to make more test matrices is: import numpy as np N = 30 square = np.random.randint(-3, 4, size=(N, N)) Timing N = 30 np.random.seed(42) big_square = randint(-3, 4, size=(N, N)) print(best_line(np.array(big_square))) takes 1 minute 55 seconds and gives the output 57. Andrej Kesely's parallel code takes 1 min 5 seconds for n=250. This is a huge improvement. Can it be made faster still?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "TL;DR: This answer provides a much faster solution than the one of @AndrejKesely. Is also makes use of Numba, but the final code is more optimized despite being also more complex. First simple implementation The initial code is not efficient because it calls Numpy function in a loop. In such a case, Numpy functions are very expensive. Numba and Cython are the way to go to make the code significantly faster. Still, operations on Nx2 arrays are not efficient, neither in Numpy nor in Numba. Moreover, generating temporary arrays (like Q) tends not to be optimal. The solution is typically to compute the array D on the fly without generating a temporary array Q. Here is a naive relatively-fast implementation we can write based on this: import numpy as np import numba as nb # Naive implementation based on the initial code: see below def generate_D(n): import fractions def slope(d): di, dj = d if dj == 0: return float('inf') if di &lt;= 0 else float('-inf'), -di return fractions.Fraction(di, dj), fractions.Fraction(-1, dj) D = [(di, dj) for di in range(-(n - 1), n) for dj in range(-(n - 1), n)] D.sort(key=slope) return np.array(D, dtype=np.int64) # Naive Numba implementation: see below @nb.njit('(int64[:,::1], int64[:,::1], int64)') def best_line_main_loop_seq(grid, D, s_max_so_far): n, m = grid.shape s_max = s_max_so_far left_sum = 0 for j in range(m): left_sum += grid[:,j].sum() for i in range(n): s = left_sum for k in range(D.shape[0]): qi = D[k, 0] + i qj = D[k, 1] + j if 0 &lt;= qi and qi &lt; n and 0 &lt;= qj and qj &lt; m: if qi == 0 and qj == 0: break val = grid[qi,qj] s += -val if qj &lt;= j else val s_max = max(s_max, s) return s_max # Main computing function def best_line(grid): n, m = grid.shape D = generate_D(n) s_max = grid.sum() s_max = max(s_max, best_line_main_loop_par_unroll_transposed(grid.T.copy(), D, s_max)) s_max = max(s_max, best_line_main_loop_par_unroll_transposed(grid, D, s_max)) return s_max # Benchmark N = 30 np.random.seed(42) big_square = np.random.randint(-3, 4, size=(N, N)) grid = np.array(big_square).astype(np.int64) assert best_line(grid) == 57 %time best_line(grid) The performance of this sequential code is not far from the one of the parallel implementation of @AndrejKesely (a bit slower on my machine, especially with a large N). Faster generation of the array D The above code is limited by the slow sort in generate_D(n) due to the rather inefficient fractions module, especially for small N values. The code can be made faster by comparing fractions in Numba directly. However, Numba unfortunately do not support keywords for the sort function so this feature needs to be reimplemented. This is a bit cumbersome to do manually, but the speed up worth the effort. Here is the resulting implementation: @nb.njit('(UniTuple(int64,2), UniTuple(int64,2))') def compare_fraction(a, b): a_top, a_bot = a b_top, b_bot = b if a_bot &lt; 0: a_top = -a_top a_bot = -a_bot if b_bot &lt; 0: b_top = -b_top b_bot = -b_bot fixed_a = a_top * b_bot fixed_b = b_top * a_bot if fixed_a &lt; fixed_b: return -1 if fixed_a == fixed_b: return 0 return 1 @nb.njit('(int64[::1], int64[::1])') def compare_slope(a, b): ai, aj = a bi, bj = b if aj == 0: # slope_a is special if ai &lt;= 0: # slope_a = (INF,-ai) if bj == 0 and bi &lt;= 0: if -ai &lt; -bi: return -1 elif -ai == -bi: return 0 else: return 1 else: return 1 else: # slope_a = (-INF,-ai) if bj == 0 and bi &gt; 0: # slope_b = (-INF,-bi) if -ai &lt; -bi: return -1 elif -ai == -bi: return 0 else: return 1 else: return -1 else: if bj == 0: # slope_b is special if bi &lt;= 0: # slope_b = (INF,-bi) return -1 else: # slope_b = (-INF,-bi) return 1 slope_a = ((ai,aj), (-1,aj)) slope_b = ((bi,bj), (-1,bj)) res = compare_fraction(slope_a[0], slope_b[0]) if res == 0: return compare_fraction(slope_a[1], slope_b[1]) return res # Quite naive quick-sort, but simple one @nb.njit('(int64[:,::1],)') def sort_D(arr): if len(arr) &lt;= 1: return else: pivot = arr[0].copy() left = 1 right = len(arr) - 1 while True: while left &lt;= right and compare_slope(arr[left], pivot) &lt;= 0: left = left + 1 while compare_slope(arr[right], pivot) &gt;= 0 and right &gt;= left: right = right - 1 if right &lt; left: break else: arr[left], arr[right] = arr[right].copy(), arr[left].copy() arr[0], arr[right] = arr[right].copy(), arr[0].copy() sort_D(arr[:right]) sort_D(arr[right+1:]) @nb.njit('(int64,)') def generate_D(n): D_list = [(di, dj) for di in range(-(n - 1), n) for dj in range(-(n - 1), n)] D_arr = np.array(D_list, dtype=np.int64) sort_D(D_arr) return D_arr The above generate_D function is more than 10 times faster on my machine. Faster main loop The naive main loop code provided above can be improved. First of all, it can be parallelized though the code does not scale well (certainly due to the work imbalance caused by the break in the loop). This can be done efficiently with prange on the outer loop (using prange on the inner loop is not optimal because the amount of work is small and creating/joining threads is expensive). Moreover, the loop can be unrolled so to reduce the number of conditional checks, especially on j. While the performance improvement can be significant (e.g. 2 times faster), this resulting code is also clearly less readable/maintainable (actually pretty ugly). This this a trade-off to pay since the operation is too complex for the Numba JIT to do it (and more generally most compilers). Finally, the array access can be made more cache-friendly by virtually transposing the array so to improve the locality of memory accesses (i.e. accessing many items of a same row in the target grid rather than accessing different rows). This optimization is especially useful for large N values (e.g. &gt;200). Here is the resulting optimized main code: @nb.njit('(int64[:,::1], int64[:,::1], int64)', parallel=True, cache=True) def best_line_main_loop_par_unroll_transposed(grid, D, s_max_so_far): m, n = grid.shape s_max = s_max_so_far all_left_sum = np.empty(m, dtype=np.int64) left_sum = 0 for j in range(m): left_sum += grid[j,:].sum() all_left_sum[j] = left_sum for j in nb.prange(m): left_sum = all_left_sum[j] for i in range(0, n//4*4, 4): i1 = i i2 = i + 1 i3 = i + 2 i4 = i + 3 s1 = left_sum s2 = left_sum s3 = left_sum s4 = left_sum continue_loop_i1 = True continue_loop_i2 = True continue_loop_i3 = True continue_loop_i4 = True for k in range(D.shape[0]): qj = D[k, 1] + j if 0 &lt;= qj and qj &lt; m: qi1 = D[k, 0] + i1 qi2 = D[k, 0] + i2 qi3 = D[k, 0] + i3 qi4 = D[k, 0] + i4 mult = np.int64(-1 if qj &lt;= j else 1) if qj != 0: if continue_loop_i1 and 0 &lt;= qi1 and qi1 &lt; n: s1 += mult * grid[qj,qi1] if continue_loop_i2 and 0 &lt;= qi2 and qi2 &lt; n: s2 += mult * grid[qj,qi2] if continue_loop_i3 and 0 &lt;= qi3 and qi3 &lt; n: s3 += mult * grid[qj,qi3] if continue_loop_i4 and 0 &lt;= qi4 and qi4 &lt; n: s4 += mult * grid[qj,qi4] s_max = max(s_max, max(max(s1, s2), max(s3, s4))) else: if continue_loop_i1 and 0 &lt;= qi1 and qi1 &lt; n: if qi1 == 0 and qj == 0: continue_loop_i1 = False else: s1 += mult * grid[qj,qi1] s_max = max(s_max, s1) if continue_loop_i2 and 0 &lt;= qi2 and qi2 &lt; n: if qi2 == 0 and qj == 0: continue_loop_i2 = False else: s2 += mult * grid[qj,qi2] s_max = max(s_max, s2) if continue_loop_i3 and 0 &lt;= qi3 and qi3 &lt; n: if qi3 == 0 and qj == 0: continue_loop_i3 = False else: s3 += mult * grid[qj,qi3] s_max = max(s_max, s3) if continue_loop_i4 and 0 &lt;= qi4 and qi4 &lt; n: if qi4 == 0 and qj == 0: continue_loop_i4 = False else: s4 += mult * grid[qj,qi4] s_max = max(s_max, s4) if not continue_loop_i1 and not continue_loop_i2 and not continue_loop_i3 and not continue_loop_i4: break for i in range(n//4*4, n): s = left_sum for k in range(D.shape[0]): qi = D[k, 0] + i qj = D[k, 1] + j mult = np.int64(-1 if qj &lt;= j else 1) if 0 &lt;= qi and qi &lt; n and 0 &lt;= qj and qj &lt; m: if qi == 0 and qj == 0: break s += mult * grid[qj,qi] s_max = max(s_max, s) return s_max def best_line(grid): n, m = grid.shape D = generate_D(n) s_max = grid.sum() s_max = max(s_max, best_line_main_loop_par_unroll_transposed(grid.T.copy(), D, s_max)) s_max = max(s_max, best_line_main_loop_par_unroll_transposed(grid, D, s_max)) return s_max Note that Numba takes a significant time to compile this function, hence the cache=True flag to avoid recompiling it over and over. Performance results Here are performance results on my machine (with a i5-9600KF CPU, CPython 3.8.1 on Windows, and Numba 0.58.1): With N = 30: - Initial code: 6461 ms - AndrejKesely's code: 54 ms - This code: 4 ms &lt;----- With N = 300: - Initial code: TOO LONG - AndrejKesely's code: 109 s - This code: 12 s &lt;----- Thus, this implementation is more than 1600 times faster than the initial implementation, and also 9 times faster than the one of @AndrejKesely. This is the fastest one by a large margin. Notes The provided implementation can theoretically be optimized a bit further thanks to SIMD instructions. However, this is AFAIK not possible to easily do that with Numba. A native language need to be used to do that (e.g. C, C++, Rust). SIMD-friendly native languages (e.g. CUDA, ISPC) are certainly the way to go to do that quite easily. Indeed, doing that manually with native SIMD intrinsics or SIMD library is cumbersome and it will likely make the code completely unreadable. I expect this to be 2x-4x faster but certainly not much more. On CPU, this requires a hardware supporting fast SIMD masked-load instructions and blending/masking ones (e.g. quite-recent mainstream AMD/Intel x86-64 CPUs). On GPU, one need to care about maintaining SIMD lanes mainly active (not so simple since GPUs have very wide SIMD registers and warp divergence tends to increase due to the break and conditionals), not to mention memory accesses should be as contiguous as possible without bank conflits to get a fast implementation (this is probably far from being easy to do though here).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I was able to make a numba version of the code above + make it run parallel with numba.prange: import fractions import numpy as np from numba import njit, prange @njit def np_all_axis1(x): out = np.zeros(x.shape[0], dtype=np.bool_) for i in range(x.shape[0]): for j in range(x.shape[1]): if not x[i, j]: break else: out[i] = True return out def calculate_directions(grid): def slope(d): di, dj = d if dj == 0: return float(&quot;inf&quot;) if di &lt;= 0 else float(&quot;-inf&quot;), -di else: return fractions.Fraction(di, dj), fractions.Fraction(-1, dj) n = len(grid) D = [(di, dj) for di in range(-(n - 1), n) for dj in range(-(n - 1), n)] D.sort(key=slope) return np.array(D, dtype=np.int32) @njit(parallel=True) def best_line(grid, D): n, m = grid.shape gs = np.array(grid.shape) s_max = grid.sum() l = [grid, grid.T] for grid in l: left_sum = 0 for j in range(grid.shape[1]): left_sum += grid[:, j].sum() for i in prange(grid.shape[0]): p = np.array([i, j], dtype=np.int32) Q = p + D Q = Q[np_all_axis1((0 &lt;= Q) &amp; (Q &lt; gs))] s = left_sum for q in Q: if q[0] == 0 and q[1] == 0: break if q[1] &lt;= j: s -= grid[q[0], q[1]] else: s += grid[q[0], q[1]] s_max = max(s_max, s) return s_max grid = [[3, -1, -2, -1], [0, 1, -1, 1], [1, 1, 3, 0], [3, 3, -1, -1]] out = best_line(np.array(grid), calculate_directions(grid)) print(out) Prints: 14 Quick benchmark on my AMD 5700X: from timeit import timeit N = 30 np.random.seed(42) big_square = np.random.randint(-3, 4, size=(N, N)) assert best_line(np.array(big_square), calculate_directions(big_square)) == 57 t1 = timeit( &quot;best_line(np.array(big_square), calculate_directions(big_square))&quot;, setup=&quot;N = 30;np.random.seed(42);big_square = np.random.randint(-3, 4, size=(N, N))&quot;, number=1, globals=globals(), ) print(t1) Prints: 0.023645485984161496 With N = 250 the result is 22.5 seconds.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "performance",
        "optimization"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2023-12-23T16:15:30",
      "question_id": 77708266,
      "answer_id": 77728934
    }
  },
  {
    "question": "From R array to Numpy array",
    "expected_answer": "Basically the difference is in the ordering of the array. Arrays can be row-major or column-major. See here about some info. R by default constructs column-major arrays, and Python constructs row-major arrays. That's why the default constructors you use will not give the same output. To fix that you can tell Python to construct the array in column-major by saying it should be in Fortran contiguous (column-major): np.reshape(np.arange(1,19), (3,3,2), &quot;F&quot;) array([[[ 1, 10], [ 4, 13], [ 7, 16]], [[ 2, 11], [ 5, 14], [ 8, 17]], [[ 3, 12], [ 6, 15], [ 9, 18]]]) It looks different, but the underlying data is the same as in R. If you do indexing you can see that it performs exactly the same: R: a[1,1,] [1] 1 10 Python: a[0,0,] &gt;&gt;&gt; array([ 1, 10]) Comparing that to the formerly top-rated answer, which will not give you the same values. It may visually look the same, but it does not function the same. b = np.arange(1, 19).reshape((2, 3, 3)).transpose(0,2,1) b[0,0,] &gt;&gt;&gt; array([1, 4, 7]) See also this great vignette by the reticulate team about the differences: https://cran.r-project.org/web/packages/reticulate/vignettes/arrays.html",
    "context_chunks": [
      {
        "text": "Lets say, I have a following R array a &lt;- array(1:18, dim = c(3, 3, 2)) r$&gt; a , , 1 [,1] [,2] [,3] [1,] 1 4 7 [2,] 2 5 8 [3,] 3 6 9 , , 2 [,1] [,2] [,3] [1,] 10 13 16 [2,] 11 14 17 [3,] 12 15 18 and now I want to have the same array in Python numpy. I use a = np.arange(1, 19).reshape((3, 3, 2)) array([[[ 1, 2], [ 3, 4], [ 5, 6]], [[ 7, 8], [ 9, 10], [11, 12]], [[13, 14], [15, 16], [17, 18]]]) But somehow, those two do not look like the same. how can one replicate the same array in Python? I also tried a = np.arange(1, 19).reshape((2, 3, 3)) array([[[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]]) which is also not identical.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Basically the difference is in the ordering of the array. Arrays can be row-major or column-major. See here about some info. R by default constructs column-major arrays, and Python constructs row-major arrays. That's why the default constructors you use will not give the same output. To fix that you can tell Python to construct the array in column-major by saying it should be in Fortran contiguous (column-major): np.reshape(np.arange(1,19), (3,3,2), &quot;F&quot;) array([[[ 1, 10], [ 4, 13], [ 7, 16]], [[ 2, 11], [ 5, 14], [ 8, 17]], [[ 3, 12], [ 6, 15], [ 9, 18]]]) It looks different, but the underlying data is the same as in R. If you do indexing you can see that it performs exactly the same: R: a[1,1,] [1] 1 10 Python: a[0,0,] &gt;&gt;&gt; array([ 1, 10]) Comparing that to the formerly top-rated answer, which will not give you the same values. It may visually look the same, but it does not function the same. b = np.arange(1, 19).reshape((2, 3, 3)).transpose(0,2,1) b[0,0,] &gt;&gt;&gt; array([1, 4, 7]) See also this great vignette by the reticulate team about the differences: https://cran.r-project.org/web/packages/reticulate/vignettes/arrays.html",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The order of the items in NumPy must match the column-major order in order to reproduce the same array as in R. This may be accomplished by reshaping first, then transposing the axes. Here is how I go about it: import numpy as np # Create the array in NumPy with the desired shape and order a = np.arange(1, 19).reshape((3, 3, 2), order='F') print(a) This will give you an array in NumPy that matches the order of the R array: array([[[ 1, 10], [ 2, 11], [ 3, 12]], [[ 4, 13], [ 5, 14], [ 6, 15]], [[ 7, 16], [ 8, 17], [ 9, 18]]])",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "r",
        "numpy"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2023-10-11T09:01:56",
      "question_id": 77271626,
      "answer_id": 77271744
    }
  },
  {
    "question": "Create a Polars Dataframe by appending multiple row at a time",
    "expected_answer": "Brainstorming idea =&gt; adding a list of the dictionary, converting to DF, and concatenating: new_rows = [{'abc': None, 'def': value, 'ghi': None} for value in values_to_set] # list of the dictionary new_df = pl.DataFrame(new_rows) # new DataFrame with the new rows df_polars = pl.concat([df_polars, new_df]) # Concatenate",
    "context_chunks": [
      {
        "text": "How can I make DataFrame and then more rows one at a time? import polars as pl df_polars = pl.DataFrame({ 'abc': [], 'def': [], 'ghi': [] }) value_to_set = 10.0 df_polars = df_polars.extend([ pl.lit(None).alias('abc'), pl.lit(value_to_set).alias('def'), pl.lit(None).alias('ghi') ]) print(df_polars) It only works for one field at a time. What is a more efficient approach to add several rows to my data-frame?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Brainstorming idea =&gt; adding a list of the dictionary, converting to DF, and concatenating: new_rows = [{'abc': None, 'def': value, 'ghi': None} for value in values_to_set] # list of the dictionary new_df = pl.DataFrame(new_rows) # new DataFrame with the new rows df_polars = pl.concat([df_polars, new_df]) # Concatenate",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Put the new rows into a new DataFrame with identical column names, then either DataFrame.extend or DataFrame.vstack to combine two DataFrames vertically. df = pl.DataFrame({&quot;foo&quot;: [1, 2, 3], &quot;bar&quot;: [4, 5, 6]}) new_rows_as_df = pl.DataFrame({&quot;foo&quot;: [10, 20, 30], &quot;bar&quot;: [40, 50, 60]}) df.extend(new_rows_as_df) From the extend docs: Different from vstack which adds the chunks from other to the chunks of this DataFrame, extend appends the data from other to the underlying memory locations and thus may cause a reallocation. If this does not cause a reallocation, the resulting data structure will not have any extra chunks and thus will yield faster queries. Prefer extend over vstack when you want to do a query after a single append. For instance, during online operations where you add n rows and rerun a query. Prefer vstack over extend when you want to append many times before doing a query. For instance, when you read in multiple files and want to store them in a single DataFrame. In the latter case, finish the sequence of vstack operations with a rechunk.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-polars"
      ],
      "question_score": 8,
      "answer_score": 7,
      "created": "2023-10-17T09:27:13",
      "question_id": 77307743,
      "answer_id": 77307802
    }
  },
  {
    "question": "Unable to use nltk functions",
    "expected_answer": "This error appears to be due to a change introduced in nltk version 3.8.2 to address pickle file vulnerabilities. See: https://github.com/nltk/nltk/issues/3293 for the nltk issue created for this breaking change and a discussion there about moving the change to the next major release. Two possible fixes: use version 3.8.1 and run with known vulnerability. as Chrispresso mentions in the comment, download &quot;punkt_tab&quot; instead of &quot;punkt&quot;, i.e., nltk.download('punkt_tab') As mentioned in the issue report linked above, fix 2 may be problematic on Windows.",
    "context_chunks": [
      {
        "text": "I was trying to run some nltk functions on the UCI spam message dataset but ran into this problem of word_tokenize not working even after downloading dependencies. import nltk nltk.download('punkt') from nltk.tokenize import word_tokenize df['text'].apply(lambda x: len(nltk.word_tokenize(x))) following is the error: { &quot;name&quot;: &quot;LookupError&quot;, &quot;message&quot;: &quot; ********************************************************************** Resource punkt_tab not found. Please use the NLTK Downloader to obtain the resource: &gt;&gt;&gt; import nltk &gt;&gt;&gt; nltk.download('punkt_tab') For more information see: https://www.nltk.org/data.html Attempted to load tokenizers/punkt_tab/english/ Searched in: - 'C:\\\\\\\\Users\\\\\\\\user/nltk_data' - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\ ltk_data' - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\\share\\\\\\ ltk_data' - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\\lib\\\\\\ ltk_data' - 'C:\\\\\\\\Users\\\\\\\\user\\\\\\AppData\\\\\\\\Roaming\\\\\\ ltk_data' - 'C:\\\\\\ ltk_data' - 'D:\\\\\\ ltk_data' - 'E:\\\\\\ ltk_data' ********************************************************************** &quot;, &quot;stack&quot;: &quot;--------------------------------------------------------------------------- LookupError Traceback (most recent call last) Cell In[1024], line 3 1 #finding no. of words ----&gt; 3 df['text'].apply(lambda x: len(nltk.word_tokenize(x))) File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\series.py:4915, in Series.apply(self, func, convert_dtype, args, by_row, **kwargs) 4780 def apply( 4781 self, 4782 func: AggFuncType, (...) 4787 **kwargs, 4788 ) -&gt; DataFrame | Series: 4789 \\&quot;\\&quot;\\&quot; 4790 Invoke function on values of Series. 4791 (...) 4906 dtype: float64 4907 \\&quot;\\&quot;\\&quot; 4908 return SeriesApply( 4909 self, 4910 func, 4911 convert_dtype=convert_dtype, 4912 by_row=by_row, 4913 args=args, 4914 kwargs=kwargs, -&gt; 4915 ).apply() File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\apply.py:1427, in SeriesApply.apply(self) 1424 return self.apply_compat() 1426 # self.func is Callable -&gt; 1427 return self.apply_standard() File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\apply.py:1507, in SeriesApply.apply_standard(self) 1501 # row-wise access 1502 # apply doesn't have a `na_action` keyword and for backward compat reasons 1503 # we need to give `na_action=\\&quot;ignore\\&quot;` for categorical data. 1504 # TODO: remove the `na_action=\\&quot;ignore\\&quot;` when that default has been changed in 1505 # Categorical (GH51645). 1506 action = \\&quot;ignore\\&quot; if isinstance(obj.dtype, CategoricalDtype) else None -&gt; 1507 mapped = obj._map_values( 1508 mapper=curried, na_action=action, convert=self.convert_dtype 1509 ) 1511 if len(mapped) and isinstance(mapped[0], ABCSeries): 1512 # GH#43986 Need to do list(mapped) in order to get treated as nested 1513 # See also GH#25959 regarding EA support 1514 return obj._constructor_expanddim(list(mapped), index=obj.index) File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\base.py:921, in IndexOpsMixin._map_values(self, mapper, na_action, convert) 918 if isinstance(arr, ExtensionArray): 919 return arr.map(mapper, na_action=na_action) --&gt; 921 return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert) File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\pandas\\\\core\\\\algorithms.py:1743, in map_array(arr, mapper, na_action, convert) 1741 values = arr.astype(object, copy=False) 1742 if na_action is None: -&gt; 1743 return lib.map_infer(values, mapper, convert=convert) 1744 else: 1745 return lib.map_infer_mask( 1746 values, mapper, mask=isna(values).view(np.uint8), convert=convert 1747 ) File lib.pyx:2972, in pandas._libs.lib.map_infer() Cell In[1024], line 3, in &lt;lambda&gt;(x) 1 #finding no. of words ----&gt; 3 df['text'].apply(lambda x: len(nltk.word_tokenize(x))) File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\ ltk\\\\tokenize\\\\__init__.py:129, in word_tokenize(text, language, preserve_line) 114 def word_tokenize(text, language=\\&quot;english\\&quot;, preserve_line=False): 115 \\&quot;\\&quot;\\&quot; 116 Return a tokenized copy of *text*, 117 using NLTK's recommended word tokenizer (...) 127 :type preserve_line: bool 128 \\&quot;\\&quot;\\&quot; --&gt; 129 sentences = [text] if preserve_line else sent_tokenize(text, language) 130 return [ 131 token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent) 132 ] File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\ ltk\\\\tokenize\\\\__init__.py:106, in sent_tokenize(text, language) 96 def sent_tokenize(text, language=\\&quot;english\\&quot;): 97 \\&quot;\\&quot;\\&quot; 98 Return a sentence-tokenized copy of *text*, 99 using NLTK's recommended sentence tokenizer (...) 104 :param language: the model name in the Punkt corpus 105 \\&quot;\\&quot;\\&quot; --&gt; 106 tokenizer = PunktTokenizer(language) 107 return tokenizer.tokenize(text) File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\ ltk\\\\tokenize\\\\punkt.py:1744, in PunktTokenizer.__init__(self, lang) 1742 def __init__(self, lang=\\&quot;english\\&quot;): 1743 PunktSentenceTokenizer.__init__(self) -&gt; 1744 self.load_lang(lang) File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\ ltk\\\\tokenize\\\\punkt.py:1749, in PunktTokenizer.load_lang(self, lang) 1746 def load_lang(self, lang=\\&quot;english\\&quot;): 1747 from nltk.data import find -&gt; 1749 lang_dir = find(f\\&quot;tokenizers/punkt_tab/{lang}/\\&quot;) 1750 self._params = load_punkt_params(lang_dir) 1751 self._lang = lang File ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\ ltk\\\\data.py:582, in find(resource_name, paths) 580 sep = \\&quot;*\\&quot; * 70 581 resource_not_found = f\\&quot;\\ {sep}\\ {msg}\\ {sep}\\ \\&quot; --&gt; 582 raise LookupError(resource_not_found) LookupError: ********************************************************************** Resource punkt_tab not found. Please use the NLTK Downloader to obtain the resource: &gt;&gt;&gt; import nltk &gt;&gt;&gt; nltk.download('punkt_tab') For more information see: https://www.nltk.org/data.html Attempted to load tokenizers/punkt_tab/english/ Searched in: - 'C:\\\\\\\\Users\\\\\\\\user/nltk_data' - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\ ltk_data' - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\\share\\\\\\ ltk_data' - 'C:\\\\\\\\Program Files\\\\\\\\WindowsApps\\\\\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\\\\\lib\\\\\\ ltk_data' - 'C:\\\\\\\\Users\\\\\\\\user\\\\\\\\AppData\\\\\\\\Roaming\\\\\\ ltk_data' - 'C:\\\\\\ ltk_data' - 'D:\\\\\\ ltk_data' - 'E:\\\\\\ ltk_data' ********************************************************************** &quot; } I tried re installing nltk and try and download a few other dependency files but nothing works. What am I doing wrong?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This error appears to be due to a change introduced in nltk version 3.8.2 to address pickle file vulnerabilities. See: https://github.com/nltk/nltk/issues/3293 for the nltk issue created for this breaking change and a discussion there about moving the change to the next major release. Two possible fixes: use version 3.8.1 and run with known vulnerability. as Chrispresso mentions in the comment, download &quot;punkt_tab&quot; instead of &quot;punkt&quot;, i.e., nltk.download('punkt_tab') As mentioned in the issue report linked above, fix 2 may be problematic on Windows.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Possible solutions to resolve this issue: import nltk custom_nltk_path = &quot;&quot; nltk.data.path.append(custom_nltk_path) nltk.download('stopwords', download_dir=custom_nltk_path) nltk.download('punkt', download_dir=custom_nltk_path) nltk.download('wordnet', download_dir=custom_nltk_path) nltk.download('omw-1.4', download_dir=custom_nltk_path) nltk.download('punkt_tab',download_dir=custom_nltk_path) print(&quot;NLTK Data Paths:&quot;, nltk.data.path)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "machine-learning",
        "nlp",
        "nltk"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2024-08-12T15:17:29",
      "question_id": 78862426,
      "answer_id": 78867612
    }
  },
  {
    "question": "No module named &#39;urllib3.packages.six.moves&#39;",
    "expected_answer": "For me, it was a version compatibility issue. Upgrading urllib3 (which made me upgrade requests) solved the problem. Try in this order and test between installs: pip install requests --upgrade pip install urllib3 --upgrade pip install boto3 --upgrade pip install django-storages --upgrade pip install django --upgrade Caution: You will be upgrading lots os components. This could lead to break your application, so make a backup before. Also, make a: pip freeze &gt;requirements.txt And save this file somewere, so you can roll back.",
    "context_chunks": [
      {
        "text": "I'm trying to use the groupme API with GroupyAPI, but I'm having some issues with urllib3. import urllib3 Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;C:\\Users\\smlac\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\__init__.py&quot;, line 8, in &lt;module&gt; from .connectionpool import ( File &quot;C:\\Users\\smlac\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 11, in &lt;module&gt; from .exceptions import ( File &quot;C:\\Users\\smlac\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\exceptions.py&quot;, line 2, in &lt;module&gt; from .packages.six.moves.http_client import ( ModuleNotFoundError: No module named 'urllib3.packages.six.moves' I have tried this on both my laptop and PC, for some reason it works on my PC, but I need to use it on my laptop as well. I have uninstalled urllib and reinstalled it, I have verified the versions match with what I'm using on my PC and they all match. I made sure that python is in my environmental variables path. I just don't understand why it only works on my PC. Any help would be greatly appreciated.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "For me, it was a version compatibility issue. Upgrading urllib3 (which made me upgrade requests) solved the problem. Try in this order and test between installs: pip install requests --upgrade pip install urllib3 --upgrade pip install boto3 --upgrade pip install django-storages --upgrade pip install django --upgrade Caution: You will be upgrading lots os components. This could lead to break your application, so make a backup before. Also, make a: pip freeze &gt;requirements.txt And save this file somewere, so you can roll back.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "i got the same question today,and i found it's python version,when i use version3.12.3, it cant work whether pip install urllib3,then i uninstall it and install version3.7.2,it work normally:)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "urllib3",
        "six"
      ],
      "question_score": 8,
      "answer_score": 8,
      "created": "2024-03-14T20:27:37",
      "question_id": 78163280,
      "answer_id": 78492694
    }
  },
  {
    "question": "What&#39;s the correct way to use user local python environment under PEP668?",
    "expected_answer": "I finally found the best solution is use third-party tools (pyenv, conda, mini-forge). Pyenv use it's own python and pip independent with system's and use it by default. see $ where pip /home/john/.pyenv/shims/pip /home/john/.local/bin/pip /usr/local/bin/pip /usr/bin/pip /bin/pip So, system python and daily used python are separated. User can use pip install xxx like in previous days.",
    "context_chunks": [
      {
        "text": "I have tried to install any python packages on Ubuntu 24.04, but found I cannot do that as in 22.04, with --user PEP668 said it is for avoiding package conflict between system-wide package and user installed package. example: $ pip install setuptools --user error: externally-managed-environment × This environment is externally managed ╰─&gt; To install Python packages system-wide, try apt install python3-xyz, where xyz is the package you are trying to install. If you wish to install a non-Debian-packaged Python package, create a virtual environment using python3 -m venv path/to/venv. Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make sure you have python3-full installed. If you wish to install a non-Debian packaged Python application, it may be easiest to use pipx install xyz, which will manage a virtual environment for you. Make sure you have pipx installed. See /usr/share/doc/python3.11/README.venv for more information. note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages. hint: See PEP 668 for the detailed specification. I am really confused with current rules and can not install any package to user local env. How can I manage my user local environment now? And how can I use latest pip (not linux-distro version) and other packages by default for current user? My Environment (dockerfile just for reproduce): FROM ubuntu:24.04 # add python RUN apt install -y python3-pip python3-venv python-is-python3 pipx USER ubuntu WORKDIR /app I know I can use some env manage tools (pyenv) to do that, but is there any built-in method to bring my user local env back?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I finally found the best solution is use third-party tools (pyenv, conda, mini-forge). Pyenv use it's own python and pip independent with system's and use it by default. see $ where pip /home/john/.pyenv/shims/pip /home/john/.local/bin/pip /usr/local/bin/pip /usr/bin/pip /bin/pip So, system python and daily used python are separated. User can use pip install xxx like in previous days.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You tried to follow the instructions for installing an application. You need to follow the instructions for installing a package: If you wish to install a non-Debian-packaged Python package, create a virtual environment using python3 -m venv path/to/venv. Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make sure you have python3-full installed. Use python3 -m venv whateverpath to create a virtual environment in the specified path, and whateverpath/bin/python and whateverpath/bin/pip to run that virtual environment's Python or to install packages in that virtual environment. Alternatively, if you are absolutely sure you would rather take the risk of breaking your system than use a virtual environment, you can run pip with the --break-system-packages flag, as the pip output said. This will not be safe, even if you use --user.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 8,
      "answer_score": 3,
      "created": "2023-12-18T01:23:40",
      "question_id": 77676556,
      "answer_id": 78401394
    }
  },
  {
    "question": "InvalidDimensionException: Embedding dimension 384 does not match collection dimensionality 1536",
    "expected_answer": "I faced the same error when I first persisted documents using the OpenAI embedding function in a file. embedding = OpenAIEmbeddings() vectordb = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=persist_directory) Then, in a second file, I had opened the database and got the error when querying the previously created collection. The solution was to pass the OpenAI embedding function to the get_collection method. openai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=&quot;YOUR_API_KEY&quot;, model_name=&quot;text-embedding-ada-002&quot; ) client = chromadb.PersistentClient(path='db') collection = client.get_collection(name=&quot;langchain&quot;, embedding_function=openai_ef) It is using a different embedding method by default and, I think, they are using different dimensions.",
    "context_chunks": [
      {
        "text": "I am writing the python code on the Chromadb to create the vector database I try to create collection in the chromadb that include embedding. during my index creation with vector database that include embedding i face this problem",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I faced the same error when I first persisted documents using the OpenAI embedding function in a file. embedding = OpenAIEmbeddings() vectordb = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=persist_directory) Then, in a second file, I had opened the database and got the error when querying the previously created collection. The solution was to pass the OpenAI embedding function to the get_collection method. openai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=&quot;YOUR_API_KEY&quot;, model_name=&quot;text-embedding-ada-002&quot; ) client = chromadb.PersistentClient(path='db') collection = client.get_collection(name=&quot;langchain&quot;, embedding_function=openai_ef) It is using a different embedding method by default and, I think, they are using different dimensions.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The reason for this problem is that the dimension of your previous collection is inconsistent with that of the data you save now. So you can solve this problem by deleting the previous collection.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python"
      ],
      "question_score": 8,
      "answer_score": 4,
      "created": "2023-12-20T22:47:41",
      "question_id": 77694864,
      "answer_id": 77850056
    }
  },
  {
    "question": "Using poetry to install from a private repository with a token",
    "expected_answer": "I've just tested it locally and it looks like you can securely setup the authentication in a number of different ways. 1. Using POETRY_HTTP_BASIC_* environment variables The idea is to pass the authorization credentials via environment variables. Poetry picks up credentials from the following variables: POETRY_HTTP_BASIC_{SOURCE_NAME}_USERNAME - for user name, in your case it should be the token you have. POETRY_HTTP_BASIC_{SOURCE_NAME}_PASSWORD - the password, in your case you can omit this variable, since you're using token based authentication. Here is an example of command you can use in order to provide the token: POETRY_HTTP_BASIC_INTERNAL_PACKAGE_USERNAME=&quot;{**some-token**}&quot; poetry install If you want to avoid adding token variable each time you may choose to append your shell rc file, e.g .zshrc, .bashrc with the following: export POETRY_HTTP_BASIC_INTERNAL_PACKAGE_USERNAME=&quot;{**some-token**}&quot; This way next time you open a terminal the token will be automatically fulfilled for you. 2. Set the token to the config Poetry also allows engineers to save the token in the config (~/Library/Application Support/pypoetry/auth.toml for Mac OS). In order to do that you just need to simply execute the following command: poetry config -- http-basic.internal-package &quot;{**some-token**}&quot; &quot;&quot; Note: The last argument value is &quot;&quot; for empty password. I hope this helps you. Let me know if it works for you or you need any clarifications.",
    "context_chunks": [
      {
        "text": "I have published a package on packagecloud with a token. To install this package, this URL will be used: https://{some-token}:@packagecloud.io/{some-domain}. How do I configure poetry to download from this URL on top of the normal pypi url? One solution that I have tried is to set the pyproject.toml manually like this: [[tool.poetry.source]] name = &quot;internal-package&quot; url = &quot;https://{**some-token**}:@packagecloud.io/{some-domain}&quot; priority = &quot;supplemental&quot; This works but is a bad practice since I am exposing the token in the .toml file. Ideally I want to use a bash script that will do the following: read the token from .env file instruct poetry to look from the url Thanks.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I've just tested it locally and it looks like you can securely setup the authentication in a number of different ways. 1. Using POETRY_HTTP_BASIC_* environment variables The idea is to pass the authorization credentials via environment variables. Poetry picks up credentials from the following variables: POETRY_HTTP_BASIC_{SOURCE_NAME}_USERNAME - for user name, in your case it should be the token you have. POETRY_HTTP_BASIC_{SOURCE_NAME}_PASSWORD - the password, in your case you can omit this variable, since you're using token based authentication. Here is an example of command you can use in order to provide the token: POETRY_HTTP_BASIC_INTERNAL_PACKAGE_USERNAME=&quot;{**some-token**}&quot; poetry install If you want to avoid adding token variable each time you may choose to append your shell rc file, e.g .zshrc, .bashrc with the following: export POETRY_HTTP_BASIC_INTERNAL_PACKAGE_USERNAME=&quot;{**some-token**}&quot; This way next time you open a terminal the token will be automatically fulfilled for you. 2. Set the token to the config Poetry also allows engineers to save the token in the config (~/Library/Application Support/pypoetry/auth.toml for Mac OS). In order to do that you just need to simply execute the following command: poetry config -- http-basic.internal-package &quot;{**some-token**}&quot; &quot;&quot; Note: The last argument value is &quot;&quot; for empty password. I hope this helps you. Let me know if it works for you or you need any clarifications.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The right way to configure a private git repo as dependency is documented in the poetry official documentation. # ensure git token is available in the environment variable export GIT_TOKEN=&quot;****&quot; poetry config repositories.internal-package https://packagecloud.io/some-domain.git poetry config http-basic.internal-package x-token-auth $GIT_TOKEN Add package to the pyproject.toml file poetry add &quot;git+https://packagecloud.io/some-domain.git&quot; This will create an entry in the pyproject.toml file like [tool.poetry.dependencies] myproject = {git = &quot;https://packagecloud.io/some-domain.git&quot; } Run poetry install, you will see the magic happens!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pip",
        "package",
        "software-design",
        "python-poetry"
      ],
      "question_score": 8,
      "answer_score": 5,
      "created": "2023-11-27T15:23:52",
      "question_id": 77558055,
      "answer_id": 77659341
    }
  },
  {
    "question": "Add startup/shutdown handlers to FastAPI app with lifespan API",
    "expected_answer": "I found a solution, but I'm not sure if I like it... It accesses the existing lifespan generator via app.router.lifespan_context and wraps it with additional startup/shutdown commands: from contextlib import asynccontextmanager ... main_app_lifespan = app.router.lifespan_context @asynccontextmanager async def lifespan_wrapper(app): print(&quot;sub startup&quot;) async with main_app_lifespan(app) as maybe_state: yield maybe_state print(&quot;sub shutdown&quot;) app.router.lifespan_context = lifespan_wrapper Output: INFO: Waiting for application startup. sub startup startup INFO: Application startup complete. ... INFO: Shutting down INFO: Waiting for application shutdown. shutdown sub shutdown INFO: Application shutdown complete.",
    "context_chunks": [
      {
        "text": "Consider a FastAPI using the lifespan parameter like this: def lifespan(app): print('lifespan start') yield print('lifespan end') app = FastAPI(lifespan=lifespan) Now I want to register a sub app with its own lifecycle functions: app.mount(mount_path, sub_app) How can I register startup/shutdown handlers for the sub app? All solutions I could find either require control over the lifespan generator (which I don't have) or involve deprecated methods like add_event_handler (which doesn't work when lifespan is set). Update Minimal reproducible example: from fastapi import FastAPI # --- main app --- def lifespan(_): print(&quot;startup&quot;) yield print(&quot;shutdown&quot;) app = FastAPI(lifespan=lifespan) @app.get(&quot;/&quot;) async def root(): return {&quot;message&quot;: &quot;Hello World&quot;} # --- sub app --- sub_app = FastAPI() @sub_app.get(&quot;/&quot;) async def sub_root(): return {&quot;message&quot;: &quot;Hello Sub World&quot;} app.mount(&quot;/sub&quot;, sub_app) app.on_event(&quot;startup&quot;)(lambda: print(&quot;sub startup&quot;)) # doesn't work app.on_event(&quot;shutdown&quot;)(lambda: print(&quot;sub shutdown&quot;)) # doesn't work Run with: uvicorn my_app:app --port 8000",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I found a solution, but I'm not sure if I like it... It accesses the existing lifespan generator via app.router.lifespan_context and wraps it with additional startup/shutdown commands: from contextlib import asynccontextmanager ... main_app_lifespan = app.router.lifespan_context @asynccontextmanager async def lifespan_wrapper(app): print(&quot;sub startup&quot;) async with main_app_lifespan(app) as maybe_state: yield maybe_state print(&quot;sub shutdown&quot;) app.router.lifespan_context = lifespan_wrapper Output: INFO: Waiting for application startup. sub startup startup INFO: Application startup complete. ... INFO: Shutting down INFO: Waiting for application shutdown. shutdown sub shutdown INFO: Application shutdown complete.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Try the add_event_handler function. This works for me. app.add_event_handler('startup', lambda: print(&quot;API startup&quot;)) app.add_event_handler('shutdown', lambda: print(&quot;API shutdown&quot;))",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "fastapi",
        "lifecycle",
        "starlette"
      ],
      "question_score": 8,
      "answer_score": 7,
      "created": "2023-10-25T18:56:54",
      "question_id": 77362216,
      "answer_id": 77364729
    }
  },
  {
    "question": "Polars - How to use `pl.when` to output a string value",
    "expected_answer": "You need to wrap the string into a pl.lit to ensure it is interpreted as a literal value. pl.when(cond).then(pl.lit('string a')).otherwise(pl.lit('string b'))",
    "context_chunks": [
      {
        "text": "I am trying to use polars' pl.when to create a new string column. Based on a condition, the column should contain &quot;string a&quot; or &quot;string b&quot;. Polars seems to interpret &quot;string a&quot; as a column, when I try the following. pl.when(condition) .then('string a') .otherwise('string b') .alias('new column') Output. ColumnNotFoundError: string a I am expecting an output like this: new column string a string b",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You need to wrap the string into a pl.lit to ensure it is interpreted as a literal value. pl.when(cond).then(pl.lit('string a')).otherwise(pl.lit('string b'))",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Expanding on @Hericks answers, here is a full example for reference: import polars as pl df = pl.DataFrame({&quot;values&quot;: [5, 15, 10]}) result = df.with_columns( pl.when(pl.col(&quot;values&quot;) &gt; 10) .then(pl.lit(&quot;string a&quot;)) .otherwise(pl.lit(&quot;string b&quot;)) .alias(&quot;new column&quot;) ) print(result) The output is the following: shape: (3, 2) ┌────────┬────────────┐ │ values ┆ new column │ │ --- ┆ --- │ │ i64 ┆ str │ ╞════════╪════════════╡ │ 5 ┆ string b │ │ 15 ┆ string a │ │ 10 ┆ string b │ └────────┴────────────┘",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-polars"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2024-02-26T17:43:22",
      "question_id": 78062891,
      "answer_id": 78062900
    }
  },
  {
    "question": "Create groups in time series data based on potential start and end boolean columns (vectorized solution)",
    "expected_answer": "import pandas as pd import numpy as np df = pd.DataFrame({ 'group': ['ABC'] * 8 + ['DEF'] * 11, 'maybe_start': [False, True, False, False, True, False, False, False, False, False, True, False, False, False, False, False, True, False, False], 'maybe_end': [False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, True, False, False, True] }) def apply_func(df): df['group2'] = np.nan counter = 0 started = False for idx, row in df.iterrows(): if row['maybe_start'] and not started: counter += 1 started = True if started: df.loc[idx, 'group2'] = counter if row['maybe_end']: started = False return df df = df.groupby('group').apply(apply_func) print(df)",
    "context_chunks": [
      {
        "text": "I have a data frame structured like below: group maybe_start maybe_end 0 ABC False False 1 ABC True False 2 ABC False False 3 ABC False False 4 ABC True False 5 ABC False False 6 ABC False True 7 ABC False False 8 DEF False False 9 DEF False False 10 DEF True False 11 DEF False False 12 DEF False True 13 DEF False False 14 DEF False False 15 DEF False True 16 DEF True False 17 DEF False False 18 DEF False True I need to create a separate column, let's say group2, that will note the group defined by the moments of start and end. Therefore, every group in group2 should start, whenever there's first True value in maybe_start column after previous maybe_end==True and end on the first occurrance of maybe_end==True after the start. In other words, we start a new value in group2 at maybe_start==True (in this example in row 1) and every next row of group2 will get the same value until there's occurance of maybe_end==True (here, in row 6). All of this needs to be done within groupby where groups are created based on the group column. Therefore, the expected output should look as follows: group maybe_start maybe_end group2 0 ABC False False NaN 1 ABC True False 1.0 2 ABC False False 1.0 3 ABC False False 1.0 4 ABC True False 1.0 5 ABC False False 1.0 6 ABC False True 1.0 7 ABC False False NaN 0 DEF False False NaN 1 DEF False False NaN 2 DEF True False 1.0 3 DEF False False 1.0 4 DEF False True 1.0 5 DEF False False NaN 6 DEF False False NaN 7 DEF False True NaN 8 DEF True False 2.0 9 DEF False False 2.0 10 DEF False True 2.0 How can I achieve this in vectorised way in Pandas?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "import pandas as pd import numpy as np df = pd.DataFrame({ 'group': ['ABC'] * 8 + ['DEF'] * 11, 'maybe_start': [False, True, False, False, True, False, False, False, False, False, True, False, False, False, False, False, True, False, False], 'maybe_end': [False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, True, False, False, True] }) def apply_func(df): df['group2'] = np.nan counter = 0 started = False for idx, row in df.iterrows(): if row['maybe_start'] and not started: counter += 1 started = True if started: df.loc[idx, 'group2'] = counter if row['maybe_end']: started = False return df df = df.groupby('group').apply(apply_func) print(df)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I'm posting another answer as StackOverflow won't me allow edit the old answer. As stated, it's hard do vectorize this code, but you can try to speed up the computation using numba: from numba import njit @njit def numba_fn(out, x_start, x_end): g = 1.0 state = 0 for i in range(len(out)): start = x_start[i] end = x_end[i] if start and end: if state: out[i] = g state = False else: out[i] = g g += 1 elif start: out[i] = g state = True elif end: if state: out[i] = g state = False g += 1 elif state: out[i] = g def fn(x): x[&quot;group2&quot;] = np.nan numba_fn(x[&quot;group2&quot;].values, x[&quot;maybe_start&quot;].values, x[&quot;maybe_end&quot;].values) return x print(df.groupby(&quot;group&quot;, group_keys=False).apply(fn)) Prints: group maybe_start maybe_end group2 0 ABC False False NaN 1 ABC True False 1.0 2 ABC False False 1.0 3 ABC False False 1.0 4 ABC True False 1.0 5 ABC False False 1.0 6 ABC False True 1.0 7 ABC False False NaN 8 DEF False False NaN 9 DEF False False NaN 10 DEF True False 1.0 11 DEF False False 1.0 12 DEF False True 1.0 13 DEF False False NaN 14 DEF False False NaN 15 DEF False True NaN 16 DEF True False 2.0 17 DEF False False 2.0 18 DEF False True 2.0 19 DEF True True 3.0 20 DEF False False NaN 21 DEF True False 4.0 22 DEF True True 4.0 23 DEF False False NaN BUT The main bottleneck seems to be the pd.Groupby (it creates new dataframe for each group, which slows down the computation). You can use np.bincount to simulate the .groupby() (assuming the dataframe is sorted by &quot;group&quot;): d = np.bincount( pd.Categorical(df[&quot;group&quot;]).codes, ) d = [0, *d.cumsum()] df[&quot;group2&quot;] = np.nan values_group2 = df[&quot;group2&quot;].to_numpy() values_start = df[&quot;maybe_start&quot;].to_numpy() values_end = df[&quot;maybe_end&quot;].to_numpy() for a, b in zip(d, d[1:]): numba_fn(values_group2[a:b], values_start[a:b], values_end[a:b]) print(df) A benchmark (with dataframe of 1000 groups with each group has 1000 elements): from timeit import timeit import numpy as np import pandas as pd from numba import njit @njit def numba_fn(out, x_start, x_end): g = 1.0 state = 0 for i in range(len(out)): start = x_start[i] end = x_end[i] if start and end: if state: out[i] = g state = False else: out[i] = g g += 1 elif start: out[i] = g state = True elif end: if state: out[i] = g state = False g += 1 elif state: out[i] = g def normal_fn(x): out, g, state = [], 1, False for start, end in zip(x.maybe_start, x.maybe_end): if start and end: if state: out.append(g) state = False else: out.append(g) g += 1 elif start: out.append(g) state = True elif end: if state: out.append(g) state = False g += 1 else: out.append(np.nan) elif state: out.append(g) else: out.append(np.nan) x[&quot;group2&quot;] = out return x def fn(x): x[&quot;group2&quot;] = np.nan numba_fn(x[&quot;group2&quot;].values, x[&quot;maybe_start&quot;].values, x[&quot;maybe_end&quot;].values) return x def test_groupby_normal_fn(df): return df.groupby(&quot;group&quot;, group_keys=False).apply(normal_fn) def test_groupby_numba_fn(df): return df.groupby(&quot;group&quot;, group_keys=False).apply(fn) def test_numpy_groupby_numba_fn(df): d = np.bincount( pd.Categorical(df[&quot;group&quot;]).codes, ) d = [0, *d.cumsum()] df[&quot;group2&quot;] = np.nan values_group2 = df[&quot;group2&quot;].to_numpy() values_start = df[&quot;maybe_start&quot;].to_numpy() values_end = df[&quot;maybe_end&quot;].to_numpy() for a, b in zip(d, d[1:]): numba_fn(values_group2[a:b], values_start[a:b], values_end[a:b]) return df def generate_df(num_groups=1000, elements_in_group=1000): from random import randint, seed seed(42) out = [] for g in range(num_groups): for _ in range(elements_in_group): out.append((str(g), bool(randint(0, 1)), bool(randint(0, 1)))) return pd.DataFrame(out, columns=[&quot;group&quot;, &quot;maybe_start&quot;, &quot;maybe_end&quot;]) df = generate_df() # test if the algorithm is correct: df1 = test_groupby_normal_fn(df.copy()) df2 = test_groupby_numba_fn(df.copy()) df3 = test_numpy_groupby_numba_fn(df.copy()) np.testing.assert_equal(df1[&quot;group2&quot;].values, df2[&quot;group2&quot;].values) np.testing.assert_equal(df1[&quot;group2&quot;].values, df3[&quot;group2&quot;].values) t1 = timeit( &quot;test_groupby_normal_fn(x)&quot;, setup=&quot;x=df.copy()&quot;, number=1, globals=globals() ) t2 = timeit( &quot;test_groupby_numba_fn(x)&quot;, setup=&quot;x=df.copy()&quot;, number=1, globals=globals() ) t3 = timeit( &quot;test_numpy_groupby_numba_fn(x)&quot;, setup=&quot;x=df.copy()&quot;, number=1, globals=globals() ) print(&quot;test_groupby_normal_fn =&quot;, t1) print(&quot;test_groupby_numba_fn =&quot;, t2) print(&quot;test_numpy_groupby_numba_fn =&quot;, t3) Prints on my machine (AMD 5700x, python==3.11.4, pandas=2.0.3, numpy==1.24.4, numba==0.57.1): test_groupby_normal_fn = 0.47854723408818245 test_groupby_numba_fn = 0.30540501209907234 test_numpy_groupby_numba_fn = 0.03548573097214103 The numpy groupby + numba JIT is ~14x faster than normal pd.Groupby + .apply",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "vectorization"
      ],
      "question_score": 8,
      "answer_score": 3,
      "created": "2023-07-29T09:59:17",
      "question_id": 76793035,
      "answer_id": 76812957
    }
  },
  {
    "question": "Error Installing Package to Python - Error with requirements to build wheel",
    "expected_answer": "Try using upgrade command for wheel: pip install --upgrade setuptools wheel then the installation command: pip install [PACKAGE]",
    "context_chunks": [
      {
        "text": "newbie to Python, am trying to install a package to explore Python, but can't get it installed. Below is the error output. I've already tried installing wheel, which did not fix the issue. can anyone help pls? Installing build dependencies ... done Getting requirements to build wheel ... error error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; [28 lines of output] Traceback (most recent call last): File &quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 353, in &lt;module&gt; main() File &quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 118, in get_requires_for_build_wheel return hook(config_settings) ^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-05ydr5uy\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py&quot;, line 355, in get_requires_for_build_wheel return self._get_build_requires(config_settings, requirements=['wheel']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-05ydr5uy\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py&quot;, line 325, in _get_build_requires self.run_setup() File &quot;C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-05ydr5uy\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py&quot;, line 507, in run_setup super(_BuildMetaLegacyBackend, self).run_setup(setup_script=setup_script) File &quot;C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-05ydr5uy\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py&quot;, line 341, in run_setup exec(code, locals()) File &quot;&lt;string&gt;&quot;, line 47, in &lt;module&gt; File &quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py&quot;, line 389, in call with Popen(*popenargs, **kwargs) as p: ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py&quot;, line 1026, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File &quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py&quot;, line 1538, in _execute_child hp, ht, pid, tid = _winapi.CreateProcess(executable, args, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ FileNotFoundError: [WinError 2] The system cannot find the file specified [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error Attempted to install a package, but got an error stating that the requirements to build wheel did not run successfully",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Try using upgrade command for wheel: pip install --upgrade setuptools wheel then the installation command: pip install [PACKAGE]",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It’s important to note that the term “package” in this context is being used to describe a bundle of software to be installed (i.e. as a synonym for a distribution). It does not refer to the kind of package that you import in your Python source code (i.e. a container of modules). It is common in the Python community to refer to a distribution using the term “package”. Using the term “distribution” is often not preferred, because it can easily be confused with a Linux distribution, or another larger software distribution like Python itself.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "installation",
        "package"
      ],
      "question_score": 8,
      "answer_score": 8,
      "created": "2023-10-20T22:49:24",
      "question_id": 77334106,
      "answer_id": 77812527
    }
  },
  {
    "question": "AttributeError: `np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead.. Did you mean: &#39;strings&#39;?",
    "expected_answer": "Your code seems to be written in numpy &lt; 2.0 compatible way, but it looks like you are running it with numpy &gt;=2. Try downgrading the numpy version to &lt; 2.0.",
    "context_chunks": [
      {
        "text": "I m interested in seeing neural network as graph using tensorboard. I have constructed a network in pytorch with following code- import torch BATCH_SIZE = 16 DIM_IN = 1000 HIDDEN_SIZE = 100 DIM_OUT = 10 class TinyModel(torch.nn.Module): def __init__(self): super(TinyModel, self).__init__() self.layer1 = torch.nn.Linear(DIM_IN, HIDDEN_SIZE) self.relu = torch.nn.ReLU() self.layer2 = torch.nn.Linear(HIDDEN_SIZE, DIM_OUT) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False) ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False) model = TinyModel() Setting-up tensorboard from torch.utils.tensorboard import SummaryWriter # Create a SummaryWriter writer = SummaryWriter(&quot;checkpoint&quot;) # Add the graph to TensorBoard writer.add_graph(model, some_input) writer.close() While I run tensorboard --logdir=checkpoint on terminal , I receive the following error - Traceback (most recent call last): File &quot;/home/k/python_venv/bin/tensorboard&quot;, line 5, in &lt;module&gt; from tensorboard.main import run_main File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/main.py&quot;, line 27, in &lt;module&gt; from tensorboard import default File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/default.py&quot;, line 39, in &lt;module&gt; from tensorboard.plugins.hparams import hparams_plugin File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/plugins/hparams/hparams_plugin.py&quot;, line 30, in &lt;module&gt; from tensorboard.plugins.hparams import backend_context File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/plugins/hparams/backend_context.py&quot;, line 26, in &lt;module&gt; from tensorboard.plugins.hparams import metadata File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/plugins/hparams/metadata.py&quot;, line 32, in &lt;module&gt; NULL_TENSOR = tensor_util.make_tensor_proto( File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/util/tensor_util.py&quot;, line 405, in make_tensor_proto numpy_dtype = dtypes.as_dtype(nparray.dtype) File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py&quot;, line 677, in as_dtype if type_value.type == np.string_ or type_value.type == np.unicode_: File &quot;/home/k/python_venv/lib/python3.10/site-packages/numpy/__init__.py&quot;, line 397, in __getattr__ raise AttributeError( AttributeError: `np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead.. Did you mean: 'strings'? Probably the issue will be fixed in future releases, but is there a fix for now?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Your code seems to be written in numpy &lt; 2.0 compatible way, but it looks like you are running it with numpy &gt;=2. Try downgrading the numpy version to &lt; 2.0.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This was caused by a compatibility issue between tensorboard 2.17 and numpy &gt;=2.0. See github issue here: https://github.com/tensorflow/tensorboard/issues/6874. If numpy 2.0 compatibility is desired, try updating tensorboard to 2.18.0 or higher.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "pytorch",
        "tensorboard"
      ],
      "question_score": 8,
      "answer_score": 4,
      "created": "2024-07-08T13:44:22",
      "question_id": 78721195,
      "answer_id": 78721296
    }
  },
  {
    "question": "Find minimum and maximum values in OHLC data",
    "expected_answer": "This is a straightforward solution by splitting each daily OHLC line into four (day, value) entries. Then we process each entry (order dependent on direction) while recording the local minima/maxima (&quot;peaks&quot;), merging continuous runs and skipping insignificant movements. There are two NamedTuple's: Entry (for a (day, value) pair) and Movement (for each line of the results). I could have used tuples, but NamedTuple's give clear names for each field. It also doesn't depend on numpy, pandas, or any other library, and the type hints help catch mistakes at compile time if used with a static checker like mypy. It should also be fairly fast for a pure-Python solution, as it computes all movements in one pass. from typing import Iterator, NamedTuple Entry = NamedTuple('Entry', [('value', float), ('date', str)]) Movement = NamedTuple('Movement', [('start', Entry), ('end', Entry), ('percentage', float)]) get_change = lambda a, b: (b.value - a.value) / a.value def get_movements(data_str: str, min_change_percent: float = 0.05) -&gt; Iterator[Movement]: &quot;&quot;&quot; Return all movements with changes above a threshold. &quot;&quot;&quot; peaks: list[Entry] = [] for line in data_str.strip().split('\\n'): # Read lines from input and split into date and values. date, open, high, low, close = line.split() # Order values according to movement direction. values_str = [open, low, high, close] if close &gt; open else [open, high, low, close] for value_str in values_str: entry = Entry(float(value_str), date) if len(peaks) &gt;= 2 and (entry &gt; peaks[-1]) == (peaks[-1] &gt; peaks[-2]): # Continue movement of same direction by replacing last peak. peaks[-1] = entry elif not peaks or abs(get_change(peaks[-1], entry)) &gt;= min_change_percent: # New peak is above minimum threshold. peaks.append(entry) # Convert every pair of remaining peaks to a `Movement`. for start, end in zip(peaks, peaks[1:]): yield Movement(start, end, percentage=get_change(start, end)) Usage for first example: data_str = &quot;&quot;&quot; 2023-07-02 0.12800000 0.12800000 0.12090000 0.12390000 2023-07-03 0.12360000 0.13050000 0.12220000 0.12830000 2023-07-04 0.12830000 0.12830000 0.12320000 0.12410000 2023-07-05 0.12410000 0.12530000 0.11800000 0.11980000 2023-07-06 0.11990000 0.12270000 0.11470000 0.11500000 &quot;&quot;&quot; for mov in get_movements(data_str, 0.05): print(f'{mov.start.date} {mov.start.value:.4f} {mov.end.date} {mov.end.value:.4f} {mov.percentage:.2%}') # 2023-07-02 0.1280 2023-07-02 0.1209 -5.55% # 2023-07-02 0.1209 2023-07-03 0.1305 7.94% # 2023-07-03 0.1305 2023-07-06 0.1147 -12.11% Usage for second example: data_str = &quot;&quot;&quot; 2022-02-25 38340.4200 39699.0000 38038.4600 39237.0600 2022-02-26 39237.0700 40300.0000 38600.4600 39138.1100 2022-02-27 39138.1100 39881.7700 37027.5500 37714.4300 2022-02-28 37714.4200 44200.0000 37468.2800 43181.2700 2022-03-01 43176.4100 44968.1300 42838.6800 44434.0900 &quot;&quot;&quot; for mov in get_movements(data_str, 0.03): print(f'{mov.start.date} {int(mov.start.value)} {mov.end.date} {int(mov.end.value)} {mov.percentage:.2%}') # 2022-02-25 38340 2022-02-26 40300 5.11% # 2022-02-26 40300 2022-02-26 38600 -4.22% # 2022-02-26 38600 2022-02-27 39881 3.32% # 2022-02-27 39881 2022-02-27 37027 -7.16% # 2022-02-27 37027 2022-02-28 44200 19.37% # 2022-02-28 44200 2022-03-01 42838 -3.08% # 2022-03-01 42838 2022-03-01 44968 4.97% The first result of the second example doesn't agree with the value you provided, but it's not clear to me why it started at 38038 instead of 38340. All other values match perfectly.",
    "context_chunks": [
      {
        "text": "I would like to find (in python) the local minimum and maximum values in OHLC data, under the condition that the distance between these values is at least +-5%. Temporal Condition Note that for an UP movement (close&gt;open), low price comes BEFORE high price for a DOWN movement (close&lt;open), low price comes AFTER high price The best way to explain what I would like to achieve is by a graphical example: OHLC data is in this format: open_time open high low close 2023-07-02 0.12800000 0.12800000 0.12090000 0.12390000 2023-07-03 0.12360000 0.13050000 0.12220000 0.12830000 2023-07-04 0.12830000 0.12830000 0.12320000 0.12410000 2023-07-05 0.12410000 0.12530000 0.11800000 0.11980000 2023-07-06 0.11990000 0.12270000 0.11470000 0.11500000 The result should be something like: date1 val1 date2 val2 &lt;---up date2 val2 date3 val3 &lt;---down date3 val3 date4 val4 &lt;---up date4 val4 date5 val5 &lt;---down . . . As for the data in the example the result should be: 2023-07-02 0.1280 2023-07-02 0.1209 -5.55% 2023-07-02 0.1209 2023-07-03 0.1305 7.94% 2023-07-03 0.1305 2023-07-06 0.1147 -12.11% Is there a name for this task? ADDENDUM I add a new example, with a different condition (+-3%). This is the data: 2022-02-25 38340.4200 39699.0000 38038.4600 39237.0600 2022-02-26 39237.0700 40300.0000 38600.4600 39138.1100 2022-02-27 39138.1100 39881.7700 37027.5500 37714.4300 2022-02-28 37714.4200 44200.0000 37468.2800 43181.2700 2022-03-01 43176.4100 44968.1300 42838.6800 44434.0900 And the final result shold be: 2022-02-25 38038 2022-02-26 40300 5.95% 2022-02-26 40300 2022-02-26 38600 -4.22% 2022-02-26 38600 2022-02-27 39881 3.32% 2022-02-27 39881 2022-02-27 37027 -7.16% 2022-02-27 37027 2022-02-28 44200 19.37% 2022-02-28 44200 2022-03-01 42838 -3.08%",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is a straightforward solution by splitting each daily OHLC line into four (day, value) entries. Then we process each entry (order dependent on direction) while recording the local minima/maxima (&quot;peaks&quot;), merging continuous runs and skipping insignificant movements. There are two NamedTuple's: Entry (for a (day, value) pair) and Movement (for each line of the results). I could have used tuples, but NamedTuple's give clear names for each field. It also doesn't depend on numpy, pandas, or any other library, and the type hints help catch mistakes at compile time if used with a static checker like mypy. It should also be fairly fast for a pure-Python solution, as it computes all movements in one pass. from typing import Iterator, NamedTuple Entry = NamedTuple('Entry', [('value', float), ('date', str)]) Movement = NamedTuple('Movement', [('start', Entry), ('end', Entry), ('percentage', float)]) get_change = lambda a, b: (b.value - a.value) / a.value def get_movements(data_str: str, min_change_percent: float = 0.05) -&gt; Iterator[Movement]: &quot;&quot;&quot; Return all movements with changes above a threshold. &quot;&quot;&quot; peaks: list[Entry] = [] for line in data_str.strip().split('\\n'): # Read lines from input and split into date and values. date, open, high, low, close = line.split() # Order values according to movement direction. values_str = [open, low, high, close] if close &gt; open else [open, high, low, close] for value_str in values_str: entry = Entry(float(value_str), date) if len(peaks) &gt;= 2 and (entry &gt; peaks[-1]) == (peaks[-1] &gt; peaks[-2]): # Continue movement of same direction by replacing last peak. peaks[-1] = entry elif not peaks or abs(get_change(peaks[-1], entry)) &gt;= min_change_percent: # New peak is above minimum threshold. peaks.append(entry) # Convert every pair of remaining peaks to a `Movement`. for start, end in zip(peaks, peaks[1:]): yield Movement(start, end, percentage=get_change(start, end)) Usage for first example: data_str = &quot;&quot;&quot; 2023-07-02 0.12800000 0.12800000 0.12090000 0.12390000 2023-07-03 0.12360000 0.13050000 0.12220000 0.12830000 2023-07-04 0.12830000 0.12830000 0.12320000 0.12410000 2023-07-05 0.12410000 0.12530000 0.11800000 0.11980000 2023-07-06 0.11990000 0.12270000 0.11470000 0.11500000 &quot;&quot;&quot; for mov in get_movements(data_str, 0.05): print(f'{mov.start.date} {mov.start.value:.4f} {mov.end.date} {mov.end.value:.4f} {mov.percentage:.2%}') # 2023-07-02 0.1280 2023-07-02 0.1209 -5.55% # 2023-07-02 0.1209 2023-07-03 0.1305 7.94% # 2023-07-03 0.1305 2023-07-06 0.1147 -12.11% Usage for second example: data_str = &quot;&quot;&quot; 2022-02-25 38340.4200 39699.0000 38038.4600 39237.0600 2022-02-26 39237.0700 40300.0000 38600.4600 39138.1100 2022-02-27 39138.1100 39881.7700 37027.5500 37714.4300 2022-02-28 37714.4200 44200.0000 37468.2800 43181.2700 2022-03-01 43176.4100 44968.1300 42838.6800 44434.0900 &quot;&quot;&quot; for mov in get_movements(data_str, 0.03): print(f'{mov.start.date} {int(mov.start.value)} {mov.end.date} {int(mov.end.value)} {mov.percentage:.2%}') # 2022-02-25 38340 2022-02-26 40300 5.11% # 2022-02-26 40300 2022-02-26 38600 -4.22% # 2022-02-26 38600 2022-02-27 39881 3.32% # 2022-02-27 39881 2022-02-27 37027 -7.16% # 2022-02-27 37027 2022-02-28 44200 19.37% # 2022-02-28 44200 2022-03-01 42838 -3.08% # 2022-03-01 42838 2022-03-01 44968 4.97% The first result of the second example doesn't agree with the value you provided, but it's not clear to me why it started at 38038 instead of 38340. All other values match perfectly.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I sat out determined to give this a go with using as much pandas as possible. I couldn't figure out a better way than @BoppreH to actually implement the business logic of the peak determination. I create a configurable filter to be applied to the rows of the DataFrame with a decorator for state storage: def min_percent_change_filter(min_change_percent=0.05): peaks = [] get_change = lambda a, b: (b - a) / a def add_entry(row): &quot;&quot;&quot;By @BoppreH, with slight modifications Update list of peaks with one new entry.&quot;&quot;&quot; if len(peaks) &gt;= 2 and (row[&quot;data&quot;] &gt; peaks[-1][&quot;data&quot;]) == ( peaks[-1][&quot;data&quot;] &gt; peaks[-2][&quot;data&quot;] ): # Continue movement of same direction by replacing last peak. peaks[-1] = row.copy() return peaks elif ( not peaks or abs(get_change(peaks[-1][&quot;data&quot;], row[&quot;data&quot;])) &gt;= min_change_percent ): # New peak is above minimum threshold. peaks.append(row.copy()) return peaks return peaks return add_entry The pandas part requires quite some manipulation to get the data into the right shape. After it's in the right shape, we apply the filter across rows. Finally we put the DataFrame in the desired output format: import pandas as pd def pandas_approach(data, min_pct_change): df = pd.DataFrame(data) df[&quot;open_time&quot;] = pd.to_datetime(df[&quot;open_time&quot;]) # Respect termporal aspect, create new columns first and second # set them to the respective value depending on whether we're # moving down or up df[&quot;first&quot;] = df[&quot;low&quot;].where(df[&quot;open&quot;] &lt;= df[&quot;close&quot;], df[&quot;high&quot;]) df[&quot;second&quot;] = df[&quot;high&quot;].where(df[&quot;open&quot;] &lt;= df[&quot;close&quot;], df[&quot;low&quot;]) # Create a new representation of the data, by stacking first and second # on the index, then sorting by 'open_time' and whether it came first # or second (Note: assert 'first' &lt; 'second') stacked_representation = ( df.set_index(&quot;open_time&quot;)[[&quot;first&quot;, &quot;second&quot;]] .stack() .reset_index() .sort_values([&quot;open_time&quot;, &quot;level_1&quot;])[[&quot;open_time&quot;, 0]] ) stacked_representation.columns = [&quot;open_time&quot;, &quot;data&quot;] # Now we can go to work with our filter results = pd.DataFrame( stacked_representation.apply(min_percent_change_filter(min_pct_change), axis=1)[ 0 ] ) # We reshape /rename/reorder our data to fit the desired output format results[&quot;begin&quot;] = results[&quot;data&quot;].shift() results[&quot;begin_date&quot;] = results[&quot;open_time&quot;].shift() results = results.dropna()[[&quot;begin_date&quot;, &quot;begin&quot;, &quot;open_time&quot;, &quot;data&quot;]] results.columns = [&quot;begin_date&quot;, &quot;begin&quot;, &quot;end_date&quot;, &quot;end&quot;] # Lastly add the pct change results[&quot;pct_change&quot;] = (results.end - results.begin) / results.begin # This returns the styler for output formatting purposes, but you can return the # DataFrame instead by commenting/deleting it def format_datetime(dt): return pd.to_datetime(dt).strftime(&quot;%Y-%m-%d&quot;) def price_formatter(value): return &quot;{:.4f}&quot;.format(value) if abs(value) &lt; 10000 else &quot;{:.0f}&quot;.format(value) return results.style.format( { &quot;pct_change&quot;: &quot;{:,.2%}&quot;.format, &quot;begin_date&quot;: format_datetime, &quot;end_date&quot;: format_datetime, &quot;begin&quot;: price_formatter, &quot;end&quot;: price_formatter, } ) Output for the first example:: import pandas as pd data = { &quot;open_time&quot;: [&quot;2023-07-02&quot;, &quot;2023-07-03&quot;, &quot;2023-07-04&quot;, &quot;2023-07-05&quot;, &quot;2023-07-06&quot;], &quot;open&quot;: [0.12800000, 0.12360000, 0.12830000, 0.12410000, 0.11990000], &quot;high&quot;: [0.12800000, 0.13050000, 0.12830000, 0.12530000, 0.12270000], &quot;low&quot;: [0.12090000, 0.12220000, 0.12320000, 0.11800000, 0.11470000], &quot;close&quot;: [0.12390000, 0.12830000, 0.12410000, 0.11980000, 0.11500000], } pandas_approach(data,0.05) begin_date begin end_date end pct_change 1 2023-07-02 0.1280 2023-07-02 0.1209 -5.55% 3 2023-07-02 0.1209 2023-07-03 0.1305 7.94% 9 2023-07-03 0.1305 2023-07-06 0.1147 -12.11% Output for the second example: data_2 = { &quot;open_time&quot;: [&quot;2022-02-25&quot;, &quot;2022-02-26&quot;, &quot;2022-02-27&quot;, &quot;2022-02-28&quot;, &quot;2022-03-01&quot;], &quot;open&quot;: [38340.4200, 39237.0700, 39138.1100, 37714.4200, 43176.4100], &quot;high&quot;: [39699.0000, 40300.0000, 39881.7700, 44200.0000, 44968.1300], &quot;low&quot;: [38038.4600, 38600.4600, 37027.5500, 37468.2800, 42838.6800], &quot;close&quot;: [39237.0600, 39138.1100, 37714.4300, 43181.2700, 44434.0900], } pandas_approach(data_2, 0.03) begin_date begin end_date end pct_change 2 2022-02-25 38038 2022-02-26 40300 5.95% 3 2022-02-26 40300 2022-02-26 38600 -4.22% 4 2022-02-26 38600 2022-02-27 39882 3.32% 5 2022-02-27 39882 2022-02-27 37028 -7.16% 7 2022-02-27 37028 2022-02-28 44200 19.37% 8 2022-02-28 44200 2022-03-01 42839 -3.08% 9 2022-03-01 42839 2022-03-01 44968 4.97%",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "time",
        "time-series",
        "ohlc"
      ],
      "question_score": 8,
      "answer_score": 4,
      "created": "2023-07-06T13:20:58",
      "question_id": 76629307,
      "answer_id": 76643981
    }
  },
  {
    "question": "Tensorflow 2.13.1, no matching distribution found for tensorflow-text 2.13.0",
    "expected_answer": "I resolved the issue, by installing Python 3.10 Tensorflow 2.13.0 tensorflow-text 2.13.0 tensorflow-models-official 2.13.1 Everything works with these versions, but I did not find a way to make it work with Python 3.11 atm. The issue is best described here (https://github.com/yaml/pyyaml/issues/724) and has to do with Cython and PyYAML 5.4 dependency issues.",
    "context_chunks": [
      {
        "text": "I am trying to install the latest Tensorflow models 2.13.1 (pip install tf-models-official==2.13.1), with Python 3.11. There seems to be an issue with Cython and PyYAML not playing nice together since last week in Tensorflow models 2.13.0, so it won't install. But 2.13.1 is giving me an error that the corresponding tensorflow-text version 2.13.0 is not found. The error I am receiving is as follows: (tensorflow-env) username@DESKTOP:~/projects/tensorflow/models-master/research$ pip install tf-models-official==2.13.1 INFO: pip is looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while. ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python &gt;=3.7,&lt;3.11; 1.21.3 Requires-Python &gt;=3.7,&lt;3.11; 1.21.4 Requires-Python &gt;=3.7,&lt;3.11; 1.21.5 Requires-Python &gt;=3.7,&lt;3.11; 1.21.6 Requires-Python &gt;=3.7,&lt;3.11; 1.6.2 Requires-Python &gt;=3.7,&lt;3.10; 1.6.3 Requires-Python &gt;=3.7,&lt;3.10; 1.7.0 Requires-Python &gt;=3.7,&lt;3.10; 1.7.1 Requires-Python &gt;=3.7,&lt;3.10; 1.7.2 Requires-Python &gt;=3.7,&lt;3.11; 1.7.3 Requires-Python &gt;=3.7,&lt;3.11; 1.8.0 Requires-Python &gt;=3.8,&lt;3.11; 1.8.0rc1 Requires-Python &gt;=3.8,&lt;3.11; 1.8.0rc2 Requires-Python &gt;=3.8,&lt;3.11; 1.8.0rc3 Requires-Python &gt;=3.8,&lt;3.11; 1.8.0rc4 Requires-Python &gt;=3.8,&lt;3.11; 1.8.1 Requires-Python &gt;=3.8,&lt;3.11 ERROR: Could not find a version that satisfies the requirement tensorflow-text~=2.13.0 (from tf-models-official) (from versions: 2.12.0rc0, 2.12.0, 2.12.1, 2.13.0rc0) ERROR: No matching distribution found for tensorflow-text~=2.13.0 But the release history on pypi.org shows that the 2.13.0 version of tensorflow-text is out: https://pypi.org/project/tensorflow-text/2.13.0/#history What am I doing wrong?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I resolved the issue, by installing Python 3.10 Tensorflow 2.13.0 tensorflow-text 2.13.0 tensorflow-models-official 2.13.1 Everything works with these versions, but I did not find a way to make it work with Python 3.11 atm. The issue is best described here (https://github.com/yaml/pyyaml/issues/724) and has to do with Cython and PyYAML 5.4 dependency issues.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I faced the exact same problem, when trying to install the mediapipe-model-maker pip package. It turns out that the maintainers of tensorflow-text stopped providing pre-built python wheels for several operating systems (now supporting only Linux or Intel-based macs). That's why pip install can't find a matching distribution for tensorflow-text~=2.13.0. Instead, you must either build it youself or rely on Official Build Collaborators, who generously build and upload them for us. For instance, for Arm-based macs you can download and manually install the whl file from this Github repository.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "tensorflow",
        "pip",
        "tensorflow2.0"
      ],
      "question_score": 8,
      "answer_score": 3,
      "created": "2023-07-25T08:11:14",
      "question_id": 76760682,
      "answer_id": 76763236
    }
  },
  {
    "question": "Randomly getting &quot;ValueError: PyCapsule_New called with null pointer&quot; when making plots with Python 3.12 fresh install on new PC",
    "expected_answer": "I encountered this issue with matplotlib 3.9.0. It's a known bug and the advice is to either wait for the next update or install matplotlib from the github repo. I did the latter and it resolved the issue. For anyone pip installing from a requirements.txt file, simply replace matplotlib with matplotlib@git+https://github.com/matplotlib/matplotlib.git.",
    "context_chunks": [
      {
        "text": "I use a suite of python 3.7 code for my work, generating various plots and fitting lines. It works perfectly fine on my old windows 10 machine. However, I just got a new laptop and installed 3.12 on it. The code still works, but SOMETIMES it randomly decides to fail on a plot, giving &quot;ValueError: PyCapsule_New called with null pointer&quot; originating from a simple line of code that I use to initialize new plots: plt.figure(figsize=(figx,figy)) #generate clean figure. I can see no rhyme or reason as to why it sometimes works and occasionally fails. In a few instances, I've literally just changed the order of certain plots and it works just fine. In others, I change minor details like the initial guess for the center of a gaussian for a line fit (by one index), and it works just fine all of a sudden. I'm at a loss as to how to move forward with this, as the problem seems to be some random, arcane issue buried deep within matplotlib or python or something, clearly relating to something with the fresh install on my new machine. Any suggestions? Full error below: File &quot;D:\\astra\\ASTRA MS\\Massify.py&quot;, line 435, in &lt;module&gt; ASTRA_MS.plot_set(x_m,t,y_ms,y_blur,y_superblur,y_megablur,figtext,titletext,shotnum,plotstyle=plotstyle,savepath=path,falling=falling,vlines=vlines,vacc=vacc,xlim=xlim,showplots=showplots,invertplot=invertplot,metadata=str(values)) File &quot;D:\\astra\\ASTRA MS\\ASTRA_MS.py&quot;, line 259, in plot_set plt.figure(figsize=(figx,figy)) #generate clean figure File &quot;C:\\Users\\zachu\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\pyplot.py&quot;, line 1022, in figure manager = new_figure_manager( File &quot;C:\\Users\\zachu\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\pyplot.py&quot;, line 545, in new_figure_manager return _get_backend_mod().new_figure_manager(*args, **kwargs) File &quot;C:\\Users\\zachu\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\backend_bases.py&quot;, line 3521, in new_figure_manager return cls.new_figure_manager_given_figure(num, fig) File &quot;C:\\Users\\zachu\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\backend_bases.py&quot;, line 3526, in new_figure_manager_given_figure return cls.FigureCanvas.new_manager(figure, num) File &quot;C:\\Users\\zachu\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\backend_bases.py&quot;, line 1811, in new_manager return cls.manager_class.create_with_canvas(cls, figure, num) File &quot;C:\\Users\\zachu\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\backends\\_backend_tk.py&quot;, line 479, in create_with_canvas with _restore_foreground_window_at_end(): File &quot;C:\\Program Files\\Python312\\Lib\\contextlib.py&quot;, line 137, in __enter__ return next(self.gen) File &quot;C:\\Users\\zachu\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\backends\\_backend_tk.py&quot;, line 43, in _restore_foreground_window_at_end foreground = _c_internal_utils.Win32_GetForegroundWindow() ValueError: PyCapsule_New called with null pointer",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I encountered this issue with matplotlib 3.9.0. It's a known bug and the advice is to either wait for the next update or install matplotlib from the github repo. I did the latter and it resolved the issue. For anyone pip installing from a requirements.txt file, simply replace matplotlib with matplotlib@git+https://github.com/matplotlib/matplotlib.git.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "What are you using to run your code? IDLE? Anaconda? Something else? Was using matplotlib and seaborn for plotting scatterplots within a loop, and it kept giving me the error “ValueError: PyCapsule_New called with null pointer”. Couldn’t figure out how to fix it, I was running the code in Python IDLE. SOMETIMES a few of the graphs would show properly and then randomly would stop working for the second or third graph and give me that error. When I switched to using Anaconda Jupyter Lab for running the code, this resolved the issue right away! Suddenly it was able to plot the graphs just fine. If you’re using Python IDLE, would recommend switching to Anaconda. Download Anaconda, go to Anaconda Navigator, open Jupyter Lab, navigate to your folder where your data is located, open a new Jupyter notebook file, and copy paste your code to press the “play” (run) button there.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "matplotlib"
      ],
      "question_score": 8,
      "answer_score": 3,
      "created": "2024-05-30T21:29:35",
      "question_id": 78557321,
      "answer_id": 78707094
    }
  },
  {
    "question": "Managing ALLOWED_HOSTS in Django for Kubernetes health check",
    "expected_answer": "The ALLOWED_HOSTS setting checks the Host header sent on an HTTP request, so you can simply configure the headers sent by your liveliness probe: livenessProbe: httpGet: path: /health/ port: 8000 httpHeaders: - name: host value: your.hostname.here # Configure the appropriate host here initialDelaySeconds: 15 timeoutSeconds: 5",
    "context_chunks": [
      {
        "text": "I have a Django application running on Kubernetes, using an API for health checks. The issue I'm facing is that every time the IP associated with Django in Kubernetes changes, I have to manually update ALLOWED_HOSTS. django code: class HealthViewSet(ViewSet): @action(methods=['GET'], detail=False) def health(self, request): try: return Response('OK', status=status.HTTP_200_OK) except Exception as e: print(e) return Response({'response': 'Internal server error'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR) deployment code : livenessProbe: httpGet: path: /health/ port: 8000 initialDelaySeconds: 15 timeoutSeconds: 5 Error: Invalid HTTP_HOST header: '192.168.186.79:8000'. You may need to add '192.168.186.79' to ALLOWED_HOSTS. Traceback (most recent call last): File &quot;/usr/local/lib/python3.11/site-packages/django/core/handlers/exception.py&quot;, line 55, in inner response = get_response(request) ^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/local/lib/python3.11/site-packages/django/utils/deprecation.py&quot;, line 135, in __call__ response = self.process_request(request) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/local/lib/python3.11/site-packages/django/middleware/common.py&quot;, line 48, in process_request host = request.get_host() ^^^^^^^^^^^^^^^^^^ File &quot;/usr/local/lib/python3.11/site-packages/django/http/request.py&quot;, line 148, in get_host raise DisallowedHost(msg) django.core.exceptions.DisallowedHost: Invalid HTTP_HOST header: '192.168.186.79:8000'. You may need to add '192.168.186.79' to ALLOWED_HOSTS. Bad Request: /health/ Is there a way to dynamically use ALLOWED_HOSTS and avoid manual updates? (Every deployment IP changed.) ALLOWED_HOSTS ALLOWED_HOSTS = [localhost', '127.0.0.1'] Any guidance or suggestions for the best solution in this regard would be appreciated.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The ALLOWED_HOSTS setting checks the Host header sent on an HTTP request, so you can simply configure the headers sent by your liveliness probe: livenessProbe: httpGet: path: /health/ port: 8000 httpHeaders: - name: host value: your.hostname.here # Configure the appropriate host here initialDelaySeconds: 15 timeoutSeconds: 5",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can use a middleware to selectively disable the allowed_hosts check for the url you use for health checks.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "django",
        "kubernetes",
        "devops"
      ],
      "question_score": 8,
      "answer_score": 6,
      "created": "2024-01-15T18:16:59",
      "question_id": 77821648,
      "answer_id": 78224932
    }
  },
  {
    "question": "error: command &#39;C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX86\\x64\\cl.exe&#39; failed with exitcode2",
    "expected_answer": "After fixing &quot;Microsoft Visual C++ 14.0 is required error&quot;, I got the same issue while installing Twilio. But this works for me. for python 3.11/12: pip install aiohttp==3.9.0b0 And then try again, pip install [package] Reference: ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects",
    "context_chunks": [
      {
        "text": "so I was trying to install this package Eur-Lex on Python and had this error : error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.36.32532\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2 [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for pandas Failed to build pandas ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects I tried to find a solution on blogs or forums but it didn't work. I only have basic knowledge of Python. First of all, it was the &quot;Microsoft Visual C++ 14.0 is required error&quot; so I downloaded Build Tools and it works. Also tried to re-install Python after VSCode or Spyder but didn't work. Python version : 3.8.10 Any insights ?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "After fixing &quot;Microsoft Visual C++ 14.0 is required error&quot;, I got the same issue while installing Twilio. But this works for me. for python 3.11/12: pip install aiohttp==3.9.0b0 And then try again, pip install [package] Reference: ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This happened to me. I was trying to install the dcurves package. I tried several approaches including pip install aiohttp==3.9.0b0 and python -m pip install --upgrade pip setuptools wheel, however neither worked. Downgrading from python 3.12 to 3.11 solved the problem.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "visual-c++"
      ],
      "question_score": 8,
      "answer_score": 4,
      "created": "2023-07-25T15:04:53",
      "question_id": 76764042,
      "answer_id": 77465686
    }
  },
  {
    "question": "Generate 4 random numbers between -1.0 and 1.0 such their sum is 1 using python",
    "expected_answer": "From a quick study: import numpy as np random_state = np.random.RandomState(98238) n_samples = 100_000 n_dim = 4 numbers = random_state.uniform(-1.0, 1.0, (n_samples, n_dim - 1)) last_number = 1 - np.sum(numbers, axis=1, keepdims=True) is_valid = (-1.0 &lt;= last_number) &amp; (last_number &lt;= 1.0) samples = np.append(numbers, last_number, axis=1)[is_valid[:, 0]] print(f&quot;Acceptance ratio {is_valid.sum() / n_samples:.2f}&quot;) Which gives: Acceptance ratio 0.48 You can find that your acceptance ratio for the rejection sampling method you proposed is around 0.48. So on average the approach would be by a factor of ~2 worse compared to a perfect direct sampling method. This is not bad, given that the method is very simple. I would suggest to keep your method and change to the vectorized version I showed above if you need more than a single sample.",
    "context_chunks": [
      {
        "text": "I am trying generate 4 random numbers between -1.0 and 1.0 such that their sum is 1 using python. I initially looked at the dirichlet function in numpy but that only works for positive numbers. One other way I can think of is: def generate_random_numbers(): numbers = np.random.uniform(-1.0, 1.0, 3) last_number = 1 - np.sum(numbers) if -1.0 &lt;= last_number &lt;= 1.0: return np.append(numbers, last_number) else: return generate_random_numbers() However its not that efficient. Any other way to do this?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "From a quick study: import numpy as np random_state = np.random.RandomState(98238) n_samples = 100_000 n_dim = 4 numbers = random_state.uniform(-1.0, 1.0, (n_samples, n_dim - 1)) last_number = 1 - np.sum(numbers, axis=1, keepdims=True) is_valid = (-1.0 &lt;= last_number) &amp; (last_number &lt;= 1.0) samples = np.append(numbers, last_number, axis=1)[is_valid[:, 0]] print(f&quot;Acceptance ratio {is_valid.sum() / n_samples:.2f}&quot;) Which gives: Acceptance ratio 0.48 You can find that your acceptance ratio for the rejection sampling method you proposed is around 0.48. So on average the approach would be by a factor of ~2 worse compared to a perfect direct sampling method. This is not bad, given that the method is very simple. I would suggest to keep your method and change to the vectorized version I showed above if you need more than a single sample.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This is massively indebted to Mark Dickinson's answer in the previously-linked thread (Generate random numbers summing to a predefined value ). However, this is different insofar as the answer requires floats (so random.sample won't work), involves negative numbers, and, as far as I can see, you would have to reject some groupings and recurse occasionally. Basically, I: change the range to [0,2] by having an intermediate variable Y=X+1; everything is then positive (or 0); change the intended sum to 1+4 (or, in general, to S+n, where n is number of variables) As Mark Dickinson does, I find the DIVIDERS - in this case by uniform random variables in the range 0 to S+n, subsequently sorted. Then the intended Y values are the gaps between the dividers. I have to do a final check that the largest gap, or value of Y, doesn't exceed 2 (and recurse if it does). Finally, revert to X=Y-1 import numpy as np def generate_random_numbers( S, n ): dividers = np.concatenate( ( [0], np.sort( np.random.uniform( 0.0, S + n, n - 1 ) ), [S+n] ) ) y = np.diff( dividers ) if np.max( y ) &gt; 2: return generate_random_numbers( S, n ) return y - 1 print( generate_random_numbers( 1.0, 4 ) ) Sample output: [ 0.69312962 -0.01597064 0.77286876 -0.45002774] A bit of testing with more samples shows that the averages are OK (0.25 for each variable) but the rejection rate is annoyingly high.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "random"
      ],
      "question_score": 8,
      "answer_score": 3,
      "created": "2025-03-10T06:24:36",
      "question_id": 79497170,
      "answer_id": 79498908
    }
  },
  {
    "question": "Best way to count tokens for Anthropic Claude Models using the API?",
    "expected_answer": "From Anthropic's Python SDK: def count_tokens( self, text: str, ) -&gt; int: &quot;&quot;&quot;Count the number of tokens in a given string. Note that this is only accurate for older models, e.g. `claude-2.1`. For newer models this can only be used as a _very_ rough estimate, instead you should rely on the `usage` property in the response for exact counts. &quot;&quot;&quot; # Note: tokenizer is untyped tokenizer = self.get_tokenizer() encoded_text = tokenizer.encode(text) # type: ignore return len(encoded_text.ids) # type: ignore So I'd say that you should expect better results from anthropic_bedrock, but if you are able to find the code to compare to this one, it would be best. In other places, I've seen the suggestion to use the ChatGPT-4 token counter to have another estimate of the number of tokens, you could try it too.",
    "context_chunks": [
      {
        "text": "Summary How can I count the number of tokens before sending it to Anthropic? Question content I'm working with Anthropic's Claude models and need to accurately count the number of tokens in my prompts and responses. I'm using the anthropic_bedrock Python client but recently came across an alternative method using the anthropic client. I'm looking for advice on which approach is better and the proper way to implement token counting. Here are the two approaches I've found: Approach 1: Using anthropic_bedrock Client from anthropic_bedrock import AnthropicBedrock client = AnthropicBedrock() prompt = &quot;Hello, world!&quot; token_count = client.count_tokens(prompt) print(token_count) Approach 2: Using anthropic Client import anthropic client = anthropic.Client() token_count = client.count_tokens(&quot;Sample text&quot;) print(token_count) My Questions: Which client (anthropic_bedrock or anthropic) is better for counting tokens in prompts for Claude models? Are there any significant differences in how these clients handle token counting or any other functionalities that might influence the choice? Are there best practices I should follow when counting tokens and managing token usage in my applications? Steps to Reproduce: Install the appropriate client (anthropic_bedrock or anthropic) using pip. Authenticate the client with your AWS credentials. Use the count_tokens method to count tokens in a given prompt. Print or log the token count for analysis. References: Discord link to their get-help the question Anthropic Token Counter Documentation Anthropic API Documentation Using the Anthropic API with Python They give different outputs! &gt;&gt;&gt; client = AnthropicBedrock() prompt = &quot;Hello, world!&quot; token_count = client.count_tokens(prompt) print(token_count)&gt;&gt;&gt; prompt = &quot;Hello, world!&quot; &gt;&gt;&gt; token_count = client.count_tokens(prompt) &gt;&gt;&gt; print(token_count) 4 &gt;&gt;&gt; import anthropic Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'anthropic' &gt;&gt;&gt; &gt;&gt;&gt; client = anthropic.Client() Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; NameError: name 'anthropic' is not defined &gt;&gt;&gt; &gt;&gt;&gt; token_count = client.count_tokens(&quot;Sample text&quot;) &gt;&gt;&gt; print(token_count) 2 2 vs 4. Which one to use?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "From Anthropic's Python SDK: def count_tokens( self, text: str, ) -&gt; int: &quot;&quot;&quot;Count the number of tokens in a given string. Note that this is only accurate for older models, e.g. `claude-2.1`. For newer models this can only be used as a _very_ rough estimate, instead you should rely on the `usage` property in the response for exact counts. &quot;&quot;&quot; # Note: tokenizer is untyped tokenizer = self.get_tokenizer() encoded_text = tokenizer.encode(text) # type: ignore return len(encoded_text.ids) # type: ignore So I'd say that you should expect better results from anthropic_bedrock, but if you are able to find the code to compare to this one, it would be best. In other places, I've seen the suggestion to use the ChatGPT-4 token counter to have another estimate of the number of tokens, you could try it too.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It looks like anthropic-bedrock (https://github.com/anthropics/anthropic-bedrock-python/tree/main) is no longer an actively maintained repo. It has been merged into the main anthropic repo at ( https://github.com/anthropics/anthropic-sdk-python/tree/main)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "anthropic"
      ],
      "question_score": 8,
      "answer_score": 1,
      "created": "2024-07-19T01:47:58",
      "question_id": 78767238,
      "answer_id": 78782599
    }
  },
  {
    "question": "PyLance in Visual Studio Code does not recognise Poetry virtual env dependencies",
    "expected_answer": "Some other people were having the same issue. Pylance does not show auto import information from site-packages directory #3281 Pylance not indexing all files and symbols for sqlalchemy even with package depth of 4 Visual Studio Code's PyLance implementation seems to have some internal limits that may prevent indexing all files. However, this was not the case for me. Instead, PyLance was somehow corrupted. Running: PyLance: Clear all persistent indices from the command palette fixed the issue for. After this, PyLance seemed to behave.",
    "context_chunks": [
      {
        "text": "I am using Poetry to manage a Python project. I create a virtual environment for Poetry using a normal poetry install and pyproject.toml workflow. Visual Studio Code and its PyLance does not pick up project dependencies in Jupyter Notebook. Python stdlib modules are recognised The modules of my application are recognised The modules in the dependencies and libraries my application uses are not recognised Instead, you get an error Import &quot;xxx&quot; could not be resolved Pylance (reportMissingImports) An example screenshot with some random imports that show what is recognised and what is not (tradeexecutor package is Poetry project, then some random Python packages dependency are not recognised).: The notebook still runs fine within Visual Studio Code, so the problem is specific to PyLance, the virtual environment is definitely correctly set up. Some Python Language Server output (if relevant): 2024-03-01 10:15:40.628 [info] [Info - 10:15:40] (28928) Starting service instance &quot;trade-executor&quot; 2024-03-01 10:15:40.656 [info] [Info - 10:15:40] (28928) Setting pythonPath for service &quot;trade-executor&quot;: &quot;/Users/moo/code/ts/trade-executor&quot; 2024-03-01 10:15:40.657 [info] [Info - 10:15:40] (28928) Setting environmentName for service &quot;trade-executor&quot;: &quot;3.10.13 (trade-executor-8Oz1GdY1-py3.10 venv)&quot; 2024-03-01 10:15:40.657 [info] [Info - 10:15:40] (28928) Loading pyproject.toml file at /Users/moo/code/ts/trade-executor/pyproject.toml 2024-03-01 10:15:40.657 [info] [Info - 10:15:40] (28928) Pyproject file &quot;/Users/moo/code/ts/trade-executor/pyproject.toml&quot; has no &quot;[tool.pyright]&quot; section. 2024-03-01 10:15:41.064 [info] [Info - 10:15:41] (28928) Found 763 source files 2024-03-01 10:15:41.158 [info] [Info - 10:15:41] (28928) Background analysis(4) root directory: file:///Users/moo/.vscode/extensions/ms-python.vscode-pylance-2024.2.2/dist 2024-03-01 10:15:41.158 [info] [Info - 10:15:41] (28928) Background analysis(4) started 2024-03-01 10:15:41.411 [info] [Info - 10:15:41] (28928) Indexer background runner(5) root directory: file:///Users/moo/.vscode/extensions/ms-python.vscode-pylance-2024.2.2/dist (index) 2024-03-01 10:15:41.411 [info] [Info - 10:15:41] (28928) Indexing(5) started 2024-03-01 10:15:41.662 [info] [Info - 10:15:41] (28928) scanned(5) 1 files over 1 exec env 2024-03-01 10:15:42.326 [info] [Info - 10:15:42] (28928) indexed(5) 1 files over 1 exec Also looks like PyLance correctly finds the virtual environment in the earlier Python Language Server output: 2024-03-03 19:36:56.784 [info] [Info - 19:36:56] (41658) Pylance language server 2024.2.2 (pyright version 1.1.348, commit cfb1de0c) starting 2024-03-03 19:36:56.789 [info] [Info - 19:36:56] (41658) Server root directory: file:///Users/moo/.vscode/extensions/ms-python.vscode-pylance-2024.2.2/dist 2024-03-03 19:36:56.789 [info] [Info - 19:36:56] (41658) Starting service instance &quot;trade-executor&quot; 2024-03-03 19:36:57.091 [info] [Info - 19:36:57] (41658) Setting pythonPath for service &quot;trade-executor&quot;: &quot;/Users/moo/Library/Caches/pypoetry/virtualenvs/trade-executor-8Oz1GdY1-py3.10/bin/python&quot; 2024-03-03 19:36:57.093 [info] [Info - 19:36:57] (41658) Setting environmentName for service &quot;trade-executor&quot;: &quot;3.10.13 (trade-executor-8Oz1GdY1-py3.10 venv)&quot; 2024-03-03 19:36:57.096 [info] [Info - 19:36:57] (41658) Loading pyproject.toml file at /Users/moo/code/ts/trade-executor/pyproject.toml How to diagnose the issue further and then fix the issue?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Some other people were having the same issue. Pylance does not show auto import information from site-packages directory #3281 Pylance not indexing all files and symbols for sqlalchemy even with package depth of 4 Visual Studio Code's PyLance implementation seems to have some internal limits that may prevent indexing all files. However, this was not the case for me. Instead, PyLance was somehow corrupted. Running: PyLance: Clear all persistent indices from the command palette fixed the issue for. After this, PyLance seemed to behave.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For me I was able to resolve this by selecting the interpreter in VSCode that matched the path. This was actually not the recommended interpreter so it was not the most intuitive. I ran ctrl, shift, p to open the palette and then up above chose the interpreter that had been created for my project specifically and not the global. This has been a problem for two projects so far so I assume it is fairly common. This fixed my error both times.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code",
        "python-poetry",
        "pylance"
      ],
      "question_score": 8,
      "answer_score": 2,
      "created": "2024-03-03T18:22:15",
      "question_id": 78097487,
      "answer_id": 78130688
    }
  },
  {
    "question": "How to efficiently carry out a priority-based $or query in MongoDB with 10 billion records?",
    "expected_answer": "You can achieve this with some Boolean reduction. For starters, since number: { $exists: true } is in all the criteria, separate that from $or clauses and use $and to include it. Like: data.find({ $and: [ { number: { $exists: true } }, { $or: [ { prefix: prefixChar, latter: latterword } ... // all the other criteria here ] } ] }) In the 3rd &amp; 4th clauses, the difference is only latter: { $regex: `^${latterword[0]}${latterword[1]}` } vs latter: { $regex: `^${latterword[0]}` } So the 4th case already covers the 3rd case and is more broad, so the 3rd case can be removed. Now, your remaining query is this. For a regex to match, or for a string to match - as required by the remaining clauses - the field has to exist. So the exists requirements can be moved into the first number-exists check anyway. Now the you're left with these in the $or part: { $or: [ { prefix: &quot;prefixChar&quot;, latter: &quot;latterword&quot; }, { latter: &quot;latterword&quot; }, { prefix: &quot;prefixChar&quot;, latter: { $regex: `^${latterword[0]}` } }, { prefix: &quot;prefixChar&quot; } ] } Excluding the regex, that means either prefix matches or latter matches, or both but that's covered in the either case already: { $or: [ { latter: &quot;latterword&quot; }, { prefix: &quot;prefixChar&quot;, latter: { $regex: `^${latterword[0]}` } }, { prefix: &quot;prefixChar&quot; } ] } So, finally it's: data.find({ $and: [ { prefix: { $exists: true }, number: { $exists: true }, latter: { $exists: true } }, { $or: [ { latter: &quot;latterword&quot; }, { prefix: &quot;prefixChar&quot;, latter: { $regex: `^${latterword[0]}` } }, { prefix: &quot;prefixChar&quot; } ] } ] }) Also, you should sort when using limit; otherwise results will be in a random order every time you execute it.",
    "context_chunks": [
      {
        "text": "I have a MongoDdb collection of about 10 billion documents. I want to be able to search in the collection using an $or query, but give priority over the conditions. Here is my existing code: const prefixChar = 'A'; const latterword = 'XYZ'; await data.find({ $or: [ { prefix: prefixChar, number: { $exists: true }, latter: latterword }, { prefix: { $exists: true }, number: { $exists: true }, latter: latterword }, { prefix: prefixChar, number: { $exists: true }, latter: { $regex: `^${latterword[0]}${latterword[1]}` } }, { prefix: prefixChar, number: { $exists: true }, latter: { $regex: `^${latterword[0]}` } }, { prefix: prefixChar, number: { $exists: true }, latter: { $exists: true } } ] }).limit(12); I have also tried to execute the queries one by one. It takes too much time. I need the response time between 0 to 2000 milliseconds. I have already put an index on those three fields and tried parallel execution of a query in Python, but with billions of records, it's still too slow.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can achieve this with some Boolean reduction. For starters, since number: { $exists: true } is in all the criteria, separate that from $or clauses and use $and to include it. Like: data.find({ $and: [ { number: { $exists: true } }, { $or: [ { prefix: prefixChar, latter: latterword } ... // all the other criteria here ] } ] }) In the 3rd &amp; 4th clauses, the difference is only latter: { $regex: `^${latterword[0]}${latterword[1]}` } vs latter: { $regex: `^${latterword[0]}` } So the 4th case already covers the 3rd case and is more broad, so the 3rd case can be removed. Now, your remaining query is this. For a regex to match, or for a string to match - as required by the remaining clauses - the field has to exist. So the exists requirements can be moved into the first number-exists check anyway. Now the you're left with these in the $or part: { $or: [ { prefix: &quot;prefixChar&quot;, latter: &quot;latterword&quot; }, { latter: &quot;latterword&quot; }, { prefix: &quot;prefixChar&quot;, latter: { $regex: `^${latterword[0]}` } }, { prefix: &quot;prefixChar&quot; } ] } Excluding the regex, that means either prefix matches or latter matches, or both but that's covered in the either case already: { $or: [ { latter: &quot;latterword&quot; }, { prefix: &quot;prefixChar&quot;, latter: { $regex: `^${latterword[0]}` } }, { prefix: &quot;prefixChar&quot; } ] } So, finally it's: data.find({ $and: [ { prefix: { $exists: true }, number: { $exists: true }, latter: { $exists: true } }, { $or: [ { latter: &quot;latterword&quot; }, { prefix: &quot;prefixChar&quot;, latter: { $regex: `^${latterword[0]}` } }, { prefix: &quot;prefixChar&quot; } ] } ] }) Also, you should sort when using limit; otherwise results will be in a random order every time you execute it.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "From the scale of the dataset, I am guessing it is likely to be some datalake collection that might have a high read-write ratio. i.e. read frequently but not updated frequently. If that is the case and your parameter does not change frequently, you may consider precomputing the priority and store each execution's results in a separate collection such that you can refer the same execution result in future. db.collection.aggregate([ { &quot;$set&quot;: { priority: { $let: { vars: { prefixChar: &quot;A&quot;, latterword: &quot;XYZ&quot; }, &quot;in&quot;: { &quot;$switch&quot;: { &quot;branches&quot;: [ { &quot;case&quot;: { &quot;$and&quot;: [ { &quot;$eq&quot;: [ &quot;$prefix&quot;, &quot;$$prefixChar&quot; ] }, { &quot;$ne&quot;: [ { &quot;$ifNull&quot;: [ &quot;$number&quot;, null ] }, null ] }, { &quot;$eq&quot;: [ &quot;$latter&quot;, &quot;$$latterword&quot; ] } ] }, &quot;then&quot;: 1 }, { &quot;case&quot;: { &quot;$and&quot;: [ { &quot;$ne&quot;: [ { &quot;$ifNull&quot;: [ &quot;$prefix&quot;, null ] }, null ] }, { &quot;$ne&quot;: [ { &quot;$ifNull&quot;: [ &quot;$number&quot;, null ] }, null ] }, { &quot;$eq&quot;: [ &quot;$latter&quot;, &quot;$$latterword&quot; ] } ] }, &quot;then&quot;: 2 }, { &quot;case&quot;: { &quot;$and&quot;: [ { &quot;$eq&quot;: [ &quot;$prefix&quot;, &quot;$$prefixChar&quot; ] }, { &quot;$ne&quot;: [ { &quot;$ifNull&quot;: [ &quot;$number&quot;, null ] }, null ] }, { $eq: [ 0, { &quot;$indexOfCP&quot;: [ &quot;$latter&quot;, { &quot;$substrCP&quot;: [ &quot;$$latterword&quot;, 0, 2 ] } ] } ] } ] }, &quot;then&quot;: 3 }, { &quot;case&quot;: { &quot;$and&quot;: [ { &quot;$eq&quot;: [ &quot;$prefix&quot;, &quot;$$prefixChar&quot; ] }, { &quot;$ne&quot;: [ { &quot;$ifNull&quot;: [ &quot;$number&quot;, null ] }, null ] }, { $eq: [ 0, { &quot;$indexOfCP&quot;: [ &quot;$latter&quot;, { &quot;$substrCP&quot;: [ &quot;$$latterword&quot;, 0, 1 ] } ] } ] } ] }, &quot;then&quot;: 4 }, { &quot;case&quot;: { &quot;$and&quot;: [ { &quot;$eq&quot;: [ &quot;$prefix&quot;, &quot;$$prefixChar&quot; ] }, { &quot;$ne&quot;: [ { &quot;$ifNull&quot;: [ &quot;$number&quot;, null ] }, null ] }, { &quot;$ne&quot;: [ { &quot;$ifNull&quot;: [ &quot;$latter&quot;, null ] }, null ] } ] }, &quot;then&quot;: 5 } ], &quot;default&quot;: 6 } } } } } }, { &quot;$group&quot;: { &quot;_id&quot;: &quot;$priority&quot;, &quot;docIds&quot;: { &quot;$push&quot;: &quot;$_id&quot; } } }, { &quot;$set&quot;: { &quot;_id&quot;: { &quot;executionId&quot;: &quot;fill in your preferred id&quot;, &quot;priority&quot;: &quot;$_id&quot;, &quot;prefixChar&quot;: &quot;A&quot;, &quot;latterword&quot;: &quot;XYZ&quot; } } }, { &quot;$merge&quot;: { &quot;into&quot;: &quot;precomputed&quot; } } ]) Mongo Playground Afterwards, when you are trying to fetch the result, you can start a $lookup from your precomputed collection. The precomputed collection can be indexed to boost the performance. db.precomputed.aggregate([ { &quot;$match&quot;: { &quot;_id.executionId&quot;: &quot;fill in your preferred id&quot; } }, { &quot;$sort&quot;: { &quot;_id.priority&quot;: 1 } }, { &quot;$unwind&quot;: &quot;$docIds&quot; }, { &quot;$limit&quot;: 12 }, { &quot;$lookup&quot;: { &quot;from&quot;: &quot;collection&quot;, &quot;localField&quot;: &quot;docIds&quot;, &quot;foreignField&quot;: &quot;_id&quot;, &quot;as&quot;: &quot;rawDocsLookup&quot; } }, { &quot;$unwind&quot;: &quot;$rawDocsLookup&quot; }, { &quot;$replaceRoot&quot;: { &quot;newRoot&quot;: { &quot;$mergeObjects&quot;: [ &quot;$rawDocsLookup&quot;, { priority: &quot;$_id.priority&quot; } ] } } } ]) Mongo Playground",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "node.js",
        "mongodb",
        "search"
      ],
      "question_score": 8,
      "answer_score": 3,
      "created": "2024-08-21T17:19:36",
      "question_id": 78898259,
      "answer_id": 78898458
    }
  },
  {
    "question": "Pylance in VS Code crashes when adding or deleting cells in a Jupyter notebook",
    "expected_answer": "This seems to be an instance of this known issue: Pylance crashing on Jupyter Notebook Cell Deletion #4685. The issue is known to exist for v2023.8.10 of the Pylance extension. Also related is microsoft/pylance-release#5071. The issue is apparently fixed in prerelease version 2023.8.51. Try installing that release (if it's still in pre-release, switch to the pre-release channel). Older info: Apparently for many (but not all) users, the issue does not reproduce in Pylance v2023.7.40, so downgrading might be a possible workaround for now. You may or may not need to downgrade further than that to get a fix. See also VS Code - how to rollback extension/install specific extension version. According to the maintainers, some of the issue-cases will be fixed by pull request #4733, which will be released in the upcoming pre-release version of the Pylance extension, so please give that version a try. You can also try using this setting &quot;python.analysis.enableSyncServer&quot;: true to see if that makes any difference- it forces the extension to handle one LSP message at a time (you need to restart VS Code once for that setting to take effect). If you can determine a procedure to reliably reproduce the issue, it would be greatly useful to the maintainers of this extension in the resolution process. If none of that works, one user found that they could work around the issue for the time being is to reload the VS Code window with the Developer: Reload Window command in the command palette (note that this may require re-running cells). For your reference and learning purposes, I found the above issue ticket by googling &quot;github vscode issues jupyter crash &quot;Chained file path undefined doesn't match cellFilePaths&quot;&quot;.",
    "context_chunks": [
      {
        "text": "I'm encountering an issue with the Pylance extension in VS Code when working on a Python notebook. Sometimes if I add or delete a cell, the language server crashes and throws the following error: Error: Debug Failure. False expression: Chained file path undefined doesn't match cellFilePaths [REDACTED PATH].ipynb:pylance-notebook-cell:W5sZmlsZQ==.py at _0x225e6a ([REDACTED PATH]\\server.bundle.js:1:1015920) at [REDACTED PATH]\\server.bundle.js:1:1023143 at _0x32da0b.&lt;computed&gt; ([REDACTED PATH]\\server.bundle.js:1:1023175) at _0x382c6c._onDidChangeNotebookDocumentAsync ([REDACTED PATH]\\server.bundle.js:1:1001957) Here's my setup: VS Code Version: June 2023 (version 1.80) Pylance Extension Version: 2023.8.21 Operating System: Windows 10 and Linux Steps to reproduce: Open a Jupyter notebook in VS Code. Add or delete a cell. The Pylance language server crashes with the above error. Has anyone encountered this issue before? Any guidance or solutions would be greatly appreciated. Thank you in advance! I've already tried several troubleshooting steps: Reinstalled VS Code. Reinstalled the Pylance extension. Updated both VS Code and Pylance to their latest versions. Despite these efforts, the issue persists. My primary goal is to have Pylance running smoothly within Jupyter notebooks in VS Code. If anyone has encountered and resolved this problem, or has any suggestions, I'd be very appreciative.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This seems to be an instance of this known issue: Pylance crashing on Jupyter Notebook Cell Deletion #4685. The issue is known to exist for v2023.8.10 of the Pylance extension. Also related is microsoft/pylance-release#5071. The issue is apparently fixed in prerelease version 2023.8.51. Try installing that release (if it's still in pre-release, switch to the pre-release channel). Older info: Apparently for many (but not all) users, the issue does not reproduce in Pylance v2023.7.40, so downgrading might be a possible workaround for now. You may or may not need to downgrade further than that to get a fix. See also VS Code - how to rollback extension/install specific extension version. According to the maintainers, some of the issue-cases will be fixed by pull request #4733, which will be released in the upcoming pre-release version of the Pylance extension, so please give that version a try. You can also try using this setting &quot;python.analysis.enableSyncServer&quot;: true to see if that makes any difference- it forces the extension to handle one LSP message at a time (you need to restart VS Code once for that setting to take effect). If you can determine a procedure to reliably reproduce the issue, it would be greatly useful to the maintainers of this extension in the resolution process. If none of that works, one user found that they could work around the issue for the time being is to reload the VS Code window with the Developer: Reload Window command in the command palette (note that this may require re-running cells). For your reference and learning purposes, I found the above issue ticket by googling &quot;github vscode issues jupyter crash &quot;Chained file path undefined doesn't match cellFilePaths&quot;&quot;.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It is a known issue I faced as well. I tried downgrading with version 2023.7.40 but Pylance still crashed when adding/deleting cells. Version v2023.7.30 solved the issue for me. EDIT: After a couple of days, the issue came back and is still persistent. Looks like they haven't solved it yet, you can check the updating status here on the official github: https://github.com/microsoft/pylance-release/issues/4685 I'm currently using the v2023.6.30 release but I'm not sure it will solve the issue. I think the best option right now, is to keep an eye on the github issue and wait to get it fixed",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code",
        "jupyter-notebook",
        "pylance"
      ],
      "question_score": 8,
      "answer_score": 4,
      "created": "2023-08-10T09:08:13",
      "question_id": 76874235,
      "answer_id": 76878340
    }
  },
  {
    "question": "Type checking python function signatures of Protocol subclass with mypy",
    "expected_answer": "I just want to point out, as was established in the comments, it is not in fact true that if you explicitly subtype SupportsPublish, mypy does not report a type error. The problem is that you weren't type annotating your method, which essentially tells mypy &quot;don't check this&quot;. If you do, for example: from dataclasses import dataclass from typing import Protocol class SupportsPublish(Protocol): def publish(self, topic: str) -&gt; None: ... def publish(m: SupportsPublish): m.publish(&quot;topic&quot;) @dataclass class Publishable: foo: str = &quot;bar&quot; def publish(self) -&gt; None: print(self) Then mypy will complain: (py311) Juans-MBP:~ juan$ mypy foo.py foo.py:18: error: Argument 1 to &quot;publish&quot; has incompatible type &quot;Publishable&quot;; expected &quot;SupportsPublish&quot; [arg-type] foo.py:18: note: Following member(s) of &quot;Publishable&quot; have conflicts: foo.py:18: note: Expected: foo.py:18: note: def publish(self, topic: str) -&gt; None foo.py:18: note: Got: foo.py:18: note: def publish(self) -&gt; None Found 1 error in 1 file (checked 1 source file) Because this is a requirement of just regular subclassing with method overriding. If you aren't going to run mypy with full --strict mode, at least somehow (through how it is invoked or by using a mypy.ini) make sure you have --disallow-untyped-defs or --disallow-untyped-calls",
    "context_chunks": [
      {
        "text": "Is there a way to safely type-check a python class which subclasses a protocol? If I define a protocol with a certain method function signature, then implicit subclasses must define a method with a compatible signature: # protocols.py from abc import abstractmethod from dataclasses import dataclass from typing import Protocol class SupportsPublish(Protocol): @abstractmethod def publish(self, topic: str): ... def publish(m: SupportsPublish): m.publish(&quot;topic&quot;) @dataclass class Publishable: foo: str = &quot;bar&quot; def publish(self): print(self) publish(Publishable()) # ✗ mypy protocols.py # protocols.py:24: error: Argument 1 to &quot;publish&quot; has incompatible type &quot;Publishable&quot;; expected &quot;SupportsPublish&quot; [arg-type] # protocols.py:24: note: Following member(s) of &quot;Publishable&quot; have conflicts: # protocols.py:24: note: Expected: # protocols.py:24: note: def publish(self, topic: str) -&gt; Any # protocols.py:24: note: Got: # protocols.py:24: note: def publish(self) -&gt; Any # Found 1 error in 1 file (checked 1 source file) But, if I explicitly subtype SupportsPublish, mypy does not report a type error: ... @dataclass class Publishable(SupportsPublish): ... # ✗ mypy protocols.py # Success: no issues found in 1 source file Based on this blurb from the PEP, I expected the type checker to find the function signature mismatch: Note that there is little difference between explicit and implicit subtypes, the main benefit of explicit subclassing is to get some protocol methods “for free”. In addition, type checkers can statically verify that the class actually implements the protocol correctly: This is my environment: &gt; mypy --version mypy 1.3.0 (compiled: yes) &gt; python --version Python 3.9.17 I expected mypy to point out the function signature mismatch.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I just want to point out, as was established in the comments, it is not in fact true that if you explicitly subtype SupportsPublish, mypy does not report a type error. The problem is that you weren't type annotating your method, which essentially tells mypy &quot;don't check this&quot;. If you do, for example: from dataclasses import dataclass from typing import Protocol class SupportsPublish(Protocol): def publish(self, topic: str) -&gt; None: ... def publish(m: SupportsPublish): m.publish(&quot;topic&quot;) @dataclass class Publishable: foo: str = &quot;bar&quot; def publish(self) -&gt; None: print(self) Then mypy will complain: (py311) Juans-MBP:~ juan$ mypy foo.py foo.py:18: error: Argument 1 to &quot;publish&quot; has incompatible type &quot;Publishable&quot;; expected &quot;SupportsPublish&quot; [arg-type] foo.py:18: note: Following member(s) of &quot;Publishable&quot; have conflicts: foo.py:18: note: Expected: foo.py:18: note: def publish(self, topic: str) -&gt; None foo.py:18: note: Got: foo.py:18: note: def publish(self) -&gt; None Found 1 error in 1 file (checked 1 source file) Because this is a requirement of just regular subclassing with method overriding. If you aren't going to run mypy with full --strict mode, at least somehow (through how it is invoked or by using a mypy.ini) make sure you have --disallow-untyped-defs or --disallow-untyped-calls",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The publish method must have the same signature in the class as in the protocol to conform to the protocol. Also, you shouldn't subclass protocols except possibly to create other protocols; they are implicit in the sense that a class (or other thing) &quot;conforms to&quot; the protocol if it matches a superset of the protocol definition, not by explicitly linking to it: A concrete type X is a subtype of protocol P if and only if X implements all protocol members of P with compatible types. In other words, for a class to be a subtype of SupportsPublish it is necessary and sufficient for it to have a method with signature publish(self, topic: str) -&gt; None. Here's a version which supports mypy --strict: from abc import abstractmethod from dataclasses import dataclass from typing import Protocol class SupportsPublish(Protocol): @abstractmethod def publish(self, topic: str) -&gt; None: ... def publish(m: SupportsPublish) -&gt; None: m.publish(&quot;topic&quot;) @dataclass class Publishable: foo: str = &quot;bar&quot; def publish(self, topic: str) -&gt; None: print(topic) publish(Publishable())",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "types",
        "mypy"
      ],
      "question_score": 8,
      "answer_score": 3,
      "created": "2023-08-29T16:55:44",
      "question_id": 77002238,
      "answer_id": 77004537
    }
  },
  {
    "question": "Create Literal type from constant variables",
    "expected_answer": "The problem is that the type is not the same as the value. One is a Literal type and the value is a string. Just like @STerliakov is mentioning in a comment, the best way is to turn it on its head, by defining the type first and the constant string based on the type, as follows: from typing import Literal, get_args FOO_TYPE = Literal[&quot;foo&quot;] FOO = get_args(FOO_TYPE)[0] BAR_TYPE = Literal[&quot;bar&quot;] BAR = get_args(BAR_TYPE)[0] It's a bit more verbose than I'd like and not so elegant, but at least you don't repeat yourself. One could also put this in a class and write a decorator to do this automatically: from typing import Literal def const_literals(cls): # make a copy of vars because we change the dictionary during iteration for const, lit in dict(**vars(c)).items(): setattr(cls, f&quot;{const}_TYPE&quot;, Literal[lit]) return cls @const_literals class Constants: FOO = &quot;foo&quot; BAR = &quot;bar&quot; print(Constants.FOO) # prints &quot;foo&quot; print(Constants.FOO_TYPE) # prints &quot;typing.Literal['foo']&quot;",
    "context_chunks": [
      {
        "text": "I want to use &quot;constant&quot; variables in my typing definitions, something like this: FOO = &quot;foo&quot; BAR = &quot;bar&quot; @dataclass class Event(): name: Literal[FOO, BAR] But that is illegal code for mypy. This works, but then I cannot use the variables in my code: FOO = Literal[&quot;foo&quot;] BAR = Literal[&quot;bar&quot;] @dataclass class Event(): name: FOO | BAR Event(FOO) # gives: Event(name=typing.Literal['foo']) Is there a way to get this to work without defining FOO and BAR twice? Impelenting Marks answer (https://stackoverflow.com/a/76382060/5386216) works, but then mypy is not able to distinguish types based on the event name: class FooBar(Enum): FOO = &quot;foo&quot; BAR = &quot;bar&quot; @dataclass class StringEvent: name: Literal[FooBar.FOO] value: str @dataclass class NumberEvent: name: Literal[FooBar.BAR] value: int def handle_event(event: StringEvent | NumberEvent): if event.name == FooBar.FOO: event.value.upper() # should not give a mypy error if event.name == FooBar.BAR: event.value.upper() # Should give a mypy error Both both cases of event.value.upper() give the following mypy error: Item &quot;int&quot; of &quot;Union[str, int]&quot; has no attribute &quot;upper&quot;",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The problem is that the type is not the same as the value. One is a Literal type and the value is a string. Just like @STerliakov is mentioning in a comment, the best way is to turn it on its head, by defining the type first and the constant string based on the type, as follows: from typing import Literal, get_args FOO_TYPE = Literal[&quot;foo&quot;] FOO = get_args(FOO_TYPE)[0] BAR_TYPE = Literal[&quot;bar&quot;] BAR = get_args(BAR_TYPE)[0] It's a bit more verbose than I'd like and not so elegant, but at least you don't repeat yourself. One could also put this in a class and write a decorator to do this automatically: from typing import Literal def const_literals(cls): # make a copy of vars because we change the dictionary during iteration for const, lit in dict(**vars(c)).items(): setattr(cls, f&quot;{const}_TYPE&quot;, Literal[lit]) return cls @const_literals class Constants: FOO = &quot;foo&quot; BAR = &quot;bar&quot; print(Constants.FOO) # prints &quot;foo&quot; print(Constants.FOO_TYPE) # prints &quot;typing.Literal['foo']&quot;",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "A bit more messy, but with less boilerplate is just to unpack the Literal. e.g.: MyType = Literal[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] A, B, C = get_args(MyType)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "mypy",
        "python-typing"
      ],
      "question_score": 8,
      "answer_score": 2,
      "created": "2023-06-01T12:46:48",
      "question_id": 76382022,
      "answer_id": 76928031
    }
  },
  {
    "question": "Is it possible to run autograd backward one node at a time?",
    "expected_answer": "TLDR Copy the code at the end, run it, follow code comments ...but I think the motivation behind this solution would help TLDR how it's done: Use a backward hook to call the optimizer immediately after a gradient was generated, then clear said gradient PyTorch's register_post_accumulate_grad_hook function allows you to hook into the right location Introduction Now when I run loss.backward() it would calculate gradients for all layers at once. This is not true. loss.backward() calculates gradients layer by layer, starting from the loss layer and going backwards through the network, each gradient is dependent on the previous layer's gradient. But is it possible to run backward() one layer at a time? That's what the backpropagation kind of does. So what I'm trying to do is to obtain gradients for layer 1 first, pass them to optimizer, And then immediately set grads to None in order to free the memory. It is not possible, as for you to obtain gradients for layer 1, you need to calculate gradients for all layers after it before (layer3 -&gt; layer2 -&gt; layer1 in your case). You could use other algorithm than backpropagation, for example Forward Forward, with some implementations available (here or here), please note this approach was tested only on small networks, and it is a research topic. See other possible alternatives here. How PyTorch's backward() conceptually works? Actual backward() process is more complex, this is a simplified description: After calling backward() on a torch.tensor (loss in this case), an implicit torch.tensor(torch.ones_like(tensor) is created and used as an actual starting gradient for the backward() process. Tensors grad_fn attribute is called (if it is not None!) with the gradient tensor(s) which calculates the gradient of an operation with respect to the input tensors. This one is multiplied with the gradient from layer above (according to the chain rule). tensor.grad_fn.next_functions is used to find next nodes in the computational graph, which are grad_fn themselves Repeat steps 2-4 until you reach the leaf tensors (which have grad_fn=None). Save the gradients in the tensor.grad attribute. Now, the very important thing is that backward() will clear the intermediate gradients (unless you specify retain_graph=True), hence you cannot save memory here. Only gradients in the leaf nodes are saved (e.g. model.layer3.weight.grad will not be None, but out.grad will be), the rest is aggressively cleared. To verify, you can check the out.grad_fn after calling loss.backward() (you should get a warning + None as a result). Additional Resources backward() should have as many inputs as there were outputs for each operation in the chain. Same thing happens for grad_fn(), which calculates a single step Extending PyTorch (specifically autograd) here for more information PyTorch hooks for autograd used later (thanks @Green绿色!) Possible improvements + implementation I think the only place where you could (realistically) save memory is when the gradients on the leaf nodes are set, one could use PyTorch's register_post_accumulate_grad_hook which will simply run after the .grad field was computed. PyTorch's backward hooks are explained in a bit more detail this helpful article from the PyTorch documentation. In order to do that, one has to obtain gradient of a given layer, apply the correction to the weights, and then set the .grad field to None. An example SGD optimizer with this feature could look like this (please read code comments!): class SGD: # You could specify additional parameters, different optimizers etc. # This one is a very basic implementation only for demonstration purposes # See [here](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD) # for PyTorch's original implementation def __init__(self, lr: float = 3e-4, *debug_modules: torch.nn.Module): self.lr = lr # Debug modules are used to verify that only one module has gradient set to None # at any given time, see __call__ below self.debug_modules = debug_modules def __call__(self, param: torch.Tensor) -&gt; None: if self.debug_modules: # Assert every module has gradient set to None # EXCEPT THE CURRENT ONE # You could also print the layers to see for yourself assert ( sum([module.weight.grad is not None for module in self.debug_modules]) == 1 ) param.add_(param.grad, alpha=-self.lr) # Clear the gradient immediately after the update param.grad = None Here is how one could register it for your neural network (please read code comments!): model = NeuralNetwork() # Used 20 so one could see the difference in weights easily sgd = SGD(20, *[c for c in model.children() if isinstance(c, torch.nn.Linear)]) # Register the hook for every linear layer for module in model.children(): if isinstance(module, torch.nn.Linear): module.weight.register_post_accumulate_grad_hook(sgd) Full code Full code I used so you could test it yourself: import torch torch.manual_seed(0) class NeuralNetwork(torch.nn.Module): def __init__(self): super().__init__() self.flatten = torch.nn.Flatten() self.layer1 = torch.nn.Linear(5, 5) self.layer2 = torch.nn.Linear(5, 5) self.layer3 = torch.nn.Linear(5, 5) def forward(self, x): x = self.flatten(x) layer1_out = self.layer1(x) layer2_out = self.layer2(layer1_out) return self.layer3(layer2_out + layer1_out) class SGD: # You could specify additional parameters, different optimizers etc. # This one is a very basic implementation only for demonstration purposes # See [here](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD) # for PyTorch's original implementation def __init__(self, lr: float = 3e-4, *debug_modules: torch.nn.Module): self.lr = lr # Debug modules are used to verify that only one module has gradient set to None # at any given time, see __call__ below self.debug_modules = debug_modules def __call__(self, param: torch.Tensor) -&gt; None: if self.debug_modules: # Assert every module has gradient set to None # EXCEPT THE CURRENT ONE # You could also print the layers to see for yourself assert ( sum([module.weight.grad is not None for module in self.debug_modules]) == 1 ) param.add_(param.grad, alpha=-self.lr) # Clear the gradient immediately after the update param.grad = None model = NeuralNetwork() # Used 20 so one could see the difference in weights easily sgd = SGD(20, *[c for c in model.children() if isinstance(c, torch.nn.Linear)]) # Register the hook for every linear layer for module in model.children(): if isinstance(module, torch.nn.Linear): module.weight.register_post_accumulate_grad_hook(sgd) batch, labels = torch.randn((5, 5)), torch.randn((5, 5)) criterion = torch.nn.MSELoss() out = model(batch) loss = criterion(out, labels) / 2 # Now calling backward on the loss suffices print(model.layer1.weight) loss.backward() print(model.layer1.weight) Please note: Difference in the weights (as expected) Assert is passing, meaning only one layer has gradient set to None at any given time One could implement different optimizers this way (too much for a SO post though) You don't have to modify the model source code, smarter registering would be possible Didn't test for performance at all",
    "context_chunks": [
      {
        "text": "Let say I have a complex model with many many layers. When I obtain the output of the model I calculate the loss. Now when I run loss.backward() it would calculate gradients for all layers at once. But is it possible to run backward() one layer at a time? So what I'm trying to do is to obtain gradients for layer 1 first, pass them to optimizer, and then immediately set grads to None in order to free the memory. Then move on to calculate gradients for the layer 2, and so on until it reaches the final layer using a loop. Is this possible? Edit: added a toy example to demonstrate the problem. for complexity I also used an architecture that somewhat resembles unet, ie output of layer 1 may be used in layer 2 but also in layer 3 import torch from torch.optim.optimizer import Optimizer, _use_grad_for_differentiable class NeuralNetwork(torch.nn.Module): def __init__(self): super().__init__() self.flatten = torch.nn.Flatten() self.layer1 = torch.nn.Linear(5, 5) self.layer2 = torch.nn.Linear(5, 5) self.layer3 = torch.nn.Linear(5, 5) def forward(self, x): x = self.flatten(x) layer1_out = self.layer1(x) layer2_out = self.layer2(layer1_out) return self.layer3(layer2_out + layer1_out) class OptimS(Optimizer): def __init__(self, params=None, lr=1e-3): super().__init__(params, dict(lr=lr,differentiable=False,)) @_use_grad_for_differentiable def step(self, closure=None): for group in self.param_groups: for param in group[&quot;params&quot;]: param.add_(param.grad, alpha=-group['lr']) model = NeuralNetwork() batch, labels= torch.randn((5,5)), torch.randn((5,5)) criterion = torch.nn.MSELoss() paramteres =model.parameters() optimizer = OptimS(paramteres) out = model(batch) loss = criterion(out,labels) /2 loss.backward() optimizer.step() optimizer.zero_grad(set_to_none=True) # you realize optimizer step is doing a just doing a loop, # so we could also run it per layer, something like: out = model(batch) loss = criterion(out,labels) /2 loss.backward() def layerwise_optimizer_step(param, lr=1e-3): param.add_(param.grad, alpha=-lr) param.grad = None for param in paramteres: layerwise_optimizer_step(param, 1e-3) # so the goal is to create some custom backward function, # that upon calculating the gradinet for each layer would # pass the corresponding parameter to our layerwise optimizer",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "TLDR Copy the code at the end, run it, follow code comments ...but I think the motivation behind this solution would help TLDR how it's done: Use a backward hook to call the optimizer immediately after a gradient was generated, then clear said gradient PyTorch's register_post_accumulate_grad_hook function allows you to hook into the right location Introduction Now when I run loss.backward() it would calculate gradients for all layers at once. This is not true. loss.backward() calculates gradients layer by layer, starting from the loss layer and going backwards through the network, each gradient is dependent on the previous layer's gradient. But is it possible to run backward() one layer at a time? That's what the backpropagation kind of does. So what I'm trying to do is to obtain gradients for layer 1 first, pass them to optimizer, And then immediately set grads to None in order to free the memory. It is not possible, as for you to obtain gradients for layer 1, you need to calculate gradients for all layers after it before (layer3 -&gt; layer2 -&gt; layer1 in your case). You could use other algorithm than backpropagation, for example Forward Forward, with some implementations available (here or here), please note this approach was tested only on small networks, and it is a research topic. See other possible alternatives here. How PyTorch's backward() conceptually works? Actual backward() process is more complex, this is a simplified description: After calling backward() on a torch.tensor (loss in this case), an implicit torch.tensor(torch.ones_like(tensor) is created and used as an actual starting gradient for the backward() process. Tensors grad_fn attribute is called (if it is not None!) with the gradient tensor(s) which calculates the gradient of an operation with respect to the input tensors. This one is multiplied with the gradient from layer above (according to the chain rule). tensor.grad_fn.next_functions is used to find next nodes in the computational graph, which are grad_fn themselves Repeat steps 2-4 until you reach the leaf tensors (which have grad_fn=None). Save the gradients in the tensor.grad attribute. Now, the very important thing is that backward() will clear the intermediate gradients (unless you specify retain_graph=True), hence you cannot save memory here. Only gradients in the leaf nodes are saved (e.g. model.layer3.weight.grad will not be None, but out.grad will be), the rest is aggressively cleared. To verify, you can check the out.grad_fn after calling loss.backward() (you should get a warning + None as a result). Additional Resources backward() should have as many inputs as there were outputs for each operation in the chain. Same thing happens for grad_fn(), which calculates a single step Extending PyTorch (specifically autograd) here for more information PyTorch hooks for autograd used later (thanks @Green绿色!) Possible improvements + implementation I think the only place where you could (realistically) save memory is when the gradients on the leaf nodes are set, one could use PyTorch's register_post_accumulate_grad_hook which will simply run after the .grad field was computed. PyTorch's backward hooks are explained in a bit more detail this helpful article from the PyTorch documentation. In order to do that, one has to obtain gradient of a given layer, apply the correction to the weights, and then set the .grad field to None. An example SGD optimizer with this feature could look like this (please read code comments!): class SGD: # You could specify additional parameters, different optimizers etc. # This one is a very basic implementation only for demonstration purposes # See [here](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD) # for PyTorch's original implementation def __init__(self, lr: float = 3e-4, *debug_modules: torch.nn.Module): self.lr = lr # Debug modules are used to verify that only one module has gradient set to None # at any given time, see __call__ below self.debug_modules = debug_modules def __call__(self, param: torch.Tensor) -&gt; None: if self.debug_modules: # Assert every module has gradient set to None # EXCEPT THE CURRENT ONE # You could also print the layers to see for yourself assert ( sum([module.weight.grad is not None for module in self.debug_modules]) == 1 ) param.add_(param.grad, alpha=-self.lr) # Clear the gradient immediately after the update param.grad = None Here is how one could register it for your neural network (please read code comments!): model = NeuralNetwork() # Used 20 so one could see the difference in weights easily sgd = SGD(20, *[c for c in model.children() if isinstance(c, torch.nn.Linear)]) # Register the hook for every linear layer for module in model.children(): if isinstance(module, torch.nn.Linear): module.weight.register_post_accumulate_grad_hook(sgd) Full code Full code I used so you could test it yourself: import torch torch.manual_seed(0) class NeuralNetwork(torch.nn.Module): def __init__(self): super().__init__() self.flatten = torch.nn.Flatten() self.layer1 = torch.nn.Linear(5, 5) self.layer2 = torch.nn.Linear(5, 5) self.layer3 = torch.nn.Linear(5, 5) def forward(self, x): x = self.flatten(x) layer1_out = self.layer1(x) layer2_out = self.layer2(layer1_out) return self.layer3(layer2_out + layer1_out) class SGD: # You could specify additional parameters, different optimizers etc. # This one is a very basic implementation only for demonstration purposes # See [here](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD) # for PyTorch's original implementation def __init__(self, lr: float = 3e-4, *debug_modules: torch.nn.Module): self.lr = lr # Debug modules are used to verify that only one module has gradient set to None # at any given time, see __call__ below self.debug_modules = debug_modules def __call__(self, param: torch.Tensor) -&gt; None: if self.debug_modules: # Assert every module has gradient set to None # EXCEPT THE CURRENT ONE # You could also print the layers to see for yourself assert ( sum([module.weight.grad is not None for module in self.debug_modules]) == 1 ) param.add_(param.grad, alpha=-self.lr) # Clear the gradient immediately after the update param.grad = None model = NeuralNetwork() # Used 20 so one could see the difference in weights easily sgd = SGD(20, *[c for c in model.children() if isinstance(c, torch.nn.Linear)]) # Register the hook for every linear layer for module in model.children(): if isinstance(module, torch.nn.Linear): module.weight.register_post_accumulate_grad_hook(sgd) batch, labels = torch.randn((5, 5)), torch.randn((5, 5)) criterion = torch.nn.MSELoss() out = model(batch) loss = criterion(out, labels) / 2 # Now calling backward on the loss suffices print(model.layer1.weight) loss.backward() print(model.layer1.weight) Please note: Difference in the weights (as expected) Assert is passing, meaning only one layer has gradient set to None at any given time One could implement different optimizers this way (too much for a SO post though) You don't have to modify the model source code, smarter registering would be possible Didn't test for performance at all",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Yes, it is possible but it is not a standard approach and may require some manual intervention. The typical way PyTorch handles gradients is by computing them all at once with loss.backward(). You can achieve what you're asking for by manually controlling the computation graph and gradients. first a Forward Pass then a Layer-by-Layer Backward Pass",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pytorch",
        "torch",
        "autograd"
      ],
      "question_score": 8,
      "answer_score": 5,
      "created": "2024-08-21T04:09:45",
      "question_id": 78895109,
      "answer_id": 78946033
    }
  },
  {
    "question": "Creating a TypedDict with enum keys",
    "expected_answer": "This is not compatible with the TypedDict specification as laid out in PEP 589. Let me quote: (emphasis mine) A TypedDict type represents dictionary objects with a specific set of string keys, and with specific value types for each valid key. So using arbitrary enum members for defining TypedDict keys is invalid. While TypedDict does also support an alternative, functional definition syntax and you could theoretically make your enum have the str data type by doing class MyEnum(str, Enum): ..., you would still probably not be able to define a TypedDict with those enum members in a way that your type checker understands. That is because only actual string literals are officially accepted as keys as mentioned in the section on the Use of Final Values and Literal Types. Quote: (again, emphasis mine) Type checkers are only expected to support actual string literals, not final names or literal types, for specifying keys in a TypedDict type definition. [...] The motivation for this is to make type declarations self-contained, and to simplify the implementation of type checkers. In other words, whether something like the following is supported depends entirely on any given type checker: from enum import Enum from typing import TypedDict class OneObject: pass class TwoObject: pass class MyEnum(str, Enum): ONE = &quot;1&quot; TWO = &quot;2&quot; CustomDict = TypedDict( &quot;CustomDict&quot;, {MyEnum.ONE: list[OneObject], MyEnum.TWO: list[TwoObject]} ) Mypy (currently) does not and gives the output: error: Invalid TypedDict() field name. (By the way, I tested it with Final variables as keys and those are also rejected.) So depending on what your use case is, you will probably have to bite the bullet and explicitly type out the enum/key names again or just not use an enum for that in the first place, as suggested by @artem in his answer.",
    "context_chunks": [
      {
        "text": "I am trying to create a TypedDict for better code completion and am running into an issue. I want to have a fixed set of keys (an Enum) and the values to match a specific list of objects depending on the key. For example: from enum import Enum class OneObject: pass class TwoObject: pass class MyEnum(Enum): ONE: 1 TWO: 2 I am looking to have something like this: from typing import TypedDict class CustomDict(TypedDict): MyEnum.ONE: list[OneObject] MyEnum.TWO: list[TwoObject] However, I am getting Non-self attribute could not be type hinted and it doesn't really work. What are my options?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is not compatible with the TypedDict specification as laid out in PEP 589. Let me quote: (emphasis mine) A TypedDict type represents dictionary objects with a specific set of string keys, and with specific value types for each valid key. So using arbitrary enum members for defining TypedDict keys is invalid. While TypedDict does also support an alternative, functional definition syntax and you could theoretically make your enum have the str data type by doing class MyEnum(str, Enum): ..., you would still probably not be able to define a TypedDict with those enum members in a way that your type checker understands. That is because only actual string literals are officially accepted as keys as mentioned in the section on the Use of Final Values and Literal Types. Quote: (again, emphasis mine) Type checkers are only expected to support actual string literals, not final names or literal types, for specifying keys in a TypedDict type definition. [...] The motivation for this is to make type declarations self-contained, and to simplify the implementation of type checkers. In other words, whether something like the following is supported depends entirely on any given type checker: from enum import Enum from typing import TypedDict class OneObject: pass class TwoObject: pass class MyEnum(str, Enum): ONE = &quot;1&quot; TWO = &quot;2&quot; CustomDict = TypedDict( &quot;CustomDict&quot;, {MyEnum.ONE: list[OneObject], MyEnum.TWO: list[TwoObject]} ) Mypy (currently) does not and gives the output: error: Invalid TypedDict() field name. (By the way, I tested it with Final variables as keys and those are also rejected.) So depending on what your use case is, you will probably have to bite the bullet and explicitly type out the enum/key names again or just not use an enum for that in the first place, as suggested by @artem in his answer.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I'm not sure what your motivation for using Enum is, but I can suggest a solution without Enum. I personally think that it does not add any value in this case. class OneObject: pass class TwoObject: pass class CustomDict(TypedDict): ONE: list[OneObject] TWO: list[TwoObject] test: CustomDict = {&quot;ONE&quot;: [OneObject()], &quot;TWO&quot;: [TwoObject()]}",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "enums",
        "python-typing",
        "typeddict"
      ],
      "question_score": 8,
      "answer_score": 3,
      "created": "2023-06-04T05:38:12",
      "question_id": 76399078,
      "answer_id": 76400797
    }
  },
  {
    "question": "Custom loss in XGBoost is not updating",
    "expected_answer": "The problem is that following the docs the custom loss function need the following parameters as input: .... def f_smooth_loss(beta: float): &quot;&quot;&quot; Custom loss function for maximising F score&quot;&quot;&quot; def custom_loss( predt: np.ndarray, dtrain: xgb.DMatrix ) -&gt; Tuple[np.ndarray, np.ndarray]: # Actual custom loss b = beta # Compute grad grad = - gradient(dtrain, predt, b) # Compute hessian hess = - hessian(dtrain, predt, b) return grad, hess return custom_los Update: following the documentation referenced about it seems that you need to pass the function in the .train() of the class not when initializing the model, e.g.: xgb.train({'tree_method': 'hist', 'seed': 1994}, # any other tree method is fine. dtrain=dtrain, num_boost_round=10, obj=f_smooth_loss(5)) Also, notice that the .fit() method is a wrapper that XGBoost has as a interface to interact with other sklearn objects (e.g. sklearn.pipeline) so it might lack this functionality, so it's better to use the native method .train().",
    "context_chunks": [
      {
        "text": "Context I am trying to use a custom loss function for an XGBoost binary classifier. The idea was to implement in XGBoost the soft-Fbeta loss, which I read about here. Simply put: instead of using the standard logloss, use a loss function that directly optimises the Fbeta score. Caveat Of course, the Fbeta itself is not differentiable, so it can't be used straight out of the box. However, the idea is to use the probabilities (hence, before thresholding) to create some sort of continuous TP, FP and FN. Find more details in the referenced Medium article. Attempt My attempt was the following (inspired by few different people). import numpy as np import xgboost as xgb def gradient(y: np.array, p: np.array, beta: float): &quot;&quot;&quot;Compute the gradient of the loss function. y is the true label, p the probability predicted by the model &quot;&quot;&quot; # Define the denominator D = p.sum() + beta**2 * y.sum() # Compute the gradient grad = (1 + beta**2) * y / D - (1 + beta**2) * (np.dot(p, y)) / D**2 return grad def hessian(y: np.array, p: np.array, beta: float): &quot;&quot;&quot;Compute the Hessian of the loss function. y is the true label, p the probability predicted by the model &quot;&quot;&quot; # Define the denominator D = p.sum() + beta**2 * y.sum() # Tensor sum y_i + y_j tensor_sum = y + y[:, None] # Compute the hessian hess = (1 + beta**2) / D**2 * (-tensor_sum + 2*np.dot(p, y) / D) return hess def f_smooth_loss(beta: float): &quot;&quot;&quot; Custom loss function for maximising F score&quot;&quot;&quot; def custom_loss(y: np.array, p: np.array): # Actual custom loss b = beta # Compute grad grad = - gradient(y, p, b) # Compute hessian hess = - hessian(y, p, b) return grad, hess return custom_loss # Random train dataset X_train = np.random.rand(100, 100) y_train = np.random.randint(0, 2, 100) # Random validation dataset X_validation = np.random.rand(1000, 100) y_validation = np.random.randint(0, 2, 1000) # Define a classifier trying to maximise F5 score model = xgb.XGBClassifier(objective=f_smooth_loss(5)) # Fit model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_validation, y_validation)]) Output The model runs, but the output is apparently stuck, no matter what: [0] validation_0-logloss:0.69315 validation_1-logloss:0.69315 [1] validation_0-logloss:0.69315 validation_1-logloss:0.69315 [2] validation_0-logloss:0.69315 validation_1-logloss:0.69315 [3] validation_0-logloss:0.69315 validation_1-logloss:0.69315 Comments It is possible my derivatives are not correct, even though I double checked them. However, even changing the grad and hess to constant numbers, nothing changes. The Hessian here is a matrix (which would be its mathematical definition), but I think XGBoost expects a 1D array (I think it is the diagonal). However, because of point 1., nothing changes even if I change it to a 1d-array Essentially, this model always predicts zeros, and does not update at all. Changing the size of the (fake) dataset does not lead to any change in the logloss (even more, the numbers are exactly the same). Curiously, the logloss is the same in the validation and train, this being yet another signal that there is something deeply wrong somewhere. If I switch to the standard logloss (built-in), it updates (outputs are random, as the dataset is random). Question What is wrong in my implementation? XGB docs are pretty hard to decipher, and I can't really tell if I am missing a simple building block here.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The problem is that following the docs the custom loss function need the following parameters as input: .... def f_smooth_loss(beta: float): &quot;&quot;&quot; Custom loss function for maximising F score&quot;&quot;&quot; def custom_loss( predt: np.ndarray, dtrain: xgb.DMatrix ) -&gt; Tuple[np.ndarray, np.ndarray]: # Actual custom loss b = beta # Compute grad grad = - gradient(dtrain, predt, b) # Compute hessian hess = - hessian(dtrain, predt, b) return grad, hess return custom_los Update: following the documentation referenced about it seems that you need to pass the function in the .train() of the class not when initializing the model, e.g.: xgb.train({'tree_method': 'hist', 'seed': 1994}, # any other tree method is fine. dtrain=dtrain, num_boost_round=10, obj=f_smooth_loss(5)) Also, notice that the .fit() method is a wrapper that XGBoost has as a interface to interact with other sklearn objects (e.g. sklearn.pipeline) so it might lack this functionality, so it's better to use the native method .train().",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Please change the classifier from objective=f_smooth_loss(5) to scoring=f_smooth_loss(5): model = xgb.XGBClassifier(scoring = f_smooth_loss(5))",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "xgboost",
        "loss-function",
        "xgbclassifier"
      ],
      "question_score": 8,
      "answer_score": 2,
      "created": "2023-06-15T11:22:35",
      "question_id": 76481951,
      "answer_id": 76736042
    }
  },
  {
    "question": "ERROR: Could not build wheels for fasttext, which is required to install pyproject.toml-based projects",
    "expected_answer": "Using pip install fasttext-wheel instead solved the problem for me.",
    "context_chunks": [
      {
        "text": "I'm trying to install fasttext using pip install fasttext in python 3.11.4 but I'm running into trouble when building wheels. The error reads as follows: error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.37.32822\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2 [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for fasttext Running setup.py clean for fasttext Failed to build fasttext ERROR: Could not build wheels for fasttext, which is required to install pyproject.toml-based projects I've searched the web and most hits indicated that the error has something to do with the build tools of visual studio (which the error above also indicated). I've installed/updated all my build tools and I've also installed the latest SDK as suggested here, but the error persists. Has anyone solved this problem before and can share any potential solution?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Using pip install fasttext-wheel instead solved the problem for me.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Some have reported managing to compile Facebook's fasttext package on Windows, and perhaps the issues mentioned in @phd's comment will have some tips that work for you. But note that the project's official homepage at https://github.com/facebookresearch/fastText only says &quot;Generally, fastText builds on modern Mac OS and Linux distributions&quot; – with no mention of official Windows support. So, other than using a workaround, some options could include: switching OSes (which may help with other packages that may be better-supported off of Windows) using an alternative FastText implementation, such as that in the gensim package - which matches the core fasttext word-modeling (&amp; even has some additional options) but misses some things in Facebook's implementation (mainly the supervised mode &amp; word n-grams).",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-wheel",
        "fasttext"
      ],
      "question_score": 7,
      "answer_score": 19,
      "created": "2023-10-11T13:23:15",
      "question_id": 77273461,
      "answer_id": 77321665
    }
  },
  {
    "question": "How to fix - UserWarning: Pydantic serializer warnings in Pydantic V2?",
    "expected_answer": "I had a similar issue when there are cyclic nested models. If that is your problem, you need to call: YourSchemaContainingNestedModel.model_rebuild() after declaring the model. Not sure if this is your case, though, since I cannot see the schemas you are using.",
    "context_chunks": [
      {
        "text": "Using datamodel-codegen command with JSON data as input type and generating Pydantic schema as output, during this process I was seeing warnings. What is the meaning of these warnings and how to fix them? What kind of issues it can create(or why this is a warning)? UserWarning: Expected `Union[list[definition-ref], definition-ref, bool]` but got `JsonSchemaObject` - serialized value may not be as expected Expected `Union[definition-ref, bool]` but got `JsonSchemaObject` - serialized value may not be as expected Expected `Union[definition-ref, bool]` but got `JsonSchemaObject` - serialized value may not be as expected Expected `Union[definition-ref, bool]` but got `JsonSchemaObject` - serialized value may not be as expected return self.__pydantic_serializer__.to_python( To reproduce: pets.json { &quot;pets&quot;: [ { &quot;name&quot;: &quot;dog&quot;, &quot;age&quot;: 2 }, { &quot;name&quot;: &quot;cat&quot;, &quot;age&quot;: 1 }, { &quot;name&quot;: &quot;snake&quot;, &quot;age&quot;: 3, &quot;nickname&quot;: &quot;python&quot; } ], &quot;status&quot;: 200 } Command: datamodel-codegen --input pets.json --input-file-type json --output model.py Versions: python - 3.11.4 pydantic==2.1.1 datamodel-code-generator==0.21.4 genson==1.2.2",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I had a similar issue when there are cyclic nested models. If that is your problem, you need to call: YourSchemaContainingNestedModel.model_rebuild() after declaring the model. Not sure if this is your case, though, since I cannot see the schemas you are using.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In my case updating pydantic and datamodel-code-generator helped pip install -U pydantic datamodel-code-generator For python 3.10",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "code-generation",
        "pydantic",
        "genson"
      ],
      "question_score": 8,
      "answer_score": 0,
      "created": "2023-08-11T06:16:30",
      "question_id": 76881106,
      "answer_id": 77053681
    }
  },
  {
    "question": "Regex a string with a space between words",
    "expected_answer": "Starting with your original pattern: ABC(?!\\.png)|ABC(?! thumb\\.png) (Note: Dot is a regex metacharacter and should be escaped with backslash) This will match ABC which is not followed by .png or ABC not followed by thumb.png. Every possible occurrence of ABC will match this pattern. Therefore, all occurrences of ABC will be match, because every extension will match at least one of the two conditions. We can write the following correction: \\bABC(?!\\.png| thumb\\.png) This pattern says to match: \\b word boundary ABC match ABC (?!\\.png| thumb\\.png) neither .png or thumb.png follows The negative lookahead used here basically has AND flavored logic, and will exclude both following extensions.",
    "context_chunks": [
      {
        "text": "import re texto = &quot;ABC ABC. ABC.png ABC thumb.png&quot; regex = r&quot;ABC(?!.png)|ABC(?! thumb.png)&quot; novo = re.sub(regex, &quot;bueno&quot;, texto) print(novo) I'm trying to replace the ABC word with exceptions. I only want to replace it if it doesn't follow the word &quot;.png&quot; or &quot; thumb.png&quot;. The string would be then &quot;ABC thumb.png&quot; I expected bueno bueno. ABC.png ABC thumb.png But the output is this bueno bueno. bueno.png bueno thumb.png It isn't detecting the space and it actually messes up the first condition.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Starting with your original pattern: ABC(?!\\.png)|ABC(?! thumb\\.png) (Note: Dot is a regex metacharacter and should be escaped with backslash) This will match ABC which is not followed by .png or ABC not followed by thumb.png. Every possible occurrence of ABC will match this pattern. Therefore, all occurrences of ABC will be match, because every extension will match at least one of the two conditions. We can write the following correction: \\bABC(?!\\.png| thumb\\.png) This pattern says to match: \\b word boundary ABC match ABC (?!\\.png| thumb\\.png) neither .png or thumb.png follows The negative lookahead used here basically has AND flavored logic, and will exclude both following extensions.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can make thumb optional inside the lookahead: \\bABC(?!(?: thumb)?\\.png) The regex matches: \\bABC Match ABC preceded by a word boundary to prevent a partial word match (?! Negative lookahead, assert that what is directly to the right is not (?: thumb)? Optionally match thumb \\.png Match .png ) Close the lookahead See a regex 101 demo Note that if .png should also not have a partial word match at the end, you can also a word boundary like .png\\b",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "regex"
      ],
      "question_score": 7,
      "answer_score": 8,
      "created": "2025-02-05T05:41:14",
      "question_id": 79413756,
      "answer_id": 79413814
    }
  },
  {
    "question": "&quot;ERROR: Could not build wheels for dlib, which is required to install pyproject.toml-based projects&quot; while installing dlib in PowerShell",
    "expected_answer": "It looks like you are using Python 3.11. Here's a solution which works for Python versions: 3.7, 3.8 and 3.9: Create a virtual environment via venv or Anaconda i.e. conda create -n your_env python = 3.8. I tested with Python 3.8 and it works. Download the wheel file from this repo for your specific Python version Open a terminal and install Dlib via: python -m pip install dlib-19.22.99-cp38-cp38-win_amd64.whl This should help you to install dlib 19.22.99! UPDATE (DEC 2023): For anyone who's having an issue with building wheels for the Dlib library, please use this repo which also contains the wheels for Python 3.10 and 3.11. UPDATE (MAY 2024): The repo now includes wheel for Python 3.12. NOTE: These wheel files can only be used to install Dlib on Windows x64 OS. BUILD FROM SOURCE: If you'd like to build it from source, follow these exact steps as per their docs: Install Visual Studio 2022 with the option Desktop Development with C++ Create a virtual env as suggested above Execute these commands: git clone https://github.com/davisking/dlib.git cd dlib pip install build python -m build --wheel # Upon successful run, a &quot;.whl&quot; binary will be created under &quot;dlib/dist/&quot; pip install dist/dlib-&lt;version&gt;.whl # replace &lt;version&gt; with the exact name of the &quot;.whl&quot; file",
    "context_chunks": [
      {
        "text": "I have been building a facial recognition tool and I am trying to install dlib. Every time I have tried to install dlib, I get this message: × Building wheel for dlib (pyproject.toml) did not run successfully. │ exit code: 1 ╰─&gt; [78 lines of output] running bdist_wheel running build running build_ext &lt;string&gt;:125: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead. Building extension for Python 3.11.4 (tags/v3.11.4:d2340ef, Jun 7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)] Invoking CMake setup: 'cmake C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-install-tl9fdcf0\\dlib_fee2183dfbb9404395bfedaca5728c1a\\tools\\python -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-install-tl9fdcf0\\dlib_fee2183dfbb9404395bfedaca5728c1a\\build\\lib.win-amd64-cpython-311 -DPYTHON_EXECUTABLE=C:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-install-tl9fdcf0\\dlib_fee2183dfbb9404395bfedaca5728c1a\\build\\lib.win-amd64-cpython-311 -A x64' -- Building for: NMake Makefiles CMake Error at CMakeLists.txt:5 (message): !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! You must use Visual Studio to build a python extension on windows. If you are getting this error it means you have not installed Visual C++. Note that there are many flavors of Visual Studio, like Visual Studio for C# development. You need to install Visual Studio for C++. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -- Configuring incomplete, errors occurred! Traceback (most recent call last): File &quot;C:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 353, in &lt;module&gt; main() File &quot;C:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py&quot;, line 251, in build_wheel return _build_backend().build_wheel(wheel_directory, config_settings, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py&quot;, line 416, in build_wheel return self._build_with_temp_dir(['bdist_wheel'], '.whl', ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py&quot;, line 401, in _build_with_temp_dir self.run_setup() File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py&quot;, line 338, in run_setup exec(code, locals()) File &quot;&lt;string&gt;&quot;, line 218, in &lt;module&gt; File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\__init__.py&quot;, line 107, in setup return distutils.core.setup(**attrs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py&quot;, line 185, in setup return run_commands(dist) ^^^^^^^^^^^^^^^^^^ File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py&quot;, line 201, in run_commands dist.run_commands() File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py&quot;, line 969, in run_commands self.run_command(cmd) File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\dist.py&quot;, line 1234, in run_command super().run_command(command) File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py&quot;, line 988, in run_command cmd_obj.run() File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\wheel\\bdist_wheel.py&quot;, line 343, in run self.run_command(&quot;build&quot;) File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py&quot;, line 318, in run_command self.distribution.run_command(command) File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\dist.py&quot;, line 1234, in run_command super().run_command(command) File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py&quot;, line 988, in run_command cmd_obj.run() File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\build.py&quot;, line 131, in run self.run_command(cmd_name) File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py&quot;, line 318, in run_command self.distribution.run_command(command) File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\dist.py&quot;, line 1234, in run_command super().run_command(command) File &quot;C:\\Users\\Jack\\AppData\\Local\\Temp\\pip-build-env-ar6t1xkr\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py&quot;, line 988, in run_command cmd_obj.run() File &quot;&lt;string&gt;&quot;, line 130, in run File &quot;&lt;string&gt;&quot;, line 167, in build_extension File &quot;C:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py&quot;, line 413, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command '['cmake', 'C:\\\\Users\\\\Jack\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tl9fdcf0\\\\dlib_fee2183dfbb9404395bfedaca5728c1a\\\\tools\\\\python', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\\\\Users\\\\Jack\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tl9fdcf0\\\\dlib_fee2183dfbb9404395bfedaca5728c1a\\\\build\\\\lib.win-amd64-cpython-311', '-DPYTHON_EXECUTABLE=C:\\\\Users\\\\Jack\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\\\\Users\\\\Jack\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tl9fdcf0\\\\dlib_fee2183dfbb9404395bfedaca5728c1a\\\\build\\\\lib.win-amd64-cpython-311', '-A', 'x64']' returned non-zero exit status 1. [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for dlib Failed to build dlib ERROR: Could not build wheels for dlib, which is required to install pyproject.toml-based projects I have already installed CMake the correct way. Visual Studio is also installed. How do I fix this error?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "It looks like you are using Python 3.11. Here's a solution which works for Python versions: 3.7, 3.8 and 3.9: Create a virtual environment via venv or Anaconda i.e. conda create -n your_env python = 3.8. I tested with Python 3.8 and it works. Download the wheel file from this repo for your specific Python version Open a terminal and install Dlib via: python -m pip install dlib-19.22.99-cp38-cp38-win_amd64.whl This should help you to install dlib 19.22.99! UPDATE (DEC 2023): For anyone who's having an issue with building wheels for the Dlib library, please use this repo which also contains the wheels for Python 3.10 and 3.11. UPDATE (MAY 2024): The repo now includes wheel for Python 3.12. NOTE: These wheel files can only be used to install Dlib on Windows x64 OS. BUILD FROM SOURCE: If you'd like to build it from source, follow these exact steps as per their docs: Install Visual Studio 2022 with the option Desktop Development with C++ Create a virtual env as suggested above Execute these commands: git clone https://github.com/davisking/dlib.git cd dlib pip install build python -m build --wheel # Upon successful run, a &quot;.whl&quot; binary will be created under &quot;dlib/dist/&quot; pip install dist/dlib-&lt;version&gt;.whl # replace &lt;version&gt; with the exact name of the &quot;.whl&quot; file",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "pip install cmake==3.25.2 pip install dlib==19.24.2 I also encountered the same problem, My solution is to install the specified version mentioned above (the latest version cannot be installed)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "dlib"
      ],
      "question_score": 7,
      "answer_score": 18,
      "created": "2023-07-06T13:51:51",
      "question_id": 76629574,
      "answer_id": 76630254
    }
  },
  {
    "question": "VS Code Jupyter Notebook Output Cell Word Wrapping Not Working",
    "expected_answer": "You can enable word wrap on VS code Jupyter notebooks by setting notebook.output.wordWrap within VS code settings. https://github.com/microsoft/vscode-jupyter/issues/13510",
    "context_chunks": [
      {
        "text": "I'm selecting text data from an SFrame and printing it. The text is really long, and the cell gets a horizontal scrollbar to view it. I would like to have it wrap to a newline and fit in my window, not to have a horizontal scrollbar. I tried enabling/disabling the vscode command View: Toggle Word Wrap, but that didn't change the output, even upon rerunning the script.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can enable word wrap on VS code Jupyter notebooks by setting notebook.output.wordWrap within VS code settings. https://github.com/microsoft/vscode-jupyter/issues/13510",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Unfortunately, VS Code doesn't currently support word wrap in the output or terminal windows. You can try to use textwrap package manually. Add the following codes to your script: import textwrap wrapped_text = textwrap.fill(text, width=80) #text is the object which you want to print print(wrapped_text)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "visual-studio-code",
        "jupyter-notebook"
      ],
      "question_score": 7,
      "answer_score": 18,
      "created": "2024-01-09T16:31:50",
      "question_id": 77788310,
      "answer_id": 78409650
    }
  },
  {
    "question": "AttributeError: &#39;super&#39; object has no attribute &#39;init&#39;",
    "expected_answer": "this turns out to be a little tricky. and this is a workaround! hope works for you. Under the hood, this module pyttsx3 uses PyObjC as a bridge between Python and Objective-C. Step 1: Check that pyobjc is installed(pip show pyobjc), if not install as pip install pyobjc. Step 2: open this file /usr/local/lib/python3.11/site-packages/pyttsx3/drivers/nsss.py and change the following: #self = super(NSSpeechDriver, self).init() comment this line , and add the following self = objc.super(NSSpeechDriver, self).init() Note: from Foundation import * imports NSObject and objc from foundation, which has been consumed. after the change, your following program would run okay. import pyttsx3 engine = pyttsx3.init() engine.say('How are you today?') engine.runAndWait()",
    "context_chunks": [
      {
        "text": "I was making a personal assistant. I got an error in starting code: import pyttsx3 engine = pyttsx3.init() engine.say('How are you today?') engine.runAndWait() Error: /usr/local/lib/python3.11/site-packages/pyttsx3/drivers/nsss.py:12: ObjCSuperWarning: Objective-C subclass uses super(), but super is not objc.super class NSSpeechDriver(NSObject): Traceback (most recent call last): File &quot;/usr/local/lib/python3.11/site-packages/pyttsx3/__init__.py&quot;, line 20, in init eng = _activeEngines[driverName] ~~~~~~~~~~~~~~^^^^^^^^^^^^ File &quot;/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py&quot;, line 136, in __getitem__ o = self.data[key]() ~~~~~~~~~^^^^^ KeyError: None During handling of the above exception, another exception occurred: Traceback (most recent call last): File &quot;/Users/anshtyagi/Documents/personal assistant/main.py&quot;, line 5, in &lt;module&gt; engine = pyttsx3.init() ^^^^^^^^^^^^^^ File &quot;/usr/local/lib/python3.11/site-packages/pyttsx3/__init__.py&quot;, line 22, in init eng = Engine(driverName, debug) ^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/local/lib/python3.11/site-packages/pyttsx3/engine.py&quot;, line 30, in __init__ self.proxy = driver.DriverProxy(weakref.proxy(self), driverName, debug) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/local/lib/python3.11/site-packages/pyttsx3/driver.py&quot;, line 52, in __init__ self._driver = self._module.buildDriver(weakref.proxy(self)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/local/lib/python3.11/site-packages/pyttsx3/drivers/nsss.py&quot;, line 9, in buildDriver return NSSpeechDriver.alloc().initWithProxy(proxy) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/local/lib/python3.11/site-packages/pyttsx3/drivers/nsss.py&quot;, line 15, in initWithProxy self = super(NSSpeechDriver, self).init() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ AttributeError: 'super' object has no attribute 'init' sys:1: UninitializedDeallocWarning: leaking an uninitialized object of type NSSpeechDriver I don't know what is the problem. One more thing: due to some issue I had to uninstall old python version on Mac and installed new one using Homebrew. Mac OS ventura 13.4 Python 3.11",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "this turns out to be a little tricky. and this is a workaround! hope works for you. Under the hood, this module pyttsx3 uses PyObjC as a bridge between Python and Objective-C. Step 1: Check that pyobjc is installed(pip show pyobjc), if not install as pip install pyobjc. Step 2: open this file /usr/local/lib/python3.11/site-packages/pyttsx3/drivers/nsss.py and change the following: #self = super(NSSpeechDriver, self).init() comment this line , and add the following self = objc.super(NSSpeechDriver, self).init() Note: from Foundation import * imports NSObject and objc from foundation, which has been consumed. after the change, your following program would run okay. import pyttsx3 engine = pyttsx3.init() engine.say('How are you today?') engine.runAndWait()",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "It seems that the supported Python versions are 3.5 to 3.7, as stated in the documentation. I can confirm that version 3.6.15 works fine with my Mac. (However, version 3.7.12 doesn't work.)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pyttsx3"
      ],
      "question_score": 7,
      "answer_score": 28,
      "created": "2023-06-08T17:58:21",
      "question_id": 76434535,
      "answer_id": 76444434
    }
  },
  {
    "question": "ModuleNotFoundError while importing moviepy.editor",
    "expected_answer": "try: from moviepy import VideoFileClip instead of: from moviepy.editor import VideoFileClip It works.",
    "context_chunks": [
      {
        "text": "I'm trying to work with VideoFileClip and vfx functions from the Moviepy library but my interpreter keeps throwing a 'ModuleNotFoundError: No module named 'moviepy.editor''. I've installed and reinstalled Moviepy several times but to no avail Tried: from moviepy.editor import VideoFileClip, vfx Expected: import statement to work Edit: Other imports like 'from moviepy.video.io.VideoFileClip import VideoFileClip' seem to work except the moviepy.editor library imageio and the program ffmpeg have already been successfully downloaded so the replies from this post aren't helping T~T",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "try: from moviepy import VideoFileClip instead of: from moviepy.editor import VideoFileClip It works.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Newest verison of moviepy doesn't really contain editor.py file whether if you download it with pip, clone it from git etc. You can always go back to a previous version of the library, not the ideal but definetly worked for me. pip uninstall moviepy pip install moviepy==1.0.3",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-import",
        "modulenotfounderror",
        "moviepy"
      ],
      "question_score": 7,
      "answer_score": 14,
      "created": "2024-12-16T10:03:15",
      "question_id": 79284285,
      "answer_id": 79505175
    }
  },
  {
    "question": "Removing elements based on nested dictionary values",
    "expected_answer": "As a dictionary comprehension A dictionary comprehension can be used to create dictionaries from arbitrary key and value expressions. new_dict2 = { key: value for key, value in my_dict.items() if value['category']['name'] == 'Fruit' } new_dict2 == new_dict # True Using filter() The filter() function is used to: Construct an iterator from those elements of iterable for which function is true. The dict.items() returns an iterable where each element is a tuple of length 2. We can supply each item to a lambda function, where item[0] will be the key and item[1] the value. filter() returns an iterator of the tuples which match the condition. We can wrap this in dict() to get a dictionary (in the same way that dict([(&quot;key1&quot;, &quot;value1&quot;), (&quot;key2&quot;, &quot;value2&quot;)]) returns {'key1': 'value1', 'key2': 'value2'}). new_dict3 = dict( filter( lambda item: item[1]['category']['name'] == 'Fruit', my_dict.items() ) ) new_dict3 == new_dict # True Most Pythonic way Achieving the nebulous goal of Pythonicness (Pythonicity?) is always somewhat subjective. I think a dictionary comprehension is clean and neat but it can be hard to see what it's doing, especially if the dict is deeply nested or the condition is complex. It's probably clearest if you wrap it in an appropriately-named function so you can see what's going on. I've added type annotations for clarity: def find_fruit(d: dict[str, dict]) -&gt; dict[str, dict]: def is_fruit(key: str, value: dict) -&gt; bool: return value[&quot;category&quot;][&quot;name&quot;] == &quot;Fruit&quot; return {key: value for key, value in d.items() if is_fruit(key, value)} fruit_dict = find_fruit(my_dict) new_dict == fruit_dict # True This is fundamentally the same as the first approach but easier on the eyes.",
    "context_chunks": [
      {
        "text": "I have a complex nested dictionary structure and I need to remove elements based on the values in a nested dictionary. My dictionary looks like this: my_dict = { 'item1': {'name': 'Apple', 'price': 1.0, 'category': {'id': 1, 'name': 'Fruit'}}, 'item2': {'name': 'Banana', 'price': 0.5, 'category': {'id': 1, 'name': 'Fruit'}}, 'item3': {'name': 'Carrot', 'price': 0.75, 'category': {'id': 2, 'name': 'Vegetable'}}, 'item4': {'name': 'Broccoli', 'price': 1.5, 'category': {'id': 2, 'name': 'Vegetable'}} } I want to filter this dictionary to only include items belonging to the 'Fruit' category. I tried the following code: new_dict = {} for key, value in my_dict.items(): if value['category']['name'] == 'Fruit': new_dict[key] = value print(new_dict) This works, but I'm wondering if there's a more concise or Pythonic way to achieve this, perhaps using dictionary comprehension or a filtering function like filter().",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "As a dictionary comprehension A dictionary comprehension can be used to create dictionaries from arbitrary key and value expressions. new_dict2 = { key: value for key, value in my_dict.items() if value['category']['name'] == 'Fruit' } new_dict2 == new_dict # True Using filter() The filter() function is used to: Construct an iterator from those elements of iterable for which function is true. The dict.items() returns an iterable where each element is a tuple of length 2. We can supply each item to a lambda function, where item[0] will be the key and item[1] the value. filter() returns an iterator of the tuples which match the condition. We can wrap this in dict() to get a dictionary (in the same way that dict([(&quot;key1&quot;, &quot;value1&quot;), (&quot;key2&quot;, &quot;value2&quot;)]) returns {'key1': 'value1', 'key2': 'value2'}). new_dict3 = dict( filter( lambda item: item[1]['category']['name'] == 'Fruit', my_dict.items() ) ) new_dict3 == new_dict # True Most Pythonic way Achieving the nebulous goal of Pythonicness (Pythonicity?) is always somewhat subjective. I think a dictionary comprehension is clean and neat but it can be hard to see what it's doing, especially if the dict is deeply nested or the condition is complex. It's probably clearest if you wrap it in an appropriately-named function so you can see what's going on. I've added type annotations for clarity: def find_fruit(d: dict[str, dict]) -&gt; dict[str, dict]: def is_fruit(key: str, value: dict) -&gt; bool: return value[&quot;category&quot;][&quot;name&quot;] == &quot;Fruit&quot; return {key: value for key, value in d.items() if is_fruit(key, value)} fruit_dict = find_fruit(my_dict) new_dict == fruit_dict # True This is fundamentally the same as the first approach but easier on the eyes.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You could use a dictionary comprehension which would be more concise but, arguably, harder to read: my_dict = { 'item1': {'name': 'Apple', 'price': 1.0, 'category': {'id': 1, 'name': 'Fruit'}}, 'item2': {'name': 'Banana', 'price': 0.5, 'category': {'id': 1, 'name': 'Fruit'}}, 'item3': {'name': 'Carrot', 'price': 0.75, 'category': {'id': 2, 'name': 'Vegetable'}}, 'item4': {'name': 'Broccoli', 'price': 1.5, 'category': {'id': 2, 'name': 'Vegetable'}} } new_dict = {k: v for (k, v) in my_dict.items() if v[&quot;category&quot;][&quot;name&quot;] == &quot;Fruit&quot;} print(new_dict)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "function",
        "dictionary"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2025-04-14T15:01:04",
      "question_id": 79573449,
      "answer_id": 79573457
    }
  },
  {
    "question": "Anaconda Navigator Cannot initalize GLX",
    "expected_answer": "I’m using Arch Linux and encountered an issue after upgrading Anaconda Navigator. I reinstalled both Anaconda and Navigator, but it didn’t help. Here’s the error I got when trying to run Anaconda Navigator: 2024-10-17 14:18:25,806 - WARNING linux_scaling.get_scaling_factor_using_dbus:32 An exception occurred during fetching list of system display settings. qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags&lt;QSurfaceFormat::FormatOption&gt;(), depthBufferSize -1, redBufferSize 1, greenBufferSi ze 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile QSurfaceFormat::NoProfile) qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags&lt;QSurfaceFormat::FormatOption&gt;(), depthBufferSize -1, redBufferSize 1, greenBufferSi ze 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile QSurfaceFormat::NoProfile) Could not initialize GLX zsh: IOT instruction (core dumped) Steps that solved the above for me: conda update anaconda-navigator The command produced the below output Channels: - defaults Platform: linux-64 Collecting package metadata (repodata.json): done Solving environment: done # All requested packages already installed. The final command: export QT_XCB_GL_INTEGRATION=none I also changed the following flag in ~/.anaconda/navigator/anaconda-navigator.ini enable_high_dpi_scaling = True",
    "context_chunks": [
      {
        "text": "I use Fedora Nobara. I made an update to Nobara 40. Before that anaconda navigator was working perfectly and now it gives me this error &gt; conda activate TheWorld &gt; anaconda-navigator Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway. qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags&lt;QSurfaceFormat::FormatOption&gt;(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile QSurfaceFormat::NoProfile) qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags&lt;QSurfaceFormat::FormatOption&gt;(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile QSurfaceFormat::NoProfile) Could not initialize GLX zsh: IOT instruction (core dumped) anaconda-navigator I can't filter it down to which is the real cause for this issue. And took the basic measures still having the same issue. How to resolve this?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I’m using Arch Linux and encountered an issue after upgrading Anaconda Navigator. I reinstalled both Anaconda and Navigator, but it didn’t help. Here’s the error I got when trying to run Anaconda Navigator: 2024-10-17 14:18:25,806 - WARNING linux_scaling.get_scaling_factor_using_dbus:32 An exception occurred during fetching list of system display settings. qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags&lt;QSurfaceFormat::FormatOption&gt;(), depthBufferSize -1, redBufferSize 1, greenBufferSi ze 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile QSurfaceFormat::NoProfile) qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags&lt;QSurfaceFormat::FormatOption&gt;(), depthBufferSize -1, redBufferSize 1, greenBufferSi ze 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile QSurfaceFormat::NoProfile) Could not initialize GLX zsh: IOT instruction (core dumped) Steps that solved the above for me: conda update anaconda-navigator The command produced the below output Channels: - defaults Platform: linux-64 Collecting package metadata (repodata.json): done Solving environment: done # All requested packages already installed. The final command: export QT_XCB_GL_INTEGRATION=none I also changed the following flag in ~/.anaconda/navigator/anaconda-navigator.ini enable_high_dpi_scaling = True",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Go to your terminal and do this: export QT_XCB_GL_INTEGRATION=none It worked for me.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "anaconda",
        "fedora",
        "gnome"
      ],
      "question_score": 7,
      "answer_score": 14,
      "created": "2024-09-02T04:09:05",
      "question_id": 78938890,
      "answer_id": 79097588
    }
  },
  {
    "question": "Finding the last row that meets conditions of a mask",
    "expected_answer": "Instead of checking if the cumulative sum is greater than 1, you should check if the cumulative sum is equal to the maximum cumulative sum value which is equivalent of getting the mask.sum()[suggested by @Onyambu]. df.loc[(mask.cumsum() == mask.sum()) &amp; mask, 'c'] = 'x' CODE DEMO",
    "context_chunks": [
      {
        "text": "This is my dataframe: df = pd.DataFrame({'a': [20, 21, 333, 444], 'b': [20, 20, 20, 20]}) I want to create column c by using this mask: mask = (df.a &gt;= df.b) And I want to get the last row that meets this condition and create column c. The output that I want looks like this: a b c 0 20 20 NaN 1 21 20 NaN 2 333 20 NaN 3 444 20 x I tried the code below but it didn't work: df.loc[mask.cumsum().gt(1) &amp; mask, 'c'] = 'x'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Instead of checking if the cumulative sum is greater than 1, you should check if the cumulative sum is equal to the maximum cumulative sum value which is equivalent of getting the mask.sum()[suggested by @Onyambu]. df.loc[(mask.cumsum() == mask.sum()) &amp; mask, 'c'] = 'x' CODE DEMO",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For a mask to flag the last value satisfying a condition, use duplicated() by keeping last. We know that mask consists of at most 2 values (True/False). If we can create another mask that flags the last occurrences these values as True, then we can chain it with mask itself for the desired mask. This is accomplished by ~mask.duplicated(keep='last') because mask.duplicated(keep='last') flags duplicates as True except for the last occurrence, so its negation gives us what we want. df = pd.DataFrame({'a': [20, 21, 333, 444], 'b': [20, 20, 20, 20]}) mask = (df.a &gt;= df.b) df['c'] = pd.Series('x', df.index).where(mask &amp; ~mask.duplicated(keep='last')) If you want to slice/assign, then you can use this chained mask as well. df.loc[mask &amp; ~mask.duplicated(keep='last'), 'c'] = 'x' A shorter version of @mandy8055's answer is to call idxmax() to get the index of the highest cum sum (although this is showing a FutureWarning on pandas 2.1.0). As pointed out by @mozway, this works as long as there's at least one True value in mask. df.loc[mask.cumsum().idxmax(), 'c'] = 'x'",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "indexing",
        "conditional-statements",
        "duplicates"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2023-11-05T05:11:20",
      "question_id": 77424774,
      "answer_id": 77424801
    }
  },
  {
    "question": "Is there a known working configuration for using Selenium on linux-arm64?",
    "expected_answer": "Disclaimer: The solution below is outdated as Firefox now officially supports Linux ARM64, as mentioned in xxldoener's answer. Additionally, there may be compatibility and stability issues with the approach below, as highlighted in vk6's answer. It is recommended to use the official ARM64 versions of geckodriver available in your distribution's repositories. Here is what I got after spending a whole evening trying to solve this problem: To use Selenium on linux-arm64, we'll need to obtain three old Debian packages: chromium-codecs-ffmpeg-extra_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb chromium-browser_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb chromium-chromedriver_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb Below is a step-by-step guide. Installation Download the packages To fetch the necessary Debian packages directly from the launchpad, we can use the wget command. # Fetch chromium-codecs-ffmpeg-extra wget http://launchpadlibrarian.net/660838579/chromium-codecs-ffmpeg-extra_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb # Fetch chromium-browser wget http://launchpadlibrarian.net/660838574/chromium-browser_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb # Fetch chromium-chromedriver wget http://launchpadlibrarian.net/660838578/chromium-chromedriver_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb Once ready, can install them using gdebi-core, which will handle the dependencies. Install gdebi-core sudo apt-get install gdebi-core Install the Debian packages sudo gdebi chromium-codecs-ffmpeg-extra_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb sudo gdebi chromium-browser_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb sudo gdebi chromium-chromedriver_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb After following these steps, should have a working configuration of Selenium with ChromeDriver on linux-arm64. Verify Installation chromium-browser --version the output should be like Chromium 112.0.5615.49 Built on Ubuntu , running on Ubuntu 22.04 To further test the installation, save the following script as test.py. Note that the arguments provided for options are essential: from selenium import webdriver # Initialize the Chrome WebDriver options = webdriver.ChromeOptions() options.add_argument('--headless') options.add_argument('--no-sandbox') # options.add_argument('--disable-dev-shm-usage') # options.add_argument('--remote-debugging-port=9222') driver = webdriver.Chrome(options=options) # Retrieve the capabilities capabilities = driver.capabilities # For Chrome: if 'browserName' in capabilities and capabilities['browserName'] == 'chrome': browser_version = capabilities.get('browserVersion', 'Unknown') chromedriver_version = capabilities.get('chrome', {}).get('chromedriverVersion', 'Unknown').split(' ')[0] print(f&quot;Browser Name: Chrome&quot;) print(f&quot;Browser Version: {browser_version}&quot;) print(f&quot;ChromeDriver Version: {chromedriver_version}&quot;) # Close the driver driver.quit() Executing python test.py should yield: Browser Name: Chrome Browser Version: 112.0.5615.49 ChromeDriver Version: 112.0.5615.49",
    "context_chunks": [
      {
        "text": "I wrote a quick program that simply opens and closes a website using Firefox at set intervals. It runs perfectly on my Intel Mac OS Ventura. I intended to keep it running on a Raspberry Pi, but I cannot find a combination of versions of Selenium, geckodriver or chromedriver, and Firefox or Chromium that will run on it. The Mac has Selenium 4.11.2, geckodriver v0.33.0, and Firefox 115.0.3 working. The Raspberry Pi has Ubuntu 22.04.3 LTS. I found out here, https://github.com/SeleniumHQ/selenium/issues/11599, that Selenium Manager doesn't work on linux-arm64, and the Raspberry Pi uses linux-arm64. I was getting errors even when I tried to code in the path to the driver, with Selenium logging that it couldn't find a driver, even while it was also in PATH. It looks like the developers say in the conversation above that the built in Selenium Manager driver manager causes errors like these. Selenium Manager was introduced in Selenium 4.6, so I rolled back to Selenium 4.5, altered my code for that version, tried to run it, and got different errors that seemed to be about incompatibility issues between the driver and the version of Firefox. I tried different combinations of them with no success. Then I decided to try Chrome instead. Google does not provide a chromedriver build for linux-arm64, so I tried to use different versions found here, https://github.com/electron/electron/releases, as well as trying to roll back Chromium. I was able to at least launch the Chromium browser with the program, which is more success than I had with Firefox, but I could not get it fully working. All along the whole process I read many answers to Selenium problems on Stack Overflow, but nothing has helped. Here is the code that runs fine on Mac with the configuration above: import datetime, logging, time from selenium import webdriver logger = logging.getLogger('selenium') logger.setLevel(logging.DEBUG) handler = logging.FileHandler(&quot;handler.log&quot;) logger.addHandler(handler) logging.getLogger('selenium.webdriver.remote').setLevel(logging.DEBUG) logging.getLogger('selenium.webdriver.common').setLevel(logging.DEBUG) logging.basicConfig(filename=&quot;program.log&quot;, level=logging.INFO) timeStarted = datetime.datetime.now() logging.info( timeStarted.strftime(&quot;%m/%d/%Y, %H:%M:%S&quot;) + &quot; started on https://google.com&quot; ) # Program starts here while True: timeOfRequest = datetime.datetime.now() try: browser = webdriver.Firefox() browser.get(&quot;https://google.com&quot;) logging.info( timeOfRequest.strftime(&quot;%m/%d/%Y, %H:%M:%S&quot;) + &quot; Success&quot; ) except: logging.exception( timeOfRequest.strftime(&quot;%m/%d/%Y, %H:%M:%S&quot;) + &quot; Something went wrong&quot; ) time.sleep(810) browser.quit() time.sleep(30)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Disclaimer: The solution below is outdated as Firefox now officially supports Linux ARM64, as mentioned in xxldoener's answer. Additionally, there may be compatibility and stability issues with the approach below, as highlighted in vk6's answer. It is recommended to use the official ARM64 versions of geckodriver available in your distribution's repositories. Here is what I got after spending a whole evening trying to solve this problem: To use Selenium on linux-arm64, we'll need to obtain three old Debian packages: chromium-codecs-ffmpeg-extra_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb chromium-browser_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb chromium-chromedriver_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb Below is a step-by-step guide. Installation Download the packages To fetch the necessary Debian packages directly from the launchpad, we can use the wget command. # Fetch chromium-codecs-ffmpeg-extra wget http://launchpadlibrarian.net/660838579/chromium-codecs-ffmpeg-extra_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb # Fetch chromium-browser wget http://launchpadlibrarian.net/660838574/chromium-browser_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb # Fetch chromium-chromedriver wget http://launchpadlibrarian.net/660838578/chromium-chromedriver_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb Once ready, can install them using gdebi-core, which will handle the dependencies. Install gdebi-core sudo apt-get install gdebi-core Install the Debian packages sudo gdebi chromium-codecs-ffmpeg-extra_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb sudo gdebi chromium-browser_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb sudo gdebi chromium-chromedriver_112.0.5615.49-0ubuntu0.18.04.1_arm64.deb After following these steps, should have a working configuration of Selenium with ChromeDriver on linux-arm64. Verify Installation chromium-browser --version the output should be like Chromium 112.0.5615.49 Built on Ubuntu , running on Ubuntu 22.04 To further test the installation, save the following script as test.py. Note that the arguments provided for options are essential: from selenium import webdriver # Initialize the Chrome WebDriver options = webdriver.ChromeOptions() options.add_argument('--headless') options.add_argument('--no-sandbox') # options.add_argument('--disable-dev-shm-usage') # options.add_argument('--remote-debugging-port=9222') driver = webdriver.Chrome(options=options) # Retrieve the capabilities capabilities = driver.capabilities # For Chrome: if 'browserName' in capabilities and capabilities['browserName'] == 'chrome': browser_version = capabilities.get('browserVersion', 'Unknown') chromedriver_version = capabilities.get('chrome', {}).get('chromedriverVersion', 'Unknown').split(' ')[0] print(f&quot;Browser Name: Chrome&quot;) print(f&quot;Browser Version: {browser_version}&quot;) print(f&quot;ChromeDriver Version: {chromedriver_version}&quot;) # Close the driver driver.quit() Executing python test.py should yield: Browser Name: Chrome Browser Version: 112.0.5615.49 ChromeDriver Version: 112.0.5615.49",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Selenium Manager doesn't work on linux-arm64, Yes because the arch is x86 for the selenium-manager, wondering why the binary has been built not for arm64 making troubles. I figure it out using the file command on selenium-manager binary (both python and node suffers from this wrong arch - nodejs selenium fails ) selenium-manager: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, stripped I found a way to solve this by installing : sudo apt install binfmt-support qemu qemu-user-static assuming you download the geckodriver https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux-aarch64.tar.gz and declare its directory you unpacked it in ~/.local/bin/ declared in you .bashrc or .profile in the PATH a working starting python code I use is from selenium import webdriver from selenium.webdriver.firefox.options import Options options = Options() options.binary_location = r'/usr/bin/firefox-esr' from selenium.webdriver.firefox.service import Service service = Service('/home/pi/.local/bin/geckodriver') driver = webdriver.Firefox(options=options, service=service)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver",
        "selenium-chromedriver",
        "arm64",
        "geckodriver"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2023-08-08T08:48:12",
      "question_id": 76857893,
      "answer_id": 76978633
    }
  },
  {
    "question": "Least common multiple of natural numbers up to a limit, say, 10&#39;000&#39;000",
    "expected_answer": "There are two keys to making this fast. First, using the fastest mult implementation you can get. For &quot;sufficiently large&quot; multiplicands, Python's Karatsuba mult is O(n^1.585). The decimal module's much fancier NTT mult is more like O(n log n). But fastest of all is to install the gmpy2 extension package, which wraps GNU's GMP library, whose chief goal is peak speed. That has essentially the same asymptotics as decimal mult, but with a smaller constant factor. Second, the advanced mult algorithms work best when multiplying two large ints of about the same size (number of bits). You can leave that to luck, or, as below, you can force it by using a priority queue and, at each step, multiplying the &quot;two smallest&quot; partial products remaining. from gmpy2 import mpz from heapq import heapreplace, heappop, heapify # Assuming your input ints are in `xs`. mpzs = list(map(mpz, xs)) heapify(mpzs) for _ in range(len(mpzs) - 1): heapreplace(mpzs, heappop(mpzs) * mpzs[0]) assert len(mpzs) == 1 # the result is mpzs[0] That's the code I'd use. Note that the cost of recursion (which this doesn't use) is trivial compared to the cost of huge-int arithmetic. Heap operations are more expensive than recursion, but still relatively cheap, and can waaaaay more than repay their cost if the input is in an order such that the &quot;by luck&quot; methods aren't lucky enough.",
    "context_chunks": [
      {
        "text": "I'm working on a small Python program for myself and I need an algorithm for fast multiplication of a huge array with prime powers (over 660 000 numbers, each is 7 digits). The result number is over 4 millions digits. Currently I'm using math.prod, which calculates it in ~10 minutes. But that's too slow, especially if I want to increase amount of numbers. I checked some algorithms for faster multiplications, for example the Schönhage–Strassen algorithm and Toom–Cook multiplication, but I didn't understand how they work or how to implement them. I tried some versions that I've found on the internet, but they're not working too well and are even slower. I wonder if someone knows how to multiply these amounts of numbers faster, or could explain how to use some math to do this?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "There are two keys to making this fast. First, using the fastest mult implementation you can get. For &quot;sufficiently large&quot; multiplicands, Python's Karatsuba mult is O(n^1.585). The decimal module's much fancier NTT mult is more like O(n log n). But fastest of all is to install the gmpy2 extension package, which wraps GNU's GMP library, whose chief goal is peak speed. That has essentially the same asymptotics as decimal mult, but with a smaller constant factor. Second, the advanced mult algorithms work best when multiplying two large ints of about the same size (number of bits). You can leave that to luck, or, as below, you can force it by using a priority queue and, at each step, multiplying the &quot;two smallest&quot; partial products remaining. from gmpy2 import mpz from heapq import heapreplace, heappop, heapify # Assuming your input ints are in `xs`. mpzs = list(map(mpz, xs)) heapify(mpzs) for _ in range(len(mpzs) - 1): heapreplace(mpzs, heappop(mpzs) * mpzs[0]) assert len(mpzs) == 1 # the result is mpzs[0] That's the code I'd use. Note that the cost of recursion (which this doesn't use) is trivial compared to the cost of huge-int arithmetic. Heap operations are more expensive than recursion, but still relatively cheap, and can waaaaay more than repay their cost if the input is in an order such that the &quot;by luck&quot; methods aren't lucky enough.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "math.prod will accumulate a product one number at a time. You can do better by recursively dividing the list, taking the product of each half, for example, which reduces the size of the intermediate products. This runs in a few seconds for me: import math def recursive_prod(ns, r): if len(r) &lt;= 10: # arbitrary small base case return math.prod(ns[i] for i in r) split_at = len(r) // 2 return recursive_prod(ns, r[:split_at]) * recursive_prod(ns, r[split_at:]) import random ns = [random.randrange(1_000_000_000) for _ in range(660_000)] p = recursive_prod(ns, range(len(ns)))",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "math",
        "optimization"
      ],
      "question_score": 7,
      "answer_score": 8,
      "created": "2024-07-15T23:55:52",
      "question_id": 78752268,
      "answer_id": 78752523
    }
  },
  {
    "question": "Installing dependencies with Python Poetry is stuck on &quot;Pending...&quot;",
    "expected_answer": "Hi I have the same problem. How I solve it: run &quot;poetry install -vvv&quot; to debug it Then I find poetry hangs instead of asking to unlock keyring Then I find https://github.com/python-poetry/poetry/issues/8623 Solution: run &quot;poetry config keyring.enabled false&quot;",
    "context_chunks": [
      {
        "text": "I am using Raspbian 11 I ran these to commands, and it says &quot;Pending&quot;. pip3 install poetry poetry install I deleted the virtual environment and retried but same error. - Updating certifi (2023.7.22 -&gt; 2022.12.7): Pending... - Updating charset-normalizer (3.2.0 -&gt; 2.1.1): Pending... - Installing markupsafe (2.1.2): Pending... - Installing pytz (2023.3): Pending... - Updating urllib3 (1.26.16 -&gt; 1.26.14): Pending... - Updating zipp (3.16.2 -&gt; 3.11.0): Pending...",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Hi I have the same problem. How I solve it: run &quot;poetry install -vvv&quot; to debug it Then I find poetry hangs instead of asking to unlock keyring Then I find https://github.com/python-poetry/poetry/issues/8623 Solution: run &quot;poetry config keyring.enabled false&quot;",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I also had very different and erratic results when downloading. Quite troublesome and not really plausible why it sometimes works and sometimes not. For me the following method did the trick. Downloads are slower, but after the change at least not pending anymore (Ubuntu 22.04). poetry config installer.max-workers 1 Before I tried using the older installers, but it seems that this is not as stable as setting max-workers to 1. poetry config installer.modern-installation false For more details see here: https://python-poetry.org/docs/configuration/#installermodern-installation For older versions of Poetry there was another flag: https://python-poetry.org/blog/announcing-poetry-1.4.0/ Strangely it was always pending with the urllib package. So I assume that the combination of packages might also influence this.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pip",
        "raspbian",
        "python-poetry"
      ],
      "question_score": 7,
      "answer_score": 17,
      "created": "2023-08-17T12:07:45",
      "question_id": 76921371,
      "answer_id": 78276067
    }
  },
  {
    "question": "Problem after upgrading to django 5.0 &quot;AttributeError: &#39;BlankChoiceIterator&#39; object has no attribute &#39;__len__&#39; &quot;",
    "expected_answer": "If you're using a package that populates the field you'll get this error. I am going to use django-countries as an example. If you're using a similar library, you want to get the actual choices and pass them into models.CharField with a choices parameter and call the choices. Let me know if this fixes it. It would be lovely to have you post your model or form from django_countries.fields import CountryField # Create your models here. class BaseModel(models.Model): # ... other fields country = models.CharField(max_length=200, null=True, choices=CountryField().choices + [('', 'Select Country')]) class Meta: abstract = True`",
    "context_chunks": [
      {
        "text": "I'm new in django triad to upgrade to version 5.0 but got this problem in my project Traceback (most recent call last): File &quot;dirto/env/lib/python3.10/site-packages/django/core/handlers/exception.py&quot;, line 55, in inner response = get_response(request) File &quot;dirto/env/lib/python3.10/site-packages/django/core/handlers/base.py&quot;, line 220, in _get_response response = response.render() File &quot;dirto/env/lib/python3.10/site-packages/django/template/response.py&quot;, line 114, in render self.content = self.rendered_content File &quot;dirto/env/lib/python3.10/site-packages/django/template/response.py&quot;, line 92, in rendered_content return template.render(context, self._request) File &quot;dirto/env/lib/python3.10/site-packages/django/template/backends/django.py&quot;, line 61, in render return self.template.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 171, in render return self._render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 163, in _render return self.nodelist.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in render return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in &lt;listcomp&gt; return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 961, in render_annotated return self.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/loader_tags.py&quot;, line 159, in render return compiled_parent._render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 163, in _render return self.nodelist.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in render return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in &lt;listcomp&gt; return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 961, in render_annotated return self.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/defaulttags.py&quot;, line 325, in render return nodelist.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in render return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in &lt;listcomp&gt; return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 961, in render_annotated return self.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/defaulttags.py&quot;, line 325, in render return nodelist.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in render return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in &lt;listcomp&gt; return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 961, in render_annotated return self.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/defaulttags.py&quot;, line 325, in render return nodelist.render(context) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in render return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 1000, in &lt;listcomp&gt; return SafeString(&quot;&quot;.join([node.render_annotated(context) for node in self])) File &quot;dirto/env/lib/python3.10/site-packages/django/template/base.py&quot;, line 961, in render_annotated return self.render(context) File &quot;dirto/env/lib/python3.10/site-packages/crispy_forms/templatetags/crispy_forms_field.py&quot;, line 125, in render return str(field) File &quot;dirto/env/lib/python3.10/site-packages/django/forms/utils.py&quot;, line 79, in __str__ return self.as_widget() File &quot;dirto/env/lib/python3.10/site-packages/django/forms/boundfield.py&quot;, line 95, in as_widget attrs = self.build_widget_attrs(attrs, widget) File &quot;dirto/env/lib/python3.10/site-packages/django/forms/boundfield.py&quot;, line 270, in build_widget_attrs widget.use_required_attribute(self.initial) File &quot;dirto/env/lib/python3.10/site-packages/django/forms/widgets.py&quot;, line 781, in use_required_attribute first_choice = next(iter(self.choices), None) File &quot;dirto/env/lib/python3.10/site-packages/django_countries/widgets.py&quot;, line 29, in get_choices self._choices: ChoiceList = list(self._choices) File &quot;dirto/env/lib/python3.10/site-packages/django/utils/functional.py&quot;, line 188, in __wrapper__ return getattr(result, __method_name)(*args, **kw) AttributeError: 'BlankChoiceIterator' object has no attribute '__len__' I triad to get help from chatgpt but didn't understand much it give me this answer The error you're encountering, AttributeError: 'BlankChoiceIterator' object has no attribute 'len', is likely related to changes in Django 5.0, specifically to the forms.ChoiceField and its handling of choices. In Django 5.0, the default behavior for the choices parameter in ChoiceField has changed. It now requires choices to be an iterable of tuples, and the first element of each tuple should be unique.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "If you're using a package that populates the field you'll get this error. I am going to use django-countries as an example. If you're using a similar library, you want to get the actual choices and pass them into models.CharField with a choices parameter and call the choices. Let me know if this fixes it. It would be lovely to have you post your model or form from django_countries.fields import CountryField # Create your models here. class BaseModel(models.Model): # ... other fields country = models.CharField(max_length=200, null=True, choices=CountryField().choices + [('', 'Select Country')]) class Meta: abstract = True`",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Found this when I ran into the same problem with django-countries, however @Dev Savvi's answer didn't work for me. There's a temporary patch you can put in your settings.py which fixes this, until such times as django-countries is updated to work with Django 5.X from django_countries.widgets import LazyChoicesMixin LazyChoicesMixin.get_choices = lambda self: self._choices LazyChoicesMixin.choices = property(LazyChoicesMixin.get_choices, LazyChoicesMixin.set_choices) Issue is being tracked here.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "django"
      ],
      "question_score": 7,
      "answer_score": 13,
      "created": "2023-12-15T15:58:09",
      "question_id": 77667419,
      "answer_id": 77676425
    }
  },
  {
    "question": "Change fields in pdf using pypdf?",
    "expected_answer": "The following code copies all the root objects (/AcroForm is a root object) from the reader to the writer fixing the raised PyPdfError(&quot;No /AcroForm dictionary in PdfWriter Object&quot;) writer.clone_reader_document_root(reader)",
    "context_chunks": [
      {
        "text": "i try to update entry-fields in a pdf using the following code with the pytho-module pypdf. At first i read the pdf-files and get all available fields on this firt pdf-page. from pypdf import PdfReader, PdfWriter reader = PdfReader(&quot;exmpl3.pdf&quot;) writer = PdfWriter() page = reader.pages[0] fields = reader.get_fields() for k,v in enumerate(fields): print (k, v) writer.add_page(page) writer.update_page_form_field_values( writer.pages[0], {0: &quot;some filled in text&quot;} ) with open(&quot;filled-out.pdf&quot;, &quot;wb&quot;) as output_stream: writer.write(output_stream) But when i run this program i get the following error-message: C:\\DEV\\Python-Diverses\\pypdf&gt;python exmpl3.py Traceback (most recent call last): File &quot;C:\\DEV\\Python-Diverses\\pypdf\\exmpl3.py&quot;, line 13, in &lt;module&gt; writer.update_page_form_field_values( File &quot;C:\\Users\\WRSPOL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pypdf\\_writer.py&quot;, line 946, in update_page_form_field_values raise PyPdfError(&quot;No /AcroForm dictionary in PdfWriter Object&quot;) pypdf.errors.PyPdfError: No /AcroForm dictionary in PdfWriter Object C:\\DEV\\Python-Diverses\\pypdf&gt;python exmpl3.py 0 form1[0].#subform[0].TextField1[0] 1 form1[0].#subform[0].TextField1[1] 2 form1[0].#subform[0].TextField1[2] 3 form1[0].#subform[0].TextField1[3] 4 form1[0].#subform[0].TextField1[4] 5 form1[0].#subform[0].TextField1[5] 6 form1[0].#subform[0].TextField1[6] 7 form1[0].#subform[0].TextField1[7] 8 form1[0].#subform[0].TextField1[8] 9 form1[0].#subform[0].CheckBox1[0] 10 form1[0].#subform[0].CheckBox2[0] 11 form1[0].#subform[0].CheckBox3[0] 12 form1[0].#subform[0].CheckBox4[0] 13 form1[0].#subform[0].CheckBox5[0] 14 form1[0].#subform[0].SSN[0] 15 form1[0].#subform[0].HouseholdIncome1[0] 16 form1[0].#subform[0].HouseholdIncome1[1] 17 form1[0].#subform[0].HouseholdIncome2[0] 18 form1[0].#subform[0].HouseholdIncome3[0] 19 form1[0].#subform[0].DateSigned[0] 20 form1[0].#subform[0].SignatureField1[0] 21 form1[0].#subform[0] 22 form1[0].#subform[1].Text38[0] 23 form1[0].#subform[1].RadioButtonList[0] 24 form1[0].#subform[1].DateRecordUpdated[0] 25 form1[0].#subform[1].DateSigned[1] 26 form1[0].#subform[1].SignatureField1[1] 27 form1[0].#subform[1].DateNotified[0] 28 form1[0].#subform[1] 29 form1[0] Traceback (most recent call last): File &quot;C:\\DEV\\Python-Diverses\\pypdf\\exmpl3.py&quot;, line 13, in &lt;module&gt; writer.update_page_form_field_values( File &quot;C:\\Users\\WRSPOL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pypdf\\_writer.py&quot;, line 946, in update_page_form_field_values raise PyPdfError(&quot;No /AcroForm dictionary in PdfWriter Object&quot;) pypdf.errors.PyPdfError: No /AcroForm dictionary in PdfWriter Object How can i update the fields in this pdf?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The following code copies all the root objects (/AcroForm is a root object) from the reader to the writer fixing the raised PyPdfError(&quot;No /AcroForm dictionary in PdfWriter Object&quot;) writer.clone_reader_document_root(reader)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Had the same issue. What I did was to use a previous version of pypdf. pip install pypdf==3.8.1",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pypdf"
      ],
      "question_score": 7,
      "answer_score": 10,
      "created": "2023-07-31T11:28:21",
      "question_id": 76803186,
      "answer_id": 77836297
    }
  },
  {
    "question": "How to skip, if starts with, but match other strings",
    "expected_answer": "You could change the pattern to use 2 capture groups, and then use a callback with re.sub. The callback checks if there is a group 1 value. If there is, use it in the replacement, else use group 2 followed by UK/ ^((?:!!|test)\\s.*)|(Street|,)(?=\\d) The regex matches ^((?:!!|test)\\s.*) Capture either !! or test at the start of the string followed by a whitespace char and then the rest of the line in group 1 | Or (Street|,)(?=\\d) Capture either Street or , in group 2 while asserting a digit to the right See a regex101 demo import re lst = ['Street1-2,4,6,8-10', '!! Street4/31/2', 'test Street4'] pattern = r'^((?:!!|test)\\s.*)|(Street|,)(?=\\d)' output = [re.sub(pattern, lambda m: m.group(1) or m.group(2) + 'UK/', line) for line in lst] print(output) Output ['StreetUK/1-2,UK/4,UK/6,UK/8-10', '!! Street4/31/2', 'test Street4']",
    "context_chunks": [
      {
        "text": "I want to match and substitute for strings as shown in the example below, but not for some strings which start with test or !!. I have used negative lookahead to skip matching unwanted strings but (Street|,)(?=\\d) matching for Street &amp; comma replacing group 1 with UK/ is not working as expected. import re input = [ 'Street1-2,4,6,8-10', '!! Street4/31/2', 'test Street4' ] pattern = r'(^(?!test\\s|!!\\s).*(Street|,)(?=\\d))' output = [re.sub(pattern, r'\\g&lt;1&gt;UK/', line) for line in input ] Actual output: ['Street1-2,4,6,UK/8-10', '!! Street4/31/2', 'test Street4'] Expected output: ['StreetUK/1-2,UK/4,UK/6,UK/8-10', '!! Street4/31/2', 'test Street4']",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You could change the pattern to use 2 capture groups, and then use a callback with re.sub. The callback checks if there is a group 1 value. If there is, use it in the replacement, else use group 2 followed by UK/ ^((?:!!|test)\\s.*)|(Street|,)(?=\\d) The regex matches ^((?:!!|test)\\s.*) Capture either !! or test at the start of the string followed by a whitespace char and then the rest of the line in group 1 | Or (Street|,)(?=\\d) Capture either Street or , in group 2 while asserting a digit to the right See a regex101 demo import re lst = ['Street1-2,4,6,8-10', '!! Street4/31/2', 'test Street4'] pattern = r'^((?:!!|test)\\s.*)|(Street|,)(?=\\d)' output = [re.sub(pattern, lambda m: m.group(1) or m.group(2) + 'UK/', line) for line in lst] print(output) Output ['StreetUK/1-2,UK/4,UK/6,UK/8-10', '!! Street4/31/2', 'test Street4']",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Here is one robust solution using the python's regex module that allows use to use PCRE features such as (*SKIP)(*F) and \\G. ^(?:!!|test)\\h.*(*SKIP)(*F)|(\\bStreet|\\G(?!^)[\\d-]*,)(?=\\d) RegEx Demo RegEx Details: ^: Start (?:!!|test): Match !! or test \\h.*: Match a horizontal whitespace followed by any text till end of line (*SKIP)(*F): Skip these matches altogether |: OR (: Start capture group #1 \\b: Match word boundary Street: Match Street |: OR \\G: Start from end position of the previous match (?!^): Make sure we are NOT at the start position [\\d-]*: Match 0 or more of digit or hyphen characters ,: Match a comma ): Close capture group #1 (?=\\d): Lookahead to assert that we have a digit ahead Code import regex arr = [ 'Street1-2,4,6,8-10', '!! Street4/31/2', 'test Street4', '1,2,3' ] rx = regex.compile(r'^(?:!!|test)\\h.*(*SKIP)(*F)|(\\bStreet|\\G(?!^)[\\d-]*,)(?=\\d)') output = [rx.sub(r'\\1UK/', s) for s in arr] print(output) Output: ['StreetUK/1-2,UK/4,UK/6,UK/8-10', '!! Street4/31/2', 'test Street4', '1,2,3']",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "regex"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2025-03-19T07:04:19",
      "question_id": 79519395,
      "answer_id": 79519765
    }
  },
  {
    "question": "FastAPI with uvicorn doesn&#39;t update",
    "expected_answer": "I will elaborate more on the comment written by Yurii Motov. My solution may not be perfect but it solved my problem and I think it adds some value. I stopped all instances of my app running, then I started again my app it worked out. My solution is based on Windows. First, I checked all uvicorn instances running by the following command tasklist /FI &quot;IMAGENAME eq python.exe&quot; In my case, there were two Python instances running. I terminated them with the following commands. PID can be found from the previous image (red arrow). taskkill /PID 24508 /F taskkill /PID 16024 /F Then I started again my app and all changes could be seen on fastapi swagger. uvicorn main:app --reload",
    "context_chunks": [
      {
        "text": "I am using FastAPI with Uvicorn. Everything was working perfectly, but now it seems that nothing is updating. I have changed a method and it does not recognize the change, I have created a new method and it does not recognize it either. Execution: uvicorn API:app --reload. For example, this is a simple code that does not recognize: @app.get(&quot;/clientes/cumple&quot;, response_model=List[Cliente]) async def get_cliente_cumple(dia: int, mes: int): clientes = db.obtener_clientes_por_fecha_nacimiento(dia, mes) if not clientes: raise HTTPException(status_code=404, detail=&quot;Cliente no encontrado&quot;) return [Cliente(codigoclientes=c[&quot;codigoclientes&quot;],titular=c[&quot;titular&quot;],fnacim=c[&quot;fnacim&quot;],dni_cif=c[&quot;dni_cif&quot;], telf=c[&quot;telf&quot;]) for c in clientes] I finish it and restart, but nothing changes. In the terminal where I execute the API, it looks like it recognize the changes, but usually when I do a request, the terminal also shows the HTTP Code result and right now only shows the alleged changes. Does anyone know what might be happening?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I will elaborate more on the comment written by Yurii Motov. My solution may not be perfect but it solved my problem and I think it adds some value. I stopped all instances of my app running, then I started again my app it worked out. My solution is based on Windows. First, I checked all uvicorn instances running by the following command tasklist /FI &quot;IMAGENAME eq python.exe&quot; In my case, there were two Python instances running. I terminated them with the following commands. PID can be found from the previous image (red arrow). taskkill /PID 24508 /F taskkill /PID 16024 /F Then I started again my app and all changes could be seen on fastapi swagger. uvicorn main:app --reload",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I had a similar problem with PyCharm FastAPI. When introducing a change, it used to get stuck on reload. (most likely couldn't kill the current process.) The server was still running on the old version, although I could see it is trying to reload. However, I could resolve it by just using the terminal and running uvicorn main:app --reload It then worked like a charm. You should not use the FastAPI launcher. Instead launch it directly by Uvicorn. I noticed that no matter what I do in &quot;Run/Debug Configuration&quot; of PyCharm, it won't be fixed there. I.E. if I choose Uvicorn in &quot;Run/Debug Config&quot; window of PyCharm, I still face the same problem, it does not reload. For me it works only when I launch the server from the terminal using Uvicorn. UPDATE: I am a windows user. When I use Win+R, and enter cmd, it by default open the terminal in C:/Users/MyUser. I made a .bat file in C:/Users/MyUser to make it easier to run it every time. You can automatically activate your virtual environment and get the code running. This is the .bat file. Please note that based on the location of the file on your machine, you need to change some of the steps. cd .. cd .. cd [Now you should be at C:\\] cd [Navigate to where you store your project] cd .venv cd Scripts call activate cd .. cd .. call uvicorn main:app --reload Therefore, now I just need to use &quot;Win+R&quot;, type cmd, Enter, [.bat_file_name], Enter.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "fastapi"
      ],
      "question_score": 7,
      "answer_score": 13,
      "created": "2023-11-21T09:16:39",
      "question_id": 77521600,
      "answer_id": 78694274
    }
  },
  {
    "question": "What is the point of usedforsecurity?",
    "expected_answer": "TL;DR For almost everyone, ignore the flag, it has no effect whatsoever. The full story involves FIPS and how that gets exposed as a python API. For our purposes, FIPS is a standard that supposedly specifies a safe set of practices. In certain scenarios (e.g. writing software for US government agencies), you are forced to comply with FIPS. To comply with FIPS, your python would have had FIPS mode turned on by building python with FIPS enabled from source. This is the &quot;restricted environment&quot; mentioned in the documentation. If you have a standard python build, then you aren't complying with FIPS and the flag literally does nothing. One aspect of FIPS is restricting the hash functions you are allowed to use. In particular, MD5 is not allowed under FIPS. When you use MD5 in a FIPS environment, you will encounter an error. That is what the introduction of usedforsecurity is supposed to fix: give you an escape hatch in the case that you truly want to use MD5 in a FIPS environment. The parameter is designed to be specified at each call site so it can be audited on a case by case basis. There seems to be confusion on many sides that usedforsecurity has anything to do with security. It's not. Having it set to False doesn't reduce your security. On regular python builds, you use the exact same hash function regardless of usedforsecurity. On FIPS enabled environments, it does however switch your implementation of (allowed) hash functions between those that were explicitly certified† or not. In conclusion, for all intents and purposes, the parameter might as well have been called exceptionforfips because that's the singular purpose it serves: as an escape hatch if you happen to work under a FIPS environment and still need to use a FIPS non-compliant hash. It is quite unfortunate it is part of the API for all users with a seriously misleading name. † However, the certified version doesn't use a different algorithm, certification is very much bureaucratic in nature.",
    "context_chunks": [
      {
        "text": "The parameter usedforsecurity was added to every hash function in hashlib in Python 3.9. Changed in version 3.9: All hashlib constructors take a keyword-only argument usedforsecurity with default value True. A false value allows the use of insecure and blocked hashing algorithms in restricted environments. False indicates that the hashing algorithm is not used in a security context, e.g. as a non-cryptographic one-way compression function. However, this provides zero guidance on When you should use usedforsecurity When you should not use usedforsecurity What &quot;restricted environments&quot; are And while I'm not a security researcher, I darn well know md5 is not secure in any sense of the word. Consequently, the name usedforsecurity boggles my mind in more ways than one. What is the point of usedforsecurity?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "TL;DR For almost everyone, ignore the flag, it has no effect whatsoever. The full story involves FIPS and how that gets exposed as a python API. For our purposes, FIPS is a standard that supposedly specifies a safe set of practices. In certain scenarios (e.g. writing software for US government agencies), you are forced to comply with FIPS. To comply with FIPS, your python would have had FIPS mode turned on by building python with FIPS enabled from source. This is the &quot;restricted environment&quot; mentioned in the documentation. If you have a standard python build, then you aren't complying with FIPS and the flag literally does nothing. One aspect of FIPS is restricting the hash functions you are allowed to use. In particular, MD5 is not allowed under FIPS. When you use MD5 in a FIPS environment, you will encounter an error. That is what the introduction of usedforsecurity is supposed to fix: give you an escape hatch in the case that you truly want to use MD5 in a FIPS environment. The parameter is designed to be specified at each call site so it can be audited on a case by case basis. There seems to be confusion on many sides that usedforsecurity has anything to do with security. It's not. Having it set to False doesn't reduce your security. On regular python builds, you use the exact same hash function regardless of usedforsecurity. On FIPS enabled environments, it does however switch your implementation of (allowed) hash functions between those that were explicitly certified† or not. In conclusion, for all intents and purposes, the parameter might as well have been called exceptionforfips because that's the singular purpose it serves: as an escape hatch if you happen to work under a FIPS environment and still need to use a FIPS non-compliant hash. It is quite unfortunate it is part of the API for all users with a seriously misleading name. † However, the certified version doesn't use a different algorithm, certification is very much bureaucratic in nature.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The explanation given in the docs is pretty clear on when you should or shouldn't use usedforsecurity: pass False if you're not using the algorithm for security purposes. The effects do deserve more explanation. On most Python builds, this parameter does nothing. Insecure algorithms are enabled by default. &quot;Restricted environments&quot; would mostly be rare builds of Python using the OpenSSL FIPS module (or OpenSSL FIPS mode on pre-3.0 OpenSSL) to disable insecure algorithms. If you pass usedforsecurity=False, Python will tell OpenSSL to allow insecure algorithms anyway (see here, here and here, if you'd like to see the code). If you're genuinely not using the algorithm for anything that needs cryptographic-strength guarantees, this is okay. If you'd like to read more, you can also check the issue discussion that originally led to introducing this flag.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "security",
        "hash"
      ],
      "question_score": 7,
      "answer_score": 10,
      "created": "2023-11-05T11:14:07",
      "question_id": 77425682,
      "answer_id": 77461494
    }
  },
  {
    "question": "PydanticImportError: `BaseSettings` has been moved to the `pydantic-settings` package",
    "expected_answer": "First you need to install ydata-profiling: pip install ydata-profiling Then: import pandas as pd from ydata_profiling import ProfileReport # Read the data from a csv file df = pd.read_csv(&quot;data.csv&quot;) # Generate the data profiling report report = ProfileReport(df, title='My Data') report.to_file(&quot;my_report.html&quot;)",
    "context_chunks": [
      {
        "text": "I have titanic dataset downloaded from kaggle. I am implementing pandas's profiling by installing pandas_profiling your contribution will be appreciated! import pandas as pd df = pd.read_csv('E:/pythonWorkspace/excelFiles/train.csv') df.head() from pandas_profiling import ProfileReport prof = ProfileReport(df) #object created! prof.to_file(output_file='output.html') Error : PydanticImportError: `BaseSettings` has been moved to the `pydantic-settings` package. See https://docs.pydantic.dev/2.0.2/migration/#basesettings-has-moved-to-pydantic-settings for more details. For further information visit https://errors.pydantic.dev/2.0.2/u/import-error",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "First you need to install ydata-profiling: pip install ydata-profiling Then: import pandas as pd from ydata_profiling import ProfileReport # Read the data from a csv file df = pd.read_csv(&quot;data.csv&quot;) # Generate the data profiling report report = ProfileReport(df, title='My Data') report.to_file(&quot;my_report.html&quot;)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The reason you're getting this issue is due to pydantic v2 having breaking changes compared to from v1. Solution Revert to Pydantic v1 by running pip install &quot;pydantic==1.*&quot; Alternative Solution Change the failing part of the code to use the new import structure found in Pydantic v2 from pydantic_settings import BaseSettings",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "pandas-profiling"
      ],
      "question_score": 7,
      "answer_score": 11,
      "created": "2023-07-12T06:16:51",
      "question_id": 76667611,
      "answer_id": 76678838
    }
  },
  {
    "question": "How to compose functions through purely using Python&#39;s standard library?",
    "expected_answer": "Well, since you're saying I want to &quot;abuse&quot; the language and only use existing definitions from the standard library starting with Python 3.12, the test suite happens to contain the gadget you want: import functools import operator from test.test_zipfile._path._functools import compose increment = functools.partial(operator.add, 1) double = functools.partial(operator.mul, 2) increment_and_double = compose(increment, double) print(increment_and_double(10)) (I found this by way of a strategic ag compose in my local CPython checkout.)",
    "context_chunks": [
      {
        "text": "Python's standard library is vast, and my intuition tells that there must be a way in it to accomplish this, but I just can't figure it out. This is purely for curiosity and learning purposes: I have two simple functions: def increment(x): return x + 1 def double(x): return x * 2 and I want to compose them into a new function double_and_increment. I could of course simply do that as such: double_and_increment = lambda x: increment(double(x)) but I could also do it in a more convoluted but perhaps more &quot;ergonomically scalable&quot; way: import functools double_and_increment = functools.partial(functools.reduce, lambda acc, f: f(acc), [double, increment]) Both of the above work fine: &gt;&gt;&gt; double_and_increment(1) 3 Now, the question is, is there tooling in the standard library that would allow achieving the composition without any user-defined lambdas, regular functions, or classes. The first intuition is to replace the lambda acc, f: f(acc) definition in the functools.reduce call with operator.call, but that unfortunately takes the arguments in the reverse order: &gt;&gt;&gt; (lambda acc, f: f(acc))(1, str) # What we want to replace. &gt;&gt;&gt; '1' &gt;&gt;&gt; import operator &gt;&gt;&gt; operator.call(str, 1) # Incorrect argument order. &gt;&gt;&gt; '1' I have a hunch that using functools.reduce is still the way to accomplish the composition, but for the life of me I can't figure out a way to get rid of the user-defined lambda. Few out-of-the-box methods that got me close: import functools, operator # Curried form, can't figure out how to uncurry. functools.partial(operator.methodcaller, '__call__')(1)(str) # The arguments needs to be in the middle of the expression, which does not work. operator.call(*reversed(operator.attrgetter('args')(functools.partial(functools.partial, operator.call)(1, str)))) Have looked through all the existing questions, but they are completely different and rely on using user-defined functions and/or lambdas.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Well, since you're saying I want to &quot;abuse&quot; the language and only use existing definitions from the standard library starting with Python 3.12, the test suite happens to contain the gadget you want: import functools import operator from test.test_zipfile._path._functools import compose increment = functools.partial(operator.add, 1) double = functools.partial(operator.mul, 2) increment_and_double = compose(increment, double) print(increment_and_double(10)) (I found this by way of a strategic ag compose in my local CPython checkout.)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "While it is cool that @AKX found the function test.test_zipfile._path._functools.compose in the CPython code tree that perfectly implements the OP's desired functionality of function composition, it does not actually belong to the standard library as required by the rules of the question, for reasons that: It belongs to a helper module in a test suite of the CPython implementation of the Python language. A test suite is not part of the standard library of the language; it is just code that validates a particular implementation of the language and its standard library. A test suite, let alone any helper functions within the test suite, may be removed at any time without any normal due process of advanced deprecation warnings. Other implementations of Python does not need to include any of CPython's test suite in order to conform to Python's specifications. So, without the helper function in the test suite of CPython 3.12 that is not part of the standard library, I believe the OP is indeed correct in the assessment that there is no out-of-the-box tooling in Python's standard library that can implement function composition. BUT, that doesn't mean we can't achieve it by modifying existing tooling, since the OP's rules are simply to use &quot;tooling in the standard library that would allow achieving the composition without any user-defined lambdas, regular functions, or classes&quot;. Since the OP almost got it already with: double_and_increment = partial(reduce, lambda acc, f: f(acc), [double, increment]) and: &gt;&gt;&gt; (lambda acc, f: f(acc))(1, str) # What we want to replace. &gt;&gt;&gt; '1' &gt;&gt;&gt; import operator &gt;&gt;&gt; operator.call(str, 1) # Incorrect argument order. &gt;&gt;&gt; '1' The real question here is then how we can modify an existing function in the standard library such that it becomes: def rcall(value, obj): return obj(value) To do that, let's take a look at the bytecode of the above function, as well as relevant attributes of the code object that defines the parameters: &gt;&gt;&gt; import dis &gt;&gt;&gt; def call(value, obj): ... return obj(value) ... &gt;&gt;&gt; dis.dis(call) 1 0 RESUME 0 2 2 PUSH_NULL 4 LOAD_FAST 1 (obj) 6 LOAD_FAST 0 (value) 8 PRECALL 1 12 CALL 1 22 RETURN_VALUE &gt;&gt;&gt; c = call.__code__ &gt;&gt;&gt; c.co_varnames ('value', 'obj') &gt;&gt;&gt; c.co_argcount 2 &gt;&gt;&gt; c.co_nlocals 2 &gt;&gt;&gt; No surprise there. A simple function body that loads the second argument (obj) and the first argument (value) onto the stack, then make a call with the callable and the argument in the stack, and finally returns the value at the top of the stack to the caller. Now, let's find a similarly simple function in the standard library that takes an argument or two and make a call with it/them, so it can more easily be modified into our desired function. As it turns out, operator.abs is one such function, which takes one argument and makes a wrapper call to the built-in _abs function: def abs(a): &quot;Same as abs(a).&quot; return _abs(a) We'd want to disassemble it for comparison, and yet unfortunately, if we try accessing operator.abs.__code__, you would get an error: &gt;&gt;&gt; import operator &gt;&gt;&gt; operator.abs.__code__ Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; AttributeError: 'builtin_function_or_method' object has no attribute '__code__'. Did you mean: '__call__'? &gt;&gt;&gt; This is because CPython's implementation of the operator module includes an _operator module, which overrides all of the operator.py's pure-Python functions with ones implemented in C, wit a try block in operator.py: try: from _operator import * except ImportError: pass Functions implemented in C do not have __code__ objects and therefore cannot be modified. What we need is the pure Python version of operator.call, before it's overridden by _operator.call. But how do we avoid the override? Well, we can import _operator module ourselves first and and delete the call attribute from it so that the modified module is cached in sys.modules such that when operator.py imports _operator, it is our modified version that it gets, without call in it: &gt;&gt;&gt; try: # other Python implementations may not have _operator.py ... import _operator ... del _operator.call ... except ImportError: ... pass ... &gt;&gt;&gt; import operator &gt;&gt;&gt; operator.call.__code__ &lt;code object call at 0x000001F68F4FADB0, file &quot;C:\\python311\\Lib\\operator.py&quot;, line 226&gt; Great! Now we can finally get to look at the bytecode and relevant attributes of the code object of operator.abs: &gt;&gt;&gt; dis.dis(operator.abs) 71 0 RESUME 0 73 2 LOAD_GLOBAL 1 (NULL + _abs) 14 LOAD_FAST 0 (a) 16 PRECALL 1 20 CALL 1 30 RETURN_VALUE 71 0 RESUME 0 &gt;&gt;&gt; c = operator.abs.__code__ &gt;&gt;&gt; c.co_varnames ('a',) &gt;&gt;&gt; c.co_argcount 1 &gt;&gt;&gt; c.co_nlocals 1 &gt;&gt;&gt; As can be seen, all we need to modify to turn operator.abs into our desired function object is to replace the LOAD_GLOBAL instruction into PUSH_NULL (to indicate a regular function call for CALL) and LOAD_FAST 1 (to load the second argument, the callable), as well as co_varnames, co_argcount and co_nlocals to add a second parameter obj. To obtain a modified code object from the existing code object of operator.abs we can call its replace method: try: import _operator del _operator.abs except ImportError: pass from operator import abs as rcall from opcode import opmap from functools import partial, reduce code = bytearray(rcall.__code__.co_code) code[code.find(opmap['LOAD_GLOBAL']):code.find(opmap['LOAD_FAST'])] = \\ opmap['PUSH_NULL'], 0, opmap['LOAD_FAST'], 1 rcall.__code__ = rcall.__code__.replace( co_code=bytes(code), co_varnames=('value', 'obj'), co_argcount=2, co_nlocals=2 ) print(rcall(1, str)) This correctly outputs: 1 So it then becomes trivial to implement the composite function that the OP wants, by plugging in the modified operator.call into the OP's close attempt: def increment(x): return x + 1 def double(x): return x * 2 double_and_increment = partial(reduce, rcall, [double, increment]) print(double_and_increment(1)) This outputs: 3 Demo: here",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "functional-programming",
        "standard-library",
        "language-features",
        "function-composition"
      ],
      "question_score": 7,
      "answer_score": 8,
      "created": "2023-11-05T12:48:12",
      "question_id": 77425962,
      "answer_id": 77426442
    }
  },
  {
    "question": "Local Azure Function: Customer packages not in sys path. This should never happen",
    "expected_answer": "This warning will usually appear when you develop and run azure function locally. If you look at the source code of Python worker for Azure Functions this logging is happening here. if CUSTOMER_PACKAGES_PATH not in sys.path: logger.warning(&quot;Customer packages not in sys path.&quot;) CUSTOMER_PACKAGES_PATH is defined as: CUSTOMER_PACKAGES_PATH = &quot;/home/site/wwwroot/.python_packages/lib/site-packages&quot; As you can see the path starts with /home/site/wwwroot which is how the file path looks like post deployment. So, this check is meant to ensure that the paths are configured correctly after the deployment. You can ignore the warning when developing locally.",
    "context_chunks": [
      {
        "text": "I'm encountering a weird warning with azure functions locally. Whenever I func start my function, I get these error messages: Found Python version 3.10.12 (python3). Azure Functions Core Tools Core Tools Version: 4.0.5455 Commit hash: N/A (64-bit) Function Runtime Version: 4.27.5.21554 [2023-11-14T10:02:39.795Z] Customer packages not in sys path. This should never happen! [2023-11-14T10:02:42.194Z] Worker process started and initialized. In the host.json, extensionBundle version is [3.*, 4.0.0) In the local.settings.json, &quot;FUNCTIONS_WORKER_RUNTIME&quot;: &quot;python&quot; The function app is based on the new model of python azure function (func init MyProjFolder --worker-runtime python --model V2 https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=linux%2Cisolated-process%2Cnode-v4%2Cpython-v2%2Chttp-trigger%2Ccontainer-apps&amp;pivots=programming-language-python) My first interrogation is the first warning: Customer packages not in sys path. This should never happen!. I'm using a virtual environment. The function is starting correctly, but what is this warning? local.settings.json: { &quot;IsEncrypted&quot;: false, &quot;Values&quot;: { &quot;FUNCTIONS_WORKER_RUNTIME&quot;: &quot;python&quot;, &quot;AzureWebJobsStorage&quot;: &quot;UseDevelopmentStorage=true&quot;, &quot;AzureWebJobsFeatureFlags&quot;: &quot;EnableWorkerIndexing&quot; } }",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This warning will usually appear when you develop and run azure function locally. If you look at the source code of Python worker for Azure Functions this logging is happening here. if CUSTOMER_PACKAGES_PATH not in sys.path: logger.warning(&quot;Customer packages not in sys path.&quot;) CUSTOMER_PACKAGES_PATH is defined as: CUSTOMER_PACKAGES_PATH = &quot;/home/site/wwwroot/.python_packages/lib/site-packages&quot; As you can see the path starts with /home/site/wwwroot which is how the file path looks like post deployment. So, this check is meant to ensure that the paths are configured correctly after the deployment. You can ignore the warning when developing locally.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This seems to be the issue with the latest version of Azure function Core tools (4.0.5455), which is published recently (6 days ago) as mentioned in the official doc. I have created a python Azure function to check the same: Python Version: 3.11.5 Azure Functions Core Tools Core Tools Version: 4.0.5348 Commit hash: N/A (64-bit) Function Runtime Version: 4.24.5.21262 Didn't get any such warning: Updated the Azure function Core tools version to 4.0.5455. Running the same Azure Function again with below versions: Python version 3.11.6 (py). Azure Functions Core Tools Core Tools Version: 4.0.5455 Commit hash: N/A (64-bit) Function Runtime Version: 4.27.5.21554",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "azure-functions",
        "azure-functions-core-tools"
      ],
      "question_score": 7,
      "answer_score": 10,
      "created": "2023-11-14T09:47:27",
      "question_id": 77479584,
      "answer_id": 77481190
    }
  },
  {
    "question": "Pythonic way of dropping columns used in assign (i.e. Pandas equivalent of `.keep = &quot;unused&quot;`)",
    "expected_answer": "I've never used R but based on the definition of unused and AFIK, to simulate the same behaviour in pandas, you will need to pop each column from a copy of the original DataFrame : &quot;unused&quot; retains only the columns not used in ... to create new columns. This is useful if you generate new columns, but no longer need the columns used to generate them. DataFrame.pop(item) returns item and drops from frame. Raises KeyError if not found. ( iris.copy().assign( new_col= lambda x: x.pop('sepal length (cm)') + x.pop('petal length (cm)') * x.pop('petal width (cm)')) ) Output : sepal width (cm) new_col 0 3.5 5.38 1 3.0 5.18 2 3.2 4.96 3 3.1 4.90 4 3.6 5.28 .. ... ... 145 3.0 18.66 146 2.5 15.80 147 3.0 16.90 148 3.4 18.62 149 3.0 15.08 [150 rows x 2 columns]",
    "context_chunks": [
      {
        "text": "In dplyr package of R, there's the option .keep = &quot;unused&quot; when creating new columns with the function mutate() (which is their equivalent of assign). An example, for those who haven't used it: &gt; head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa # any column used in creating `new_col` is dropped afterwards automatically &gt; mutate(.data = head(iris), new_col = Sepal.Length + Petal.Length * Petal.Width, .keep = &quot;unused&quot;) Sepal.Width Species new_col 1 3.5 setosa 5.38 2 3.0 setosa 5.18 3 3.2 setosa 4.96 4 3.1 setosa 4.90 5 3.6 setosa 5.28 6 3.9 setosa 6.08 I say they are equivalent, but there doesn't appear to be the option for doing this with assign in the Pandas documentation so I assume it doesn't exist. I was curious about creating a way of doing something similar then. One way I can think of to do this is to create a list of names beforehand, and drop them afterwards, like this: from sklearn import datasets import pandas as pd used_columns = ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)'] iris = pd.DataFrame(datasets.load_iris().data, columns=datasets.load_iris().feature_names) iris.assign(new_col = lambda x: x['sepal length (cm)'] + x['petal length (cm)'] * x['petal width (cm)']).drop(used_columns, axis=1) or iris.assign(new_col = lambda x: x[used_columns[0]] + x[used_columns[1]] * x[used_columns[2]]).drop(used_columns, axis=1) Which seems ~fine~, but requires a separate list, and with the first one, keeping two things updated, and with the second, the cognitive load of keeping track of what the nth list item is in my head. So I was curious if there's another way I'm not aware of of doing this, that would be easier to maintain? Both of the ones above seem not very Pythonic? Research I've done: I did a bunch of googling around this, with no luck. It seems there's plenty of ways of dropping columns, but none I've found seem particularly well-suited to this type of situation. Any help you could provide would be much appreciated! Answers which use other Python packages (e.g. janitor) are okay too.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I've never used R but based on the definition of unused and AFIK, to simulate the same behaviour in pandas, you will need to pop each column from a copy of the original DataFrame : &quot;unused&quot; retains only the columns not used in ... to create new columns. This is useful if you generate new columns, but no longer need the columns used to generate them. DataFrame.pop(item) returns item and drops from frame. Raises KeyError if not found. ( iris.copy().assign( new_col= lambda x: x.pop('sepal length (cm)') + x.pop('petal length (cm)') * x.pop('petal width (cm)')) ) Output : sepal width (cm) new_col 0 3.5 5.38 1 3.0 5.18 2 3.2 4.96 3 3.1 4.90 4 3.6 5.28 .. ... ... 145 3.0 18.66 146 2.5 15.80 147 3.0 16.90 148 3.4 18.62 149 3.0 15.08 [150 rows x 2 columns]",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Similar to @Timeless's answer: iris[&quot;new_col&quot;] = iris.pop(&quot;sepal length (cm)&quot;) + iris.pop(&quot;petal length (cm)&quot;) * iris.pop(&quot;petal width (cm)&quot;) print(iris.head()) Prints: sepal width (cm) new_col 0 3.5 5.38 1 3.0 5.18 2 3.2 4.96 3 3.1 4.90 4 3.6 5.28",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ],
      "question_score": 7,
      "answer_score": 9,
      "created": "2023-09-24T22:47:55",
      "question_id": 77169204,
      "answer_id": 77169246
    }
  },
  {
    "question": "How can I install Python library in ChatGPT Code Interpreter",
    "expected_answer": "You can upload a wheel file taken from PyPI, select cp38-manylinux_..._x86_64.whl and upload it. Tell ChatGPT to unzip and move it to /home/sandbox/.local/lib/python3.8/site-packages/ Then you can import and use it normally. Here's an example case where I install DuckDB: https://chat.openai.com/share/fa3df390-8a25-45d3-997a-c19d4f19df67",
    "context_chunks": [
      {
        "text": "ChatGPT is the newest platform for running Python in a Jupyter-like environment. However the installed libraries are limited. You cannot access the internet too. So, I cannot use pip to install. How can I install a new library?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can upload a wheel file taken from PyPI, select cp38-manylinux_..._x86_64.whl and upload it. Tell ChatGPT to unzip and move it to /home/sandbox/.local/lib/python3.8/site-packages/ Then you can import and use it normally. Here's an example case where I install DuckDB: https://chat.openai.com/share/fa3df390-8a25-45d3-997a-c19d4f19df67",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Go to the https://pypi.org/ page for the package you went to install and download the &quot;.whl&quot; file for the package. Go back to GPT and upload the file as an attachment and then add the question. &quot;What is this?&quot; THe AI will answer you and tell you that it cannot install it. I want you to say this &quot;I know you can't install it but can you try anyway. I want to see the resulting error from the installation. &quot; Then the AI should try and install it. I hope this helps! Please be advised it sometimes triggers an &quot;Our systems have detected unusual activity from your system. Please try again later.&quot; message.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "jupyter",
        "chat-gpt-4",
        "code-interpreter"
      ],
      "question_score": 7,
      "answer_score": 12,
      "created": "2023-07-12T06:58:34",
      "question_id": 76667874,
      "answer_id": 76667875
    }
  },
  {
    "question": "Can you have a progress bar for sorting a list?",
    "expected_answer": "Given the interface provided by sort, you don't have too many options for hooking into the actual sorting algoithm. However, if 50K keys is slow, it is most likely that invoking the key function is slow, which is computed prior to the actual sorting. From the docs: The key corresponding to each item in the list is calculated once and then used for the entire sorting process. So if you keep count of how many times the key method is invoked, you could gain a rough estimate for the whole sorting process. To do so, you could create a wrapper for the key function to manage this book keeping: def progress_sort(data, *, key=lambda v: v, on_increment=None): total = len(data) if on_increment is None: start = time.time() def on_increment(c): print(f&quot;{time.time() - start}: {c/total * 100}%&quot;) count = 0 def progress_key(val): nonlocal count if count % int(total / 10) == 0: on_increment(count) count += 1 return key(val) data.sort(key=progress_key) on_increment(total) Example with some dummy data and a slow key method def slow_key(val): time.sleep(1.0/500_000) return val data = [random.randint(-50_000, 50_000)/1.0 for i in range(50_000)] progress_sort(data, key=slow_key) 0.0: 0.0% 0.5136210918426514: 10.0% 1.0435900688171387: 20.0% 1.6074442863464355: 30.0% 2.156496524810791: 40.0% 2.9734878540039062: 50.0% 3.4794368743896484: 60.0% 4.016523599624634: 70.0% 4.558118104934692: 80.0% 5.047779083251953: 90.0% 5.545809030532837: 100.0% This method could then be combined with whatever type of library you wish to use for updating status. You may wish to further configure the data supplied to the provided hooks, however, the principle remains the same. Here's an example that uses tqdm: def slow_key(val): time.sleep(1.0/500_000) return val data = [random.randint(-50_000, 50_000)/1.0 for i in range(50_001)] with tqdm(total=len(data), desc=&quot;sorting&quot;) as pbar: progress_sort(data, key=slow_key, on_increment=lambda c: pbar.update(c - pbar.n)) pbar.set_description(&quot;Finished&quot;) sorting: 80%|███████▉ | 40000/50001 [00:05&lt;00:01, 5802.30it/s] Finished: 100%|██████████| 50001/50001 [00:07&lt;00:00, 6489.14it/s]",
    "context_chunks": [
      {
        "text": "I have a list containing ~50k elements of a custom data type (the latter is probably not important for my question) I'm sorting the list using pythons builtin list.sort() method. myList: List[Foo] = ... myList.sort(key=Foo.x) Since the sorting takes a couple of minutes, I would like to have a progress bar for the sorting process. I haven't found any solutions online. Is this even possible? I'm aware sorting algorithms may be complex and it might not be possible to measure the sorting progress at all. However, it would be fine for my usecase to have a &quot;rough&quot; measurement, like 25%, 50%, 75%...",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Given the interface provided by sort, you don't have too many options for hooking into the actual sorting algoithm. However, if 50K keys is slow, it is most likely that invoking the key function is slow, which is computed prior to the actual sorting. From the docs: The key corresponding to each item in the list is calculated once and then used for the entire sorting process. So if you keep count of how many times the key method is invoked, you could gain a rough estimate for the whole sorting process. To do so, you could create a wrapper for the key function to manage this book keeping: def progress_sort(data, *, key=lambda v: v, on_increment=None): total = len(data) if on_increment is None: start = time.time() def on_increment(c): print(f&quot;{time.time() - start}: {c/total * 100}%&quot;) count = 0 def progress_key(val): nonlocal count if count % int(total / 10) == 0: on_increment(count) count += 1 return key(val) data.sort(key=progress_key) on_increment(total) Example with some dummy data and a slow key method def slow_key(val): time.sleep(1.0/500_000) return val data = [random.randint(-50_000, 50_000)/1.0 for i in range(50_000)] progress_sort(data, key=slow_key) 0.0: 0.0% 0.5136210918426514: 10.0% 1.0435900688171387: 20.0% 1.6074442863464355: 30.0% 2.156496524810791: 40.0% 2.9734878540039062: 50.0% 3.4794368743896484: 60.0% 4.016523599624634: 70.0% 4.558118104934692: 80.0% 5.047779083251953: 90.0% 5.545809030532837: 100.0% This method could then be combined with whatever type of library you wish to use for updating status. You may wish to further configure the data supplied to the provided hooks, however, the principle remains the same. Here's an example that uses tqdm: def slow_key(val): time.sleep(1.0/500_000) return val data = [random.randint(-50_000, 50_000)/1.0 for i in range(50_001)] with tqdm(total=len(data), desc=&quot;sorting&quot;) as pbar: progress_sort(data, key=slow_key, on_increment=lambda c: pbar.update(c - pbar.n)) pbar.set_description(&quot;Finished&quot;) sorting: 80%|███████▉ | 40000/50001 [00:05&lt;00:01, 5802.30it/s] Finished: 100%|██████████| 50001/50001 [00:07&lt;00:00, 6489.14it/s]",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I also assumed that the key determination was the slow part of the sort, which is expected with such a relatively small list size (50k). This answer's approach takes the solution of crafting an intermediate list of just the keys and the object reference. This can be quantified and progress can be shown after each object has its key determined. For this demonstration this was made slow by using a key routine with a 100ms sleep inside of it. At the end, the real sort, will hopefully be able to be run extremely quickly as the keys have all been precalculated. #!/usr/bin/python import time import random from operator import attrgetter, itemgetter class Custom: def __init__(self): self._key = random.randint(1,50000) @property def key(self): # print('slow key') time.sleep(0.1) return self._key def __repr__(self): return f&quot;Custom(key={self._key})&quot; mylist = [Custom() for i in range(40)] print(mylist) mylist2 = [] display_inc = 5 display = 0 for x, ele in enumerate(mylist): mylist2.append((ele.key,ele)) if x/len(mylist) * 100 &gt;= display: print(f&quot;{display}% done&quot;) # stop displaying after 90 if display &gt;= 90: display = 110 display += display_inc print(&quot;95% done&quot;) mylist2.sort(key=itemgetter(0)) mylist = [i[1] for i in mylist2] print(&quot;100% done&quot;) print(mylist) returns $ python slowsort.py [Custom(key=22549), Custom(key=5431), Custom(key=8895), Custom(key=10837), Custom(key=12652), Custom(key=43897), Custom(key=24724), Custom(key=16014), Custom(key=46022), Custom(key=25979), Custom(key=45115), Custom(key=45442), Custom(key=42306), Custom(key=17611), Custom(key=25113), Custom(key=12924), Custom(key=21902), Custom(key=1661), Custom(key=6475), Custom(key=41993), Custom(key=40334), Custom(key=44407), Custom(key=20747), Custom(key=7635), Custom(key=38258), Custom(key=45187), Custom(key=13048), Custom(key=18952), Custom(key=46592), Custom(key=10790), Custom(key=24978), Custom(key=5349), Custom(key=47924), Custom(key=12413), Custom(key=7147), Custom(key=17528), Custom(key=3035), Custom(key=16639), Custom(key=17059), Custom(key=25630)] 0% done 5% done 10% done 15% done 20% done 25% done 30% done 35% done 40% done 45% done 50% done 55% done 60% done 65% done 70% done 75% done 80% done 85% done 90% done 95% done 100% done [Custom(key=1661), Custom(key=3035), Custom(key=5349), Custom(key=5431), Custom(key=6475), Custom(key=7147), Custom(key=7635), Custom(key=8895), Custom(key=10790), Custom(key=10837), Custom(key=12413), Custom(key=12652), Custom(key=12924), Custom(key=13048), Custom(key=16014), Custom(key=16639), Custom(key=17059), Custom(key=17528), Custom(key=17611), Custom(key=18952), Custom(key=20747), Custom(key=21902), Custom(key=22549), Custom(key=24724), Custom(key=24978), Custom(key=25113), Custom(key=25630), Custom(key=25979), Custom(key=38258), Custom(key=40334), Custom(key=41993), Custom(key=42306), Custom(key=43897), Custom(key=44407), Custom(key=45115), Custom(key=45187), Custom(key=45442), Custom(key=46022), Custom(key=46592), Custom(key=47924)] For some reason if the actual sort on the precalculated keys were slow, then you could partition the list into smaller lists, then resorted into one bigger list but that's kinda messy, so I would want to understand if that would be necessary. Hopefully the significant slowness is in the key generation.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "list",
        "sorting"
      ],
      "question_score": 7,
      "answer_score": 14,
      "created": "2023-07-11T10:30:56",
      "question_id": 76661046,
      "answer_id": 76796565
    }
  },
  {
    "question": "Python `typing.cast` method vs colon type hints",
    "expected_answer": "I will answer you through a well-written example: def get_name(can_return_name: bool) -&gt; str | None: in_name = input('Enter your name: ') return in_name if can_return_name else None def do_something_with_name() -&gt; None: name: str = get_name(True) print(name.capitalize()) Here, you'd expect Pyright or mypy to say that name's type is str, even if the return type of get_name is str | None, but this is not the case: if the function you are calling (in this case get_name) has a return type, the column notation type will be ignored completely, and Pyright will tell you that you can't call .capitalize on a name, as it could be None In this case though, you are completely sure that name will be str and not None, so you can use cast, like such: def do_something_with_name() -&gt; None: name: str = cast(str, get_name(True)) print(name.capitalize()) get_name's value will be unchanged, but static type checkers will treat them as the first argument's type. in fact, as python's documentation states, the cast function does in fact nothing at all. It can be defined with the new Python 3.12 generics as such (will be valid when PEP 718 will be accepted, thanks to @InSync): def cast[T](var: object) -&gt; T: return var cast[str](get_name(True))",
    "context_chunks": [
      {
        "text": "What the difference between using the Python typing.cast method x = cast(str, x) compared with using type hints/colon notation/left hand side type annotation? x: str I've seen the use of the cast method in codebases but don't have an apparent reason to use it instead of type hint notation, which is more concise.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I will answer you through a well-written example: def get_name(can_return_name: bool) -&gt; str | None: in_name = input('Enter your name: ') return in_name if can_return_name else None def do_something_with_name() -&gt; None: name: str = get_name(True) print(name.capitalize()) Here, you'd expect Pyright or mypy to say that name's type is str, even if the return type of get_name is str | None, but this is not the case: if the function you are calling (in this case get_name) has a return type, the column notation type will be ignored completely, and Pyright will tell you that you can't call .capitalize on a name, as it could be None In this case though, you are completely sure that name will be str and not None, so you can use cast, like such: def do_something_with_name() -&gt; None: name: str = cast(str, get_name(True)) print(name.capitalize()) get_name's value will be unchanged, but static type checkers will treat them as the first argument's type. in fact, as python's documentation states, the cast function does in fact nothing at all. It can be defined with the new Python 3.12 generics as such (will be valid when PEP 718 will be accepted, thanks to @InSync): def cast[T](var: object) -&gt; T: return var cast[str](get_name(True))",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I think it's simply for inlining a type annotation on an expression. The PEP contains the following line: return cast(str, a[index]) There is no variable to annotate here, since it's a[index] that's being &quot;casted&quot;. Without cast, you'd need to either ensure that a is properly typed so it can be inferred, or create a new variable just to use :-style annotations.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-typing"
      ],
      "question_score": 7,
      "answer_score": 9,
      "created": "2023-10-09T15:31:54",
      "question_id": 77260102,
      "answer_id": 77260290
    }
  },
  {
    "question": "FutureWarning: &#39;DataFrame.swapaxes&#39; is deprecated and will be removed in a future version",
    "expected_answer": "According to the numpy issue on github, this &quot;bug&quot; will not be fixed in numpy. The official statement is that np.split should not be used to split pandas DataFrames anymore. Instead, iloc should be used to split DataFrames as it is described in this answer. As it looks that you are splitting the DataFrame for machine learning reasons, please be aware that there are built-in methods in many machine learning libraries doing the test/train(/validation) split for you. Here the example of sklearn.",
    "context_chunks": [
      {
        "text": "Looks like numpy is using deprecated function DataFrame.swapaxes in fromnumeric.py. Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead. return bound(*args, **kwds) I am getting this warning from the following line of code in Jupyter Notebook: train, val, test = np.split(df.sample(frac=1), [int(0.8*len(df)), int(0.9*len(df))]) This is the structure of the dataframe I am using: What exactly is raising this warning and what should I change in my code to get rid of this Warning? I also found that this is currently an open issue of numpy in github. It will be great if anybody could help. Thanks in advance.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "According to the numpy issue on github, this &quot;bug&quot; will not be fixed in numpy. The official statement is that np.split should not be used to split pandas DataFrames anymore. Instead, iloc should be used to split DataFrames as it is described in this answer. As it looks that you are splitting the DataFrame for machine learning reasons, please be aware that there are built-in methods in many machine learning libraries doing the test/train(/validation) split for you. Here the example of sklearn.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can keep using np.split (or more generally, np.array_split) by splitting instead on the index: for chunk_idx in np.array_split(df.index, num_chunks): df_chunk = df.loc[chunk_idx]",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy"
      ],
      "question_score": 7,
      "answer_score": 7,
      "created": "2023-11-30T07:56:36",
      "question_id": 77576750,
      "answer_id": 77858849
    }
  },
  {
    "question": "Check if Series has Values in Range",
    "expected_answer": "Another possible solution: df['User Class'] = ( df.groupby('UserName')['Permissions'] .transform(lambda x: 'Admin' if (x &lt; 10).all() else 'User' if (x &gt;= 10).all() else 'Admin/User')) Output: UserName Permissions User Class 0 John Doe 2 Admin/User 1 John Doe 11 Admin/User 2 Example 9 Admin 3 Example 8 Admin 4 User3 11 User",
    "context_chunks": [
      {
        "text": "I have a Pandas dataframe that has user information and also has a column for their permissions: UserName Permissions John Doe 02 John Doe 11 Example 09 Example 08 User3 11 I am trying to create a new column called User Class that is based on their Permissions (looking at all of the users permissions). If a user has all permissions &lt;10, they are considered Admin. If a user has all permission &gt;=10, they are considered User. However if they have permissions that are both &lt;10 and &gt;=10, then they will be coded as Admin/User. So my resulting output would be: UserName Permissions User Class John Doe 02 Admin/User John Doe 11 Admin/User Example 09 Admin Example 08 Admin User3 11 User What would be the best way to do this? My original idea was to do: for UserName, User_df in df.groupby(by='UserName'): LT10 = (User_df['Permissions'] &lt; 10).any() GTE10 = (User_df['Permissions'] &gt;= 10).any() if (LT10 &amp; GTE10): UserClass = 'Admin/User' elif LT10: UserClass = 'Admin' elif GTE10: UserClass = 'User' df.at[User_df.index, 'User Class'] = UserClass However these seems very inefficient because df has ~800K records",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Another possible solution: df['User Class'] = ( df.groupby('UserName')['Permissions'] .transform(lambda x: 'Admin' if (x &lt; 10).all() else 'User' if (x &gt;= 10).all() else 'Admin/User')) Output: UserName Permissions User Class 0 John Doe 2 Admin/User 1 John Doe 11 Admin/User 2 Example 9 Admin 3 Example 8 Admin 4 User3 11 User",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Group by the username and use transform to compute the min/max permission values per group. Then use this to compute User Class using np.select: import numpy as np import pandas as pd data = { &quot;UserName&quot;: [&quot;John Doe&quot;, &quot;John Doe&quot;, &quot;Example&quot;, &quot;Example&quot;, &quot;User3&quot;], &quot;Permissions&quot;: [2, 11, 9, 8, 11], } df = pd.DataFrame(data) permissions = df.groupby(&quot;UserName&quot;)[&quot;Permissions&quot;] min_permission = permissions.transform(&quot;min&quot;) max_permission = permissions.transform(&quot;max&quot;) df[&quot;User Class&quot;] = np.select( [ (min_permission &lt; 10) &amp; (max_permission &lt; 10), (min_permission &gt;= 10) &amp; (max_permission &gt;= 10), ], [&quot;Admin&quot;, &quot;User&quot;], default=&quot;Admin/User&quot;, ) print(df) Output: UserName Permissions User Class 0 John Doe 2 Admin/User 1 John Doe 11 Admin/User 2 Example 9 Admin 3 Example 8 Admin 4 User3 11 User",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2024-09-03T18:32:34",
      "question_id": 78945659,
      "answer_id": 78945687
    }
  },
  {
    "question": "ModuleNotFoundError: No module named &#39;google.generative&#39;",
    "expected_answer": "To install from PyPI, run pip install google-generativeai. Obtain an API key from AI Studio, then configure it here. Also, your import statement is wrong. You did import google.generative as genai but it should be import google.generativeai as genai. See PyPi.",
    "context_chunks": [
      {
        "text": "Traceback: File &quot;C:\\Users\\lucifer\\AppData\\Roaming\\Python\\Python310\\site-packages\\streamlit\\runtime\\scriptrunner\\script_runner.py&quot;, line 535, in _run_script exec(code, module.__dict__) File &quot;D:\\mlproject\\app.py&quot;, line 9, in &lt;module&gt; import google.generative as genai This error is shown. For generativeai, I tried all methods to install by typing command pip install google-generativeai in cmd, but still generativea ai is not coming in generativea. I am trying this on Python 3.10 version. Tried to install the library by giving command pip install google-generativeai still no results.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "To install from PyPI, run pip install google-generativeai. Obtain an API key from AI Studio, then configure it here. Also, your import statement is wrong. You did import google.generative as genai but it should be import google.generativeai as genai. See PyPi.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Make sure to create a fresh virtual environment then : the correct installation: pip install google-generativeai and the correct import import google.generativeai as genai Link to doc: https://ai.google.dev/api/python/google/generativeai",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pip",
        "modulenotfounderror",
        "google-generativeai"
      ],
      "question_score": 7,
      "answer_score": 10,
      "created": "2024-01-23T18:45:46",
      "question_id": 77868611,
      "answer_id": 77876991
    }
  },
  {
    "question": "Langchain / ChromaDB: Why does VectorStore return so many duplicates?",
    "expected_answer": "the issue is here: Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) everytime you execute the file, you are inserting the same documents into the database. you could comment out that part of code if you are inserting from same file. or you could detect the similar vectors using EmbeddingsRedundantFilter Filter that drops redundant documents by comparing their embeddings.",
    "context_chunks": [
      {
        "text": "import os from langchain.llms import OpenAI import bs4 import langchain from langchain import hub from langchain.document_loaders import UnstructuredFileLoader from langchain.embeddings import OpenAIEmbeddings from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import Chroma os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;KEY&quot; loader = UnstructuredFileLoader( 'path_to_file' ) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, add_start_index=True ) all_splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) retriever = vectorstore.as_retriever(search_type=&quot;similarity&quot;, search_kwargs={&quot;k&quot;: 6}) retrieved_docs = retriever.get_relevant_documents( &quot;What is X?&quot; ) This returns: [Document(page_content=&quot;...&quot;, metadata={'source': 'path_to_text', 'start_index': 16932}), Document(page_content=&quot;...&quot;, metadata={'source': 'path_to_text', 'start_index': 16932}), Document(page_content=&quot;...&quot;, metadata={'source': 'path_to_text', 'start_index': 16932}), Document(page_content=&quot;...&quot;, metadata={'source': 'path_to_text', 'start_index': 16932}), Document(page_content=&quot;...&quot;, metadata={'source': 'path_to_text', 'start_index': 16932}), Document(page_content=&quot;...&quot;, metadata={'source': 'path_to_text', 'start_index': 16932})] Which is all seemingly the same document. When I first ran this code in Google Colab/Jupyter Notebook, it returned different documents...as I ran it more, it started returning the same documents. Makes me feel like this is a database issue, where the same entry is being inserted into the database with each run. How do I return 6 different unique documents?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "the issue is here: Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) everytime you execute the file, you are inserting the same documents into the database. you could comment out that part of code if you are inserting from same file. or you could detect the similar vectors using EmbeddingsRedundantFilter Filter that drops redundant documents by comparing their embeddings.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I wrote this simple function to find the unique values of the embedded docs in a chroma db vector store, it iterates through all the source files that are duplicated and outputs the unique values: ## get list of all file URLs in vector db def get_unique_files(): db = vectordb print(&quot;\\nEmbedding keys:&quot;, db.get().keys()) print(&quot;\\nNumber of embedded docs:&quot;, len(db.get()[&quot;ids&quot;])) # Print the list of source files # for x in range(len(db.get()[&quot;ids&quot;])): # # print(db.get()[&quot;metadatas&quot;][x]) # doc = db.get()[&quot;metadatas&quot;][x] # source = doc[&quot;source&quot;] # print(source) # db.get() file_list = [] for x in range(len(db.get()[&quot;ids&quot;])): doc = db.get()[&quot;metadatas&quot;][x] source = doc[&quot;source&quot;] # print(source) file_list.append(source) ### Set only stores a value once even if it is inserted more than once. list_set = set(file_list) unique_list = (list(list_set)) print(&quot;\\nList of unique files in db:\\n&quot;) for unique_file in unique_list: print(unique_file) issue the function with: get_unique_files() This will output only the individual files that were used for the embedding content: Embedding keys: dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'uris', 'data']) Number of embedded docs: 140 List of unique files in db: pdf-files/leadership-team.pdf pdf-files/report-summary.pdf csv-files/small-csv.csv ppt-content/presentation.pptx csv-files/dataset-04-17-2024.csv",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "openai-api",
        "langchain",
        "py-langchain",
        "chromadb"
      ],
      "question_score": 7,
      "answer_score": 11,
      "created": "2023-11-27T08:01:32",
      "question_id": 77555312,
      "answer_id": 77568404
    }
  },
  {
    "question": "Getting min/max column name in Polars",
    "expected_answer": "You can build a &quot;multiple choice&quot; of when/then and pl.coalesce them to create a single column of results. df.with_columns(max_col = pl.coalesce( pl.when(pl.col(name) == pl.max_horizontal(df.columns)) .then(pl.lit(name)) for name in df.columns ) ) shape: (3, 3) ┌─────┬──────┬─────────┐ │ a ┆ b ┆ max_col │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ str │ ╞═════╪══════╪═════════╡ │ 1 ┆ 4 ┆ b │ │ 8 ┆ 5 ┆ a │ │ 3 ┆ null ┆ a │ └─────┴──────┴─────────┘",
    "context_chunks": [
      {
        "text": "In polars I can get the horizontal max (maximum value of a set of columns for reach row) like this: df = pl.DataFrame( { &quot;a&quot;: [1, 8, 3], &quot;b&quot;: [4, 5, None], } ) df.with_columns(max = pl.max_horizontal(&quot;a&quot;, &quot;b&quot;)) ┌─────┬──────┬─────┐ │ a ┆ b ┆ max │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ i64 │ ╞═════╪══════╪═════╡ │ 1 ┆ 4 ┆ 4 │ │ 8 ┆ 5 ┆ 8 │ │ 3 ┆ null ┆ 3 │ └─────┴──────┴─────┘ This corresponds to Pandas df[[&quot;a&quot;, &quot;b&quot;]].max(axis=1). Now, how do I get the column names instead of the actual max value? In other words, what is the Polars version of Pandas' df[CHANGE_COLS].idxmax(axis=1)? The expected output would be: ┌─────┬──────┬─────┐ │ a ┆ b ┆ max │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ str │ ╞═════╪══════╪═════╡ │ 1 ┆ 4 ┆ b │ │ 8 ┆ 5 ┆ a │ │ 3 ┆ null ┆ a │ └─────┴──────┴─────┘",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can build a &quot;multiple choice&quot; of when/then and pl.coalesce them to create a single column of results. df.with_columns(max_col = pl.coalesce( pl.when(pl.col(name) == pl.max_horizontal(df.columns)) .then(pl.lit(name)) for name in df.columns ) ) shape: (3, 3) ┌─────┬──────┬─────────┐ │ a ┆ b ┆ max_col │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ str │ ╞═════╪══════╪═════════╡ │ 1 ┆ 4 ┆ b │ │ 8 ┆ 5 ┆ a │ │ 3 ┆ null ┆ a │ └─────┴──────┴─────────┘",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can concatenate the elements into a list using pl.concat_list, get the index of the largest element using pl.Expr.list.arg_max, and replace the index with the column name using pl.Expr.replace. mapping = {0: &quot;a&quot;, 1: &quot;b&quot;} ( df .with_columns( pl.concat_list([&quot;a&quot;, &quot;b&quot;]).list.arg_max().replace(mapping).alias(&quot;max_col&quot;) ) ) This can all be wrapped into a function to also handle the creation of the mapping dict. def max_col(cols) -&gt; str: mapping = dict(enumerate(cols)) return pl.concat_list(cols).list.arg_max().replace(mapping) df.with_columns(max_col([&quot;a&quot;, &quot;b&quot;]).alias(&quot;max_col&quot;)) Output. shape: (3, 3) ┌─────┬──────┬─────────┐ │ a ┆ b ┆ max_col │ │ --- ┆ --- ┆ --- │ │ i64 ┆ i64 ┆ str │ ╞═════╪══════╪═════════╡ │ 1 ┆ 4 ┆ b │ │ 8 ┆ 5 ┆ a │ │ 3 ┆ null ┆ a │ └─────┴──────┴─────────┘",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "max",
        "python-polars"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2024-02-09T10:09:12",
      "question_id": 77967334,
      "answer_id": 77968254
    }
  },
  {
    "question": "Problem with initializing AzureChatOpenAI()",
    "expected_answer": "In my environment, I used package versions openai=0.27.0 and langchain=0.0.341. I tried with the below code to call AzureChatOpenAI() from langchain using Python SDK. Code: from langchain.chat_models import AzureChatOpenAI from langchain.schema import HumanMessage OPENAI_API_BASE=&quot;&lt;Your azure openai endpoint&gt;&quot; GPT_DEPLOYMENT_NAME=&quot;&lt;Your deployment name&gt;&quot; OPENAI_API_KEY=&quot;&lt;Your api key&gt;&quot; model = AzureChatOpenAI( openai_api_base=OPENAI_API_BASE, openai_api_version=&quot;2023-07-01-preview&quot;, azure_deployment=GPT_DEPLOYMENT_NAME, openai_api_key=OPENAI_API_KEY, openai_api_type=&quot;azure&quot;, ) message = HumanMessage( content=&quot;Translate this sentence from English to Spanish.MS Dhoni as the greatest finisher in the history of the sport&quot; ) print(model([message])) Output: content='MS Dhoni como el mejor finalizador en la historia del deporte.' Reference: Azure OpenAI | 🦜️🔗 Langchain Update: I got a similar error when I had OPENAI_* and AZURE_* environment variables in openai version greater than 1.0.0. You can refer to this GitHub page about this issue. So try using AZURE_OPENAI_ENDPOINT in your environment. Code: from langchain.chat_models import AzureChatOpenAI from langchain.schema import HumanMessage import os GPT_DEPLOYMENT_NAME=&quot;deploymentname1&quot; os.environ[&quot;AZURE_OPENAI_API_KEY&quot;] = &quot;2xxxxx1&quot; os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;] = &quot;https://xxxx.openai.azure.com/&quot; model = AzureChatOpenAI( azure_endpoint=os.getenv(&quot;AZURE_OPENAI_ENDPOINT&quot;), openai_api_version=&quot;2023-03-15-preview&quot;, azure_deployment=GPT_DEPLOYMENT_NAME, ) message = HumanMessage( content=&quot;Translate this sentence from English to Spanish.MS Dhoni as the greatest finisher in the history of the sport&quot; ) print(model([message]))",
    "context_chunks": [
      {
        "text": "I try to call AzureChatOpenAI() from langchain. Normally I would do: model = AzureChatOpenAI( openai_api_base=os.getenv(&quot;OPENAI_API_BASE&quot;), openai_api_version=&quot;2023-03-15-preview&quot;, deployment_name=os.getenv(&quot;GPT_DEPLOYMENT_NAME&quot;), openai_api_key=os.getenv(&quot;OPENAI_API_KEY&quot;), openai_api_type=&quot;azure&quot;, ) But I get the warnings python3.9/site-packages/langchain/chat_models/azure_openai.py:155: UserWarning: As of openai&gt;=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://xxxx.openai.azure.com/ to https://xxxx.openai.azure.com/openai. warnings.warn( python3.9/site-packages/langchain/chat_models/azure_openai.py:162: UserWarning: As of openai&gt;=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`. warnings.warn( python3.9/site-packages/langchain/chat_models/azure_openai.py:170: UserWarning: As of openai&gt;=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://xxxx.openai.azure.com/ to https://xxxx.openai.azure.com/openai. warnings.warn( But if I follow the instructions and change it to: model = AzureChatOpenAI( azure_endpoint=os.getenv(&quot;OPENAI_API_BASE&quot;), openai_api_version=&quot;2023-03-15-preview&quot;, azure_deployment=os.getenv(&quot;GPT_DEPLOYMENT_NAME&quot;), openai_api_key=os.getenv(&quot;OPENAI_API_KEY&quot;), openai_api_type=&quot;azure&quot;, ) I get the error ValidationError: 1 validation error for AzureChatOpenAI __root__ base_url and azure_endpoint are mutually exclusive (type=value_error) What am I doing wrong?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In my environment, I used package versions openai=0.27.0 and langchain=0.0.341. I tried with the below code to call AzureChatOpenAI() from langchain using Python SDK. Code: from langchain.chat_models import AzureChatOpenAI from langchain.schema import HumanMessage OPENAI_API_BASE=&quot;&lt;Your azure openai endpoint&gt;&quot; GPT_DEPLOYMENT_NAME=&quot;&lt;Your deployment name&gt;&quot; OPENAI_API_KEY=&quot;&lt;Your api key&gt;&quot; model = AzureChatOpenAI( openai_api_base=OPENAI_API_BASE, openai_api_version=&quot;2023-07-01-preview&quot;, azure_deployment=GPT_DEPLOYMENT_NAME, openai_api_key=OPENAI_API_KEY, openai_api_type=&quot;azure&quot;, ) message = HumanMessage( content=&quot;Translate this sentence from English to Spanish.MS Dhoni as the greatest finisher in the history of the sport&quot; ) print(model([message])) Output: content='MS Dhoni como el mejor finalizador en la historia del deporte.' Reference: Azure OpenAI | 🦜️🔗 Langchain Update: I got a similar error when I had OPENAI_* and AZURE_* environment variables in openai version greater than 1.0.0. You can refer to this GitHub page about this issue. So try using AZURE_OPENAI_ENDPOINT in your environment. Code: from langchain.chat_models import AzureChatOpenAI from langchain.schema import HumanMessage import os GPT_DEPLOYMENT_NAME=&quot;deploymentname1&quot; os.environ[&quot;AZURE_OPENAI_API_KEY&quot;] = &quot;2xxxxx1&quot; os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;] = &quot;https://xxxx.openai.azure.com/&quot; model = AzureChatOpenAI( azure_endpoint=os.getenv(&quot;AZURE_OPENAI_ENDPOINT&quot;), openai_api_version=&quot;2023-03-15-preview&quot;, azure_deployment=GPT_DEPLOYMENT_NAME, ) message = HumanMessage( content=&quot;Translate this sentence from English to Spanish.MS Dhoni as the greatest finisher in the history of the sport&quot; ) print(model([message]))",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "For langchain_openai==0.0.5 this setup seems to be working: llm = AzureChatOpenAI( openai_api_key=os.getenv(&quot;KEY&quot;), azure_endpoint=os.getenv(&quot;ENDPOINT&quot;), openai_api_version=os.getenv(&quot;API_VERSION&quot;), deployment_name=os.getenv(&quot;MODEL_NAME&quot;), ) I intentionally didn't use the suggested environment variables from docs to be sure that only explicitly passed parameters are used.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "azure",
        "openai-api",
        "langchain"
      ],
      "question_score": 7,
      "answer_score": 9,
      "created": "2023-11-25T17:10:08",
      "question_id": 77548802,
      "answer_id": 77561939
    }
  },
  {
    "question": "Why is numba popcount code twice as fast as equivalent C code?",
    "expected_answer": "TL;DR: the performance gap between the GCC and the Clang version is due to the use of scalar instructions versus SIMD instructions. The performance gap between the Numba and the Clang version comes from the size of the integers that is not the same between the two version : 64-bit versus 32-bits. Performance Results First of all, I am also able to reproduce the problem on my Intel i5-9600KF. Here are the results (and the versions): Numba 0.56.4: 170.089 ms Clang 14.0.6: 190.350 ms GCC 12.2.0: 328.133 ms To understand what happens, we need to analyze the assembly code produce by all compilers. Assembly code Here is the assembly code of the hot loop produced by GCC: .L5: xorl %edx, %edx popcntl %eax, %edx incl %eax addl %edx, %ebx cmpl %ecx, %eax jne .L5 Here is the one produced by Clang: .LBB1_3: # =&gt;This Inner Loop Header: Depth=1 vpand %ymm5, %ymm0, %ymm12 vpshufb %ymm12, %ymm6, %ymm12 vpsrlw $4, %ymm0, %ymm13 vpand %ymm5, %ymm13, %ymm13 vpshufb %ymm13, %ymm6, %ymm13 vpaddb %ymm12, %ymm13, %ymm12 vpunpckhdq %ymm1, %ymm12, %ymm13 # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7] vpsadbw %ymm1, %ymm13, %ymm13 vpunpckldq %ymm1, %ymm12, %ymm12 # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5] vpsadbw %ymm1, %ymm12, %ymm12 vpackuswb %ymm13, %ymm12, %ymm12 vpaddd %ymm2, %ymm0, %ymm13 vpaddd %ymm12, %ymm8, %ymm8 vpand %ymm5, %ymm13, %ymm12 vpshufb %ymm12, %ymm6, %ymm12 vpsrlw $4, %ymm13, %ymm13 vpand %ymm5, %ymm13, %ymm13 vpshufb %ymm13, %ymm6, %ymm13 vpaddb %ymm12, %ymm13, %ymm12 vpunpckhdq %ymm1, %ymm12, %ymm13 # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7] vpsadbw %ymm1, %ymm13, %ymm13 vpunpckldq %ymm1, %ymm12, %ymm12 # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5] vpsadbw %ymm1, %ymm12, %ymm12 vpackuswb %ymm13, %ymm12, %ymm12 vpaddd %ymm3, %ymm0, %ymm13 vpaddd %ymm12, %ymm9, %ymm9 vpand %ymm5, %ymm13, %ymm12 vpshufb %ymm12, %ymm6, %ymm12 vpsrlw $4, %ymm13, %ymm13 vpand %ymm5, %ymm13, %ymm13 vpshufb %ymm13, %ymm6, %ymm13 vpaddb %ymm12, %ymm13, %ymm12 vpunpckhdq %ymm1, %ymm12, %ymm13 # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7] vpsadbw %ymm1, %ymm13, %ymm13 vpunpckldq %ymm1, %ymm12, %ymm12 # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5] vpsadbw %ymm1, %ymm12, %ymm12 vpackuswb %ymm13, %ymm12, %ymm12 vpaddd %ymm4, %ymm0, %ymm13 vpaddd %ymm12, %ymm10, %ymm10 vpand %ymm5, %ymm13, %ymm12 vpshufb %ymm12, %ymm6, %ymm12 vpsrlw $4, %ymm13, %ymm13 vpand %ymm5, %ymm13, %ymm13 vpshufb %ymm13, %ymm6, %ymm13 vpaddb %ymm12, %ymm13, %ymm12 vpunpckhdq %ymm1, %ymm12, %ymm13 # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7] vpsadbw %ymm1, %ymm13, %ymm13 vpunpckldq %ymm1, %ymm12, %ymm12 # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5] vpsadbw %ymm1, %ymm12, %ymm12 vpackuswb %ymm13, %ymm12, %ymm12 vpaddd %ymm12, %ymm11, %ymm11 vpaddd %ymm7, %ymm0, %ymm0 addl $-32, %edx jne .LBB1_3 Here is the one produced by Numba: .LBB0_8: vpand %ymm0, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm0, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq -40(%rsp), %ymm0, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm1, %ymm1 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq -72(%rsp), %ymm0, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm2, %ymm2 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq %ymm0, %ymm8, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm3, %ymm3 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm4, %ymm4 vpaddq %ymm0, %ymm11, %ymm6 vpand %ymm6, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpsrlw $4, %ymm6, %ymm6 vpand %ymm6, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpaddb %ymm7, %ymm6, %ymm6 vpaddq %ymm0, %ymm12, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm1, %ymm1 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq %ymm0, %ymm13, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm2, %ymm2 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq %ymm0, %ymm14, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm3, %ymm3 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm4, %ymm4 vpaddq %ymm0, %ymm15, %ymm0 addq $-2, %rbx jne .LBB0_8 Analysis First of all, we can see that the GCC code use the popcntl instruction which is very fast, at least for scalar operations. Clang generate a assembly code using the AVX-2 SIMD instruction set on my machine. This is why the program produced by Clang is so fast compared to GCC : it operates on many items in parallel thanks to SIMD instructions. Numba generate a code very similar to Clang. This is not surprising since Numba is based on LLVM-Lite (and so LLVM), while Clang is also based on LLVM. However, there are small differences explaining the performance impact. Indeed, the Numba assembly code operates on twice more items than the Clang counterpart. This can be seen by counting the number of vpsrlw instructions (8 VS 4). I do not expect this to make the difference since the Clang loop is already well unrolled and the benefit of unrolling it more is tiny. Actually, this more aggressive unrolling is a side effect. The key difference is that Numba operates on 64-bit integers while the C code operates on 32-bit integers! This is why Clang unroll the loop differently and generate different instructions. In fact, the smaller integers causes Clang to generate a sequence of instructions to convert integers of different size which is less efficient. IMHO, this is a side effect impacting the optimizer since operating on smaller items can generally be used generate faster SIMD code. The code produced by LLVM seems sub-optimal in this case : it saturates the port 5 (ie. shuffle/permute execution unit) on my machine while one can write a code not saturating it (not easy though). Faster C implementation You can fix the C++ implementation so to operate 64-bit integers: #include &lt;stdio.h&gt; #include &lt;time.h&gt; #include &lt;stdint.h&gt; // Function to calculate the population count (number of set bits) of an integer using __builtin_popcount uint64_t popcount(uint64_t num) { return __builtin_popcountl(num); } int main() { int64_t n; printf(&quot;Enter the value of n: &quot;); scanf(&quot;%ld&quot;, &amp;n); // Variables to store start and end times struct timespec start_time, end_time; // Get the current time as the start time clock_gettime(CLOCK_MONOTONIC, &amp;start_time); int64_t sum = 0; for (int64_t i = 0; i &lt; n; i++) { sum += popcount(i); } // Get the current time as the end time clock_gettime(CLOCK_MONOTONIC, &amp;end_time); // Calculate the elapsed time in microseconds long long elapsed_time = (end_time.tv_sec - start_time.tv_sec) * 1000000LL + (end_time.tv_nsec - start_time.tv_nsec) / 1000; printf(&quot;Sum of population counts from 0 to %ld-1 is: %ld\\n&quot;, n, sum); printf(&quot;Elapsed time: %lld microseconds\\n&quot;, elapsed_time); return 0; } This produces a program as fast as Numba on my machine using Clang (GCC still generate a slow scalar implementation). Notes SIMD versions only make sense if your real-world code is SIMD-friendly, that is if popcount can be applied on multiple contiguous items. Otherwise, results of the scalar implementation can be drastically different (in fact, the three compilers generate a very close code which I expect to be equally fast). AVX-512 provides the SIMD instruction VPOPCNTDQ that should clearly outperforms the code generated by LLVM using (only) AVX-2.. Since I do not have AVX-512 on my machine, and AVX-2 does not provide such an instruction, it makes sense of LLVM to produce an assembly code using AVX-2. The AVX-512 instruction can count the number of 1 in 16 x 32-bit integers in parallel while taking about the same number of cycle than its scalar counterpart. To be more precise, the instruction is only available using the instruction set AVX512VPOPCNTDQ + AVX512VL (which AFAIK is not available on all CPU supporting AVX-512). As of now, this instruction is only available on few x86-64 micro-architectures (eg. Intel Ice-Lake, Intel Sapphire Rapids and AMD Zen4).",
    "context_chunks": [
      {
        "text": "I have this simple python/numba code: from numba import njit import numba as nb @nb.njit(nb.uint64(nb.uint64)) def popcount(x): b=0 while(x &gt; 0): x &amp;= x - nb.uint64(1) b+=1 return b @njit def timed_loop(n): summand = 0 for i in range(n): summand += popcount(i) return summand It just adds the popcounts for integers 0 to n - 1. When I time it I get: %timeit timed_loop(1000000) 340 µs ± 1.08 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) It turns out that llvm cleverly converts the popcount function into the native CPU POPCNT instruction so we should expect it to be fast. But the question is, how fast. I thought I would compare it to a C version to see the speed difference. #include &lt;stdio.h&gt; #include &lt;time.h&gt; // Function to calculate the population count (number of set bits) of an integer using __builtin_popcount int popcount(int num) { return __builtin_popcount(num); } int main() { unsigned int n; printf(&quot;Enter the value of n: &quot;); scanf(&quot;%d&quot;, &amp;n); // Variables to store start and end times struct timespec start_time, end_time; // Get the current time as the start time clock_gettime(CLOCK_MONOTONIC, &amp;start_time); int sum = 0; for (unsigned int i = 0; i &lt; n; i++) { sum += popcount(i); } // Get the current time as the end time clock_gettime(CLOCK_MONOTONIC, &amp;end_time); // Calculate the elapsed time in microseconds long long elapsed_time = (end_time.tv_sec - start_time.tv_sec) * 1000000LL + (end_time.tv_nsec - start_time.tv_nsec) / 1000; printf(&quot;Sum of population counts from 0 to %d-1 is: %d\\n&quot;, n, sum); printf(&quot;Elapsed time: %lld microseconds\\n&quot;, elapsed_time); return 0; } I then compiled this with -march=native -Ofast. I tried both gcc and clang and the results were very similar. ./popcount Enter the value of n: 1000000 Sum of population counts from 0 to 1000000-1 is: 9884992 Elapsed time: 732 microseconds Why is the numba twice as fast as the C code?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "TL;DR: the performance gap between the GCC and the Clang version is due to the use of scalar instructions versus SIMD instructions. The performance gap between the Numba and the Clang version comes from the size of the integers that is not the same between the two version : 64-bit versus 32-bits. Performance Results First of all, I am also able to reproduce the problem on my Intel i5-9600KF. Here are the results (and the versions): Numba 0.56.4: 170.089 ms Clang 14.0.6: 190.350 ms GCC 12.2.0: 328.133 ms To understand what happens, we need to analyze the assembly code produce by all compilers. Assembly code Here is the assembly code of the hot loop produced by GCC: .L5: xorl %edx, %edx popcntl %eax, %edx incl %eax addl %edx, %ebx cmpl %ecx, %eax jne .L5 Here is the one produced by Clang: .LBB1_3: # =&gt;This Inner Loop Header: Depth=1 vpand %ymm5, %ymm0, %ymm12 vpshufb %ymm12, %ymm6, %ymm12 vpsrlw $4, %ymm0, %ymm13 vpand %ymm5, %ymm13, %ymm13 vpshufb %ymm13, %ymm6, %ymm13 vpaddb %ymm12, %ymm13, %ymm12 vpunpckhdq %ymm1, %ymm12, %ymm13 # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7] vpsadbw %ymm1, %ymm13, %ymm13 vpunpckldq %ymm1, %ymm12, %ymm12 # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5] vpsadbw %ymm1, %ymm12, %ymm12 vpackuswb %ymm13, %ymm12, %ymm12 vpaddd %ymm2, %ymm0, %ymm13 vpaddd %ymm12, %ymm8, %ymm8 vpand %ymm5, %ymm13, %ymm12 vpshufb %ymm12, %ymm6, %ymm12 vpsrlw $4, %ymm13, %ymm13 vpand %ymm5, %ymm13, %ymm13 vpshufb %ymm13, %ymm6, %ymm13 vpaddb %ymm12, %ymm13, %ymm12 vpunpckhdq %ymm1, %ymm12, %ymm13 # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7] vpsadbw %ymm1, %ymm13, %ymm13 vpunpckldq %ymm1, %ymm12, %ymm12 # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5] vpsadbw %ymm1, %ymm12, %ymm12 vpackuswb %ymm13, %ymm12, %ymm12 vpaddd %ymm3, %ymm0, %ymm13 vpaddd %ymm12, %ymm9, %ymm9 vpand %ymm5, %ymm13, %ymm12 vpshufb %ymm12, %ymm6, %ymm12 vpsrlw $4, %ymm13, %ymm13 vpand %ymm5, %ymm13, %ymm13 vpshufb %ymm13, %ymm6, %ymm13 vpaddb %ymm12, %ymm13, %ymm12 vpunpckhdq %ymm1, %ymm12, %ymm13 # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7] vpsadbw %ymm1, %ymm13, %ymm13 vpunpckldq %ymm1, %ymm12, %ymm12 # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5] vpsadbw %ymm1, %ymm12, %ymm12 vpackuswb %ymm13, %ymm12, %ymm12 vpaddd %ymm4, %ymm0, %ymm13 vpaddd %ymm12, %ymm10, %ymm10 vpand %ymm5, %ymm13, %ymm12 vpshufb %ymm12, %ymm6, %ymm12 vpsrlw $4, %ymm13, %ymm13 vpand %ymm5, %ymm13, %ymm13 vpshufb %ymm13, %ymm6, %ymm13 vpaddb %ymm12, %ymm13, %ymm12 vpunpckhdq %ymm1, %ymm12, %ymm13 # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7] vpsadbw %ymm1, %ymm13, %ymm13 vpunpckldq %ymm1, %ymm12, %ymm12 # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5] vpsadbw %ymm1, %ymm12, %ymm12 vpackuswb %ymm13, %ymm12, %ymm12 vpaddd %ymm12, %ymm11, %ymm11 vpaddd %ymm7, %ymm0, %ymm0 addl $-32, %edx jne .LBB1_3 Here is the one produced by Numba: .LBB0_8: vpand %ymm0, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm0, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq -40(%rsp), %ymm0, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm1, %ymm1 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq -72(%rsp), %ymm0, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm2, %ymm2 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq %ymm0, %ymm8, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm3, %ymm3 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm4, %ymm4 vpaddq %ymm0, %ymm11, %ymm6 vpand %ymm6, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpsrlw $4, %ymm6, %ymm6 vpand %ymm6, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpaddb %ymm7, %ymm6, %ymm6 vpaddq %ymm0, %ymm12, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm1, %ymm1 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq %ymm0, %ymm13, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm2, %ymm2 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpaddq %ymm0, %ymm14, %ymm7 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm3, %ymm3 vpand %ymm7, %ymm9, %ymm6 vpshufb %ymm6, %ymm10, %ymm6 vpsrlw $4, %ymm7, %ymm7 vpand %ymm7, %ymm9, %ymm7 vpshufb %ymm7, %ymm10, %ymm7 vpaddb %ymm6, %ymm7, %ymm6 vpsadbw %ymm5, %ymm6, %ymm6 vpaddq %ymm6, %ymm4, %ymm4 vpaddq %ymm0, %ymm15, %ymm0 addq $-2, %rbx jne .LBB0_8 Analysis First of all, we can see that the GCC code use the popcntl instruction which is very fast, at least for scalar operations. Clang generate a assembly code using the AVX-2 SIMD instruction set on my machine. This is why the program produced by Clang is so fast compared to GCC : it operates on many items in parallel thanks to SIMD instructions. Numba generate a code very similar to Clang. This is not surprising since Numba is based on LLVM-Lite (and so LLVM), while Clang is also based on LLVM. However, there are small differences explaining the performance impact. Indeed, the Numba assembly code operates on twice more items than the Clang counterpart. This can be seen by counting the number of vpsrlw instructions (8 VS 4). I do not expect this to make the difference since the Clang loop is already well unrolled and the benefit of unrolling it more is tiny. Actually, this more aggressive unrolling is a side effect. The key difference is that Numba operates on 64-bit integers while the C code operates on 32-bit integers! This is why Clang unroll the loop differently and generate different instructions. In fact, the smaller integers causes Clang to generate a sequence of instructions to convert integers of different size which is less efficient. IMHO, this is a side effect impacting the optimizer since operating on smaller items can generally be used generate faster SIMD code. The code produced by LLVM seems sub-optimal in this case : it saturates the port 5 (ie. shuffle/permute execution unit) on my machine while one can write a code not saturating it (not easy though). Faster C implementation You can fix the C++ implementation so to operate 64-bit integers: #include &lt;stdio.h&gt; #include &lt;time.h&gt; #include &lt;stdint.h&gt; // Function to calculate the population count (number of set bits) of an integer using __builtin_popcount uint64_t popcount(uint64_t num) { return __builtin_popcountl(num); } int main() { int64_t n; printf(&quot;Enter the value of n: &quot;); scanf(&quot;%ld&quot;, &amp;n); // Variables to store start and end times struct timespec start_time, end_time; // Get the current time as the start time clock_gettime(CLOCK_MONOTONIC, &amp;start_time); int64_t sum = 0; for (int64_t i = 0; i &lt; n; i++) { sum += popcount(i); } // Get the current time as the end time clock_gettime(CLOCK_MONOTONIC, &amp;end_time); // Calculate the elapsed time in microseconds long long elapsed_time = (end_time.tv_sec - start_time.tv_sec) * 1000000LL + (end_time.tv_nsec - start_time.tv_nsec) / 1000; printf(&quot;Sum of population counts from 0 to %ld-1 is: %ld\\n&quot;, n, sum); printf(&quot;Elapsed time: %lld microseconds\\n&quot;, elapsed_time); return 0; } This produces a program as fast as Numba on my machine using Clang (GCC still generate a slow scalar implementation). Notes SIMD versions only make sense if your real-world code is SIMD-friendly, that is if popcount can be applied on multiple contiguous items. Otherwise, results of the scalar implementation can be drastically different (in fact, the three compilers generate a very close code which I expect to be equally fast). AVX-512 provides the SIMD instruction VPOPCNTDQ that should clearly outperforms the code generated by LLVM using (only) AVX-2.. Since I do not have AVX-512 on my machine, and AVX-2 does not provide such an instruction, it makes sense of LLVM to produce an assembly code using AVX-2. The AVX-512 instruction can count the number of 1 in 16 x 32-bit integers in parallel while taking about the same number of cycle than its scalar counterpart. To be more precise, the instruction is only available using the instruction set AVX512VPOPCNTDQ + AVX512VL (which AFAIK is not available on all CPU supporting AVX-512). As of now, this instruction is only available on few x86-64 micro-architectures (eg. Intel Ice-Lake, Intel Sapphire Rapids and AMD Zen4).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I benchmarked your code using nanobench benchmarking library: #define ANKERL_NANOBENCH_IMPLEMENT #include &lt;nanobench.h&gt; // Function to calculate the population count (number of set bits) of an integer using __builtin_popcount int popcount(int num) { return __builtin_popcount(num); } int main() { unsigned int n = 1000000; int sum = 0; ankerl::nanobench::Bench().minEpochIterations(100).run(&quot;popcount bench&quot;, [&amp;] { for (unsigned int i = 0; i &lt; n; i++) { sum += popcount(i); } ankerl::nanobench::doNotOptimizeAway(sum); }); return 0; } Compiling it with g++ -Ofast -march=native -I./nanobench/src/include/ ./main.cpp this gives me this output: | ns/op | op/s | err% | total | benchmark |--------------------:|--------------------:|--------:|----------:|:---------- | 216,476.86 | 4,619.43 | 0.1% | 0.26 | `popcount bench` So if I interpret the results correctly, in one second we do 4619 operations (one iteration ~216us) In comparison, using numba + timeit: import numba as nb from numba import njit @nb.njit(nb.uint64(nb.uint64)) def popcount(x): b = 0 while x &gt; 0: x &amp;= x - nb.uint64(1) b += 1 return b @njit def timed_loop(n): summand = 0 for i in range(n): summand += popcount(i) return summand # warm the cache timed_loop(1) from timeit import timeit t = timeit(&quot;timed_loop(n)&quot;, setup=&quot;n = 1000000&quot;, globals=globals(), number=1000) print(t) Prints: 0.13498791516758502 So 1000 iterations is 0.135 seconds, 1 iteration took ~135us. So yes, your observation is correct. It seems numba is ~2x fast. My spec: Python 3.11/Ubuntu 20.04/AMD 5700x/g++ 9.4.0",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "c",
        "performance",
        "x86-64",
        "numba"
      ],
      "question_score": 7,
      "answer_score": 9,
      "created": "2023-09-14T11:36:18",
      "question_id": 77104513,
      "answer_id": 77108220
    }
  },
  {
    "question": "Why is the continued fraction expansion of arctangent combined with half-angle formula not working with Machin-like series?",
    "expected_answer": "The loss of precision is here: c = (x**2 + y**2) ** 0.5 a, b = c.as_integer_ratio() This code works with float type, which is the result of power calculation ** 0.5. As soon as you use float, you lose arbitrary precision. You might want to use the Decimal.sqrt method instead.",
    "context_chunks": [
      {
        "text": "Sorry for the long title. I don't know if this is more of a math problem or programming problem, but I think my math is extremely rusty and I am better at programming. So I have this continued fraction expansion of arctangent: I got it from Wikipedia I tried to find a simple algorithm to calculate it: And I did it, I have written an infinite precision implementation of the continued fraction expansion without using any libraries, using only basic integer arithmetic: import json import math import random from decimal import Decimal, getcontext from typing import Callable, List, Tuple Fraction = Tuple[int, int] def arctan_cf(y: int, x: int, lim: int) -&gt; Fraction: y_sq = y**2 a1, a2 = y, 3 * x * y b1, b2 = x, 3 * x**2 + y_sq odd = 5 for i in range(2, 2 + lim): t1, t2 = odd * x, i**2 * y_sq a1, a2 = a2, t1 * a2 + t2 * a1 b1, b2 = b2, t1 * b2 + t2 * b1 odd += 2 return a2, b2 And it converges faster than Newton's arctangent series which I previously used. Now I think if I combine it with the half-angle formula of arctangent it should converge faster. def half_arctan_cf(y: int, x: int, lim: int) -&gt; Fraction: c = (x**2 + y**2) ** 0.5 a, b = c.as_integer_ratio() a, b = arctan_cf(a - b * x, b * y, lim) return 2 * a, b And indeed, it does converge even faster: def test_accuracy(lim: int) -&gt; dict: result = {} for _ in range(lim): x, y = random.sample(range(1024), 2) while not x or not y: x, y = random.sample(range(1024), 2) atan2 = math.atan2(y, x) entry = {&quot;atan&quot;: atan2} for fname, func in zip( (&quot;arctan_cf&quot;, &quot;half_arctan_cf&quot;), (arctan_cf, half_arctan_cf) ): i = 1 while True: a, b = func(y, x, i) if math.isclose(deci := a / b, atan2): break i += 1 entry[fname] = (i, deci) result[f&quot;{y} / {x}&quot;] = entry return result print(json.dumps(test_accuracy(8), indent=4)) for v in test_accuracy(128).values(): assert v[&quot;half_arctan_cf&quot;][0] &lt;= v[&quot;arctan_cf&quot;][0] { &quot;206 / 136&quot;: { &quot;atan&quot;: 0.9872880750087898, &quot;arctan_cf&quot;: [ 16, 0.9872880746658675 ], &quot;half_arctan_cf&quot;: [ 6, 0.9872880746018052 ] }, &quot;537 / 308&quot;: { &quot;atan&quot;: 1.0500473287277563, &quot;arctan_cf&quot;: [ 18, 1.0500473281360896 ], &quot;half_arctan_cf&quot;: [ 7, 1.0500473288158192 ] }, &quot;331 / 356&quot;: { &quot;atan&quot;: 0.7490241118247137, &quot;arctan_cf&quot;: [ 10, 0.7490241115996227 ], &quot;half_arctan_cf&quot;: [ 5, 0.749024111913438 ] }, &quot;744 / 613&quot;: { &quot;atan&quot;: 0.8816364228048325, &quot;arctan_cf&quot;: [ 13, 0.8816364230439662 ], &quot;half_arctan_cf&quot;: [ 6, 0.8816364227495634 ] }, &quot;960 / 419&quot;: { &quot;atan&quot;: 1.1592605364805093, &quot;arctan_cf&quot;: [ 24, 1.1592605359263286 ], &quot;half_arctan_cf&quot;: [ 7, 1.1592605371181872 ] }, &quot;597 / 884&quot;: { &quot;atan&quot;: 0.5939827714677137, &quot;arctan_cf&quot;: [ 7, 0.5939827719895824 ], &quot;half_arctan_cf&quot;: [ 4, 0.59398277135389 ] }, &quot;212 / 498&quot;: { &quot;atan&quot;: 0.40246578425167584, &quot;arctan_cf&quot;: [ 5, 0.4024657843859885 ], &quot;half_arctan_cf&quot;: [ 3, 0.40246578431841773 ] }, &quot;837 / 212&quot;: { &quot;atan&quot;: 1.322727785860997, &quot;arctan_cf&quot;: [ 41, 1.322727786922624 ], &quot;half_arctan_cf&quot;: [ 8, 1.3227277847674388 ] } } That assert block runs quite a bit long for large number of samples, but it never raises exceptions. So I think I can use the continued fraction expansion of arctangent with Machin-like series to calculate π. (I used the last series in the linked section because it converges the fastest) def sum_fractions(fractions: List[Fraction]) -&gt; Fraction: while (length := len(fractions)) &gt; 1: stack = [] for i in range(0, length - (odd := length &amp; 1), 2): num1, den1 = fractions[i] num2, den2 = fractions[i + 1] stack.append((num1 * den2 + num2 * den1, den1 * den2)) if odd: stack.append(fractions[-1]) fractions = stack return fractions[0] MACHIN_SERIES = ((44, 57), (7, 239), (-12, 682), (24, 12943)) def approximate_loop(lim: int, func: Callable) -&gt; List[Fraction]: fractions = [] for coef, denom in MACHIN_SERIES: dividend, divisor = func(1, denom, lim) fractions.append((coef * dividend, divisor)) return fractions def approximate_1(lim: int) -&gt; List[Fraction]: return approximate_loop(lim, arctan_cf) def approximate_2(lim: int) -&gt; List[Fraction]: return approximate_loop(lim, half_arctan_cf) approx_funcs = (approximate_1, approximate_2) def calculate_pi(lim: int, approx: bool = 0) -&gt; Fraction: dividend, divisor = sum_fractions(approx_funcs[approx](lim)) dividend *= 4 return dividend // (common := math.gcd(dividend, divisor)), divisor // common getcontext().rounding = 'ROUND_DOWN' def to_decimal(dividend: int, divisor: int, places: int) -&gt; str: getcontext().prec = places + len(str(dividend // divisor)) return str(Decimal(dividend) / Decimal(divisor)) def get_accuracy(lim: int, approx: bool = 0) -&gt; Tuple[int, str]: length = 12 fraction = calculate_pi(lim, approx) while True: decimal = to_decimal(*fraction, length) for i, e in enumerate(decimal): if Pillion[i] != e: return (max(0, i - 2), decimal[:i]) length += 10 with open(&quot;D:/Pillion.txt&quot;, &quot;r&quot;) as f: Pillion = f.read() Pillion.txt contains the first 1000001 digits of π, Pi + Million = Pillion. And it works, but only partially. The basic continued fraction expansion works very well with Machin-like formula, but combined with half-angle formula, I can only get 9 correct decimal places no matter what, and in fact, I get 9 correct digits on the very first iteration, and then this whole thing doesn't improve ever: In [2]: get_accuracy(16) Out[2]: (73, '3.1415926535897932384626433832795028841971693993751058209749445923078164062') In [3]: get_accuracy(32) Out[3]: (138, '3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067982148086513282306647093844609550582231') In [4]: get_accuracy(16, 1) Out[4]: (9, '3.141592653') In [5]: get_accuracy(32, 1) Out[5]: (9, '3.141592653') In [6]: get_accuracy(1, 1) Out[6]: (9, '3.141592653') But the digits do in fact change: In [7]: to_decimal(*calculate_pi(1, 1), 32) Out[7]: '3.14159265360948500093515231500093' In [8]: to_decimal(*calculate_pi(2, 1), 32) Out[8]: '3.14159265360945286794831052938917' In [9]: to_decimal(*calculate_pi(3, 1), 32) Out[9]: '3.14159265360945286857612896472974' In [10]: to_decimal(*calculate_pi(4, 1), 32) Out[10]: '3.14159265360945286857611676794770' In [11]: to_decimal(*calculate_pi(5, 1), 32) Out[11]: '3.14159265360945286857611676818392' Why is the continued fraction with half-angle formula not working with Machin-like formula? And is it possible to make it work, and if it can work, then how? I want either a proof that it is impossible, or a working example that proves it is possible. Just a sanity check, using π/4 = arctan(1) I was able to make half_arctan_cf spit out digits of π but it converges much slower: def approximate_3(lim: int) -&gt; List[Fraction]: return [half_arctan_cf(1, 1, lim)] approx_funcs = (approximate_1, approximate_2, approximate_3) In [28]: get_accuracy(16, 2) Out[28]: (15, '3.141592653589793') In [29]: get_accuracy(16, 0) Out[29]: (73, '3.1415926535897932384626433832795028841971693993751058209749445923078164062') And the same problem recurs, it reaches maximum precision of 15 digits at the 10th iteration: In [37]: get_accuracy(9, 2) Out[37]: (14, '3.14159265358979') In [38]: get_accuracy(10, 2) Out[38]: (15, '3.141592653589793') In [39]: get_accuracy(11, 2) Out[39]: (15, '3.141592653589793') In [40]: get_accuracy(32, 2) Out[40]: (15, '3.141592653589793') I just rewrote my arctangent continued fraction implementation and made it avoid doing redundant computations. In my code in each iteration t1 increases by 2 * y_sq, so there is no need to repeatedly multiply y_sq by the odd number, instead just use a cumulative variable and a step of 2 * y_sq. And the difference between each pair of consecutive square numbers is just the odd numbers, so I can use a cumulative variable of a cumulative variable. def arctan_cf_0(y: int, x: int, lim: int) -&gt; Fraction: y_sq = y**2 a1, a2 = y, 3 * x * y b1, b2 = x, 3 * x**2 + y_sq odd = 5 for i in range(2, 2 + lim): t1, t2 = odd * x, i**2 * y_sq a1, a2 = a2, t1 * a2 + t2 * a1 b1, b2 = b2, t1 * b2 + t2 * b1 odd += 2 return a2, b2 def arctan_cf(y: int, x: int, lim: int) -&gt; Fraction: y_sq = y**2 a1, a2 = y, 3 * x * y b1, b2 = x, 3 * x**2 + y_sq t1_step, t3_step = 2 * x, 2 * y_sq t1, t2 = 5 * x, 4 * y_sq t3 = t2 + y_sq for _ in range(lim): a1, a2 = a2, t1 * a2 + t2 * a1 b1, b2 = b2, t1 * b2 + t2 * b1 t1 += t1_step t2 += t3 t3 += t3_step return a2, b2 In [301]: arctan_cf_0(4, 3, 100) == arctan_cf(4, 3, 100) Out[301]: True In [302]: %timeit arctan_cf_0(4, 3, 100) 58.6 μs ± 503 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) In [303]: %timeit arctan_cf(4, 3, 100) 54.3 μs ± 816 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) While this doesn't improve the speed by much, this is definitively an improvement.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The loss of precision is here: c = (x**2 + y**2) ** 0.5 a, b = c.as_integer_ratio() This code works with float type, which is the result of power calculation ** 0.5. As soon as you use float, you lose arbitrary precision. You might want to use the Decimal.sqrt method instead.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I have implemented infinite precision continued fraction arctangent function using half-tangent formula, I did this whole thing using only integers with the new insight I gained while solving this problem. I won't try to explain my method, but basically I treated every number as (a + b√n), now I express it as (a, b), then (a, b) * (c, d) = (ac + nbd, ad + bc), (a, b) + (d, c) = (a + d, b + c). I reimplemented the whole thing and used much more complex mathematics than my original implementation, but this thing successfully combines continued fraction, integer arithmetic, algebra and half-tangent formula all in one, as a result it has &quot;infinite precision&quot; if I don't evaluate the final expressions. I have ensured that there is no division occurring throughout the entire process exception the final evaluation to get the result, and that the radicals never gets evaluated until the final evaluation, so there is no precision loss caused by multiplications of radicals during the building of the expression. I have reduced multiplications of radicals, consider the following example, (a + b√c) * (d + e√f) = (ad + bd√c + ae√f + be√c√f), I have defined the following equivalent relationship: (a, (b, c)) * (d, (e, f)) = (ad, (bd, c), (ae, f), (be, c, f)), and defined a function to repeatedly do that. So the radicals never get evaluated while building the expression, so the expression is infinitely precise, but I have to keep the radicals in order not to lose precision, and irrationals can only ever be approximated, and so if I evaluate the expression to get decimal representation I am bound to lose precision. But I have ensured that I won't lose precision anywhere but in the final conversion to decimal, and the only causes of loss of precision in the final evaluation are square rooting and multiplications of irrationals. Because I use radicals, loss of precision is guaranteed, but I have minimized the loss of precision, so that I can get arbitrarily many digits. from decimal import Decimal, getcontext from typing import Tuple MACHIN_SERIES = ((44, 57), (7, 239), (-12, 682), (24, 12943)) getcontext().rounding = &quot;ROUND_DOWN&quot; def arctan_cf_half(y: int, x: int, lim: int) -&gt; list: x_sq, y_sq = x**2, y**2 z = x_sq + y_sq a1_r, a1_i, a2_r, a2_i = y, 0, 3 * x * y, 3 * y b1_r, b1_i, b2_r, b2_i = x, 1, 2 * x_sq + 4 * z, 6 * x t1_r, t1_i, t2_r = 5 * x, 5, 4 * y_sq t3_r = t2_r + y_sq s1_r, s3_r = 2 * x, 2 * y_sq for _ in range(lim): a3_r = a1_r * t2_r + a2_r * t1_r + z * a2_i * t1_i a3_i = a1_i * t2_r + a2_r * t1_i + a2_i * t1_r b3_r = b1_r * t2_r + b2_r * t1_r + z * b2_i * t1_i b3_i = b1_i * t2_r + b2_r * t1_i + b2_i * t1_r a1_r, a1_i, a2_r, a2_i = a2_r, a2_i, a3_r, a3_i b1_r, b1_i, b2_r, b2_i = b2_r, b2_i, b3_r, b3_i t1_r += s1_r t1_i += 2 t2_r += t3_r t3_r += s3_r return [2 * a2_r, [2 * a2_i, z]], [b2_r, [b2_i, z]] def radical_poly_mult(numbers: list, mult: list) -&gt; list: a, b = numbers[0], mult[0] c, d = mult[1] return ( [a * b] + [[n[0] * b] + n[1:] for n in numbers[1:]] + [[a * c, d]] + [[n[0] * c, d] + n[1:] for n in numbers[1:]] ) def radical_poly_add(a: list, b: list) -&gt; list: return [a[0] + b[0]] + a[1:] + b[1:] def sum_radical_fractions(fractions: list) -&gt; list: while (length := len(fractions)) &gt; 1: stack = [] for i in range(0, length - (odd := length &amp; 1), 2): num1, den1 = fractions[i] num2, den2 = fractions[i + 1] stack.append( ( radical_poly_add( radical_poly_mult(num1, den2), radical_poly_mult(num2, den1) ), radical_poly_mult(den1, den2), ) ) if odd: stack.append(fractions[-1]) fractions = stack return fractions[0] def radical_poly_eval(numbers: list) -&gt; Decimal: result = numbers[0] for number in numbers[1:]: prod = number[0] for i in number[1:]: prod *= Decimal(i).sqrt() result += prod return result def half_tangent_pi(lim: int, length: int) -&gt; str: fractions = [] for coef, denom in MACHIN_SERIES: (a, (b, c)), divisor = arctan_cf_half(1, denom, lim) fractions.append([[coef * a, [coef * b, c]], divisor]) dividend, divisor = sum_radical_fractions(fractions) getcontext().prec = length + 1 return str(4 * radical_poly_eval(dividend) / radical_poly_eval(divisor)) with open(&quot;D:/Pillion.txt&quot;, &quot;r&quot;) as f: Pillion = f.read() def get_accuracy_half_tangent(lim: int) -&gt; Tuple[int, str]: length = 256 while True: decimal = half_tangent_pi(lim, length) for i, e in enumerate(decimal): if Pillion[i] != e: return (max(0, i - 2), decimal[:i]) length += 32 In [163]: get_accuracy(16) Out[163]: (73, '3.1415926535897932384626433832795028841971693993751058209749445923078164062') In [164]: get_accuracy_half_tangent(16) Out[164]: (54, '3.141592653589793238462643383279502884197169399375105820') In [165]: get_accuracy_half_tangent(32) Out[165]: (101, '3.14159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798') In [166]: get_accuracy(32) Out[166]: (138, '3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067982148086513282306647093844609550582231') In [167]: get_accuracy_half_tangent(64) Out[167]: (197, '3.14159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798214808651328230664709384460955058223172535940812848111745028410270193852110555964462294895493038') In [168]: get_accuracy(64) Out[168]: (270, '3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067982148086513282306647093844609550582231725359408128481117450284102701938521105559644622948954930381964428810975665933446128475648233786783165271201909145648566923460348610') In [169]: get_accuracy_half_tangent(128) Out[169]: (254, '3.14159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798214808651328230664709384460955058223172535940812848111745028410270193852110555964462294895493038196442881097566593344612847564823378678316527120190914564') In [170]: get_accuracy(128) Out[170]: (533, '3.14159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798214808651328230664709384460955058223172535940812848111745028410270193852110555964462294895493038196442881097566593344612847564823378678316527120190914564856692346034861045432664821339360726024914127372458700660631558817488152092096282925409171536436789259036001133053054882046652138414695194151160943305727036575959195309218611738193261179310511854807446237996274956735188575272489122793818301194912983367336244065664308602139494639')",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "algorithm",
        "math",
        "pi"
      ],
      "question_score": 7,
      "answer_score": 7,
      "created": "2025-03-22T13:38:07",
      "question_id": 79527532,
      "answer_id": 79528666
    }
  },
  {
    "question": "Python Xarray ValueError: unrecognized chunk manager dask - must be one of: []",
    "expected_answer": "For some unknown reason (to me), what I have found that works instead of performing any downgrade is to install all the packages needed (specially Xarray) and finally reinstall dask as: conda install dask --force-reinstall I have performed this in 4 environments and always has worked. PS: I would also recommend using libmamba as the solver.",
    "context_chunks": [
      {
        "text": "I am using xarray for combining multiple netcdf files using xarray.open_mfdataset. But I get the error while running the command, below are the commands and error. nc_all = xarray.open_mfdataset(files,combine = 'nested', concat_dim=&quot;time&quot;) files = glob.glob(&quot;/filepath/*&quot;) I get the following error- Traceback (most recent call last): File &quot;/home/lsrathore/GLEAM/GLEAM_HPC.py&quot;, line 85, in &lt;module&gt; nc_1980_90 = xarray.open_mfdataset(files[1:11],combine = 'nested', concat_dim=&quot;time&quot;) File &quot;/home/lsrathore/.local/lib/python3.9/site-packages/xarray/backends/api.py&quot;, line 1038, in open_mfdataset datasets = [open_(p, **open_kwargs) for p in paths] File &quot;/home/lsrathore/.local/lib/python3.9/site-packages/xarray/backends/api.py&quot;, line 1038, in &lt;listcomp&gt; datasets = [open_(p, **open_kwargs) for p in paths] File &quot;/home/lsrathore/.local/lib/python3.9/site-packages/xarray/backends/api.py&quot;, line 572, in open_dataset ds = _dataset_from_backend_dataset( File &quot;/home/lsrathore/.local/lib/python3.9/site-packages/xarray/backends/api.py&quot;, line 367, in _dataset_from_backend_dataset ds = _chunk_ds( File &quot;/home/lsrathore/.local/lib/python3.9/site-packages/xarray/backends/api.py&quot;, line 315, in _chunk_ds chunkmanager = guess_chunkmanager(chunked_array_type) File &quot;/home/lsrathore/.local/lib/python3.9/site-packages/xarray/core/parallelcompat.py&quot;, line 87, in guess_chunkmanager raise ValueError( ValueError: unrecognized chunk manager dask - must be one of: [] What is causing the problem?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "For some unknown reason (to me), what I have found that works instead of performing any downgrade is to install all the packages needed (specially Xarray) and finally reinstall dask as: conda install dask --force-reinstall I have performed this in 4 environments and always has worked. PS: I would also recommend using libmamba as the solver.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The issue was resolved when I downgraded the xarray version to 0.21.1 from 2023.5.0",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-xarray"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2023-06-05T20:50:29",
      "question_id": 76409916,
      "answer_id": 77936230
    }
  },
  {
    "question": "How do I select the top k rows of a python polars dataframe for each group?",
    "expected_answer": "groups = ['grp', 'val'] top_n = 2 ( df .sort(groups, descending=[False, True]) .filter(pl.int_range(0, pl.len()).over('grp') &lt; top_n) )",
    "context_chunks": [
      {
        "text": "The polars dataframe has a top_k method that can be used to select rows which contain the k largest values when sorting on a column. For example, the following code selects the two rows with the largest and second largest entry in the val column: df = pl.DataFrame({'grp':['a','a','a','b','b','b'], 'val':[1,2,3,10,20,30], 'etc':[0,1,2,3,4,5]}) grp val etc str i64 i64 &quot;a&quot; 1 0 &quot;a&quot; 2 1 &quot;a&quot; 3 2 &quot;b&quot; 10 3 &quot;b&quot; 20 4 &quot;b&quot; 30 5 df.top_k(2, by='val') grp val etc str i64 i64 &quot;b&quot; 30 5 &quot;b&quot; 20 4 My question is: how do I get the rows with top k values for each group? Specifically, I want the entire row and not just the value in the val column. I want to do something like this, but this doesn't work in polars because polars GroupBy doesn't have a top_k method: df.groupby('grp').top_k(2, by='val') # doesnt work in polars grp val etc str i64 i64 &quot;b&quot; 30 5 &quot;b&quot; 20 4 &quot;a&quot; 3 2 &quot;a&quot; 2 1 I was able to come up with two ways: one using map_groups and another using sorting. Both of these are not desirable for performance reasons. map_groups is generally not recommended because it's almost always significantly slower. The sorting option is also not desirable as getting the top k elements uses a faster algorithm than sorting (for small k and large n, it's basically O(n) vs O(n log n)). So even though the following below work, I'm looking for other approaches. Is there any way to directly use a top_k method with polars groupby? That would be my ideal solution. # works, but at expense of using map_groups method df.group_by('grp').map_groups(lambda df: df.top_k(2, by='val')) grp val etc str i64 i64 &quot;b&quot; 30 5 &quot;b&quot; 20 4 &quot;a&quot; 3 2 &quot;a&quot; 2 1 # works, but at expense of sorting entire groups df.group_by('grp').agg(pl.all().sort_by('val', descending=True).head(2)).explode('val','etc') grp val etc str i64 i64 &quot;a&quot; 3 2 &quot;a&quot; 2 1 &quot;b&quot; 30 5 &quot;b&quot; 20 4 df.group_by('grp').top_k(2, by='val'), which doesn't work in polars df.group_by('grp').map_groups(lambda df: df.top_k(2, by='val')), which works at the cost of using map_groups df.group_by('grp').agg(pl.all().sort_by('val', descending=True).head(2)).explode('val','etc'), which works at the cost of sorting",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "groups = ['grp', 'val'] top_n = 2 ( df .sort(groups, descending=[False, True]) .filter(pl.int_range(0, pl.len()).over('grp') &lt; top_n) )",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The latest release (version 0.20.24) of polars introduced pl.Expr.top_k_by (and also pl.Expr.bottom_k_by) with optimised runtime complexity O(n + k log n - k / 2) for precisely the use-case mentioned in the question. It can be used jointly with pl.Expr.over and mapping_strategy=&quot;explode&quot; to obtain the desired result. df.select( pl.all().top_k_by(&quot;val&quot;, k=2).over(&quot;grp&quot;, mapping_strategy=&quot;explode&quot;) ) shape: (4, 3) ┌─────┬─────┬─────┐ │ grp ┆ val ┆ etc │ │ --- ┆ --- ┆ --- │ │ str ┆ i64 ┆ i64 │ ╞═════╪═════╪═════╡ │ a ┆ 3 ┆ 2 │ │ a ┆ 2 ┆ 1 │ │ b ┆ 30 ┆ 5 │ │ b ┆ 20 ┆ 4 │ └─────┴─────┴─────┘ Note. The call to pl.Expr.over with mapping_strategy=&quot;explode&quot; is equivalent to the following aggregation. df.group_by(&quot;grp&quot;).agg(pl.all().top_k_by(&quot;val&quot;, k=2)).explode(pl.exclude(&quot;grp&quot;))",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-polars"
      ],
      "question_score": 7,
      "answer_score": 4,
      "created": "2023-07-01T23:56:52",
      "question_id": 76596952,
      "answer_id": 78066884
    }
  },
  {
    "question": "Format datetime in polars",
    "expected_answer": "you can use strftime like this: df = df.with_columns(pl.col(&quot;dates&quot;).dt.strftime(&quot;%Y%m&quot;)) print(df) ┌─────┬────────┐ │ ID ┆ dates │ │ --- ┆ --- │ │ i64 ┆ str │ ╞═════╪════════╡ │ 1 ┆ 202401 │ │ 2 ┆ 202401 │ │ 3 ┆ 202401 │ └─────┴────────┘",
    "context_chunks": [
      {
        "text": "I have a polars dataframe that contains a datetime column. I want to convert this column to strings in the format %Y%m. For example, all dates in January 2024 should be converted to &quot;202401&quot;. from datetime import datetime import polars as pl data = { &quot;ID&quot; : [1,2,3], &quot;dates&quot; : [datetime(2024,1,2),datetime(2024,1,3),datetime(2024,1,4)], } df = pl.DataFrame(data) I have tried using strftime. However, the following AttributeError is raised. AttributeError: 'Expr' object has no attribute 'strftime'",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "you can use strftime like this: df = df.with_columns(pl.col(&quot;dates&quot;).dt.strftime(&quot;%Y%m&quot;)) print(df) ┌─────┬────────┐ │ ID ┆ dates │ │ --- ┆ --- │ │ i64 ┆ str │ ╞═════╪════════╡ │ 1 ┆ 202401 │ │ 2 ┆ 202401 │ │ 3 ┆ 202401 │ └─────┴────────┘",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Note that pl.Expr.dt.strftime is available under the pl.Expr.dt namespace. Hence, it is called on the dt attribute of an expression and not the expression directly. df.with_columns( pl.col(&quot;dates&quot;).dt.strftime(&quot;%Y%m&quot;) ) shape: (3, 2) ┌─────┬────────┐ │ ID ┆ dates │ │ --- ┆ --- │ │ i64 ┆ str │ ╞═════╪════════╡ │ 1 ┆ 202401 │ │ 2 ┆ 202401 │ │ 3 ┆ 202401 │ └─────┴────────┘",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "datetime",
        "python-polars",
        "strftime"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2024-08-06T21:09:10",
      "question_id": 78841010,
      "answer_id": 78841036
    }
  },
  {
    "question": "Stable diffusion: AttributeError: module &#39;jax.random&#39; has no attribute &#39;KeyArray&#39;",
    "expected_answer": "jax.random.KeyArray was deprecated in JAX v0.4.16 and removed in JAX v0.4.24. Given this, it sounds like the HuggingFace stable diffusion code only works JAX v0.4.23 or earlier. You can install JAX v0.4.23 with GPU support like this: pip install &quot;jax[cuda12_pip]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html or, if you prefer targeting a local CUDA installation, like this: pip install &quot;jax[cuda12_local]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html For more information on GPU installation, see JAX Installation: NVIDIA GPU. From the colab tutorial, update the second segment into: !pip install &quot;jax[cuda12_local]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html !pip install diffusers==0.11.1 !pip install transformers scipy ftfy accelerate",
    "context_chunks": [
      {
        "text": "When I run the stable diffusion on colab https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb with no modification, it fails on the line from diffusers import StableDiffusionPipeline The error log is AttributeError: module 'jax.random' has no attribute 'KeyArray' How can I fix this or any clue ? The import should work, the ipynb should run with no error.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "jax.random.KeyArray was deprecated in JAX v0.4.16 and removed in JAX v0.4.24. Given this, it sounds like the HuggingFace stable diffusion code only works JAX v0.4.23 or earlier. You can install JAX v0.4.23 with GPU support like this: pip install &quot;jax[cuda12_pip]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html or, if you prefer targeting a local CUDA installation, like this: pip install &quot;jax[cuda12_local]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html For more information on GPU installation, see JAX Installation: NVIDIA GPU. From the colab tutorial, update the second segment into: !pip install &quot;jax[cuda12_local]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html !pip install diffusers==0.11.1 !pip install transformers scipy ftfy accelerate",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "In the end, we need to downgrade the jax, Try each from the lateset to ealier, and luckily it works for jax==0.4.23 jaxlib==0.4.23",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "google-colaboratory",
        "jax",
        "stable-diffusion"
      ],
      "question_score": 7,
      "answer_score": 10,
      "created": "2024-04-10T03:43:22",
      "question_id": 78302031,
      "answer_id": 78304430
    }
  },
  {
    "question": "OpenAI from Langchain requires &quot;openai_api_key&quot; even though it is loaded",
    "expected_answer": "I was getting the error in Jupyter Notebook Did not find openai_api_key, please add an environment variable OPENAI_API_KEY which contains it, or pass openai_api_key as a named parameter. (type=value_error) For the people who are using Jupyter Notebook: %env OPENAI_API_KEY=&quot;sk-XXXXXXXXX&quot; Solved the issue for me.",
    "context_chunks": [
      {
        "text": "this is my code: import os from dotenv import load_dotenv,find_dotenv load_dotenv(find_dotenv()) print(os.environ.get(&quot;OPEN_AI_KEY&quot;)) from langchain.llms import OpenAI llm=OpenAI(model_name=&quot;text-davinci-003&quot;,temperature=0.7,max_tokens=512) print(llm) when I execute above code I get this error ValidationError: 1 validation error for OpenAI __root__ Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error) docs say If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class: But already set it and it prints correctly When I set the llm by passing named param: llm=OpenAI(openai_api_key=&quot;PASSINGCORRECTKEY&quot;, model_name=&quot;text-davinci-003&quot;,temperature=0.7,max_tokens=512) llm(&quot;Tell me a joke&quot;) then I get this error: raise ValueError( &quot;Argument `prompt` is expected to be a string. Instead found &quot; f&quot;{type(prompt)}. If you want to run the LLM on multiple prompts, use &quot; &quot;`generate` instead.&quot; ) UPDATE env variable initially was set as OPEN_AI_KEY since I copied and pasted from one of my other project which calls chat/completions api. I changed the env to OPENAI_API_KEY not I get this error: AuthenticationError: Incorrect API key provided: org-Wz3J****************2XK6. You can find your API key at https://platform.openai.com/account/api-keys. But same api key works when i call &quot;https://api.openai.com/v1/chat/completions&quot; endpoint",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I was getting the error in Jupyter Notebook Did not find openai_api_key, please add an environment variable OPENAI_API_KEY which contains it, or pass openai_api_key as a named parameter. (type=value_error) For the people who are using Jupyter Notebook: %env OPENAI_API_KEY=&quot;sk-XXXXXXXXX&quot; Solved the issue for me.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You may need to store the OpenAI token and then pass it to the llm variable you have here, or just rename your environment variable to openai_api_key. A possible example of passing a key directly is this: import os from dotenv import load_dotenv,find_dotenv load_dotenv(find_dotenv()) prompt = &quot;Your Prompt Here&quot; OpenAI_key = os.environ.get(&quot;OPEN_AI_KEY&quot;) print(OpenAI_token) from langchain.llms import OpenAI llm=openai.Completion.create(model_name=&quot;text-davinci-003&quot;,temperature=0.7,max_tokens=512,openai_api_key=OpenAI_key, prompt=prompt, stop=None) print(llm) It should work now",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "openai-api",
        "langchain",
        "large-language-model"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2023-07-31T02:38:50",
      "question_id": 76800382,
      "answer_id": 77537612
    }
  },
  {
    "question": "How to write to a SQLite database using Polars in Python?",
    "expected_answer": "Here is an example for writing / reading sqlite tables using polars. Write table to database (sqlalchemy needs to be installed). import polars as pl df = pl.DataFrame({&quot;a&quot;: [1, 2, 3]}) df.write_database( &quot;my_table&quot;, connection=&quot;sqlite:///database.db&quot;, if_table_exists=&quot;replace&quot; ) Read table from database (connectorx needs to be installed). pl.read_database_uri(query=&quot;SELECT * FROM my_table&quot;, uri=&quot;sqlite://database.db&quot;) Note the format of the connection URI. sqlite:///:memory: (or, sqlite://) sqlite:///relative/path/to/file.db sqlite:////absolute/path/to/file.db sqlite:///c:/absolute/path/on/windows/file.db (credit to @Thomas)",
    "context_chunks": [
      {
        "text": "import polars as pl import sqlite3 conn = sqlite3.connect(&quot;test.db&quot;) df = pl.DataFrame({&quot;col1&quot;: [1, 2, 3]}) According to the documentation of pl.write_database, I need to pass a connection URI string e.g. &quot;sqlite:////path/to/database.db&quot; for SQLite database: df.write_database(&quot;test_table&quot;, f&quot;sqlite:////test.db&quot;, if_table_exists=&quot;replace&quot;) However, I got the following error: OperationalError: (sqlite3.OperationalError) unable to open database file EDIT: Based on the answer, install SQLAlchemy with the pip install polars[sqlalchemy] command.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Here is an example for writing / reading sqlite tables using polars. Write table to database (sqlalchemy needs to be installed). import polars as pl df = pl.DataFrame({&quot;a&quot;: [1, 2, 3]}) df.write_database( &quot;my_table&quot;, connection=&quot;sqlite:///database.db&quot;, if_table_exists=&quot;replace&quot; ) Read table from database (connectorx needs to be installed). pl.read_database_uri(query=&quot;SELECT * FROM my_table&quot;, uri=&quot;sqlite://database.db&quot;) Note the format of the connection URI. sqlite:///:memory: (or, sqlite://) sqlite:///relative/path/to/file.db sqlite:////absolute/path/to/file.db sqlite:///c:/absolute/path/on/windows/file.db (credit to @Thomas)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "When working with SQLite, the number of slashes depends on how you are accessing the sqlite file. If you use two slashes as suggested in the comments, you'll see this: sqlalchemy.exc.ArgumentError: Invalid SQLite URL: sqlite://test.db Valid SQLite URL forms are: sqlite:///:memory: (or, sqlite://) sqlite:///relative/path/to/file.db sqlite:////absolute/path/to/file.db It appears that in your case, you want to use 3 slashes and not 4. Please try this instead: df.write_database(&quot;test_table&quot;, f&quot;sqlite:///test.db&quot;, if_table_exists=&quot;replace&quot;) This would be a working example, based on the sample code snippets you provided: import polars as pl import sqlite3 conn = sqlite3.connect(&quot;test.db&quot;) df = pl.DataFrame({&quot;col1&quot;: [1, 2, 3]}) df.write_database(&quot;test_table&quot;, f&quot;sqlite:///test.db&quot;, if_table_exists=&quot;replace&quot;)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "sqlite",
        "python-polars"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2024-02-28T15:37:56",
      "question_id": 78075720,
      "answer_id": 78075979
    }
  },
  {
    "question": "HTML/XML: Understanding How &quot;Scroll Bars&quot; Work",
    "expected_answer": "I see that you updated your question to include a Python answer, so here's how it's done in Python. you can use the same method for R. The page is lazy loaded which means, as you scroll the data is paginated and loaded. So, what you need to do, is to keep finding the last HTML tag of the data which will therefore load more content. Finding how more data is loaded You need to find out how the data is loaded. Here's what I did: First, disable internet access for your browser in the Network calls (F12 -&gt; Network -&gt; Offline) Then, scroll to the last loaded element, you will see a loading indicator (since there is no internet, it will just hang) Now, here comes the important part, find out under what HTML tag this loading indicator is: As you can see that element is under the div.qjESne CSS selector. Working with Selenium You can call the javascript code scrollIntoView() function which will scroll a particular element into view within the browser's viewport. Finding out when to break To find out when to stop scrolling in order to load more data, we need to find out what element appears when theres no data. If you scroll until there are no more results, you will see: which is an element under the CSS selector span.HlvSq. Code examples Scrolling the page from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC URL = &quot;https://www.google.com/maps/search/Restaurants/@40.7256843,-74.1138399,14z/data=!4m8!2m7!3m5!1sRestaurants!2s40.7256456,-74.0909442!4m2!1d-74.0909442!2d40.7256456!6e5?entry=ttu&quot; driver = webdriver.Chrome() driver.get(URL) # Waits 10 seconds for the elements to load before scrolling wait = WebDriverWait(driver, 10) elements = wait.until( EC.presence_of_all_elements_located((By.CSS_SELECTOR, &quot;div.qjESne&quot;)) ) while True: new_elements = wait.until( EC.presence_of_all_elements_located((By.CSS_SELECTOR, &quot;div.qjESne&quot;)) ) # Pick the last element in the list - this is the one we want to scroll to last_element = elements[-1] # Scroll to the last element driver.execute_script(&quot;arguments[0].scrollIntoView(true);&quot;, last_element) # Update the elements list elements = new_elements # Check if there are any new elements loaded - the &quot;You've reached the end of the list.&quot; message if driver.find_elements(By.CSS_SELECTOR, &quot;span.HlvSq&quot;): print(&quot;No more elements&quot;) break Getting the data If you inspect the page, you will see that the data is under the cards under the CSS selector of div.lI9IFe. What you need to do, is wait until the scrolling has finished, and then you get all the data under the CSS selector of div.lI9IFe Code example from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import pandas as pd URL = &quot;https://www.google.com/maps/search/Restaurants/@40.7256843,-74.1138399,14z/data=!4m8!2m7!3m5!1sRestaurants!2s40.7256456,-74.0909442!4m2!1d-74.0909442!2d40.7256456!6e5?entry=ttu&quot; driver = webdriver.Chrome() driver.get(URL) # Waits 10 seconds for the elements to load before scrolling wait = WebDriverWait(driver, 10) elements = wait.until( EC.presence_of_all_elements_located((By.CSS_SELECTOR, &quot;div.qjESne&quot;)) ) titles = [] links = [] addresses = [] while True: new_elements = wait.until( EC.presence_of_all_elements_located((By.CSS_SELECTOR, &quot;div.qjESne&quot;)) ) # Pick the last element in the list - this is the one we want to scroll to last_element = elements[-1] # Scroll to the last element driver.execute_script(&quot;arguments[0].scrollIntoView(true);&quot;, last_element) # Update the elements list elements = new_elements # time.sleep(0.1) # Check if there are any new elements loaded - the &quot;You've reached the end of the list.&quot; message if driver.find_elements(By.CSS_SELECTOR, &quot;span.HlvSq&quot;): # now we can parse the data since all the elements loaded for data in driver.find_elements(By.CSS_SELECTOR, &quot;div.lI9IFe&quot;): title = data.find_element( By.CSS_SELECTOR, &quot;div.qBF1Pd.fontHeadlineSmall&quot; ).text restaurant = data.find_element( By.CSS_SELECTOR, &quot;.W4Efsd &gt; span:nth-of-type(2)&quot; ).text titles.append(title) addresses.append(restaurant) # This converts the list of titles and links into a dataframe df = pd.DataFrame(list(zip(titles, addresses)), columns=[&quot;title&quot;, &quot;addresses&quot;]) print(df) break Prints: title addresses 0 Domino's Pizza · 741 Communipaw Ave A 1 Tommy's Family Restaurant · 349 Central Ave 2 VIP RESTAURANT LLC BARSHAY'S · 175 Sip Ave 3 The Hutton Restaurant and Bar · 225 Hutton St 4 Barge Inn · 324 3rd St .. ... ... 116 Bettie's Restaurant · 579 West Side Ave 117 Mahboob-E-El Ahi · 580 Montgomery St 118 Samosa Paradise · 804 Newark Ave 119 TACO DRIVE · 195 Newark Ave 120 Two Boots Pizza · 133 Newark Ave [121 rows x 2 columns]",
    "context_chunks": [
      {
        "text": "I am working with the R programming language and trying to learn about how to use Selenium to interact with webpages. For example, using Google Maps - I am trying to find the name, address and longitude/latitude of all Pizza shops around a certain area. As I understand, this would involve entering the location you are interested in, clicking the &quot;nearby&quot; button, entering what you are looking for (e.g. &quot;pizza&quot;), scrolling all the way to the bottom to make sure all pizza shops are loaded - and then copying the names, address and longitude/latitudes of all pizza locations. I have been self-teaching myself how to use Selenium in R and have been able to solve parts of this problem myself. Here is what I have done so far: Part 1: Searching for an address (e.g. Statue of Liberty, New York, USA) and returning a longitude/latitude : library(RSelenium) library(wdman) library(netstat) selenium() seleium_object &lt;- selenium(retcommand = T, check = F) remote_driver &lt;- rsDriver(browser = &quot;chrome&quot;, chromever = &quot;114.0.5735.90&quot;, verbose = F, port = free_port()) remDr&lt;- remote_driver$client remDr$navigate(&quot;https://www.google.com/maps&quot;) search_box &lt;- remDr$findElement(using = 'css selector', &quot;#searchboxinput&quot;) search_box$sendKeysToElement(list(&quot;Statue of Liberty&quot;, key = &quot;enter&quot;)) Sys.sleep(5) url &lt;- remDr$getCurrentUrl()[[1]] long_lat &lt;- gsub(&quot;.*@(-?[0-9.]+),(-?[0-9.]+),.*&quot;, &quot;\\\\1,\\\\2&quot;, url) long_lat &lt;- unlist(strsplit(long_lat, &quot;,&quot;)) &gt; long_lat [1] &quot;40.7269409&quot; &quot;-74.0906116&quot; Part 2: Searching for all Pizza shops around a certain location: library(RSelenium) library(wdman) library(netstat) selenium() seleium_object &lt;- selenium(retcommand = T, check = F) remote_driver &lt;- rsDriver(browser = &quot;chrome&quot;, chromever = &quot;114.0.5735.90&quot;, verbose = F, port = free_port()) remDr&lt;- remote_driver$client remDr$navigate(&quot;https://www.google.com/maps&quot;) Sys.sleep(5) search_box &lt;- remDr$findElement(using = 'css selector', &quot;#searchboxinput&quot;) search_box$sendKeysToElement(list(&quot;40.7256456,-74.0909442&quot;, key = &quot;enter&quot;)) Sys.sleep(5) search_box &lt;- remDr$findElement(using = 'css selector', &quot;#searchboxinput&quot;) search_box$clearElement() search_box$sendKeysToElement(list(&quot;pizza&quot;, key = &quot;enter&quot;)) Sys.sleep(5) But from here, I do not know how to proceed. I do not know how to scroll the page all the way to the bottom to view all such results that are available - and I do not know how to start extracting the names. Doing some research (i.e. inspecting the HTML code), I made the following observations: The name of a restaurant location can be found in the following tags: &lt;a class=&quot;hfpxzc&quot; aria-label= The address of a restaurant location be found in the following tags: &lt;div class=&quot;W4Efsd&quot;&gt; In the end, I would be looking for a result like this: name address longitude latitude 1 pizza land 123 fake st, city, state, zip code 45.212 -75.123 Can someone please show me how to proceed? Note: Seeing as more people likely use Selenium through Python - I am more than happy to learn how to solve this problem in Python and then try to convert the answer into R code.r Thanks! References: https://medium.com/python-point/python-crawling-restaurant-data-ab395d121247 https://www.youtube.com/watch?v=GnpJujF9dBw https://www.youtube.com/watch?v=U1BrIPmhx10 UPDATE: Some further progress with addresses remDr$navigate(&quot;https://www.google.com/maps&quot;) Sys.sleep(5) search_box &lt;- remDr$findElement(using = 'css selector', &quot;#searchboxinput&quot;) search_box$sendKeysToElement(list(&quot;40.7256456,-74.0909442&quot;, key = &quot;enter&quot;)) Sys.sleep(5) search_box &lt;- remDr$findElement(using = 'css selector', &quot;#searchboxinput&quot;) search_box$clearElement() search_box$sendKeysToElement(list(&quot;pizza&quot;, key = &quot;enter&quot;)) Sys.sleep(5) address_elements &lt;- remDr$findElements(using = 'css selector', '.W4Efsd') addresses &lt;- lapply(address_elements, function(x) x$getElementText()[[1]]) result &lt;- data.frame(name = unlist(names), address = unlist(addresses))",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I see that you updated your question to include a Python answer, so here's how it's done in Python. you can use the same method for R. The page is lazy loaded which means, as you scroll the data is paginated and loaded. So, what you need to do, is to keep finding the last HTML tag of the data which will therefore load more content. Finding how more data is loaded You need to find out how the data is loaded. Here's what I did: First, disable internet access for your browser in the Network calls (F12 -&gt; Network -&gt; Offline) Then, scroll to the last loaded element, you will see a loading indicator (since there is no internet, it will just hang) Now, here comes the important part, find out under what HTML tag this loading indicator is: As you can see that element is under the div.qjESne CSS selector. Working with Selenium You can call the javascript code scrollIntoView() function which will scroll a particular element into view within the browser's viewport. Finding out when to break To find out when to stop scrolling in order to load more data, we need to find out what element appears when theres no data. If you scroll until there are no more results, you will see: which is an element under the CSS selector span.HlvSq. Code examples Scrolling the page from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC URL = &quot;https://www.google.com/maps/search/Restaurants/@40.7256843,-74.1138399,14z/data=!4m8!2m7!3m5!1sRestaurants!2s40.7256456,-74.0909442!4m2!1d-74.0909442!2d40.7256456!6e5?entry=ttu&quot; driver = webdriver.Chrome() driver.get(URL) # Waits 10 seconds for the elements to load before scrolling wait = WebDriverWait(driver, 10) elements = wait.until( EC.presence_of_all_elements_located((By.CSS_SELECTOR, &quot;div.qjESne&quot;)) ) while True: new_elements = wait.until( EC.presence_of_all_elements_located((By.CSS_SELECTOR, &quot;div.qjESne&quot;)) ) # Pick the last element in the list - this is the one we want to scroll to last_element = elements[-1] # Scroll to the last element driver.execute_script(&quot;arguments[0].scrollIntoView(true);&quot;, last_element) # Update the elements list elements = new_elements # Check if there are any new elements loaded - the &quot;You've reached the end of the list.&quot; message if driver.find_elements(By.CSS_SELECTOR, &quot;span.HlvSq&quot;): print(&quot;No more elements&quot;) break Getting the data If you inspect the page, you will see that the data is under the cards under the CSS selector of div.lI9IFe. What you need to do, is wait until the scrolling has finished, and then you get all the data under the CSS selector of div.lI9IFe Code example from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import pandas as pd URL = &quot;https://www.google.com/maps/search/Restaurants/@40.7256843,-74.1138399,14z/data=!4m8!2m7!3m5!1sRestaurants!2s40.7256456,-74.0909442!4m2!1d-74.0909442!2d40.7256456!6e5?entry=ttu&quot; driver = webdriver.Chrome() driver.get(URL) # Waits 10 seconds for the elements to load before scrolling wait = WebDriverWait(driver, 10) elements = wait.until( EC.presence_of_all_elements_located((By.CSS_SELECTOR, &quot;div.qjESne&quot;)) ) titles = [] links = [] addresses = [] while True: new_elements = wait.until( EC.presence_of_all_elements_located((By.CSS_SELECTOR, &quot;div.qjESne&quot;)) ) # Pick the last element in the list - this is the one we want to scroll to last_element = elements[-1] # Scroll to the last element driver.execute_script(&quot;arguments[0].scrollIntoView(true);&quot;, last_element) # Update the elements list elements = new_elements # time.sleep(0.1) # Check if there are any new elements loaded - the &quot;You've reached the end of the list.&quot; message if driver.find_elements(By.CSS_SELECTOR, &quot;span.HlvSq&quot;): # now we can parse the data since all the elements loaded for data in driver.find_elements(By.CSS_SELECTOR, &quot;div.lI9IFe&quot;): title = data.find_element( By.CSS_SELECTOR, &quot;div.qBF1Pd.fontHeadlineSmall&quot; ).text restaurant = data.find_element( By.CSS_SELECTOR, &quot;.W4Efsd &gt; span:nth-of-type(2)&quot; ).text titles.append(title) addresses.append(restaurant) # This converts the list of titles and links into a dataframe df = pd.DataFrame(list(zip(titles, addresses)), columns=[&quot;title&quot;, &quot;addresses&quot;]) print(df) break Prints: title addresses 0 Domino's Pizza · 741 Communipaw Ave A 1 Tommy's Family Restaurant · 349 Central Ave 2 VIP RESTAURANT LLC BARSHAY'S · 175 Sip Ave 3 The Hutton Restaurant and Bar · 225 Hutton St 4 Barge Inn · 324 3rd St .. ... ... 116 Bettie's Restaurant · 579 West Side Ave 117 Mahboob-E-El Ahi · 580 Montgomery St 118 Samosa Paradise · 804 Newark Ave 119 TACO DRIVE · 195 Newark Ave 120 Two Boots Pizza · 133 Newark Ave [121 rows x 2 columns]",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "That is already a good start. I can name a few things I did to proceed, but note I mainly worked with python. For locating elements within the DOM tree I suggest using xpath. It has a humanreadable syntax and is quite easy to learn. https://devhints.io/xpath Here you can find an overview of all possibilities to locate elements and a linked testbench by &quot;Whitebeam.org&quot; to train. Also helps understanding how to extract names. It will look something like this: Returns an object for the given xpath expression restaurant_adr &lt;- remDr$findElement(using = 'xpath', &quot;//*/*[@class=&quot;W4Efsd&quot;]&quot;) In this object we need to reference the desired attribute, probably .text() I am not sure about the syntax in R restaurant_adr.text() To scroll there is https://www.selenium.dev/documentation/webdriver/actions_api/wheel/ but it has no documentation for R Or you could use javascript for scrolling. driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) https://cran.r-project.org/web/packages/js/vignettes/intro.html Helpful resources: https://statsandr.com/blog/web-scraping-in-r/ https://betterdatascience.com/r-web-scraping/ https://scrapfly.io/blog/web-scraping-with-r/#http-clients-crul",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "html",
        "r",
        "xml",
        "selenium-webdriver"
      ],
      "question_score": 7,
      "answer_score": 7,
      "created": "2023-07-17T03:39:03",
      "question_id": 76701351,
      "answer_id": 76726747
    }
  },
  {
    "question": "UI element for an infinite scrolling image gallery for 100k images: which data structure?",
    "expected_answer": "You can use paging to simulate scrolling 100K images, but actually you need to show at most one page of images during scrolling. create a grid of images inside a canvas widget just determine which page of images to be shown during scrolling then refresh the grid of images Below is an example with a 5x5 grid of images at a time: import os import tkinter as tk from tkinter import ttk from PIL import Image, ImageTk W = H = 100 # thumbnail size in pixels SP = 5 # space between thumbnails in pixels class InfiniteScrollApp(tk.Tk): def __init__(self, image_dir='.', *args, **kwargs): self.image_types = kwargs.pop('image_types', ('.jpg',)) self.rows = kwargs.pop('rows', 5) self.cols = kwargs.pop('columns', 5) super().__init__(*args, **kwargs) self.resizable(0, 0) self.top_row = 0 self.image_files = [] self.images = [] self.create_ui() self.load_image_files(image_dir) def create_ui(self): self.canvas = tk.Canvas(self, bg='white', highlightthickness=0, width=(W+SP)*self.cols, height=(H+SP)*self.rows, yscrollcommand=self.scrollbar_set) self.scrollbar = ttk.Scrollbar(self, orient='vertical', command=self.yview) self.scrollbar.pack(side='right', fill='y') self.canvas.pack(side='left', fill='both', expand=1) self.canvas.bind('&lt;MouseWheel&gt;', self.on_mouse_wheel) self.update() def load_image_files(self, image_dir): self.image_files = [os.path.join(root, file) for root, dirs, files in os.walk(image_dir) for file in files if file.endswith(self.image_types)] self.total_images = len(self.image_files) print(f'total {self.total_images} image files found') self.update_images() def on_mouse_wheel(self, event): self.set_top_row(self.top_row - event.delta//120) self.update_images() def update_images(self): # update total_rows self.total_rows, x = divmod(len(self.image_files), self.cols) if x: self.total_rows += 1 if self.image_files: self.canvas.delete('all') self.images.clear() idx = self.top_row * self.cols for i in range(self.rows*self.cols): if idx+i &lt; self.total_images: r, c = divmod(i, self.cols) x, y = c*(W+SP), r*(H+SP) image = Image.open(self.image_files[idx+i]) image.thumbnail((W, H)) self.images.append(ImageTk.PhotoImage(image)) self.canvas.create_image(x+W//2, y+H//2, image=self.images[-1], anchor='c', tag=f'img_{idx+i}') self.canvas.create_rectangle(x, y, x+W, y+H, outline='gray') self.scrollbar_set(self.top_row/self.total_rows, (self.top_row+self.rows)/self.total_rows) def yview(self, *args): #print('yview:', args) total = self.total_rows - self.rows if args[0] == 'scroll': delta = (self.rows if args[-1] == 'pages' else 1) * int(args[1]) self.set_top_row(self.top_row+delta) elif args[0] == 'moveto': self.set_top_row(int(float(args[-1]) * total)) self.update_images() def scrollbar_set(self, *args): #print('scrollbar_set:', args) self.scrollbar.set(*args) def set_top_row(self, row): max_top_row = self.total_rows - self.rows if row &lt; 0: row = 0 elif row &gt; max_top_row: row = max_top_row self.top_row = row InfiniteScrollApp('./demo_images').mainloop() Result when started: Result when scrolled to the end:",
    "context_chunks": [
      {
        "text": "I'm making an (infinite) scrollable GUI that displays 100x100px thumbnails of possibly 100 000 image files (JPG), in Python Tkinter. The MCVE code below works: using lazy loading: it reloads 30 new images when the scrollbar reaches the end, but it will not scale when browsing 100k images because the canvas object will become bigger and bigger and overflow memory. What UI element to use for this? Is there a tk.Frame or tk.Canvas in which you can add new content at the bottom (e.g. y = 10 000), and destroy old content at the top (e.g. at y=0..5 000), and then downsize the canvas, for example remove the first half of it (to avoid the canvas height become higher and higher) without the GUI flickering? Or maybe a canvas frame data structure that &quot;cycles&quot; like a circular buffer? This would avoid the canvas height to explode, and always stay 10 000 pixels high. When we reach y=9 000, old images at y=0..1 000 are replaced by new ones, and when scrolling at at y=10 000 in fact we are looping back to y=0. Does this exist? Or maybe another system with tiles? (but I don't see how) Notes: Many image viewers like Google Picasa (long discontinued freeware) do this: Note that the right vertical scrollbar doesn't reflect the real progress, because the scrollbar handle would be too small and be imprecise for 100k images - here it's more a joystick-like scrollbar Code: import os, tkinter as tk, tkinter.ttk as ttk, PIL.Image, PIL.ImageTk class InfiniteScrollApp(tk.Tk): def __init__(self, image_dir, *args, **kwargs): super().__init__(*args, **kwargs) self.geometry(&quot;800x600&quot;) self.image_files = [os.path.join(root, file) for root, dirs, files in os.walk(image_dir) for file in files if file.lower().endswith(('.jpg', '.jpeg'))] self.image_objects = [] self.canvas = tk.Canvas(self, bg=&quot;white&quot;) self.scrollbar = ttk.Scrollbar(self, orient=&quot;vertical&quot;, command=self.yview) self.canvas.configure(yscrollcommand=self.scrollbar.set) self.scrollbar.pack(side=&quot;right&quot;, fill=&quot;y&quot;) self.canvas.pack(side=&quot;left&quot;, fill=&quot;both&quot;, expand=True) self.frame = tk.Frame(self.canvas, width=self.canvas.winfo_width()) self.canvas.create_window((0, 0), window=self.frame, anchor=&quot;nw&quot;) self.canvas.bind_all(&quot;&lt;MouseWheel&gt;&quot;, self.on_scroll) self.load_images() def yview(self, *args): self.canvas.yview(*args) self.update() def update(self): canvas_height = self.canvas.bbox(&quot;all&quot;)[3] scroll_pos = self.canvas.yview()[1] if scroll_pos &gt; 0.9: self.load_images() def load_images(self): current_image_count = len(self.image_objects) for i in range(current_image_count, current_image_count + 30): if i &lt; len(self.image_files): image_path = self.image_files[i] img = PIL.Image.open(image_path) img.thumbnail((100, 100)) photo = PIL.ImageTk.PhotoImage(img) label = tk.Label(self.frame, image=photo) label.image = photo label.pack(pady=5, padx=10) self.image_objects.append(label) self.frame.update_idletasks() self.canvas.config(scrollregion=self.canvas.bbox(&quot;all&quot;)) def on_scroll(self, event): self.canvas.yview_scroll(-1 * (event.delta // 120), &quot;units&quot;) self.update() app = InfiniteScrollApp(&quot;D:\\images&quot;) app.mainloop() Sample images Sample code to generate 10k images quickly with a text number drawn on them: from PIL import Image, ImageDraw, ImageFont import random for i in range(10000): img = Image.new('RGB', (200, 200), (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))) draw = ImageDraw.Draw(img) font = ImageFont.truetype(&quot;arial.ttf&quot;, 30) draw.text((10, 10), str(i), font=font, fill=(0, 0, 0)) img.save(f'color_image_{i:04d}.jpg')",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can use paging to simulate scrolling 100K images, but actually you need to show at most one page of images during scrolling. create a grid of images inside a canvas widget just determine which page of images to be shown during scrolling then refresh the grid of images Below is an example with a 5x5 grid of images at a time: import os import tkinter as tk from tkinter import ttk from PIL import Image, ImageTk W = H = 100 # thumbnail size in pixels SP = 5 # space between thumbnails in pixels class InfiniteScrollApp(tk.Tk): def __init__(self, image_dir='.', *args, **kwargs): self.image_types = kwargs.pop('image_types', ('.jpg',)) self.rows = kwargs.pop('rows', 5) self.cols = kwargs.pop('columns', 5) super().__init__(*args, **kwargs) self.resizable(0, 0) self.top_row = 0 self.image_files = [] self.images = [] self.create_ui() self.load_image_files(image_dir) def create_ui(self): self.canvas = tk.Canvas(self, bg='white', highlightthickness=0, width=(W+SP)*self.cols, height=(H+SP)*self.rows, yscrollcommand=self.scrollbar_set) self.scrollbar = ttk.Scrollbar(self, orient='vertical', command=self.yview) self.scrollbar.pack(side='right', fill='y') self.canvas.pack(side='left', fill='both', expand=1) self.canvas.bind('&lt;MouseWheel&gt;', self.on_mouse_wheel) self.update() def load_image_files(self, image_dir): self.image_files = [os.path.join(root, file) for root, dirs, files in os.walk(image_dir) for file in files if file.endswith(self.image_types)] self.total_images = len(self.image_files) print(f'total {self.total_images} image files found') self.update_images() def on_mouse_wheel(self, event): self.set_top_row(self.top_row - event.delta//120) self.update_images() def update_images(self): # update total_rows self.total_rows, x = divmod(len(self.image_files), self.cols) if x: self.total_rows += 1 if self.image_files: self.canvas.delete('all') self.images.clear() idx = self.top_row * self.cols for i in range(self.rows*self.cols): if idx+i &lt; self.total_images: r, c = divmod(i, self.cols) x, y = c*(W+SP), r*(H+SP) image = Image.open(self.image_files[idx+i]) image.thumbnail((W, H)) self.images.append(ImageTk.PhotoImage(image)) self.canvas.create_image(x+W//2, y+H//2, image=self.images[-1], anchor='c', tag=f'img_{idx+i}') self.canvas.create_rectangle(x, y, x+W, y+H, outline='gray') self.scrollbar_set(self.top_row/self.total_rows, (self.top_row+self.rows)/self.total_rows) def yview(self, *args): #print('yview:', args) total = self.total_rows - self.rows if args[0] == 'scroll': delta = (self.rows if args[-1] == 'pages' else 1) * int(args[1]) self.set_top_row(self.top_row+delta) elif args[0] == 'moveto': self.set_top_row(int(float(args[-1]) * total)) self.update_images() def scrollbar_set(self, *args): #print('scrollbar_set:', args) self.scrollbar.set(*args) def set_top_row(self, row): max_top_row = self.total_rows - self.rows if row &lt; 0: row = 0 elif row &gt; max_top_row: row = max_top_row self.top_row = row InfiniteScrollApp('./demo_images').mainloop() Result when started: Result when scrolled to the end:",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Instead of trying to load all 100k images into memory, think of it like a moving window that only shows what's currently visible on screen, plus a bit extra above and below as buffer. As you scroll, load new images at the bottom and remove ones that are way off-screen at the top. The canvas stays at a fixed height, and you just shift the content around to make it feel like continuous scrolling. Boom, no more memory problem! That also matches your proposal of the scrollbar just showing your progress through the whole collection, like a percentage, depend how fancy you want it to look. Pseudo-code : Track visible window range and add buffer zone above/below. When scrolling, remove images outside buffer. Near bottom? Load next batch. Too many images loaded? Shift everything up and tweak scroll position to hide the jump. Functions you will need : canvas.delete(item_id) - Remove specific items canvas.move(item_id, dx, dy) - Shift items by x,y pixels canvas.coords(item_id) - Get/set item position canvas.bbox(&quot;all&quot;) - Get bounding box of all items canvas.yview_moveto(fraction) - Scroll to position For viewport tracking: canvas.winfo_height() - Get viewport height canvas.yview() - Get current scroll position as (start, end) fractions",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "image",
        "tkinter",
        "tkinter-canvas",
        "tile"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2025-02-21T10:35:29",
      "question_id": 79457083,
      "answer_id": 79466703
    }
  },
  {
    "question": "Explode multiple columns with different lengths",
    "expected_answer": "Here's one approach: min_length = pl.min_horizontal(pl.col('a', 'b').list.len()) out = (df.filter(min_length != 0) .with_columns( pl.col('a', 'b').list.head(min_length) ) .explode('a', 'b') ) Output: shape: (5, 2) ┌─────┬─────┐ │ a ┆ b │ │ --- ┆ --- │ │ i64 ┆ i64 │ ╞═════╪═════╡ │ 2 ┆ 8 │ │ 3 ┆ 9 │ │ 4 ┆ 10 │ │ 5 ┆ 11 │ │ 6 ┆ 12 │ └─────┴─────┘ Explanation Get the length for the lists in both columns with Expr.list.len and get the shortest for each row with pl.min_horizontal. Now, filter out the rows where min_length == 0 (df.filter) and inside df.with_columns select the first n values of each list with Expr.list.head. Finally, apply df.explode.",
    "context_chunks": [
      {
        "text": "I have a dataframe like: data = { &quot;a&quot;: [[1], [2], [3, 4], [5, 6, 7]], &quot;b&quot;: [[], [8], [9, 10], [11, 12]], } df = pl.DataFrame(data) &quot;&quot;&quot; ┌───────────┬───────────┐ │ a ┆ b │ │ --- ┆ --- │ │ list[i64] ┆ list[i64] │ ╞═══════════╪═══════════╡ │ [1] ┆ [] │ │ [2] ┆ [8] │ │ [3, 4] ┆ [9, 10] │ │ [5, 6, 7] ┆ [11, 12] │ └───────────┴───────────┘ &quot;&quot;&quot; Each pair of lists may not have the same length, and I want to &quot;truncate&quot; the explode to the shortest of both lists: &quot;&quot;&quot; ┌─────┬─────┐ │ a ┆ b │ │ --- ┆ --- │ │ i64 ┆ i64 │ ╞═════╪═════╡ │ 2 ┆ 8 │ │ 3 ┆ 9 │ │ 4 ┆ 10 │ │ 5 ┆ 11 │ │ 6 ┆ 12 │ └─────┴─────┘ &quot;&quot;&quot; I was thinking that maybe I'd have to fill the shortest of both lists with None to match both lengths, and then drop_nulls. But I was wondering if there was a more direct approach to this?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Here's one approach: min_length = pl.min_horizontal(pl.col('a', 'b').list.len()) out = (df.filter(min_length != 0) .with_columns( pl.col('a', 'b').list.head(min_length) ) .explode('a', 'b') ) Output: shape: (5, 2) ┌─────┬─────┐ │ a ┆ b │ │ --- ┆ --- │ │ i64 ┆ i64 │ ╞═════╪═════╡ │ 2 ┆ 8 │ │ 3 ┆ 9 │ │ 4 ┆ 10 │ │ 5 ┆ 11 │ │ 6 ┆ 12 │ └─────┴─────┘ Explanation Get the length for the lists in both columns with Expr.list.len and get the shortest for each row with pl.min_horizontal. Now, filter out the rows where min_length == 0 (df.filter) and inside df.with_columns select the first n values of each list with Expr.list.head. Finally, apply df.explode.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I can't squeeze it in the comments, so I'll just put another answer here as it's a bit more generic for given example: ( df.with_columns( (x := pl.all().list).head( pl.min_horizontal(x.len()) ) ).explode(&quot;*&quot;) .drop_nulls() ) shape: (5, 2) ┌─────┬─────┐ │ a ┆ b │ │ --- ┆ --- │ │ i64 ┆ i64 │ ╞═════╪═════╡ │ 2 ┆ 8 │ │ 3 ┆ 9 │ │ 4 ┆ 10 │ │ 5 ┆ 11 │ │ 6 ┆ 12 │ └─────┴─────┘",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "dataframe",
        "python-polars"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2024-09-15T17:45:39",
      "question_id": 78988010,
      "answer_id": 78988058
    }
  },
  {
    "question": "How to get the value of a specified index number from the sorting of a column and fill it with null if missing?",
    "expected_answer": "There's an open issue related to your question now - polars.Expr.take returns null if ComputeError: index out of bounds. For now you can either use shift() (maintain_order = True just to make it more readable): exp = pl.col.name.sort_by(&quot;age&quot;) ( df .group_by(&quot;country&quot;, maintain_order = True).agg( exp.first().alias(&quot;age_sort_1&quot;), exp.shift(-2).first().alias(&quot;age_sort_2&quot;), exp.last().alias(&quot;age_sort_-1&quot;), ) ) ┌─────────┬────────────┬────────────┬─────────────┐ │ country ┆ age_sort_1 ┆ age_sort_2 ┆ age_sort_-1 │ │ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ str ┆ str ┆ str │ ╞═════════╪════════════╪════════════╪═════════════╡ │ A ┆ a ┆ null ┆ b │ │ B ┆ c ┆ d ┆ d │ │ C ┆ f ┆ null ┆ f │ └─────────┴────────────┴────────────┴─────────────┘ Or, just aggregate your data into list, and then use .list.get() which allows you to use null_on_oob parameter: ( df .group_by(&quot;country&quot;).agg( pl.col.name.sort_by(&quot;age&quot;) ) .with_columns( pl.col.name .list.get(i, null_on_oob = True).alias(f&quot;age_sort_{c}&quot;) for i, c in [(0, 1), (2, 2), (-1, -1)] ).drop(&quot;name&quot;) ) Alternatively you can use list.gather() to get the list of 3 elements you need, convert it to struct via list.to_struct() method and then unnest() it to columns: ( df .group_by(&quot;country&quot;).agg( pl.col.name.sort_by(&quot;age&quot;) ) .with_columns( pl.col.name .list.gather([0,2,-1], null_on_oob = True) .list.to_struct(fields=[&quot;age_sort_1&quot;,&quot;age_sort_2&quot;,&quot;age_sort_-1&quot;]) ).unnest(&quot;name&quot;) )",
    "context_chunks": [
      {
        "text": "import polars as pl df = pl.DataFrame( {&quot;name&quot;: list(&quot;abcdef&quot;), &quot;age&quot;: [21, 31, 32, 53, 45, 26], &quot;country&quot;: list(&quot;AABBBC&quot;)} ) df.group_by(&quot;country&quot;).agg( pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).first().alias(&quot;age_sort_1&quot;), pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).get(2).alias(&quot;age_sort_2&quot;), # OutOfBoundsError: index out of bounds # pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).arr.get(2, null_on_oob=True).alias(&quot;age_2&quot;), # SchemaError: invalid series dtype: expected `FixedSizeList`, got `str` pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).last().alias(&quot;age_sort_-1&quot;) ) As shown in the code above, I want to get the name in each country whose age is in a specific order. However, Expr.get does not provide the null_on_oob parameter. How to automatically fill in null when an out-of-bounds situation occurs? In addition, the .arr.get method provides the null_on_oob parameter, but reports an error SchemaError: invalid series dtype: expected &quot;FixedSizeList&quot;, got &quot;str&quot;. I don’t know what this error refers to and how to solve it. ps: The above code uses the repeated code pl.col(&quot;name&quot;).sort_by(&quot;age&quot;) many times. Is there a more concise method?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "There's an open issue related to your question now - polars.Expr.take returns null if ComputeError: index out of bounds. For now you can either use shift() (maintain_order = True just to make it more readable): exp = pl.col.name.sort_by(&quot;age&quot;) ( df .group_by(&quot;country&quot;, maintain_order = True).agg( exp.first().alias(&quot;age_sort_1&quot;), exp.shift(-2).first().alias(&quot;age_sort_2&quot;), exp.last().alias(&quot;age_sort_-1&quot;), ) ) ┌─────────┬────────────┬────────────┬─────────────┐ │ country ┆ age_sort_1 ┆ age_sort_2 ┆ age_sort_-1 │ │ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ str ┆ str ┆ str │ ╞═════════╪════════════╪════════════╪═════════════╡ │ A ┆ a ┆ null ┆ b │ │ B ┆ c ┆ d ┆ d │ │ C ┆ f ┆ null ┆ f │ └─────────┴────────────┴────────────┴─────────────┘ Or, just aggregate your data into list, and then use .list.get() which allows you to use null_on_oob parameter: ( df .group_by(&quot;country&quot;).agg( pl.col.name.sort_by(&quot;age&quot;) ) .with_columns( pl.col.name .list.get(i, null_on_oob = True).alias(f&quot;age_sort_{c}&quot;) for i, c in [(0, 1), (2, 2), (-1, -1)] ).drop(&quot;name&quot;) ) Alternatively you can use list.gather() to get the list of 3 elements you need, convert it to struct via list.to_struct() method and then unnest() it to columns: ( df .group_by(&quot;country&quot;).agg( pl.col.name.sort_by(&quot;age&quot;) ) .with_columns( pl.col.name .list.gather([0,2,-1], null_on_oob = True) .list.to_struct(fields=[&quot;age_sort_1&quot;,&quot;age_sort_2&quot;,&quot;age_sort_-1&quot;]) ).unnest(&quot;name&quot;) )",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "import polars as pl df = pl.DataFrame( {&quot;name&quot;: list(&quot;abcdef&quot;), &quot;age&quot;: [21, 31, 32, 53, 45, 26], &quot;country&quot;: list(&quot;AABBBC&quot;)} ) df.group_by(&quot;country&quot;).agg( pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).first().alias(&quot;age_sort_1&quot;), pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).implode().list.get(2, null_on_oob=True).get(0).alias(&quot;age_sort_2&quot;), pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).last().alias(&quot;age_sort_-1&quot;) ) The above code can solve the problem. But I don’t know why the return type of pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).implode().list.get(2, null_on_oob=True) is list[str] instead of str and must perform .get(0) again to obtain the correct result.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-polars"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2024-08-14T03:01:22",
      "question_id": 78868961,
      "answer_id": 78869581
    }
  },
  {
    "question": "How to explode multiple List[_] columns with missing values in python polars?",
    "expected_answer": "You were on the right track, trying to fill the missing values with a list of null values of correct length. To make pl.Expr.repeat_by work with null, we need to ensure that the base expression is of a non-null type. This can be achieved by setting the dtype argument of pl.lit explicity. Then, the list column of (lists of) nulls can be used to fill the null values in y. From there, exploding x and y simultaneously works as usually. ( df .with_columns( pl.col(&quot;y&quot;).fill_null( pl.lit(None, dtype=pl.Boolean).repeat_by(pl.col(&quot;x&quot;).list.len()) ) ) ) shape: (3, 2) ┌───────────┬─────────────────────┐ │ x ┆ y │ │ --- ┆ --- │ │ list[i64] ┆ list[bool] │ ╞═══════════╪═════════════════════╡ │ [1] ┆ [true] │ │ [1, 2] ┆ [null, null] │ │ [1, 2, 3] ┆ [true, false, true] │ └───────────┴─────────────────────┘ From here, df.explode(&quot;x&quot;, &quot;y&quot;) should work as expected. Note. If there are more than two columns, which all might contain null values, one can combine the answer above with this answer to have a valid solution. Note.",
    "context_chunks": [
      {
        "text": "Given a Polars dataframe like below, how can I call explode() on both columns while expanding the null entry to the correct length to match up with its row? shape: (3, 2) ┌───────────┬─────────────────────┐ │ x ┆ y │ │ --- ┆ --- │ │ list[i64] ┆ list[bool] │ ╞═══════════╪═════════════════════╡ │ [1] ┆ [true] │ │ [1, 2] ┆ null │ │ [1, 2, 3] ┆ [true, false, true] │ └───────────┴─────────────────────┘ Currently calling df.explode([&quot;x&quot;, &quot;y&quot;]) will result in this error. polars.exceptions.ShapeError: exploded columns must have matching element counts I'm assuming there's not a built-in way. But I can't find/think of a way to convert that null into a list of correct length, such that the explode will work. Here, the required length is not known statically upfront. I looked into passing list.len() expressions into repeat_by(), but repeat_by() doesn't support null.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You were on the right track, trying to fill the missing values with a list of null values of correct length. To make pl.Expr.repeat_by work with null, we need to ensure that the base expression is of a non-null type. This can be achieved by setting the dtype argument of pl.lit explicity. Then, the list column of (lists of) nulls can be used to fill the null values in y. From there, exploding x and y simultaneously works as usually. ( df .with_columns( pl.col(&quot;y&quot;).fill_null( pl.lit(None, dtype=pl.Boolean).repeat_by(pl.col(&quot;x&quot;).list.len()) ) ) ) shape: (3, 2) ┌───────────┬─────────────────────┐ │ x ┆ y │ │ --- ┆ --- │ │ list[i64] ┆ list[bool] │ ╞═══════════╪═════════════════════╡ │ [1] ┆ [true] │ │ [1, 2] ┆ [null, null] │ │ [1, 2, 3] ┆ [true, false, true] │ └───────────┴─────────────────────┘ From here, df.explode(&quot;x&quot;, &quot;y&quot;) should work as expected. Note. If there are more than two columns, which all might contain null values, one can combine the answer above with this answer to have a valid solution. Note.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "See performance at bottom: I like the elegance and intuitiveness of the repeat_by approach but I'm a glutton for punishment so here's an approach that splits the data up by the condition and then puts it back together. It is worse than the simple approach but might be helpful for another operation/use case. pl.concat( [ part.lazy().select(&quot;i&quot;, &quot;x&quot;, pl.lit(None, pl.Boolean).alias(&quot;y&quot;)).explode(&quot;x&quot;) if isnull[0] else part.lazy().explode(&quot;x&quot;, &quot;y&quot;) for isnull, part in df.with_row_index(&quot;i&quot;).group_by( pl.col(&quot;y&quot;).is_null(), maintain_order=True ) ] ).sort(&quot;i&quot;).drop(&quot;i&quot;).collect() This one has an added with_row_index so you can maintain the original order but if the order isn't important you can remove that as well as the subsequent sort/drop. It also turns the parts lazy and collects at the end. This is because if you concat multiple lazyframes, it will run each of their plans in parallel. Again, if this isn't important you can remove the 2 .lazy()s and the .collect(). If you're starting from a lazy frame then you can't use group_by as an iterator directly but you can use map_groups to get the same effect. You have to make a function such as: def part_explode(part: pl.DataFrame): if part.select(pl.col('x').first().list.len()==pl.col('y').first().list.len()).item(): return part.explode('x','y') else: return part.with_columns(pl.lit(None, pl.Boolean).alias('y')).explode('x') and then you do df.group_by(pl.col(&quot;y&quot;).is_null(), maintain_order=True).map_groups( part_explode, schema={&quot;i&quot;: pl.UInt32, &quot;x&quot;: pl.Int64, &quot;y&quot;: pl.Boolean} ).sort('i').drop('i').collect() I don't think map_groups will parallelize the parts since it relies on executing the python function so don't use this approach unless you're starting from lazy and don't have the memory to materialize first. Performance Setting up with import polars as pl import numpy as np n=1_000_000 df=pl.DataFrame({ 'x':np.random.randint(0,10,n), 'y':np.random.randint(0,2,n), 'group':np.random.randint(0, 100_000, n), }).with_columns(pl.col('y').cast(pl.Boolean)).group_by('group').agg('x','y').with_columns( y=pl.when(pl.col('group').mod(20)==0).then(pl.lit(None)).otherwise('y') ).drop('group') and then the tests %%timeit ( df .with_columns( pl.col(&quot;y&quot;).fill_null( pl.lit(None, dtype=pl.Boolean).repeat_by(pl.col(&quot;x&quot;).list.len()) ) ).explode('x','y') ) 31.7 ms ± 5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) vs the concat thing from above 84.1 ms ± 6.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-polars"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2024-07-30T07:59:32",
      "question_id": 78810432,
      "answer_id": 78810497
    }
  },
  {
    "question": "Predict trajectory of a bouncing ball",
    "expected_answer": "What you are trying to do is known as system identification in the academic literature. Sys Id is a method used to identify the parameters of dynamical systems models from trajectory data. Once you have a good model fitted to the data, you can then predict the future trajectory of the ball (see also @el_pazzu's answer for using a Kalman filter although this might be tricky with your non-linear system). Your system is non-linear due to the bouncing behaviour so you will have to set up a non-linear optimization problem and try to solve it for the parameters of your model. The simplest method of system identification is the single-shooting method which involves simulating an entire trajectory and then comparing it to the outputs from the true system. This is the prediction error method, which usually involves minimizing the squared-errors between the output trajectory predicted by the model and the data. Hope that helps. There are many resources online to do this. There are some curated tutorials here: https://resourcium.org/topic/system-identification Also, MATLAB software has a comprehensive set of tools to help you do it. But I would try solving it using scipy.optimize first. If that doesn't work you may need to consider more advanced non-linear optimization algorithms (see for example, GEKKO, CasADi, Pyomo Hope that helps. Good luck, it's a very interesting problem...",
    "context_chunks": [
      {
        "text": "Key Points: I have a default ball trajectory generated using some code(provided below). Lets name this trajectory Initial Trajectory. Next I have an actual ball whose trajectory I need to estimate. Now I need to improve the Initial Trajectory iteratively (to better represent actual ball trajectory) based on the coordinates of the actual ball as it moves through the environment. For example lets say after 5 units of time, I have 5 positions of where the actual ball has been through. Now I need to uses these 5 points and the Initial Trajectory to predict the future trajectory of the actual ball. I am expecting the actual trajectory to improve with time as more positions of the actual ball comes in. Initial Trajectory Visualization: Initial Trajectory Code: import numpy as np import matplotlib.pyplot as plt # Constants g = 9.81 # gravity (m/s^2) e = 0.8 # coefficient of restitution theta = np.radians(45) # launch angle (radians) v0 = 10 # initial velocity (m/s) x0, y0 = 0, 2 # initial position (m) vx0 = v0 * np.cos(theta) # initial horizontal velocity (m/s) vy0 = v0 * np.sin(theta) # initial vertical velocity (m/s) t_step = 0.01 # time step for simulation (s) t_max = 5 # max time for simulation (s) # Initialize lists to store trajectory points x_vals = [x0] y_vals = [y0] # Simulation loop t = 0 x, y = x0, y0 vx, vy = vx0, vy0 while t &lt; t_max: t += t_step x += vx * t_step y += vy * t_step - 0.5 * g * t_step ** 2 vy -= g * t_step if y &lt;= 0: # Ball hits the ground y = 0 vy = -e * vy x_vals.append(x) y_vals.append(y) if len(x_vals) &gt; 1 and x_vals[-1] == x_vals[-2] and y_vals[-1] == y_vals[-2]: break # Stop simulation if ball stops bouncing # Plotting the trajectory plt.figure(figsize=(10, 5)) plt.plot(x_vals, y_vals, label=&quot;Trajectory of the Ball&quot;) plt.xlabel(&quot;Horizontal Distance (m)&quot;) plt.ylabel(&quot;Vertical Distance (m)&quot;) plt.title(&quot;Trajectory of a Bouncing Ball&quot;) plt.legend() plt.grid(True) plt.show() QUESTION: How can I combine a default bouncing ball trajectory and some coordinates of actual ball trajectory to predict the future trajectory of the actual ball. NOTE: I am going with this method of trajectory approximation to better represent real world ball trajectory without having to fine tune complex parameters like energy loss on ball bounce, friction, type of floor, ball elasticity etc. EDIT_1: Actual Ball Trajectory Data: X: [7.410000e-03 9.591000e-02 2.844100e-01 5.729100e-01 9.614100e-01 1.449910e+00 2.038410e+00 2.726910e+00 3.373700e+00 4.040770e+00 4.800040e+00 5.577610e+00 6.355180e+00 7.132750e+00 7.910320e+00 8.687900e+00 9.465470e+00 1.020976e+01 1.092333e+01 1.163690e+01 1.235047e+01 1.306404e+01 1.377762e+01 1.449119e+01] Y: [2.991964 2.903274 2.716584 2.431894 2.049204 1.568514 0.989824 0.313134 0.311512 0.646741 0.88397 1.0232 1.064429 1.007658 0.852887 0.600116 0.249345 0.232557 0.516523 0.702488 0.790454 0.78042 0.672385 0.466351] Graph(Actual Ball Trajectory): Note the ball being dropped from a certain height with some horizontal velocity. The graph is in 3D but I have simplified the data points to be 2D only (one of the axis is always zero) to keep the problem simpler. EDIT_2: I made use of scipy.optimize() with default method. Got the following output: What I observed(looking at the image above) that there is some loss in horizontal velocity after bounce, which I was not considering. I thought there was only loss of vertical velocity. Here is the relevant code for the graph above: import numpy as np import matplotlib.pyplot as plt from scipy.optimize import minimize # actually y act_x = np.array([0.019053200000000013, 0.08770320000000008, 0.1850550000000004, 0.2825550000000008, 0.38005500000000114, 0.4775550000000015, 0.5750549999999806, 0.6725549999999532, 0.7700549999999258, 0.8675549999998984, 0.965054999999871, 1.0625549999998436, 1.1600549999998162, 1.2575549999997888, 1.3550549999997614, 1.452554999999734, 1.5500549999997066, 1.6475549999996792, 1.7450549999996519, 1.8425549999996245, 1.940054999999597, 2.0375549999996125, 2.1234107142854164, 2.193053571428365, 2.2626964285713136, 2.332339285714262, 2.401982142857211, 2.4716250000001594, 2.541267857143108, 2.6109107142860566, 2.680553571429005, 2.750196428571954, 2.8198392857149024, 2.889482142857851, 2.9591250000007996, 3.028767857143748, 3.098410714286697, 3.1680535714296454, 3.237696428572594, 3.3073392857155426, 3.376982142858491, 3.44662500000144, 3.5162678571443884, 3.585910714287337, 3.6555535714302856, 3.725196428573234, 3.794839285716183, 3.8644821428591314, 3.93412500000208, 4.003767857145029, 4.073410714287977, 4.143053571430926, 4.212696428573874, 4.282339285716823, 4.351982142859772, 4.42162500000272, 4.491267857145669, 4.560910714288617, 4.630553571431566, 4.700196428574515, 4.769839285717463, 4.839482142860412, 4.90912500000336, 4.978767857146309, 5.048410714289258, 5.118053571432206, 5.187696428575155, 5.257339285718103, 5.326982142861052, 5.396625000004001, 5.466267857146949, 5.535910714289898, 5.605553571432846, 5.675196428575795, 5.744839285718744, 5.814482142861692, 5.884125000004641, 5.953767857147589, 6.023410714290538, 6.093053571433487, 6.162696428576435, 6.232339285719384, 6.301982142862332, 6.371625000005281, 6.44126785714823, 6.510910714291178, 6.580553571434127, 6.650196428577075, 6.719839285720024, 6.789482142862973, 6.859125000005921, 6.92876785714887, 6.998410714291818, 7.068053571434767, 7.137696428577716, 7.207339285720664, 7.276982142863613, 7.346625000006561, 7.41626785714951, 7.485910714292459, 7.555553571435407, 7.625196428578356, 7.694839285721304, 7.764482142864253]) # actually z act_y = np.array([1.0379079699999998, 1.1754534700000003, 1.3602534700000006, 1.5209239699999955, 1.6570944699999859, 1.7687649699999706, 1.8559354699999533, 1.9186059699999365, 1.95677646999992, 1.970446969999904, 1.9596174699998887, 1.9242879699998736, 1.8644584699998585, 1.7801289699998444, 1.6712994699998283, 1.537969969999807, 1.3801404699997812, 1.19781096999975, 0.9909814699997134, 0.7596519699996719, 0.5038224699996257, 0.22349296999957416, 0.13930352847394978, 0.3499050645343172, 0.5360066005946793, 0.6976081366550367, 0.8347096727153887, 0.9473112087757359, 1.0354127448360804, 1.0990142808964258, 1.1381158169567716, 1.1527173530171173, 1.1428188890774638, 1.1084204251378103, 1.0495219611981568, 0.9661234972585043, 0.8582250333188504, 0.7258265693791917, 0.5689281054395274, 0.3875296414998584, 0.18163117756018424, 0.11887053865291446, 0.2798887215524012, 0.41640690445188283, 0.5284250873513592, 0.6159432702508338, 0.6789614531503088, 0.7174796360497837, 0.7314978189492595, 0.7210160018487353, 0.6860341847482112, 0.6265523676476875, 0.5425705505471646, 0.4340887334466397, 0.3011069163461103, 0.14362509924557573, 0.10758844178113687, 0.2208353837910192, 0.309582325800899, 0.37382926781077935, 0.41357620982065996, 0.42882315183054115, 0.4195700938404224, 0.385817035850304, 0.3275639778601862, 0.24481091987006875, 0.13755786187995, 0.07905142871570557, 0.1646809455657065, 0.22581046241570793, 0.2624399792657099, 0.2745694961157122, 0.26219901296571463, 0.2253285298157173, 0.16395804666572056, 0.07808756351572432, 0.09996825173824968, 0.1511630228129375, 0.17785779388762588, 0.1800525649623144, 0.15774733603700322, 0.11094210711169239, 0.05563702598852237, 0.09995862508974714, 0.11978022419097237, 0.1151018232921977, 0.08592342239342333, 0.06022248147389577, 0.08484848858531084, 0.0849744956967261, 0.06060050280814157, 0.06605277311641561, 0.06792022730639775, 0.05243051383233173, 0.060240252169206004, 0.0532114047328211, 0.05285089806699147, 0.052793140813351076, 0.05130362411928389, 0.05057861286791727, 0.049998536419320685, 0.049997973905616416, 0.049997920093917625, 0.049997920094010155]) def get_initial_trajectory(e_1=0.8): # Constants g = 9.81 # gravity (m/s^2) e = e_1 #0.8 # coefficient of restitution theta = np.radians(45) # launch angle (radians) v0 = 10 # initial velocity (m/s) x0, y0 = 0, 1 # initial position (m) #vx0 = (v0 * np.cos(theta))*-1 # initial horizontal velocity (m/s) #vy0 = v0 * np.sin(theta) # initial vertical velocity (m/s) vx0 = 1.95 vy0 = 4.4 t_step = 0.01 # time step for simulation (s) t_max = 5 # max time for simulation (s) # Initialize lists to store trajectory points x_vals = [x0] y_vals = [y0] # Simulation loop t = 0 x, y = x0, y0 vx, vy = vx0, vy0 while t &lt; t_max: t += t_step x += vx * t_step y += vy * t_step - 0.5 * g * t_step ** 2 vy -= g * t_step if y &lt;= 0: # Ball hits the ground y = 0 vy = -e * vy x_vals.append(x) y_vals.append(y[0] if isinstance(y, np.ndarray) else y ) if len(x_vals) &gt; 1 and x_vals[-1] == x_vals[-2] and y_vals[-1] == y_vals[-2]: break # Stop simulation if ball stops bouncing return x_vals, y_vals def cost_function(params): e_1 = params x_vals, y_vals = get_initial_trajectory(e_1) print(x_vals) print(y_vals) y_temp = np.interp(act_x, x_vals, y_vals) return np.sum((act_y - y_temp)**2) param = np.array([0.8]) output = minimize(cost_function, param, bounds=[(None, None)]) print(&quot;Tuned Coefficient of Restitution: &quot;, output) x_vals, y_vals = get_initial_trajectory(0.8) x_vals_new, y_vals_new = get_initial_trajectory(7.207e-01) # Plotting the trajectory plt.figure(figsize=(10, 5)) plt.plot(x_vals, y_vals, label=&quot;Initial Trajectory&quot;) plt.scatter(act_x, act_y, color='r', label='Observed') plt.plot(x_vals_new, y_vals_new, label=&quot;Output Trajectory&quot;, color=&quot;orange&quot;) plt.xlabel(&quot;Horizontal Distance (m)&quot;) plt.ylabel(&quot;Vertical Distance (m)&quot;) plt.title(&quot;Trajectory of a Bouncing Ball&quot;) plt.legend() plt.grid(True) plt.show() To correct this, I added a parameter q for loss in horizontal velocity. And made some changes in code after bounce: Before Code: if y &lt;= 0: # Ball hits the ground y = 0 vy = -e * vy After Code: if y &lt;= 0: # Ball hits the ground y = 0 vy = -e * vy vx = vx * q # Added This change seems to improve the output(The red and orange line seems more aligned especially for the first 2 bumps): Here is the updated code: import numpy as np import matplotlib.pyplot as plt from scipy.optimize import minimize # actually y act_x = np.array([0.019053200000000013, 0.08770320000000008, 0.1850550000000004, 0.2825550000000008, 0.38005500000000114, 0.4775550000000015, 0.5750549999999806, 0.6725549999999532, 0.7700549999999258, 0.8675549999998984, 0.965054999999871, 1.0625549999998436, 1.1600549999998162, 1.2575549999997888, 1.3550549999997614, 1.452554999999734, 1.5500549999997066, 1.6475549999996792, 1.7450549999996519, 1.8425549999996245, 1.940054999999597, 2.0375549999996125, 2.1234107142854164, 2.193053571428365, 2.2626964285713136, 2.332339285714262, 2.401982142857211, 2.4716250000001594, 2.541267857143108, 2.6109107142860566, 2.680553571429005, 2.750196428571954, 2.8198392857149024, 2.889482142857851, 2.9591250000007996, 3.028767857143748, 3.098410714286697, 3.1680535714296454, 3.237696428572594, 3.3073392857155426, 3.376982142858491, 3.44662500000144, 3.5162678571443884, 3.585910714287337, 3.6555535714302856, 3.725196428573234, 3.794839285716183, 3.8644821428591314, 3.93412500000208, 4.003767857145029, 4.073410714287977, 4.143053571430926, 4.212696428573874, 4.282339285716823, 4.351982142859772, 4.42162500000272, 4.491267857145669, 4.560910714288617, 4.630553571431566, 4.700196428574515, 4.769839285717463, 4.839482142860412, 4.90912500000336, 4.978767857146309, 5.048410714289258, 5.118053571432206, 5.187696428575155, 5.257339285718103, 5.326982142861052, 5.396625000004001, 5.466267857146949, 5.535910714289898, 5.605553571432846, 5.675196428575795, 5.744839285718744, 5.814482142861692, 5.884125000004641, 5.953767857147589, 6.023410714290538, 6.093053571433487, 6.162696428576435, 6.232339285719384, 6.301982142862332, 6.371625000005281, 6.44126785714823, 6.510910714291178, 6.580553571434127, 6.650196428577075, 6.719839285720024, 6.789482142862973, 6.859125000005921, 6.92876785714887, 6.998410714291818, 7.068053571434767, 7.137696428577716, 7.207339285720664, 7.276982142863613, 7.346625000006561, 7.41626785714951, 7.485910714292459, 7.555553571435407, 7.625196428578356, 7.694839285721304, 7.764482142864253]) # actually z act_y = np.array([1.0379079699999998, 1.1754534700000003, 1.3602534700000006, 1.5209239699999955, 1.6570944699999859, 1.7687649699999706, 1.8559354699999533, 1.9186059699999365, 1.95677646999992, 1.970446969999904, 1.9596174699998887, 1.9242879699998736, 1.8644584699998585, 1.7801289699998444, 1.6712994699998283, 1.537969969999807, 1.3801404699997812, 1.19781096999975, 0.9909814699997134, 0.7596519699996719, 0.5038224699996257, 0.22349296999957416, 0.13930352847394978, 0.3499050645343172, 0.5360066005946793, 0.6976081366550367, 0.8347096727153887, 0.9473112087757359, 1.0354127448360804, 1.0990142808964258, 1.1381158169567716, 1.1527173530171173, 1.1428188890774638, 1.1084204251378103, 1.0495219611981568, 0.9661234972585043, 0.8582250333188504, 0.7258265693791917, 0.5689281054395274, 0.3875296414998584, 0.18163117756018424, 0.11887053865291446, 0.2798887215524012, 0.41640690445188283, 0.5284250873513592, 0.6159432702508338, 0.6789614531503088, 0.7174796360497837, 0.7314978189492595, 0.7210160018487353, 0.6860341847482112, 0.6265523676476875, 0.5425705505471646, 0.4340887334466397, 0.3011069163461103, 0.14362509924557573, 0.10758844178113687, 0.2208353837910192, 0.309582325800899, 0.37382926781077935, 0.41357620982065996, 0.42882315183054115, 0.4195700938404224, 0.385817035850304, 0.3275639778601862, 0.24481091987006875, 0.13755786187995, 0.07905142871570557, 0.1646809455657065, 0.22581046241570793, 0.2624399792657099, 0.2745694961157122, 0.26219901296571463, 0.2253285298157173, 0.16395804666572056, 0.07808756351572432, 0.09996825173824968, 0.1511630228129375, 0.17785779388762588, 0.1800525649623144, 0.15774733603700322, 0.11094210711169239, 0.05563702598852237, 0.09995862508974714, 0.11978022419097237, 0.1151018232921977, 0.08592342239342333, 0.06022248147389577, 0.08484848858531084, 0.0849744956967261, 0.06060050280814157, 0.06605277311641561, 0.06792022730639775, 0.05243051383233173, 0.060240252169206004, 0.0532114047328211, 0.05285089806699147, 0.052793140813351076, 0.05130362411928389, 0.05057861286791727, 0.049998536419320685, 0.049997973905616416, 0.049997920093917625, 0.049997920094010155]) def get_initial_trajectory(e_1=0.8, q_1=1): # Constants g = 9.81 # gravity (m/s^2) e = e_1 #0.8 # coefficient of restitution q = q_1 # Horizontal velocity coefficient theta = np.radians(45) # launch angle (radians) v0 = 10 # initial velocity (m/s) x0, y0 = 0, 1 # initial position (m) #vx0 = (v0 * np.cos(theta))*-1 # initial horizontal velocity (m/s) #vy0 = v0 * np.sin(theta) # initial vertical velocity (m/s) vx0 = 1.95 vy0 = 4.4 t_step = 0.01 # time step for simulation (s) t_max = 5 # max time for simulation (s) # Initialize lists to store trajectory points x_vals = [x0] y_vals = [y0] # Simulation loop t = 0 x, y = x0, y0 vx, vy = vx0, vy0 while t &lt; t_max: t += t_step x += vx * t_step y += vy * t_step - 0.5 * g * t_step ** 2 vy -= g * t_step if y &lt;= 0: # Ball hits the ground y = 0 vy = -e * vy vx = vx * q # Added x_vals.append(x) y_vals.append(y[0] if isinstance(y, np.ndarray) else y ) if len(x_vals) &gt; 1 and x_vals[-1] == x_vals[-2] and y_vals[-1] == y_vals[-2]: break # Stop simulation if ball stops bouncing return x_vals, y_vals def cost_function(params): e_1, q_1 = params x_vals, y_vals = get_initial_trajectory(e_1, q_1) print(x_vals) print(y_vals) y_temp = np.interp(act_x, x_vals, y_vals) return np.sum((act_y - y_temp)**2) param = np.array([0.8, 1]) output = minimize(cost_function, param, bounds=[(None, None)]) print(&quot;Tuned Coefficient of Restitution: &quot;, output) x_vals, y_vals = get_initial_trajectory(0.8, 1) x_vals_new, y_vals_new = get_initial_trajectory(output.x[0], output.x[1]) # Plotting the trajectory plt.figure(figsize=(10, 5)) plt.plot(x_vals, y_vals, label=&quot;Initial Trajectory&quot;) plt.scatter(act_x, act_y, color='r', label='Actual Ball Trajectory') plt.plot(x_vals_new, y_vals_new, label=&quot;Output Trajectory&quot;, color=&quot;orange&quot;) plt.xlabel(&quot;Horizontal Distance (m)&quot;) plt.ylabel(&quot;Vertical Distance (m)&quot;) plt.title(&quot;Trajectory of a Bouncing Ball&quot;) plt.legend() plt.grid(True) plt.show() But the results are not perfect. The orange line does not completely overlaps the red dotted line. Now I am completely lost of what to do next: Shall I use better optimizer (like what @Bill mentioned GEKKO, CasADi, Pyomo)? Or change the formula to add more parameters(as it helped in the last change)?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "What you are trying to do is known as system identification in the academic literature. Sys Id is a method used to identify the parameters of dynamical systems models from trajectory data. Once you have a good model fitted to the data, you can then predict the future trajectory of the ball (see also @el_pazzu's answer for using a Kalman filter although this might be tricky with your non-linear system). Your system is non-linear due to the bouncing behaviour so you will have to set up a non-linear optimization problem and try to solve it for the parameters of your model. The simplest method of system identification is the single-shooting method which involves simulating an entire trajectory and then comparing it to the outputs from the true system. This is the prediction error method, which usually involves minimizing the squared-errors between the output trajectory predicted by the model and the data. Hope that helps. There are many resources online to do this. There are some curated tutorials here: https://resourcium.org/topic/system-identification Also, MATLAB software has a comprehensive set of tools to help you do it. But I would try solving it using scipy.optimize first. If that doesn't work you may need to consider more advanced non-linear optimization algorithms (see for example, GEKKO, CasADi, Pyomo Hope that helps. Good luck, it's a very interesting problem...",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Have you tried using a Kalman filter: import numpy as np import matplotlib.pyplot as plt g = 9.81 e = 0.8 t_step = 0.01 # Initial actual ball trajectory data actual_x = np.array([7.410000e-03, 9.591000e-02, 2.844100e-01, 5.729100e-01, 9.614100e-01, 1.449910e+00, 2.038410e+00, 2.726910e+00, 3.373700e+00, 4.040770e+00, 4.800040e+00, 5.577610e+00, 6.355180e+00, 7.132750e+00, 7.910320e+00, 8.687900e+00, 9.465470e+00, 1.020976e+01, 1.092333e+01, 1.163690e+01, 1.235047e+01, 1.306404e+01, 1.377762e+01, 1.449119e+01]) actual_y = np.array([2.991964, 2.903274, 2.716584, 2.431894, 2.049204, 1.568514, 0.989824, 0.313134, 0.311512, 0.646741, 0.88397, 1.0232, 1.064429, 1.007658, 0.852887, 0.600116, 0.249345, 0.232557, 0.516523, 0.702488, 0.790454, 0.78042, 0.672385, 0.466351]) # Initial state (position &amp; velocity) initial_state = np.array([0, 2, 10 * np.cos(np.radians(45)), 10 * np.sin(np.radians(45))]) # [x, y, vx, vy] # State transition matrix dt = t_step F = np.array([[1, 0, dt, 0], [0, 1, 0, dt], [0, 0, 1, 0], [0, 0, 0, 1]]) # Measurement matrix H = np.array([[1, 0, 0, 0], [0, 1, 0, 0]]) # Process noise covariance Q = np.eye(4) * 0.001 # Measure noise covariance R = np.eye(2) * 0.01 # Initial covariance matrix P = np.eye(4) def predict(state, P): state_pred = F @ state P_pred = F @ P @ F.T + Q return state_pred, P_pred def update(state_pred, P_pred, measurement): y = measurement - H @ state_pred S = H @ P_pred @ H.T + R K = P_pred @ H.T @ np.linalg.inv(S) state_upd = state_pred + K @ y P_upd = (np.eye(4) - K @ H) @ P_pred return state_upd, P_upd # Initialize state &amp; covariance state = initial_state trajectory = [state[:2]] time = 0 # Do Kalman filtering with actual measurements for i in range(len(actual_x)): # Predict state_pred, P_pred = predict(state, P) # Measure measurement = np.array([actual_x[i], actual_y[i]]) # Update state, P = update(state_pred, P_pred, measurement) # Save trajectory.append(state[:2]) # Convert trajectory to np array for plotting trajectory = np.array(trajectory) # Plot trajectory plt.figure(figsize=(10, 5)) plt.plot(trajectory[:, 0], trajectory[:, 1], label=&quot;Predicted trajectory&quot;, color='r') plt.scatter(actual_x, actual_y, label=&quot;Actual trajectory&quot;, color='b') plt.xlabel(&quot;Horizontal distance&quot;) plt.ylabel(&quot;Vertical distance&quot;) plt.title(&quot;Trajectory of bouncing ball with Kalman filter&quot;) plt.legend() plt.grid(True) plt.show() This code uses the Kalman filter to refine the trajectory prediction iteratively as more actual data points become available. I obtained this plot:",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "optimization",
        "physics",
        "mathematical-optimization"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2024-07-23T08:36:38",
      "question_id": 78782420,
      "answer_id": 78784971
    }
  },
  {
    "question": "AttributeError: module &#39;chromadb&#39; has no attribute &#39;config&#39;",
    "expected_answer": "This happens when you import chromadb and THEN mess with the sqlite module like below __import__('pysqlite3') import pysqlite3 sys.modules['sqlite3'] = sys.modules[&quot;pysqlite3&quot;] Just restart the kernel (if you are in jupyter) and make sure you import chromadb AFTER tinkering with sys.modules",
    "context_chunks": [
      {
        "text": "so i recently started to work on chromabd and i am facing this error: &quot;module 'chromadb' has no attribute 'config'&quot; here is my code: from langchain.vectorstores import Chroma from sentence_transformers import SentenceTransformer model = SentenceTransformer('all-MiniLM-L6-v2') #Sentences are encoded by calling model.encode() embeddings = [model.encode(text[i].page_content) for i in range(len(text))] presist_directory = 'db' vectordb = Chroma.from_documents(documents=text,embedding=embeddings,persist_directory=presist_directory) i have already tried down versioning chromadb tried this solution: from langchain.indexes import VectorstoreIndexCreator from langchain.vectorstores import Chroma presist_directory = 'db' vectordb = VectorstoreIndexCreator().from_documents( documents=text, embedding=embeddings, persist_directory=presist_directory, vectorstore_cls=Chroma, ) and tried this solution too: import chromadb # Get the version of ChromaDB chroma_version = chromadb.__version__ # Check if the version is 0.4 or later if float(chroma_version[:3]) &gt;= 0.4: # Use the new configuration _client_settings = chromadb.config.Settings( chroma_db_impl=&quot;new_configuration&quot;, persist_directory=persist_directory, ) else: # Use the old configuration _client_settings = chromadb.config.Settings( chroma_db_impl=&quot;duckdb+parquet&quot;, persist_directory=persist_directory, )",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This happens when you import chromadb and THEN mess with the sqlite module like below __import__('pysqlite3') import pysqlite3 sys.modules['sqlite3'] = sys.modules[&quot;pysqlite3&quot;] Just restart the kernel (if you are in jupyter) and make sure you import chromadb AFTER tinkering with sys.modules",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "i got the solution, chromadb have compatibility issues. In my case i have to downgrade the version from 0.4.6 to 0.4.0 . in 0.3.26 it throw an error of 'Collection' object has no attribute 'upsert' and the updated version throw this error. so version 0.4.0 seems to work for me",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "nlp",
        "chatbot",
        "langchain",
        "chromadb"
      ],
      "question_score": 7,
      "answer_score": 7,
      "created": "2023-08-17T11:52:16",
      "question_id": 76921252,
      "answer_id": 78166411
    }
  },
  {
    "question": "Is it possible to get pydantic v2 to dump json with sorted keys?",
    "expected_answer": "I'm not sure whether it is an elegant solution but you could leverage the fact that dictionaries (since python 3.7) preserve an order of elements: from typing import Any, Dict from pydantic import BaseModel, model_serializer class JsonTest(BaseModel): b_field: int c_field: int a_field: str @model_serializer(when_used='json') def sort_model(self) -&gt; Dict[str, Any]: return dict(sorted(self.model_dump().items())) obj = JsonTest(b_field=1, a_field=&quot;one&quot;, c_field=0) print(obj.model_dump_json()) # {&quot;a_field&quot;:&quot;one&quot;,&quot;b_field&quot;:1,&quot;c_field&quot;:0}",
    "context_chunks": [
      {
        "text": "In the pydantic v1 there was an option to add kwargs which would get passed to json.dumps via **dumps_kwargs. However, in pydantic v2 if you try to add extra kwargs to BaseModel.json() it fails with the error TypeError: `dumps_kwargs` keyword arguments are no longer supported. Here is example code with a workaround using dict()/model_dump(). This is good enough as long as the types are simple, but it won't work for the more complex data types that pydantic knows how to serialize. Is there a way to get sort_keys to work in pydantic v2 in general? import json from pydantic import BaseModel class JsonTest(BaseModel): b_field: int a_field: str obj = JsonTest(b_field=1, a_field=&quot;one&quot;) # this worked in pydantic v1 but raises a TypeError in v2 # print(obj.json(sort_keys=True) print(obj.model_dump_json()) # {&quot;b_field&quot;:1,&quot;a_field&quot;:&quot;one&quot;} # workaround for simple objects print(json.dumps(obj.model_dump(), sort_keys=True)) # {&quot;a_field&quot;: &quot;one&quot;, &quot;b_field&quot;: 1}",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I'm not sure whether it is an elegant solution but you could leverage the fact that dictionaries (since python 3.7) preserve an order of elements: from typing import Any, Dict from pydantic import BaseModel, model_serializer class JsonTest(BaseModel): b_field: int c_field: int a_field: str @model_serializer(when_used='json') def sort_model(self) -&gt; Dict[str, Any]: return dict(sorted(self.model_dump().items())) obj = JsonTest(b_field=1, a_field=&quot;one&quot;, c_field=0) print(obj.model_dump_json()) # {&quot;a_field&quot;:&quot;one&quot;,&quot;b_field&quot;:1,&quot;c_field&quot;:0}",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Hi! Try using the model_dump method and the sort_keys parameter of json.dumps to achieve sorting of keys in the JSON output in pydantic v2. import json from pydantic import BaseModel class JsonTest(BaseModel): b_field: int a_field: str obj = JsonTest(b_field=1, a_field=&quot;one&quot;) # Use the model_dump method to get a dictionary and then sort the keys sorted_json = json.dumps(obj.model_dump(), sort_keys=True) print(sorted_json) # {&quot;a_field&quot;: &quot;one&quot;, &quot;b_field&quot;: 1} This will produce a JSON striing with sorted keys. The model_dump method returns a dictionary representation of the Pydantic model, and then json.dumps is used with the sort_keys=True parameter to sort the keys alphabeticaly in the resulting JSON string. You can read more here: https://docs.pydantic.dev/latest/usage/serialization/",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "json",
        "pydantic"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2023-09-10T14:48:01",
      "question_id": 77076597,
      "answer_id": 77077890
    }
  },
  {
    "question": "What happens to the asyncio event loop when multiple CPU-bound tasks run concurrently in a ThreadPoolExecutor given Python’s GIL?",
    "expected_answer": "In short, yes, your understanding is correct. When a thread (even when executing a CPU-bound, instead of I/O-bound, task via ThreadPoolExecutor, as you mentioned) holds the Global Interpreter Lock (GIL), no other thread or the event loop (actually, the thread in which the event loop is executed; in this case, that is the main thread, the thread from which the Python interpreter was started—each Python process is created with one) can run Python bytecode, until the GIL is voluntarily released or a timeout has passed. As mentioned in this answer, at which I would recommend having a look, if a CPU-bound operation (or an I/O-bound one that wouldn't voluntarily release the GIL) was executed inside a thread, and the GIL had not been released after 5ms, Python would (automatically) tell/force the current thread to release the GIL. The 5ms value is the interpreter’s default thread switch interval, which could be obtained as follows: import sys print(sys.getswitchinterval()) # 0.005 This floating-point value determines the ideal duration of the &quot;timeslices&quot; allocated to concurrently running Python threads and it could be configured using sys.setswitchinterval(interval) (the interval should be in seconds). Also, as noted in the linked documentation: Please note that the actual value can be higher, especially if long-running internal functions or methods are used. Also, which thread becomes scheduled at the end of the interval is the operating system's decision. The interpreter doesn't have its own scheduler. So, yes, &quot;there's no guarantee the event loop will get immediate priority&quot;, and there is currently no way to make priority requests for particular threads, in order, for instance, to favor the main thread (in which the event loop is running, in this case) over worker threads. It should also be noted that this automatic GIL release (mentioned earlier) is best-effort, not guaranteed. Certain native Python functions, such as pow(), for instance, would likely not release the GIL in certain cases (e.g., when performing large computations); thus, essentially blocking every execution, and in this case, the entire server, while they are running: pow(365,100000000000000) # this would not release the GIL Therefore, in such cases, it would be more appropriate to use a ProcessPoolExecutor instead of ThreadPoolExecutor or FastAPI/Starlette's external threadpool (see the linked answer above, as well as this answer for more details on that). You could alternatively increase the number of server workers/processes, as noted at the bottom of the first linked answer above (hence, please take a look at that part for more details and the possible constraints). Either way, a separate process would also mean a separate GIL. In the case of server workers, each worker would also have its own event loop that would run in the main thread of each process. Thus, depending on the number of server workers, a number of requests could be served in parallel without blocking the server. Example On a side note, in real-world scenarios, it might be best to create a reusable ProcessPoolExecutor, as explained and demonstrated in the linked answer above, instead of creating a new ProcessPool each time the endpoint is called, as shown in the example below; hence, please take a look at that answer for more details and examples on that. from fastapi import FastAPI import concurrent.futures import asyncio from multiprocessing import current_process from threading import current_thread app = FastAPI() def cpu_bound_task(): pid = current_process().pid tid = current_thread().ident thread_name = current_thread().name process_name = current_process().name print(f&quot;{pid} - {process_name} - {tid} - {thread_name}&quot;) pow(365,100000000000000) # this WILL block the event loop (because of `pow()`) @app.get(&quot;/blocking&quot;) async def blocking(): loop = asyncio.get_running_loop() with concurrent.futures.ThreadPoolExecutor() as pool: res = await loop.run_in_executor(pool, cpu_bound_task) return &quot;OK&quot; # this WON'T block the event loop @app.get(&quot;/non-blocking&quot;) async def non_blocking(): loop = asyncio.get_running_loop() with concurrent.futures.ProcessPoolExecutor() as pool: res = await loop.run_in_executor(pool, cpu_bound_task) return &quot;OK&quot; if __name__ == &quot;__main__&quot;: import uvicorn uvicorn.run(app) If your cpu_bound_task() does not involve functions such as pow(), you could still choose to use ThreadPoolExecutor, but, although this would prevent the event loop from getting blocked, it wouldn't, however, give you the performance improvement you would expect from running CPU-bound tasks in parallel. ThreadPoolExecutor should be prefered when dealing with blocking I/O-bound tasks instead. As for whether &quot;multiple CPU-bound tasks running concurrently in a threadpool could potentially monopolize the GIL, thus delaying the event loop and causing increased latency and reduced responsiveness&quot;, this needs to be tested for one to say with certainty and will depend on your server's system resources available, as well as the expected traffic (number of requests submitted within a certain time period) and the nature of the CPU-bound tasks. You might choose to implement a queue mechanism, in order to control the number of requests that are being processed concurrently (or in parallel); thus minimizing any potential delays and improving responsiveness, should this ever become an issue—see this answer, for instance (more options are provided later on). Further, as explained in the second linked answer earlier, it should be noted that when using ThreadPoolExecutor, the max_workers argument is by default set to None, meaning that the number of worker threads is set based on the following equation: min(32, os.cpu_count() + 4) (In Python 3.13 this is changed to min(32, (os.process_cpu_count() or 1) + 4). For ProcessPoolExecutor, on the other hand, max_workers defaults to os.process_cpu_count()). If, for instance, your machine has 4 physical cores, each with hyperthreading, then Python will see 8 CPUs and will allocate 12 threads (8 CPUs + 4) to the pool by default. This number of worker threads, however, may not be enough for your project requirements, and thus, it might need to be adjusted. While in ProcessPoolExecutor's case might not make that sense to increase the number of workers more than the default number given by Python, as explained above; however, when using ThreadPoolExecutor, it may do so, as it is common to have more threads than CPUs (physical or logical) in one's system. The reason is that threads are mainly used for I/O-bound tasks (that wait for relatively slow resources to respond, and thus, can be shuffled between I/O waits), not CPU-bound tasks. Note, though, that with too many threads active at once, your program may spend more time context switching than actually executing tasks. Thus, I would suggest performing various benchmark tests, similar to the ones described in the linked answer earlier, in order to compare the overall execution time and then choose a number of threads that gives approximately the best performance (e.g., ThreadPoolExecutor(max_workers=n)). It should also be noted that one could instead have the CPU-bound tasks run in the background (after returning an immediate response to the client with a unique ID assigned to their request) and implement a polling mechanism, in order for the client to check on the status of the task and retrieve the results when the task is completed (see Solution 2 of this answer and the citations provided there for working examples) or use websockets instead, as shown here, as well as here and here. GIL Becomes Optional in Python 3.13 I should also mention that in Python 3.13, there have been advances in disabling the GIL and making Free-threaded CPython. As noted in the docs: This is an experimental feature and therefore is not enabled by default. The free-threaded mode requires a different executable, usually called python3.13t or python3.13t.exe. Pre-built binaries marked as free-threaded can be installed as part of the official Windows and macOS installers, or CPython can be built from source with the --disable-gil option. Free-threaded execution allows for full utilization of the available processing power by running threads in parallel on available CPU cores. While not all software will benefit from this automatically, programs designed with threading in mind will run faster on multi-core hardware. The free-threaded mode is experimental and work is ongoing to improve it: expect some bugs and a substantial single-threaded performance hit. With free-threaded CPython, threads will be able to execute Python code truly in parallel, making multi-threaded applications much more efficient; especially, in CPU-bound tasks. However, as mentioned in the docs, this is still an experimental feature and one should expect bugs, etc. As for any articles and tutorials to look for, I would suggest to start by taking the official FastAPI tutorial, as well as having a look at Starlette's docs. Apart from that, I would recommend having a thorough look at all the linked answers above (and the references included in them), as they would help you better understand a lot of the concepts and technologies included in FastAPI/Starlette.",
    "context_chunks": [
      {
        "text": "I'm working on an asynchronous Python application (using FastAPI/Starlette/asyncio) that needs to offload synchronous, CPU-bound tasks to a thread pool (ThreadPoolExecutor) to avoid blocking the event loop. I understand that Python's Global Interpreter Lock (GIL) allows only one thread to execute Python bytecode at a time per process. But I want to clarify how this affects the asyncio event loop when multiple CPU-bound tasks (say 10) are submitted concurrently to the thread pool. Scenario The event loop runs in the main thread. There is a thread pool with multiple worker threads (e.g., 10 threads). Ten CPU-bound synchronous tasks are submitted almost simultaneously, each running Python code that holds the GIL while executing. The event loop also needs the GIL to run coroutines, callbacks, and other async operations. What I Understand When a thread holds the GIL, no other thread or the event loop can run Python bytecode. CPython periodically enforces time-slicing so that the GIL is released after a check interval, letting other threads acquire it. The event loop and worker threads contend for the GIL. If one worker thread holds the GIL, the event loop is effectively blocked from executing Python code. When the GIL is released, any thread (another worker thread or the event loop) may acquire it next; there’s no guarantee the event loop will get immediate priority. Therefore, multiple CPU-bound tasks running concurrently in the thread pool can serially monopolize the GIL, delaying the event loop and causing increased latency and reduced responsiveness. My Questions Is this understanding accurate regarding how the GIL contention affects the asyncio event loop? Does Python’s GIL and time-slicing mechanism indeed cause the event loop to be “starved” or blocked temporarily when multiple CPU-bound threads are running? Are there any internal scheduling mechanisms or priorities in CPython that favor the event loop thread over worker threads in such scenarios? Are there recommended best practices or architectural patterns to avoid this problem, aside from moving CPU-bound tasks to ProcessPoolExecutor or external services? Context I am considering using ThreadPoolExecutor in FastAPI to run some blocking CPU-bound tasks asynchronously but want to understand the implications on event loop responsiveness when multiple such tasks run concurrently. Thank you in advance for any clarifications or insights!",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "In short, yes, your understanding is correct. When a thread (even when executing a CPU-bound, instead of I/O-bound, task via ThreadPoolExecutor, as you mentioned) holds the Global Interpreter Lock (GIL), no other thread or the event loop (actually, the thread in which the event loop is executed; in this case, that is the main thread, the thread from which the Python interpreter was started—each Python process is created with one) can run Python bytecode, until the GIL is voluntarily released or a timeout has passed. As mentioned in this answer, at which I would recommend having a look, if a CPU-bound operation (or an I/O-bound one that wouldn't voluntarily release the GIL) was executed inside a thread, and the GIL had not been released after 5ms, Python would (automatically) tell/force the current thread to release the GIL. The 5ms value is the interpreter’s default thread switch interval, which could be obtained as follows: import sys print(sys.getswitchinterval()) # 0.005 This floating-point value determines the ideal duration of the &quot;timeslices&quot; allocated to concurrently running Python threads and it could be configured using sys.setswitchinterval(interval) (the interval should be in seconds). Also, as noted in the linked documentation: Please note that the actual value can be higher, especially if long-running internal functions or methods are used. Also, which thread becomes scheduled at the end of the interval is the operating system's decision. The interpreter doesn't have its own scheduler. So, yes, &quot;there's no guarantee the event loop will get immediate priority&quot;, and there is currently no way to make priority requests for particular threads, in order, for instance, to favor the main thread (in which the event loop is running, in this case) over worker threads. It should also be noted that this automatic GIL release (mentioned earlier) is best-effort, not guaranteed. Certain native Python functions, such as pow(), for instance, would likely not release the GIL in certain cases (e.g., when performing large computations); thus, essentially blocking every execution, and in this case, the entire server, while they are running: pow(365,100000000000000) # this would not release the GIL Therefore, in such cases, it would be more appropriate to use a ProcessPoolExecutor instead of ThreadPoolExecutor or FastAPI/Starlette's external threadpool (see the linked answer above, as well as this answer for more details on that). You could alternatively increase the number of server workers/processes, as noted at the bottom of the first linked answer above (hence, please take a look at that part for more details and the possible constraints). Either way, a separate process would also mean a separate GIL. In the case of server workers, each worker would also have its own event loop that would run in the main thread of each process. Thus, depending on the number of server workers, a number of requests could be served in parallel without blocking the server. Example On a side note, in real-world scenarios, it might be best to create a reusable ProcessPoolExecutor, as explained and demonstrated in the linked answer above, instead of creating a new ProcessPool each time the endpoint is called, as shown in the example below; hence, please take a look at that answer for more details and examples on that. from fastapi import FastAPI import concurrent.futures import asyncio from multiprocessing import current_process from threading import current_thread app = FastAPI() def cpu_bound_task(): pid = current_process().pid tid = current_thread().ident thread_name = current_thread().name process_name = current_process().name print(f&quot;{pid} - {process_name} - {tid} - {thread_name}&quot;) pow(365,100000000000000) # this WILL block the event loop (because of `pow()`) @app.get(&quot;/blocking&quot;) async def blocking(): loop = asyncio.get_running_loop() with concurrent.futures.ThreadPoolExecutor() as pool: res = await loop.run_in_executor(pool, cpu_bound_task) return &quot;OK&quot; # this WON'T block the event loop @app.get(&quot;/non-blocking&quot;) async def non_blocking(): loop = asyncio.get_running_loop() with concurrent.futures.ProcessPoolExecutor() as pool: res = await loop.run_in_executor(pool, cpu_bound_task) return &quot;OK&quot; if __name__ == &quot;__main__&quot;: import uvicorn uvicorn.run(app) If your cpu_bound_task() does not involve functions such as pow(), you could still choose to use ThreadPoolExecutor, but, although this would prevent the event loop from getting blocked, it wouldn't, however, give you the performance improvement you would expect from running CPU-bound tasks in parallel. ThreadPoolExecutor should be prefered when dealing with blocking I/O-bound tasks instead. As for whether &quot;multiple CPU-bound tasks running concurrently in a threadpool could potentially monopolize the GIL, thus delaying the event loop and causing increased latency and reduced responsiveness&quot;, this needs to be tested for one to say with certainty and will depend on your server's system resources available, as well as the expected traffic (number of requests submitted within a certain time period) and the nature of the CPU-bound tasks. You might choose to implement a queue mechanism, in order to control the number of requests that are being processed concurrently (or in parallel); thus minimizing any potential delays and improving responsiveness, should this ever become an issue—see this answer, for instance (more options are provided later on). Further, as explained in the second linked answer earlier, it should be noted that when using ThreadPoolExecutor, the max_workers argument is by default set to None, meaning that the number of worker threads is set based on the following equation: min(32, os.cpu_count() + 4) (In Python 3.13 this is changed to min(32, (os.process_cpu_count() or 1) + 4). For ProcessPoolExecutor, on the other hand, max_workers defaults to os.process_cpu_count()). If, for instance, your machine has 4 physical cores, each with hyperthreading, then Python will see 8 CPUs and will allocate 12 threads (8 CPUs + 4) to the pool by default. This number of worker threads, however, may not be enough for your project requirements, and thus, it might need to be adjusted. While in ProcessPoolExecutor's case might not make that sense to increase the number of workers more than the default number given by Python, as explained above; however, when using ThreadPoolExecutor, it may do so, as it is common to have more threads than CPUs (physical or logical) in one's system. The reason is that threads are mainly used for I/O-bound tasks (that wait for relatively slow resources to respond, and thus, can be shuffled between I/O waits), not CPU-bound tasks. Note, though, that with too many threads active at once, your program may spend more time context switching than actually executing tasks. Thus, I would suggest performing various benchmark tests, similar to the ones described in the linked answer earlier, in order to compare the overall execution time and then choose a number of threads that gives approximately the best performance (e.g., ThreadPoolExecutor(max_workers=n)). It should also be noted that one could instead have the CPU-bound tasks run in the background (after returning an immediate response to the client with a unique ID assigned to their request) and implement a polling mechanism, in order for the client to check on the status of the task and retrieve the results when the task is completed (see Solution 2 of this answer and the citations provided there for working examples) or use websockets instead, as shown here, as well as here and here. GIL Becomes Optional in Python 3.13 I should also mention that in Python 3.13, there have been advances in disabling the GIL and making Free-threaded CPython. As noted in the docs: This is an experimental feature and therefore is not enabled by default. The free-threaded mode requires a different executable, usually called python3.13t or python3.13t.exe. Pre-built binaries marked as free-threaded can be installed as part of the official Windows and macOS installers, or CPython can be built from source with the --disable-gil option. Free-threaded execution allows for full utilization of the available processing power by running threads in parallel on available CPU cores. While not all software will benefit from this automatically, programs designed with threading in mind will run faster on multi-core hardware. The free-threaded mode is experimental and work is ongoing to improve it: expect some bugs and a substantial single-threaded performance hit. With free-threaded CPython, threads will be able to execute Python code truly in parallel, making multi-threaded applications much more efficient; especially, in CPU-bound tasks. However, as mentioned in the docs, this is still an experimental feature and one should expect bugs, etc. As for any articles and tutorials to look for, I would suggest to start by taking the official FastAPI tutorial, as well as having a look at Starlette's docs. Apart from that, I would recommend having a thorough look at all the linked answers above (and the references included in them), as they would help you better understand a lot of the concepts and technologies included in FastAPI/Starlette.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "As mentioned in the comments and Chris's answer, your overall understanding is correct. But I believe there's a fairly simple solution both you and the other answer appear to be missing. Are there recommended best practices or architectural patterns to avoid this problem Yes - reduce the worker count in the thread pool, so the switch to the event loop thread is always reasonably fast. That might not be a very sophisticated solution, but I believe it's the best option available given the constraints you've laid out: use of Python with GIL, and use of multi-threading rather than multi-processing. If you're worried about latency, you could even use a thread pool with just one worker, created with ThreadPoolExecutor(max_workers=1). That is the closest you'll get to a guarantee that the event loop gets serviced each time the worker's GIL time slot expires. And your CPU-bound tasks will run no slower - instead of being serialized by the GIL, they will be serialized by the executor.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-asyncio",
        "fastapi",
        "python-multithreading",
        "gil"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2025-05-17T08:31:12",
      "question_id": 79626334,
      "answer_id": 79627561
    }
  },
  {
    "question": "Persist ParentDocumentRetriever of langchain",
    "expected_answer": "I had the same problem and found the solution here: https://github.com/langchain-ai/langchain/issues/9345 You need to use the create_kv_docstore() function like this: from langchain.storage._lc_store import create_kv_docstore fs = LocalFileStore(&quot;./store_location&quot;) store = create_kv_docstore(fs) parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000) child_splitter = RecursiveCharacterTextSplitter(chunk_size=400) vectorstore = Chroma(collection_name=&quot;split_parents&quot;, embedding_function=embeddings, persist_directory=&quot;./db&quot;) retriever = ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter, ) retriever.add_documents(documents, ids=None) You will end up with 2 folders: the chroma db &quot;db&quot; with the child chunks and the &quot;data&quot; folder with the parents documents. I think there is also a possibility of saving the documents in a Redis db or Azure blobstorage (https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_container) but I am not sure.",
    "context_chunks": [
      {
        "text": "I am using ParentDocumentRetriever of langchain. Using mostly the code from their webpage I managed to create an instance of ParentDocumentRetriever using bge_large embeddings, NLTK text splitter and chromadb. I added documents to it, so that I c embedding_function = HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5', cache_folder=hf_embed_path) # This text splitter is used to create the child documents child_splitter = NLTKTextSplitter(chunk_size=400) # The vectorstore to use to index the child chunks vectorstore = Chroma( collection_name=&quot;full_documents&quot;, embedding_function=embedding_function, persist_directory=&quot;./chroma_db_child&quot; ) # The storage layer for the parent documents store = InMemoryStore() retriever = ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, ) retriever.add_documents(docs, ids=None) I added documents to it, so that I can query using the small chunks to match but to return the full document: matching_docs = retriever.get_relevant_documents(query_text) Chromadb collection 'full_documents' was stored in /chroma_db_child. I can read the collection and query it. I get back the chunks, which is what is expected: vector_db = Chroma( collection_name=&quot;full_documents&quot;, embedding_function=embedding_function, persist_directory=&quot;./chroma_db_child&quot; ) matching_doc = vector_db.max_marginal_relevance_search('whatever', 3) len(matching_doc) &gt;&gt;3 One thing I can't figure out is how to persist the whole structure. This code uses store = InMemoryStore(), which means that once I stopped execution, it goes away. Is there a way, perhaps using something else instead of InMemoryStore(), to create ParentDocumentRetriever and persist both full documents and the chunks, so that I can restore them later without having to go through retriever.add_documents(docs, ids=None) step?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I had the same problem and found the solution here: https://github.com/langchain-ai/langchain/issues/9345 You need to use the create_kv_docstore() function like this: from langchain.storage._lc_store import create_kv_docstore fs = LocalFileStore(&quot;./store_location&quot;) store = create_kv_docstore(fs) parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000) child_splitter = RecursiveCharacterTextSplitter(chunk_size=400) vectorstore = Chroma(collection_name=&quot;split_parents&quot;, embedding_function=embeddings, persist_directory=&quot;./db&quot;) retriever = ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter, ) retriever.add_documents(documents, ids=None) You will end up with 2 folders: the chroma db &quot;db&quot; with the child chunks and the &quot;data&quot; folder with the parents documents. I think there is also a possibility of saving the documents in a Redis db or Azure blobstorage (https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_container) but I am not sure.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The guide in LangChain - Parent-Document Retriever Deepdive with Custom PgVector Store (https://www.youtube.com/watch?v=wxRQe3hhFwU) describes a custom class based on BaseStorage that may also solve the problem with persistent docstore using pgVector instead of file storage Alternatively, simply specify: byte_store=store in your ParentDocumentRetriever instead of docstore=store Then you will be able to use store = LocalFileStore(&quot;path_to_cache&quot;) directly without extra imports",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "py-langchain",
        "chromadb",
        "content-based-retrieval"
      ],
      "question_score": 7,
      "answer_score": 6,
      "created": "2023-10-29T23:47:18",
      "question_id": 77385587,
      "answer_id": 77397998
    }
  },
  {
    "question": "Apache Arrow with Apache Spark - UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer not available",
    "expected_answer": "Same issue with: Apache Spark version 3.5.1 Java JDK 21 Downgrading java to test minimum supported version. Update: Java JDK 17 resolves this issue, please see supported version of Spark: https://spark.apache.org/docs/latest/ (see Spark runs on Java)",
    "context_chunks": [
      {
        "text": "I am trying to integrate Apache Arrow with Apache Spark in a PySpark application, but I am encountering an issue related to sun.misc.Unsafe or java.nio.DirectByteBuffer during the execution. import os import pandas as pd from pyspark.sql import SparkSession extra_java_options = os.getenv(&quot;SPARK_EXECUTOR_EXTRA_JAVA_OPTIONS&quot;, &quot;&quot;) spark = SparkSession.builder \\ .appName(&quot;ArrowPySparkExample&quot;) \\ .getOrCreate() spark.conf.set(&quot;Dio.netty.tryReflectionSetAccessible&quot;, &quot;true&quot;) spark.conf.set(&quot;spark.sql.execution.arrow.pyspark.enabled&quot;, &quot;true&quot;) pdf = pd.DataFrame([&quot;midhun&quot;]) df = spark.createDataFrame(pdf) result_pdf = df.select(&quot;*&quot;).toPandas() Error Message: in stage 0.0 (TID 11) (192.168.140.22 executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available at org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174) at org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229) Environment: Apache Spark version: 3.4 Apache Arrow version: 1.5 Java version: jdk 21",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Same issue with: Apache Spark version 3.5.1 Java JDK 21 Downgrading java to test minimum supported version. Update: Java JDK 17 resolves this issue, please see supported version of Spark: https://spark.apache.org/docs/latest/ (see Spark runs on Java)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I report my experience as I was using Jetbrains Datagrip and I started getting the following error message after upgrading to the new version 2024.2.1 and connected to Databricks SQL: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available Downgrading to 2024.1.4 from the Jetbrains Toolbox solved the issue for me",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "apache-spark",
        "pyspark",
        "pyarrow"
      ],
      "question_score": 7,
      "answer_score": 3,
      "created": "2024-02-29T08:25:19",
      "question_id": 78079857,
      "answer_id": 78475331
    }
  },
  {
    "question": "Parallelizing numpy.sort",
    "expected_answer": "This answer show why a pure-Python, Numba or Cython implementation certainly cannot be used to write a (reasonably-simple) efficient implementation (this summaries the comments). It provides a C++ version which can be called from CPython. The provided version is fast independently of the Numpy version used (so Numpy 2.0 is not required). Why it is certainly not possible directly with Numba/Cython/pure-Python I do no think it is possible to call sort of Numpy in parallel with Cython/Numba because of the GIL and many additional issues. Regarding Numba, parallel loops need the GIL to be release and no object can be manipulated inside it. The Numba sorting function does not actually call Numpy functions, but its own implementation which does not use the GIL nor create any Python object (which require the GIL to be enabled). The Numba sequential implementation is inefficient anyway. While one can try to re-implement a parallel sort from scratch, the parallel features are too limited for the resulting implementation to be really fast or reasonable simple (or both). Indeed, it is limited to a simple parallel for loop called prange (no atomics, critical sections, barriers, TLS storage, etc.). Regarding Cython, prange of Cython requires the GIL to be disabled so creating Python object is not possible in the parallel loop preventing np.sort to be called... Cython provides more parallel features than Numba so re-implementing a parallel sort from scratch seems possible at first glance. However, in practice, it is really complicated (if even possible) to write a fast implementation because of many issues and opened/unknown bugs. Here are the issues I found out while trying to write such a code: OpenMP barriers are not yet available and there is no sane (portable) replacement; critical sections are also not yet available so one need to use manual locks (instead of just #pragma omp critical; arrays must be allocated and freed manually using malloc and free in parallel sections (bug prone and resulting in a more complex code); It is not possible to create views in parallel sections (only outside); Cython does not seems to support well Numpy 2.0 yet causing many compilation errors and also runtime ones (see this post which seems related to this); the documentation of OpenMP functions is rather limited (parts are simply missing); variables of a prange-based loop cannot be reused in a range-based loop outside the prange-loop I also tried to use a ThreadPoolExecutor so to call some optimized Cython/Numba functions and circumvent the aforementioned limitations of the two but it resulted in a very slow implementation (slower than just calling np.sort) mainly because of the GIL (nearly no speed up) and Numpy overhead (mainly temporary arrays and more precisely page-faults). Efficient parallel C++ solution We can write an efficient parallel C++ code performing the following steps: split the input array in N slices perform a bucket sort on each part in parallel so we get M buckets for each slice merge the resulting buckets so to get M buckets from the M x N buckets sort the M buckets in parallel using a SIMD-optimized sort -- this can be done with the x86simdsort C++ library (used internally by Numpy) though it only works on x86-64 CPUs merge the M buckets so to get the final array We need to write a BucketList data structure so to add numbers in a variable-size container. This is basically a linked list of chunks. Note a growing std::vector is not efficient because each resize put too much pressure on memory (and std::deque operations are so slow that is is even slower). Here is the resulting C++ code: // File: wrapper.cpp // Assume x86-simd-sort has been cloned in the same directory and built #include &quot;x86-simd-sort/lib/x86simdsort.h&quot; #include &lt;cstdlib&gt; #include &lt;cstring&gt; #include &lt;forward_list&gt; #include &lt;mutex&gt; #include &lt;omp.h&gt; template &lt;typename T, size_t bucketMaxSize&gt; struct BucketList { using Bucket = std::array&lt;T, bucketMaxSize&gt;; std::forward_list&lt;Bucket&gt; buckets; uint32_t bucketCount; uint32_t lastBucketSize; BucketList() : bucketCount(1), lastBucketSize(0) { buckets.emplace_front(); } void push_back(const T&amp; value) { if (lastBucketSize == bucketMaxSize) { buckets.emplace_front(); lastBucketSize = 0; bucketCount++; } Bucket* lastBucket = &amp;*buckets.begin(); (*lastBucket)[lastBucketSize] = value; lastBucketSize++; } size_t size() const { return (size_t(bucketCount) - 1lu) * bucketMaxSize + lastBucketSize; } size_t bucketSize(size_t idx) const { return idx == 0 ? lastBucketSize : bucketMaxSize; } }; extern &quot;C&quot; void parallel_sort(int64_t* arr, size_t size) { static const size_t bucketSize = 2048; static const size_t radixBits = 11; static const size_t bucketCount = 1 &lt;&lt; radixBits; struct alignas(64) Slice { int64_t* data = nullptr; size_t size = 0; size_t global_offset = 0; size_t local_offset = 0; std::mutex mutex; }; std::array&lt;Slice, bucketCount&gt; slices; #pragma omp parallel { std::array&lt;BucketList&lt;int64_t, bucketSize&gt;, bucketCount&gt; tlsBuckets; #pragma omp for nowait for (size_t i = 0; i &lt; size; ++i) { constexpr uint64_t signBit = uint64_t(1) &lt;&lt; uint64_t(63); const uint64_t idx = (uint64_t(arr[i]) ^ signBit) &gt;&gt; (64 - radixBits); tlsBuckets[idx].push_back(arr[i]); } #pragma omp critical for (size_t i = 0; i &lt; bucketCount; ++i) slices[i].size += tlsBuckets[i].size(); #pragma omp barrier #pragma omp single { size_t offset = 0; for (size_t i = 0; i &lt; bucketCount; ++i) { Slice&amp; slice = slices[i]; slice.data = &amp;arr[offset]; slice.global_offset = offset; offset += slice.size; } } for (size_t i = 0; i &lt; bucketCount; ++i) { Slice&amp; slice = slices[i]; size_t local_offset; size_t local_offset_end; { std::scoped_lock lock(slice.mutex); local_offset = slice.local_offset; slice.local_offset += tlsBuckets[i].size(); local_offset_end = slice.local_offset; } uint32_t bucketListId = 0; for(const auto&amp; kv : tlsBuckets[i].buckets) { const size_t actualBucketSize = tlsBuckets[i].bucketSize(bucketListId); memcpy(&amp;slice.data[local_offset], &amp;kv[0], sizeof(int64_t) * actualBucketSize); local_offset += actualBucketSize; bucketListId++; } } #pragma omp barrier #pragma omp for schedule(dynamic) for (size_t i = 0; i &lt; bucketCount; ++i) x86simdsort::qsort(&amp;slices[i].data[0], slices[i].size); } } A simple header can be written if you want to call this implementation from Cython (though it can be complicated due to the aforementioned Cython/Numpy-2.0 compatibility issue). Here is an example: // File: wrapper.h #include &lt;stdlib.h&gt; #include &lt;stdint.h&gt; void parallel_sort(int64_t* arr, size_t size) You can compile the code with Clang using the following command lines on Linux: clang++ -O3 -fopenmp -c wrapper.cpp -fPIC -g clang wrapper.o -o wrapper.so -fopenmp --shared -Lx86-simd-sort/build -lx86simdsortcpp The following one may also be needed to find the x86-simd-sort library at runtime once cloned and built: export LD_LIBRARY_PATH=x86-simd-sort/build:$LD_LIBRARY_PATH You can finally use the fast sorting function from a Python code. I personally use ctypes because it worked directly with no issues (except when the code is compiled with GCC for unknown strange reasons). Here is an example: import numpy as np import ctypes lib = ctypes.CDLL('./wrapper.so') parallel_sort = lib.parallel_sort parallel_sort.argtypes = [ctypes.c_voidp, ctypes.c_size_t] parallel_sort.restype = None fullCheck = False print('Generating...') a = np.random.randint(0, (1&lt;&lt;63) - 1, 1024*1024**2) if fullCheck: b = a.copy() print('Benchmark...') #%time a.sort() %time parallel_sort(a.ctypes.data, a.size) print('Full check...' if fullCheck else 'Check...') if fullCheck: b.sort() assert np.array_equal(b, a) else: assert np.all(np.diff(a) &gt;= 0) Notes and performance results Note this require a lot of memory to do the test, especially if fullCheck is set to true. Note that the C++ code is optimized for sorting huge arrays (with &gt;1e8 items). The memory consumption will be significant for smaller arrays compared to their size. The current code will even be slow for small arrays (&lt;1e5). You can tune constants/parameters regarding your needs. For tiny arrays, you can directly call the x86-simd-sort library. Once tuned properly, it should be faster than np.sort for all arrays (whatever their size). I strongly advise you to tune the parameters regarding your specific input and target CPU, especially radixBits. The current code/parameters are optimized for mainstream Intel CPUs (not recent big-little Intel ones nor AMD ones) and positive numbers. If you know there are only positive numbers in the input, you can skip the most-significantly bit (sign bit). Here is the resulting timings on my 6-core i5-9600KF CPU (with Numpy 2.0): np.sort: 19.3 s Proposed C++ code: 4.3 s The C++ parallel implementation is 4.5 times faster than the sequential optimized Numpy one. Note I did not massively test the code but basic checks like the one proposed in the provided Python script reported no error so far (even on negative numbers apparently). Note that this sort is efficient if the highest bits of the sorted numbers are different set (this is the downside of bucket/radix sorts). Ideally, numbers should be uniformly distributed and use all the highest bits. If this is not the case, then the buckets will be unbalanced resulting in a lower scalability. In the worst case, only 1 bucket is used resulting in a serial implementation. You can track the highest bit set so to mitigate this issue. More complex approaches are required when there are some rare big outliers (eg. remapping preserving the ordering).",
    "context_chunks": [
      {
        "text": "I need to sort uint64 arrays of length 1e8-1e9, which is one of the performance bottlenecks in my current project. I have just recently updated numpy v2.0 version, in which the sorting algorithm is significantly optimized. Testing it on my hardware, its about 5x faster than numpy v1.26 version. But currently numpy's sorting algorithm cannot utilize multi-core CPUs even though it uses SIMD. I tried to parallelize it and sort multiple np.array at the same time. One possible approach is to use numba prange, but numba has always had poor support for numpy sorting. numba.jit even has a slowdown effect on np.sort, and numba v0.60.0 fails to follow up on numpy v2.0's optimizations for sorting (https://github.com/numba/numba/issues/9611). The alternative is cython prange, but cython does not allow the creation of Python objects at nogil. Is there a way to sort numpy.array in parallel using cython or otherwise? If using cpp's parallel sorting libraries, are they faster than numpy's own sorting, taking into account the overhead of data type conversions? arr=np.random.randint(0,2**64,int(3e8),dtype='uint64') sorted_arr=np.sort(arr) # single thread np.sort takes 4 seconds (numpy v2.0.0)",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This answer show why a pure-Python, Numba or Cython implementation certainly cannot be used to write a (reasonably-simple) efficient implementation (this summaries the comments). It provides a C++ version which can be called from CPython. The provided version is fast independently of the Numpy version used (so Numpy 2.0 is not required). Why it is certainly not possible directly with Numba/Cython/pure-Python I do no think it is possible to call sort of Numpy in parallel with Cython/Numba because of the GIL and many additional issues. Regarding Numba, parallel loops need the GIL to be release and no object can be manipulated inside it. The Numba sorting function does not actually call Numpy functions, but its own implementation which does not use the GIL nor create any Python object (which require the GIL to be enabled). The Numba sequential implementation is inefficient anyway. While one can try to re-implement a parallel sort from scratch, the parallel features are too limited for the resulting implementation to be really fast or reasonable simple (or both). Indeed, it is limited to a simple parallel for loop called prange (no atomics, critical sections, barriers, TLS storage, etc.). Regarding Cython, prange of Cython requires the GIL to be disabled so creating Python object is not possible in the parallel loop preventing np.sort to be called... Cython provides more parallel features than Numba so re-implementing a parallel sort from scratch seems possible at first glance. However, in practice, it is really complicated (if even possible) to write a fast implementation because of many issues and opened/unknown bugs. Here are the issues I found out while trying to write such a code: OpenMP barriers are not yet available and there is no sane (portable) replacement; critical sections are also not yet available so one need to use manual locks (instead of just #pragma omp critical; arrays must be allocated and freed manually using malloc and free in parallel sections (bug prone and resulting in a more complex code); It is not possible to create views in parallel sections (only outside); Cython does not seems to support well Numpy 2.0 yet causing many compilation errors and also runtime ones (see this post which seems related to this); the documentation of OpenMP functions is rather limited (parts are simply missing); variables of a prange-based loop cannot be reused in a range-based loop outside the prange-loop I also tried to use a ThreadPoolExecutor so to call some optimized Cython/Numba functions and circumvent the aforementioned limitations of the two but it resulted in a very slow implementation (slower than just calling np.sort) mainly because of the GIL (nearly no speed up) and Numpy overhead (mainly temporary arrays and more precisely page-faults). Efficient parallel C++ solution We can write an efficient parallel C++ code performing the following steps: split the input array in N slices perform a bucket sort on each part in parallel so we get M buckets for each slice merge the resulting buckets so to get M buckets from the M x N buckets sort the M buckets in parallel using a SIMD-optimized sort -- this can be done with the x86simdsort C++ library (used internally by Numpy) though it only works on x86-64 CPUs merge the M buckets so to get the final array We need to write a BucketList data structure so to add numbers in a variable-size container. This is basically a linked list of chunks. Note a growing std::vector is not efficient because each resize put too much pressure on memory (and std::deque operations are so slow that is is even slower). Here is the resulting C++ code: // File: wrapper.cpp // Assume x86-simd-sort has been cloned in the same directory and built #include &quot;x86-simd-sort/lib/x86simdsort.h&quot; #include &lt;cstdlib&gt; #include &lt;cstring&gt; #include &lt;forward_list&gt; #include &lt;mutex&gt; #include &lt;omp.h&gt; template &lt;typename T, size_t bucketMaxSize&gt; struct BucketList { using Bucket = std::array&lt;T, bucketMaxSize&gt;; std::forward_list&lt;Bucket&gt; buckets; uint32_t bucketCount; uint32_t lastBucketSize; BucketList() : bucketCount(1), lastBucketSize(0) { buckets.emplace_front(); } void push_back(const T&amp; value) { if (lastBucketSize == bucketMaxSize) { buckets.emplace_front(); lastBucketSize = 0; bucketCount++; } Bucket* lastBucket = &amp;*buckets.begin(); (*lastBucket)[lastBucketSize] = value; lastBucketSize++; } size_t size() const { return (size_t(bucketCount) - 1lu) * bucketMaxSize + lastBucketSize; } size_t bucketSize(size_t idx) const { return idx == 0 ? lastBucketSize : bucketMaxSize; } }; extern &quot;C&quot; void parallel_sort(int64_t* arr, size_t size) { static const size_t bucketSize = 2048; static const size_t radixBits = 11; static const size_t bucketCount = 1 &lt;&lt; radixBits; struct alignas(64) Slice { int64_t* data = nullptr; size_t size = 0; size_t global_offset = 0; size_t local_offset = 0; std::mutex mutex; }; std::array&lt;Slice, bucketCount&gt; slices; #pragma omp parallel { std::array&lt;BucketList&lt;int64_t, bucketSize&gt;, bucketCount&gt; tlsBuckets; #pragma omp for nowait for (size_t i = 0; i &lt; size; ++i) { constexpr uint64_t signBit = uint64_t(1) &lt;&lt; uint64_t(63); const uint64_t idx = (uint64_t(arr[i]) ^ signBit) &gt;&gt; (64 - radixBits); tlsBuckets[idx].push_back(arr[i]); } #pragma omp critical for (size_t i = 0; i &lt; bucketCount; ++i) slices[i].size += tlsBuckets[i].size(); #pragma omp barrier #pragma omp single { size_t offset = 0; for (size_t i = 0; i &lt; bucketCount; ++i) { Slice&amp; slice = slices[i]; slice.data = &amp;arr[offset]; slice.global_offset = offset; offset += slice.size; } } for (size_t i = 0; i &lt; bucketCount; ++i) { Slice&amp; slice = slices[i]; size_t local_offset; size_t local_offset_end; { std::scoped_lock lock(slice.mutex); local_offset = slice.local_offset; slice.local_offset += tlsBuckets[i].size(); local_offset_end = slice.local_offset; } uint32_t bucketListId = 0; for(const auto&amp; kv : tlsBuckets[i].buckets) { const size_t actualBucketSize = tlsBuckets[i].bucketSize(bucketListId); memcpy(&amp;slice.data[local_offset], &amp;kv[0], sizeof(int64_t) * actualBucketSize); local_offset += actualBucketSize; bucketListId++; } } #pragma omp barrier #pragma omp for schedule(dynamic) for (size_t i = 0; i &lt; bucketCount; ++i) x86simdsort::qsort(&amp;slices[i].data[0], slices[i].size); } } A simple header can be written if you want to call this implementation from Cython (though it can be complicated due to the aforementioned Cython/Numpy-2.0 compatibility issue). Here is an example: // File: wrapper.h #include &lt;stdlib.h&gt; #include &lt;stdint.h&gt; void parallel_sort(int64_t* arr, size_t size) You can compile the code with Clang using the following command lines on Linux: clang++ -O3 -fopenmp -c wrapper.cpp -fPIC -g clang wrapper.o -o wrapper.so -fopenmp --shared -Lx86-simd-sort/build -lx86simdsortcpp The following one may also be needed to find the x86-simd-sort library at runtime once cloned and built: export LD_LIBRARY_PATH=x86-simd-sort/build:$LD_LIBRARY_PATH You can finally use the fast sorting function from a Python code. I personally use ctypes because it worked directly with no issues (except when the code is compiled with GCC for unknown strange reasons). Here is an example: import numpy as np import ctypes lib = ctypes.CDLL('./wrapper.so') parallel_sort = lib.parallel_sort parallel_sort.argtypes = [ctypes.c_voidp, ctypes.c_size_t] parallel_sort.restype = None fullCheck = False print('Generating...') a = np.random.randint(0, (1&lt;&lt;63) - 1, 1024*1024**2) if fullCheck: b = a.copy() print('Benchmark...') #%time a.sort() %time parallel_sort(a.ctypes.data, a.size) print('Full check...' if fullCheck else 'Check...') if fullCheck: b.sort() assert np.array_equal(b, a) else: assert np.all(np.diff(a) &gt;= 0) Notes and performance results Note this require a lot of memory to do the test, especially if fullCheck is set to true. Note that the C++ code is optimized for sorting huge arrays (with &gt;1e8 items). The memory consumption will be significant for smaller arrays compared to their size. The current code will even be slow for small arrays (&lt;1e5). You can tune constants/parameters regarding your needs. For tiny arrays, you can directly call the x86-simd-sort library. Once tuned properly, it should be faster than np.sort for all arrays (whatever their size). I strongly advise you to tune the parameters regarding your specific input and target CPU, especially radixBits. The current code/parameters are optimized for mainstream Intel CPUs (not recent big-little Intel ones nor AMD ones) and positive numbers. If you know there are only positive numbers in the input, you can skip the most-significantly bit (sign bit). Here is the resulting timings on my 6-core i5-9600KF CPU (with Numpy 2.0): np.sort: 19.3 s Proposed C++ code: 4.3 s The C++ parallel implementation is 4.5 times faster than the sequential optimized Numpy one. Note I did not massively test the code but basic checks like the one proposed in the provided Python script reported no error so far (even on negative numbers apparently). Note that this sort is efficient if the highest bits of the sorted numbers are different set (this is the downside of bucket/radix sorts). Ideally, numbers should be uniformly distributed and use all the highest bits. If this is not the case, then the buckets will be unbalanced resulting in a lower scalability. In the worst case, only 1 bucket is used resulting in a serial implementation. You can track the highest bit set so to mitigate this issue. More complex approaches are required when there are some rare big outliers (eg. remapping preserving the ordering).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can use the algorithms from the C++ standard library [ Microsoft standard library, thx @DavidW hahaha ] in Cython: import cython cimport cython ctypedef fused real: # overloading, very beautiful how Cython does that cython.short cython.ushort cython.int cython.uint cython.long cython.ulong cython.longlong cython.ulonglong cython.float cython.double cdef extern from &quot;&lt;ppl.h&gt;&quot; namespace &quot;concurrency&quot;: cdef void parallel_sort[T](T first, T last) nogil # import has to be nogil @cython.boundscheck(False) # much faster, but be careful! Don't go beyond the last index! @cython.wraparound(False) # much faster, but negative index == UB def parallelsort(real[:] a): parallel_sort(&amp;a[0], &amp;a[a.shape[0]]) There are a lot more, like parallel_radixsort (40 times faster than NumPy, but no floats), buffered_sort and so on. Microsoft has some comprehensible tutorials online: https://learn.microsoft.com/en-us/cpp/parallel/concrt/parallel-algorithms?view=msvc-170 They all can be easily used in Cython. By the way, Cython already supports OpenMp with native locks - the real deal, not that crappy &quot;with gil&quot; thing: However, it is not documented, I found it coincidentally. Here is an example: Parallelising a &#39;for&#39; loop in Cython: beyond prange &quot;with (no)gil:&quot; is only worth it when you know that you have only a few results. Calling the gil is very, very expensive. So expensive that it sometimes is much slower than a single CPU!! Native OpenMp is much better, you can create your own algorithm. It is not hard at all. Another option is using some old school C - qsort in Cython. Also very fast, but not parallel: https://stackoverflow.com/a/78767311/15096247",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "sorting",
        "parallel-processing",
        "cython"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2024-06-21T17:19:03",
      "question_id": 78653708,
      "answer_id": 78659422
    }
  },
  {
    "question": "New column, sampled from list, based on column value",
    "expected_answer": "Code use numpy indexing import numpy as np df['colors'] = np.array(colors)[df['values'] - 1] df values color 0 1 r 1 2 g 2 3 b 3 2 g 4 3 b 5 1 r If you want to solve this problem using only Pandas, use map function. (with @Onyambu comment) m = dict(enumerate(colors, 1)) df['colors'] = df['values'].map(m)",
    "context_chunks": [
      {
        "text": "values = [1,2,3,2,3,1] colors = ['r','g','b'] expected_output = ['r', 'g', 'b', 'g', 'b', 'r'] # how to create this in pandas? df = pd.DataFrame({'values': values}) df['colors'] = expected_output I want to make a new column in my dataframe where the colors are selected based on values in an existing column. I remember doing this in xarray with a vectorised indexing trick, but I can't remember if the same thing is possible in pandas. It feels like it should be a basic indexing task. The current answers are a nice start, thanks! They take a bit too much advantage of the numerical nature of &quot;values&quot; though. I'd rather something generic that would also work if say values = ['a', 'b', 'c', 'b', 'c', 'a'] I guess the &quot;map&quot; method probably still works.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Code use numpy indexing import numpy as np df['colors'] = np.array(colors)[df['values'] - 1] df values color 0 1 r 1 2 g 2 3 b 3 2 g 4 3 b 5 1 r If you want to solve this problem using only Pandas, use map function. (with @Onyambu comment) m = dict(enumerate(colors, 1)) df['colors'] = df['values'].map(m)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "You can use pd.cut: df[&quot;colors&quot;] = pd.cut([1, 2, 3, 2, 3, 1], 3, labels=[&quot;r&quot;, &quot;g&quot;, &quot;b&quot;]) Result: values colors 0 1 r 1 2 g 2 3 b 3 2 g 4 3 b 5 1 r",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2024-03-05T00:13:24",
      "question_id": 78104696,
      "answer_id": 78104715
    }
  },
  {
    "question": "_MissingDynamic: `license` defined outside of `pyproject.toml` is ignored",
    "expected_answer": "This was caused by the release of setuptools 69.0.0 on Nov 20, 2023, which has a list of deprecations and removals. The setup files (e.g. pyproject.toml) of sexpdata 1.0.1 or segno 1.5.3 must have some things that do not comply with the latest rules. A simple fix is to change requires = [&quot;setuptools&gt;=61.0&quot;] to requires = [&quot;setuptools==68.2.2&quot;] in the pyproject.toml file of sexpdata 1.0.1, then the package should be able to be installed again. A permanent fix should update the setup files to make them comply with the rules.",
    "context_chunks": [
      {
        "text": "For the last 24 hours or so I'm seeing the below error messages when I try to install either of sexpdata 1.0.1 or segno 1.5.3 in python. I should perhaps note that these installs usually happen via github code runners (automated tests), which is what alerted me to the issue. I can reproduce locally. My requirements.txt hasn't changed in many weeks and no substantive changes for even longer. Climbing back up the commit tree doesn't change the error. So I suspect the problem isn't in my code, but I also am not sure where the problem might lie, as sexpdata hasn't changed since June, according to its github repo. Any pointers what might be happening or what to look at? Collecting sexpdata&gt;=0.0.3 (from epc) Using cached sexpdata-1.0.1.tar.gz (8.6 kB) Installing build dependencies ... done Getting requirements to build wheel ... error error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; [71 lines of output] /tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py:75: _MissingDynamic: `license` defined outside of `pyproject.toml` is ignored. !! ******************************************************************************** The following seems to be defined outside of `pyproject.toml`: `license = 'BSD License'` According to the spec (see the link below), however, setuptools CANNOT consider this value unless `license` is listed as `dynamic`. https://packaging.python.org/en/latest/specifications/declaring-project-metadata/ To prevent this problem, you can list `license` under `dynamic` or alternatively remove the `[project]` table from your file and rely entirely on other means of configuration. ******************************************************************************** !! _handle_missing_dynamic(dist, project_table) /tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py:75: _MissingDynamic: `keywords` defined outside of `pyproject.toml` is ignored. !! ******************************************************************************** The following seems to be defined outside of `pyproject.toml`: `keywords = ['s-expression', 'lisp', 'parser']` According to the spec (see the link below), however, setuptools CANNOT consider this value unless `keywords` is listed as `dynamic`. https://packaging.python.org/en/latest/specifications/declaring-project-metadata/ To prevent this problem, you can list `keywords` under `dynamic` or alternatively remove the `[project]` table from your file and rely entirely on other means of configuration. ******************************************************************************** !! _handle_missing_dynamic(dist, project_table) Traceback (most recent call last): File &quot;/var/www/danube/venv/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 353, in &lt;module&gt; main() File &quot;/var/www/danube/venv/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) File &quot;/var/www/danube/venv/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 118, in get_requires_for_build_wheel return hook(config_settings) File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/build_meta.py&quot;, line 325, in get_requires_for_build_wheel return self._get_build_requires(config_settings, requirements=['wheel']) File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/build_meta.py&quot;, line 295, in _get_build_requires self.run_setup() File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/build_meta.py&quot;, line 311, in run_setup exec(code, locals()) File &quot;&lt;string&gt;&quot;, line 8, in &lt;module&gt; File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/__init__.py&quot;, line 103, in setup return distutils.core.setup(**attrs) File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/_distutils/core.py&quot;, line 159, in setup dist.parse_config_files() File &quot;/var/www/danube/venv/lib/python3.10/site-packages/_virtualenv.py&quot;, line 21, in parse_config_files result = old_parse_config_files(self, *args, **kwargs) File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/dist.py&quot;, line 627, in parse_config_files pyprojecttoml.apply_configuration(self, filename, ignore_option_errors) File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/config/pyprojecttoml.py&quot;, line 67, in apply_configuration return _apply(dist, config, filepath) File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py&quot;, line 56, in apply _apply_project_table(dist, config, root_dir) File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py&quot;, line 82, in _apply_project_table corresp(dist, value, root_dir) File &quot;/tmp/pip-build-env-ga37r2a6/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py&quot;, line 183, in _license _set_config(dist, &quot;license&quot;, val[&quot;text&quot;]) KeyError: 'text' [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error × Getting requirements to build wheel did not run successfully. │ exit code: 1 ╰─&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This was caused by the release of setuptools 69.0.0 on Nov 20, 2023, which has a list of deprecations and removals. The setup files (e.g. pyproject.toml) of sexpdata 1.0.1 or segno 1.5.3 must have some things that do not comply with the latest rules. A simple fix is to change requires = [&quot;setuptools&gt;=61.0&quot;] to requires = [&quot;setuptools==68.2.2&quot;] in the pyproject.toml file of sexpdata 1.0.1, then the package should be able to be installed again. A permanent fix should update the setup files to make them comply with the rules.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "pyproject.toml [build-system] requires = [&quot;setuptools == 68.2.2&quot;]",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "pip"
      ],
      "question_score": 7,
      "answer_score": 8,
      "created": "2023-11-21T13:04:53",
      "question_id": 77523055,
      "answer_id": 77527178
    }
  },
  {
    "question": "ValueError: &quot;Requirement&quot; object has no field &quot;use_pep517&quot; when installing Python packages in a Docker Image",
    "expected_answer": "I hit the same issue for several of my Python projects built in Docker. It looks like it's an issue with PyYAML. Here's the other post where I saw the fix: Docker-compose no longer building image (AttributeError: cython_sources) I went with two different workarounds for different projects. In one project, I just upgraded packages that were all working again. In a different project I ended up having to switch back to PyYAML 5.3.1. The projects are internal only so I am not concerned about security much. One thing to note is that trying to install the specific package that is failing (I tried them one by one until I found the culprit) with pip instead of pipenv will produce a more specific error message which led me to the linked answer. Hope this helps!",
    "context_chunks": [
      {
        "text": "I'm having a problem installing Python dependencies using pipenv inside a Dockerfile. Dockerfile: RUN pip install --upgrade pip RUN pip install pipenv # Copy dependencies source code WORKDIR /projects # Copy project source code WORKDIR /projects/source COPY ./projects/source . # Install packages RUN pipenv install --system --deploy We used to build the image successfully with the following pipenv dependencies: # Result of pip install pipenv Successfully installed certifi-2023.5.7 distlib-0.3.6 filelock-3.12.2 pipenv-2023.7.11 platformdirs-3.9.1 setuptools-68.0.0 virtualenv-20.24.0 virtualenv-clone-0.5.7 But now, we're running into this error when we do a pipenv install --system --deploy in our Docker image: # Result when executing pipenv install --system --deploy Traceback (most recent call last): File &quot;/usr/local/bin/pipenv&quot;, line 8, in &lt;module&gt; sys.exit(cli()) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/vendor/click/core.py&quot;, line 1130, in __call__ return self.main(*args, **kwargs) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/cli/options.py&quot;, line 58, in main return super().main(*args, **kwargs, windows_expand_args=False) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/vendor/click/core.py&quot;, line 1055, in main rv = self.invoke(ctx) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/vendor/click/core.py&quot;, line 1657, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/vendor/click/core.py&quot;, line 1404, in invoke return ctx.invoke(self.callback, **ctx.params) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/vendor/click/core.py&quot;, line 760, in invoke return __callback(*args, **kwargs) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/vendor/click/decorators.py&quot;, line 84, in new_func return ctx.invoke(f, obj, *args, **kwargs) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/vendor/click/core.py&quot;, line 760, in invoke return __callback(*args, **kwargs) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/cli/command.py&quot;, line 233, in install do_install( File &quot;/usr/local/lib/python3.10/site-packages/pipenv/routines/install.py&quot;, line 170, in do_install do_init( File &quot;/usr/local/lib/python3.10/site-packages/pipenv/routines/install.py&quot;, line 777, in do_init do_install_dependencies( File &quot;/usr/local/lib/python3.10/site-packages/pipenv/routines/install.py&quot;, line 455, in do_install_dependencies batch_install( File &quot;/usr/local/lib/python3.10/site-packages/pipenv/routines/install.py&quot;, line 596, in batch_install batch_install_iteration( File &quot;/usr/local/lib/python3.10/site-packages/pipenv/routines/install.py&quot;, line 538, in batch_install_iteration _cleanup_procs(project, procs, failed_deps_queue, retry=retry) File &quot;/usr/local/lib/python3.10/site-packages/pipenv/routines/install.py&quot;, line 651, in _cleanup_procs dep.use_pep517 = True File &quot;/usr/local/lib/python3.10/site-packages/pipenv/vendor/requirementslib/models/common.py&quot;, line 18, in __setattr__ raise ValueError(f'&quot;{self.__class__.__name__}&quot; object has no field &quot;{name}&quot;') ValueError: &quot;Requirement&quot; object has no field &quot;use_pep517&quot; I did notice a slight change of the distlib version when Docker tries to install the pipenv, but not sure if this is the problem Successfully installed certifi-2023.5.7 distlib-0.3.7 filelock-3.12.2 pipenv-2023.7.11 platformdirs-3.9.1 setuptools-68.0.0 virtualenv-20.24.0 virtualenv-clone-0.5.7 I tried upgrading my local pipenv and rebuilding the Pipfile.lock, but it still gives the same error when I tried building it inside a Docker image.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I hit the same issue for several of my Python projects built in Docker. It looks like it's an issue with PyYAML. Here's the other post where I saw the fix: Docker-compose no longer building image (AttributeError: cython_sources) I went with two different workarounds for different projects. In one project, I just upgraded packages that were all working again. In a different project I ended up having to switch back to PyYAML 5.3.1. The projects are internal only so I am not concerned about security much. One thing to note is that trying to install the specific package that is failing (I tried them one by one until I found the culprit) with pip instead of pipenv will produce a more specific error message which led me to the linked answer. Hope this helps!",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I hit the same issue in previously fine CI pipeline. For me, regenerating the lock file fixed it. You can regenerate the lock file using: pipenv lock",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "docker",
        "pip",
        "pipenv"
      ],
      "question_score": 7,
      "answer_score": 3,
      "created": "2023-07-19T02:10:16",
      "question_id": 76717537,
      "answer_id": 76724894
    }
  },
  {
    "question": "Documenting a script step by step with Sphinx",
    "expected_answer": "Take a look at the sphinx-gallery extension, which seems to do what you require. With this extension, if you have a Python script, you must start it with a header docstring, and then you can add comments that will be formatted as text rather than code using the # %% syntax, e.g., &quot;&quot;&quot; My example script. &quot;&quot;&quot; import numpy as np # %% # This will be a text block x = np.linspace(0, 10, 100) y = np.sin(2 * np.pi * x) # %% # Another block of text More details of the syntax is described here, and various examples are, e.g., here. Alternative option If the sphinx-gallery option is not appropriate (i.e., you don't really want a thumbnail-style gallery page linking to the examples), you could instead make use of the nbsphinx extension and the jupytext package. You can write your example Python scripts in jupytext's percent format, and then generate the pages via an intermediate conversion to a Jupyter notebook. For example (after installing both nbsphinx and jupytext), if you had a package structure like: . ├── docs │ ├── Makefile │ ├── conf.py │ ├── examples -&gt; ../src/examples/ │ ├── index.rst │ └── make.bat └── src └── examples └── narrative.py where in this case I've symbolic linked the src/examples directory into the docs directory, you could edit your Sphinx conf.py file to contain: # add nbsphinx to extensions extensions = [ ... &quot;nbsphinx&quot;, ] # this converts .py files with the percent format to notebooks nbsphinx_custom_formats = { '.py': ['jupytext.reads', {'fmt': 'py:percent'}], } nbsphinx_output_prompt = &quot;&quot; nbsphinx_execute = &quot;auto&quot; templates_path = ['_templates'] # add conf.py to exclude_patterns exclude_patterns = [..., 'conf.py'] and have narrative.py looking like: # %% [markdown] # # A title # %% [raw] raw_mimetype=&quot;text/restructuredtext&quot; # Import necessary package and define :meth:`make_grid` # %% import numpy as np def make_grid(a,b): &quot;&quot;&quot; Make a grid for constant by piece functions &quot;&quot;&quot; x = np.linspace(0,np.pi) xmid = (x[:-1]+x[1:])/2 h = x[1:]-x[:-1] return xmid,h # %% [markdown] # Interpolate a function # %% xmid,h = make_grid(0,np.pi) y = np.sin(xmid) # %% [markdown] # Calculate its integral # %% I = np.sum(y*h) print (&quot;Result %g&quot; % I ) then running make html should produce a narrative.html file like: which you can link to from index.rst etc. Some things to note about the narrative.py file: the start of the .py file has to contain a &quot;Title&quot; cell, which in this case, as I've set it as a Markdown cell, contains (after the initial comment string #) # A Title using the Markdown header syntax of #. If you don't have a title you won't be able to link to the output from other documents, e.g., index.rst; for most of the text cells, I have marked them as [markdown] format, i.e., they will be interpreted as containing Markdown syntax; for the cell containing restructured text, I have marked it as a [raw] cell with the meta data raw_mimetype=&quot;text/restructuredtext&quot;; the input code cells will display with an input prompt by default [1]: etc. Turning off the input prompts requires using Custom CSS.",
    "context_chunks": [
      {
        "text": "I am documenting a python library with Sphinx. I have a couple of example scripts which I'd like to document in a narrative way, something like this: #: Import necessary package and define :meth:`make_grid` import numpy as np def make_grid(a,b): &quot;&quot;&quot; Make a grid for constant by piece functions &quot;&quot;&quot; x = np.linspace(0,np.pi) xmid = (x[:-1]+x[1:])/2 h = x[1:]-x[:-1] return xmid,h #: Interpolate a function xmid,h = make_grid(0,np.pi) y = np.sin(xmid) #: Calculate its integral I = np.sum(y*h) print (&quot;Result %g&quot; % I ) Those scripts should remain present as executable scripts in the repository, and I want to avoid duplicating their code into comments. I would like to generate the corresponding documentation, something like : Is there any automated way to do so? This would allow me not to duplicate the example script in the documentation. It seems to me this was the object of this old question but in my hands viewcode extension doesn't interpret comments, it just produces an html page with quoted code, comments remain comments.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Take a look at the sphinx-gallery extension, which seems to do what you require. With this extension, if you have a Python script, you must start it with a header docstring, and then you can add comments that will be formatted as text rather than code using the # %% syntax, e.g., &quot;&quot;&quot; My example script. &quot;&quot;&quot; import numpy as np # %% # This will be a text block x = np.linspace(0, 10, 100) y = np.sin(2 * np.pi * x) # %% # Another block of text More details of the syntax is described here, and various examples are, e.g., here. Alternative option If the sphinx-gallery option is not appropriate (i.e., you don't really want a thumbnail-style gallery page linking to the examples), you could instead make use of the nbsphinx extension and the jupytext package. You can write your example Python scripts in jupytext's percent format, and then generate the pages via an intermediate conversion to a Jupyter notebook. For example (after installing both nbsphinx and jupytext), if you had a package structure like: . ├── docs │ ├── Makefile │ ├── conf.py │ ├── examples -&gt; ../src/examples/ │ ├── index.rst │ └── make.bat └── src └── examples └── narrative.py where in this case I've symbolic linked the src/examples directory into the docs directory, you could edit your Sphinx conf.py file to contain: # add nbsphinx to extensions extensions = [ ... &quot;nbsphinx&quot;, ] # this converts .py files with the percent format to notebooks nbsphinx_custom_formats = { '.py': ['jupytext.reads', {'fmt': 'py:percent'}], } nbsphinx_output_prompt = &quot;&quot; nbsphinx_execute = &quot;auto&quot; templates_path = ['_templates'] # add conf.py to exclude_patterns exclude_patterns = [..., 'conf.py'] and have narrative.py looking like: # %% [markdown] # # A title # %% [raw] raw_mimetype=&quot;text/restructuredtext&quot; # Import necessary package and define :meth:`make_grid` # %% import numpy as np def make_grid(a,b): &quot;&quot;&quot; Make a grid for constant by piece functions &quot;&quot;&quot; x = np.linspace(0,np.pi) xmid = (x[:-1]+x[1:])/2 h = x[1:]-x[:-1] return xmid,h # %% [markdown] # Interpolate a function # %% xmid,h = make_grid(0,np.pi) y = np.sin(xmid) # %% [markdown] # Calculate its integral # %% I = np.sum(y*h) print (&quot;Result %g&quot; % I ) then running make html should produce a narrative.html file like: which you can link to from index.rst etc. Some things to note about the narrative.py file: the start of the .py file has to contain a &quot;Title&quot; cell, which in this case, as I've set it as a Markdown cell, contains (after the initial comment string #) # A Title using the Markdown header syntax of #. If you don't have a title you won't be able to link to the output from other documents, e.g., index.rst; for most of the text cells, I have marked them as [markdown] format, i.e., they will be interpreted as containing Markdown syntax; for the cell containing restructured text, I have marked it as a [raw] cell with the meta data raw_mimetype=&quot;text/restructuredtext&quot;; the input code cells will display with an input prompt by default [1]: etc. Turning off the input prompts requires using Custom CSS.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Using literalinclude's options (lines, start-at, start-after, ...), you can select a specific part of the file to display. Check the documentation here: https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-option-literalinclude-lines It'd typically be used like this in your rst files: .. literalinclude:: /../../tests/my_example.py :caption: :lines: 1-15 With start-at, end-at, you could match your comments (it is less error-prone than lines). .. literalinclude:: /../../tests/my_example.py :caption: :start-at: make_grid :end-before: Interpolate",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-sphinx"
      ],
      "question_score": 7,
      "answer_score": 3,
      "created": "2025-05-06T17:40:43",
      "question_id": 79609220,
      "answer_id": 79613993
    }
  },
  {
    "question": "Is OpenTelemetry in Python safe to use with Async?",
    "expected_answer": "OpenTelemetry for Python supports asynchronous code. Went through the code for version 1.24.0/0.45b0 of opentelemetry-python. The code contains abstract context class _RuntimeContext. _RuntimeContext has single implementation ContextVarsRuntimeContext that utilizes contextvars. ContextVarsRuntimeContext is used as a default context.",
    "context_chunks": [
      {
        "text": "I want to use OpenTelemetry with an Async application, and I want to be 101% sure that it will work as intended. Specifically, I'm worried about what happens with the current_span when we switch back and forth between asynchronous functions. I have this fear that if I rely on tracer.start_as_current_span to set the span in each function, and then I pass execution to another function which also sets the current_span, then when execution passes back to the first function it won't be tied to the correct span anymore. Now, I have tried testing this a bit, and found no evidence that it breaks. But I also haven't found any documentation that says it explicitly won't break. Can anyone confirm? I've done the following basic test, but I'm worried it misses something: async def async_span(): with tracer.start_as_current_span(name=f&quot;span_{uuid.uuid4}&quot;) as span: for x in range(1000): assert trace.get_current_span() == span await asyncio.sleep(0.0001 * randint(1, 10)) async def main(): with tracer.start_as_current_span(name=&quot;parent&quot;): await asyncio.gather(*(async_span() for _ in range(10)))",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "OpenTelemetry for Python supports asynchronous code. Went through the code for version 1.24.0/0.45b0 of opentelemetry-python. The code contains abstract context class _RuntimeContext. _RuntimeContext has single implementation ContextVarsRuntimeContext that utilizes contextvars. ContextVarsRuntimeContext is used as a default context.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I hope, OpenTelemetry for Python is designed to consider asynchronous code. When you use tracer.start_as_current_span within an asynchronous context, the span becomes the current span for the duration of that context. The key point is that the association of the current span is managed on a per-task basis. Asynchronous tasks in Python have their own context, and the asyncio event loop ensures that the task-local context is preserved when tasks switch back and forth.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-asyncio",
        "open-telemetry"
      ],
      "question_score": 7,
      "answer_score": 4,
      "created": "2023-11-02T14:50:51",
      "question_id": 77410600,
      "answer_id": 78454343
    }
  },
  {
    "question": "Function to sum over repeated indices",
    "expected_answer": "Sorting is expensive, especially argsort internally called in np.unique. We can use a dictionary to remove replicates and perform the accumulation. This is fast if there are many replicates. However, this can be as slow as a sort (if not a bit worst) when all the values are almost different. Indeed, in this case, the target dictionary takes a significant space and accesses are not cache friendly at all (resulting in much slower fetches). In this case, Bloom filters can be used to speed up the computation (by quickly check if the items was never seen so far while iterating on all values). Unfortunately, bloom filters are not trivial to implement and I am not aware of any very fast Python library supporting them, especially in a Numba code, so I am not gonna use them in this question. You can see an example in this post. An alternative solution is to parallelize the Numpy sort (which use a very very fast sequential implementation on x86-64 CPU since Numpy 2.0) so to speed up the np.unique expensive function. This answer also provide a simple C++ version using parallel implementation of std::sort (though the one in the previous link should be even faster). The key point to remember is that the best algorithm is dependent of the following ratio: R = np.unique(idx).size / idx.size. R is close to 0: pick a dict-based algorithm; R is close to 1: pick a sort-based algorithm. Dict-based sequential Numba implementation Here is a dict-based algorithm implementation in Numba: import numba as nb import numpy as np @nb.njit('(uint16[:], uint16[:], uint16[:], float64[:])') def compute(x, y, l, v): n = x.size assert (y.size, l.size, v.size) == (n, n, n) # Find the max of each array dx, dy, dl = np.uint32(0), np.uint32(0), np.uint32(0) for i in range(n): dx = max(dx, np.uint32(x[i])) dy = max(dy, np.uint32(y[i])) dl = max(dl, np.uint32(l[i])) dx = dx + 1 dy = dy + 1 dl = dl + 1 assert np.uint64(dx) * np.uint64(dy) * np.uint64(dl) &lt;= np.uint64(0xFFFFFFFF) # Use a dict (hash-map) to accumulate the weights of unique values weights = {} for i in range(n): xi = np.uint32(x[i]) yi = np.uint32(y[i]) li = np.uint32(l[i]) idx = ((xi * dy) + yi) * dl + li if idx in weights: weights[idx] += v[i] else: weights[idx] = v[i] # Iterate on the unique values and store the resulting arrays m = len(weights) out_x = np.empty(m, dtype=np.uint16) out_y = np.empty(m, dtype=np.uint16) out_l = np.empty(m, dtype=np.uint16) out_w = np.empty(m, dtype=np.float64) i = 0 for idx, w in weights.items(): out_x[i] = (idx // dl) // dy out_y[i] = (idx // dl) % dy out_l[i] = idx % dl out_w[i] = w i += 1 return out_x, out_y, out_l, out_w Please note that most of the time is spend in dictionary accesses on my machine, and unfortunately, Numba dictionary are relatively slow so we should not expect a big speed up. Sort-based parallel C++ implementation A simple alternative way to make np.unique faster is simply to use a parallel sort. An argsort is not required here and it is actually quite overkill: you can just sort the pairs of index-values. Implementing a fast parallel sort manually is pretty challenging though. A simple way to reuse an existing implementation is to write a native C++ code calling std::sort with an std::execution::par_unseq policy. Then, you can easily track the replicates and do the accumulation. Here is the resulting code: #include &lt;cstdlib&gt; #include &lt;cstdio&gt; #include &lt;cassert&gt; #include &lt;cstdint&gt; #include &lt;algorithm&gt; #include &lt;execution&gt; #ifdef _WIN32 #define DLL_EXPORT __declspec(dllexport) #else #define DLL_EXPORT #endif extern &quot;C&quot; { DLL_EXPORT int32_t compute(uint16_t* x, uint16_t* y, uint16_t* l, double* v, uint16_t* out_x, uint16_t* out_y, uint16_t* out_l, double* out_w, int32_t n) { using SortedItem = std::pair&lt;uint64_t, double&gt;; std::vector&lt;SortedItem&gt; items(n); if(n == 0) return 0; for (int32_t i = 0; i &lt; n; ++i) items[i] = std::make_pair((uint64_t(x[i]) &lt;&lt; 32) | (uint64_t(y[i]) &lt;&lt; 16) | uint64_t(l[i]), v[i]); std::sort(std::execution::par_unseq, items.begin(), items.end()); int32_t m = 1; for (int32_t i = 1; i &lt; n; ++i) m += items[i-1].first != items[i].first; int32_t pos = 0; double sum = 0.0; for (int32_t i = 0; i &lt; n; ++i) { const auto curr_idx = items[i].first; const auto next_idx = items[i+1].first; sum += items[i].second; if(i == n-1 || curr_idx != next_idx) { out_x[pos] = uint16_t(curr_idx &gt;&gt; 32); out_y[pos] = uint16_t(curr_idx &gt;&gt; 16); out_l[pos] = uint16_t(curr_idx); out_w[pos] = sum; pos++; sum = 0.0; } } assert(pos == m); return m; } } You can compile this with the following Makefile: all: mkdir -p build clang++ --std=c++17 -O3 -fopenmp -c compute.cpp -o build/compute.obj clang++ -shared build/compute.obj -o build/compute.dll -fPIC -fopenmp This means you need Clang (Clang-CL on Windows) and a basic GNU-like environment (e.g. MinGW or MSys on Windows) to compile the library. You could certainly use a MSVC environment on Windows, but AFAIK the parallel STL requires the Intel TBB library. On Linux, you need to replace the &quot;dll&quot; extension by &quot;so&quot; and prefix the name with &quot;lib&quot;: &quot;libcompute.so&quot;. The generated library can be distributed to users. You can then load it directly at runtime. Here is the final Python code: import ctypes # Load the DLL and define the prototype of the native C++ function dll = ctypes.CDLL('./build/compute.dll') int32_t = ctypes.c_int32 array_u32_t = np.ctypeslib.ndpointer(dtype=np.uint16, ndim=1, flags='C_CONTIGUOUS') array_f64_t = np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS') dll.compute.argtypes = [array_u32_t, array_u32_t, array_u32_t, array_f64_t, array_u32_t, array_u32_t, array_u32_t, array_f64_t, int32_t] dll.compute.restype = int32_t # Wrapping function def compute_native(x, y, l, v): n = x.size assert (y.size, l.size, v.size) == (n, n, n) out_x = np.empty(n, dtype=np.uint16) out_y = np.empty(n, dtype=np.uint16) out_l = np.empty(n, dtype=np.uint16) out_w = np.empty(n, dtype=np.float64) m = dll.compute(x, y, l, v, out_x, out_y, out_l, out_w, n) out_x = out_x[:m].copy() out_y = out_y[:m].copy() out_l = out_l[:m].copy() out_w = out_w[:m].copy() return out_x, out_y, out_l, out_w Benchmark Here are performance results on my i5-9600KF CPU (6 cores), with arrays of size n=1_000_000, uniformly-distributed random numbers and 3 different R values (variable range for np.random.randint): With R ~= 1.00 (poor use-case for the dict-based implementation) - seq MadPhysicist's code: 8437 ms (takes 8 GiB of RAM!) - seq initial code: 212 ms - seq Numba dict-based code: 132 ms &lt;----- 1.6 times faster - seq C++ dict-based code: 121 ms - par C++ sort-based code: 32 ms &lt;----- 6.6 times faster With R ~= 0.26 (better use-case for dicts) - seq initial code: 201 ms - seq Numba dict-based code: 72 ms &lt;----- 2.8 times faster - seq C++ dict-based code: 45 ms - par C++ sort-based code: 31 ms &lt;----- 6.5 times faster - seq MadPhysicist's code: 29 ms With R ~= 0.03 (perfect use-case for dicts) - seq initial code: 192 ms - seq Numba dict-based code: 50 ms &lt;----- 3.8 times faster - par C++ sort-based code: 29 ms &lt;----- 6.6 times faster - seq MadPhysicist's code: 14 ms - seq C++ dict-based code: 12 ms Thus, the proposed sequential dict-based implementation is 1.6~3.8 times faster than the initial code on on large arrays. On smaller arrays, the same effect if visible though the speed-up are smaller. Note that &quot;seq Numba dict-based code&quot; is an equivalent of the provided Numba code rewritten in C++, and using a fast hash-map implementation (Tessil's robin-hood hash-maps). The parallel C++ sort-based implementation is 6.6 times faster than the initial code on large arrays. It is the fastest one overall. This is mainly because it runs on multiple cores as opposed to other implementations. Additional notes Please note that when R is pretty small like &lt;0.1, using multiple threads with thread-local dictionary (then merged) should result in a significantly faster implementation. When R is big, this does not worth it with Numba because it should not scale (due to allocations) and it is pretty complicated to implement. If you want to parallelize the dict-based implementation, then I strongly advise you to use a native language like C++. Doing this efficiently is far from being easy.",
    "context_chunks": [
      {
        "text": "We are looking for ways to improve this small step in a large pipeline we are developing. The problem is: Given: multiple np.ndarray objects of integer datatype that effectively describe pixel indices and a single np.ndarray of floating type that describes a weight. All of these arrays will have the same shape/size. Problem: any time a set of indices is repeated, sum the weights for the repeated tuple (which is now unique by construction). Return: the unique indices and the summed weights. We've developed a fairly robust method, but it's quite slow. Consider: x = np.array([1, 1, 2, 2, 3, 3, 1, 1, 3, 3, 4], dtype=np.uint16) y = np.array([1, 1, 2, 2, 2, 2, 1, 1, 3, 4, 5], dtype=np.uint16) l = np.array([1, 2, 2, 2, 3, 2, 1, 1, 3, 3, 6], dtype=np.uint16) v = np.array([2, 4, 6, 8, 7, 5, 3, 1, 8, 6, 4], dtype=np.float64) indices = (x, y, l) dims = [np.amax(index) + 1 for index in indices] idx = np.ravel_multi_index(indices, dims, order='F') out, uind, cinv = np.unique(idx, return_index=True, return_inverse=True) vv = np.bincount(cinv, weights=v) out = tuple(index[uind] for index in indices) ret = (vv, *out) This works and returns the expected result: print(vv, out) array([ 6., 4., 14., 5., 7., 8., 6., 4.]) array([1, 1, 2, 3, 3, 3, 3, 4], dtype=uint16) array([1, 1, 2, 2, 2, 3, 4, 5], dtype=uint16) array([1, 2, 2, 2, 3, 3, 3, 6], dtype=uint16) This is a MWE with small arrays, but in practice, these arrays will be well in excess of a million elements. And this raises the problem that the call to np.unique with these settings is very slow. We've tried a few things: using CSR_matrix or numba, and for different reasons they weren't much better. What would be a more efficient way of performing these types of calculations?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Sorting is expensive, especially argsort internally called in np.unique. We can use a dictionary to remove replicates and perform the accumulation. This is fast if there are many replicates. However, this can be as slow as a sort (if not a bit worst) when all the values are almost different. Indeed, in this case, the target dictionary takes a significant space and accesses are not cache friendly at all (resulting in much slower fetches). In this case, Bloom filters can be used to speed up the computation (by quickly check if the items was never seen so far while iterating on all values). Unfortunately, bloom filters are not trivial to implement and I am not aware of any very fast Python library supporting them, especially in a Numba code, so I am not gonna use them in this question. You can see an example in this post. An alternative solution is to parallelize the Numpy sort (which use a very very fast sequential implementation on x86-64 CPU since Numpy 2.0) so to speed up the np.unique expensive function. This answer also provide a simple C++ version using parallel implementation of std::sort (though the one in the previous link should be even faster). The key point to remember is that the best algorithm is dependent of the following ratio: R = np.unique(idx).size / idx.size. R is close to 0: pick a dict-based algorithm; R is close to 1: pick a sort-based algorithm. Dict-based sequential Numba implementation Here is a dict-based algorithm implementation in Numba: import numba as nb import numpy as np @nb.njit('(uint16[:], uint16[:], uint16[:], float64[:])') def compute(x, y, l, v): n = x.size assert (y.size, l.size, v.size) == (n, n, n) # Find the max of each array dx, dy, dl = np.uint32(0), np.uint32(0), np.uint32(0) for i in range(n): dx = max(dx, np.uint32(x[i])) dy = max(dy, np.uint32(y[i])) dl = max(dl, np.uint32(l[i])) dx = dx + 1 dy = dy + 1 dl = dl + 1 assert np.uint64(dx) * np.uint64(dy) * np.uint64(dl) &lt;= np.uint64(0xFFFFFFFF) # Use a dict (hash-map) to accumulate the weights of unique values weights = {} for i in range(n): xi = np.uint32(x[i]) yi = np.uint32(y[i]) li = np.uint32(l[i]) idx = ((xi * dy) + yi) * dl + li if idx in weights: weights[idx] += v[i] else: weights[idx] = v[i] # Iterate on the unique values and store the resulting arrays m = len(weights) out_x = np.empty(m, dtype=np.uint16) out_y = np.empty(m, dtype=np.uint16) out_l = np.empty(m, dtype=np.uint16) out_w = np.empty(m, dtype=np.float64) i = 0 for idx, w in weights.items(): out_x[i] = (idx // dl) // dy out_y[i] = (idx // dl) % dy out_l[i] = idx % dl out_w[i] = w i += 1 return out_x, out_y, out_l, out_w Please note that most of the time is spend in dictionary accesses on my machine, and unfortunately, Numba dictionary are relatively slow so we should not expect a big speed up. Sort-based parallel C++ implementation A simple alternative way to make np.unique faster is simply to use a parallel sort. An argsort is not required here and it is actually quite overkill: you can just sort the pairs of index-values. Implementing a fast parallel sort manually is pretty challenging though. A simple way to reuse an existing implementation is to write a native C++ code calling std::sort with an std::execution::par_unseq policy. Then, you can easily track the replicates and do the accumulation. Here is the resulting code: #include &lt;cstdlib&gt; #include &lt;cstdio&gt; #include &lt;cassert&gt; #include &lt;cstdint&gt; #include &lt;algorithm&gt; #include &lt;execution&gt; #ifdef _WIN32 #define DLL_EXPORT __declspec(dllexport) #else #define DLL_EXPORT #endif extern &quot;C&quot; { DLL_EXPORT int32_t compute(uint16_t* x, uint16_t* y, uint16_t* l, double* v, uint16_t* out_x, uint16_t* out_y, uint16_t* out_l, double* out_w, int32_t n) { using SortedItem = std::pair&lt;uint64_t, double&gt;; std::vector&lt;SortedItem&gt; items(n); if(n == 0) return 0; for (int32_t i = 0; i &lt; n; ++i) items[i] = std::make_pair((uint64_t(x[i]) &lt;&lt; 32) | (uint64_t(y[i]) &lt;&lt; 16) | uint64_t(l[i]), v[i]); std::sort(std::execution::par_unseq, items.begin(), items.end()); int32_t m = 1; for (int32_t i = 1; i &lt; n; ++i) m += items[i-1].first != items[i].first; int32_t pos = 0; double sum = 0.0; for (int32_t i = 0; i &lt; n; ++i) { const auto curr_idx = items[i].first; const auto next_idx = items[i+1].first; sum += items[i].second; if(i == n-1 || curr_idx != next_idx) { out_x[pos] = uint16_t(curr_idx &gt;&gt; 32); out_y[pos] = uint16_t(curr_idx &gt;&gt; 16); out_l[pos] = uint16_t(curr_idx); out_w[pos] = sum; pos++; sum = 0.0; } } assert(pos == m); return m; } } You can compile this with the following Makefile: all: mkdir -p build clang++ --std=c++17 -O3 -fopenmp -c compute.cpp -o build/compute.obj clang++ -shared build/compute.obj -o build/compute.dll -fPIC -fopenmp This means you need Clang (Clang-CL on Windows) and a basic GNU-like environment (e.g. MinGW or MSys on Windows) to compile the library. You could certainly use a MSVC environment on Windows, but AFAIK the parallel STL requires the Intel TBB library. On Linux, you need to replace the &quot;dll&quot; extension by &quot;so&quot; and prefix the name with &quot;lib&quot;: &quot;libcompute.so&quot;. The generated library can be distributed to users. You can then load it directly at runtime. Here is the final Python code: import ctypes # Load the DLL and define the prototype of the native C++ function dll = ctypes.CDLL('./build/compute.dll') int32_t = ctypes.c_int32 array_u32_t = np.ctypeslib.ndpointer(dtype=np.uint16, ndim=1, flags='C_CONTIGUOUS') array_f64_t = np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS') dll.compute.argtypes = [array_u32_t, array_u32_t, array_u32_t, array_f64_t, array_u32_t, array_u32_t, array_u32_t, array_f64_t, int32_t] dll.compute.restype = int32_t # Wrapping function def compute_native(x, y, l, v): n = x.size assert (y.size, l.size, v.size) == (n, n, n) out_x = np.empty(n, dtype=np.uint16) out_y = np.empty(n, dtype=np.uint16) out_l = np.empty(n, dtype=np.uint16) out_w = np.empty(n, dtype=np.float64) m = dll.compute(x, y, l, v, out_x, out_y, out_l, out_w, n) out_x = out_x[:m].copy() out_y = out_y[:m].copy() out_l = out_l[:m].copy() out_w = out_w[:m].copy() return out_x, out_y, out_l, out_w Benchmark Here are performance results on my i5-9600KF CPU (6 cores), with arrays of size n=1_000_000, uniformly-distributed random numbers and 3 different R values (variable range for np.random.randint): With R ~= 1.00 (poor use-case for the dict-based implementation) - seq MadPhysicist's code: 8437 ms (takes 8 GiB of RAM!) - seq initial code: 212 ms - seq Numba dict-based code: 132 ms &lt;----- 1.6 times faster - seq C++ dict-based code: 121 ms - par C++ sort-based code: 32 ms &lt;----- 6.6 times faster With R ~= 0.26 (better use-case for dicts) - seq initial code: 201 ms - seq Numba dict-based code: 72 ms &lt;----- 2.8 times faster - seq C++ dict-based code: 45 ms - par C++ sort-based code: 31 ms &lt;----- 6.5 times faster - seq MadPhysicist's code: 29 ms With R ~= 0.03 (perfect use-case for dicts) - seq initial code: 192 ms - seq Numba dict-based code: 50 ms &lt;----- 3.8 times faster - par C++ sort-based code: 29 ms &lt;----- 6.6 times faster - seq MadPhysicist's code: 14 ms - seq C++ dict-based code: 12 ms Thus, the proposed sequential dict-based implementation is 1.6~3.8 times faster than the initial code on on large arrays. On smaller arrays, the same effect if visible though the speed-up are smaller. Note that &quot;seq Numba dict-based code&quot; is an equivalent of the provided Numba code rewritten in C++, and using a fast hash-map implementation (Tessil's robin-hood hash-maps). The parallel C++ sort-based implementation is 6.6 times faster than the initial code on large arrays. It is the fastest one overall. This is mainly because it runs on multiple cores as opposed to other implementations. Additional notes Please note that when R is pretty small like &lt;0.1, using multiple threads with thread-local dictionary (then merged) should result in a significantly faster implementation. When R is big, this does not worth it with Numba because it should not scale (due to allocations) and it is pretty complicated to implement. If you want to parallelize the dict-based implementation, then I strongly advise you to use a native language like C++. Doing this efficiently is far from being easy.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "To save other people some time: my attempt with sparse.COO was 10% slower for arrays of size 10 M. import numpy as np import sparse # pip install sparse from line_profiler import profile def original(indices, values, shape): idx = np.ravel_multi_index(indices, shape, order='F') out, uind, cinv = np.unique(idx, return_index=True, return_inverse=True) vv = np.bincount(cinv, weights=values) out = tuple(index[uind] for index in indices) return vv, *out def with_sparse(indices, values, shape): coo = sparse.COO(indices, values, shape=shape) return coo.data, *coo.coords def test_correctness(): x = np.array([1, 1, 2, 2, 3, 3, 1, 1, 3, 3, 4], dtype=np.uint16) y = np.array([1, 1, 2, 2, 2, 2, 1, 1, 3, 4, 5], dtype=np.uint16) l = np.array([1, 2, 2, 2, 3, 2, 1, 1, 3, 3, 6], dtype=np.uint16) v = np.array([2, 4, 6, 8, 7, 5, 3, 1, 8, 6, 4], dtype=np.float64) indices = (x, y, l) shape = [np.amax(index) + 1 for index in indices] r1 = original(indices, v, shape) r2 = with_sparse(indices, v, shape) for a, b in zip(r1, r2): assert np.allclose(a, b) @profile def test_speed(n=10_000_000): indices = np.random.randint(0, 1000, size=(3, n)) values = np.random.rand(n) shape = [np.amax(index) + 1 for index in indices] r1 = original(indices, values, shape) r2 = with_sparse(indices, values, shape) if __name__ == &quot;__main__&quot;: test_correctness() test_speed() Profiler results: Total time: 6.04991 s File: sum_of_voxels.py Function: test_speed at line 35 Line # Hits Time Per Hit % Time Line Contents ============================================================== 35 @profile 36 def test_speed(n=10_000_000): 37 1 169152.2 169152.2 2.8 indices = np.random.randint(0, 1000, size=(3, n)) 38 1 89515.9 89515.9 1.5 values = np.random.rand(n) 39 1 17613.5 17613.5 0.3 shape = [np.amax(index) + 1 for index in indices] 40 1 2727965.6 3e+06 45.1 r1 = original(indices, values, shape) 41 1 3045661.3 3e+06 50.3 r2 = with_sparse(indices, values, shape)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "arrays",
        "numpy",
        "group-by"
      ],
      "question_score": 7,
      "answer_score": 2,
      "created": "2025-02-03T20:44:52",
      "question_id": 79409979,
      "answer_id": 79412857
    }
  },
  {
    "question": "Enable ruff rules only on specific files",
    "expected_answer": "You can invert the file selection in the (extend-)per-file-ignores section to ignore for example D103 on all files except the ones that do match the pattern. from the Ruff documentation for the per-file-ignores: A list of mappings from file pattern to rule codes or prefixes to exclude, when considering any matching files. An initial '!' negates the file pattern. And the corresponding example in a pyproject.toml: # Ignore `D` rules everywhere except for the `src/` directory. &quot;!src/**.py&quot; = [&quot;D&quot;]",
    "context_chunks": [
      {
        "text": "I work on a large project and I'd slowly like to enfore pydocstyle using ruff. However, many files will fail on e.g. D103 &quot;undocumented public function&quot;. I'd like to start with enforcing it on a few specific files, so I'd like to write something like select = [&quot;D&quot;] [tool.ruff.ignore-except-per-file] # this config does not exist # ignore D103 but on all files, except the ones that pass &quot;properly_formatted_module1.py&quot; = [&quot;D103&quot;] &quot;properly_formatted_module2.py&quot; = [&quot;D103&quot;] I don't think this is possible; the only way I see is to explicitly write down ALL of the file names in a [tool.ruff.extend-per-file-ignores]. There are a few hunderd of them so that's not really nice to do.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "You can invert the file selection in the (extend-)per-file-ignores section to ignore for example D103 on all files except the ones that do match the pattern. from the Ruff documentation for the per-file-ignores: A list of mappings from file pattern to rule codes or prefixes to exclude, when considering any matching files. An initial '!' negates the file pattern. And the corresponding example in a pyproject.toml: # Ignore `D` rules everywhere except for the `src/` directory. &quot;!src/**.py&quot; = [&quot;D&quot;]",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "That’s right, hoelzeli, but something missing from the Ruff documentation is that you can also use a comma-delimited pattern to negate specific filenames. For example, instead of negating all of src/**.py you can negate just two files like this: &quot;!src/{foo.py,bar.py}&quot; = [&quot;D&quot;] This is the only solution that works for me because it does not tolerate multiple separate negate patterns for the same lint rule.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "ruff"
      ],
      "question_score": 7,
      "answer_score": 4,
      "created": "2024-01-20T17:31:14",
      "question_id": 77852033,
      "answer_id": 78648754
    }
  },
  {
    "question": "How to install tensorflow-gpu from anaconda?",
    "expected_answer": "Honestly, if you already tried many things, maybe it's better to reinstall your ubuntu and start over? 👀 Anyways The first thing you need is to install nvidia drivers, and you can install it via Software and Updates: Just select the one with (proprietary, tested), in your case it might be higher such as 535 or whatever. You can check if nvidia-drivers are installed by running this command: nvidia-smi Install Anaconda (run these commands): wget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh shasum -a 256 Anaconda3-2023.09-0-Linux-x86_64.sh bash ~/Downloads/Anaconda3-2023.09-0-Linux-x86_64.sh Source for installing Anaconda Create conda environment and activate it by running these commands: conda create -n any_name python=3.9 conda activate any_name Run following comment to install latest pip version: pip install --upgrade pip Lastly, run this: # For installing tensorflow-gpu pip install 'tensorflow[and-cuda]' Source for installing tensorflow-gpu You might check if tensorflow is installed and using gpu by running the following command on your editor: import tensorflow as tf print(&quot;Num GPUs Available: &quot;, len(tf.config.list_physical_devices('GPU'))) Done!",
    "context_chunks": [
      {
        "text": "I have run some very basic steps (tensorflow-gpu is currently at 2.12.1): conda create --name py311_tf212 python=3.11 numpy numba scipy spyder pandas conda activate py311_tf212 time conda install -c conda-forge tensorflow-gpu After 3 hours of thinking and printing a few thousand lines of package dependencies, the installation fails. My system features are: Ubuntu 18.04, feature:/linux-64::__glibc==2.27, CUDA 11.8, Nvidia driver 520.61.05. I'm not sure what other information is relevant. I'd be happy to get any tips. Edit 2023-12-19, I gave this a new go. This time I specified the conda-forge channel at the env creation step: $ time conda create --name py311_tf2_test -c conda-forge python=3.11 numpy numba scipy spyder pandas tensorflow-gpu ... Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed ... Package expat conflicts for: numpy -&gt; pypy3.9[version='&gt;=7.3.13'] -&gt; expat[version='&gt;=2.2.9,&lt;3.0.0a0|&gt;=2.3.0,&lt;3.0a0|&gt;=2.4.1,&lt;3.0a0|&gt;=2.4.8,&lt;3.0a0|&gt;=2.4.9,&lt;3.0a0|&gt;=2.5.0,&lt;3.0a0|&gt;=2.4.7,&lt;3.0a0'] scipy -&gt; pypy3.9[version='&gt;=7.3.13'] -&gt; expat[version='&gt;=2.2.9,&lt;3.0.0a0|&gt;=2.3.0,&lt;3.0a0|&gt;=2.4.1,&lt;3.0a0|&gt;=2.4.8,&lt;3.0a0|&gt;=2.4.9,&lt;3.0a0|&gt;=2.5.0,&lt;3.0a0'] pandas -&gt; pypy3.9[version='&gt;=7.3.13'] -&gt; expat[version='&gt;=2.2.9,&lt;3.0.0a0|&gt;=2.3.0,&lt;3.0a0|&gt;=2.4.1,&lt;3.0a0|&gt;=2.4.8,&lt;3.0a0|&gt;=2.4.9,&lt;3.0a0|&gt;=2.5.0,&lt;3.0a0'] spyder -&gt; python[version='&gt;=3.12,&lt;3.13.0a0'] -&gt; expat[version='&gt;=2.5.0,&lt;3.0a0'] ... Package typing_extensions conflicts for: numba -&gt; importlib-metadata -&gt; typing_extensions[version='&gt;=3.6.4'] spyder -&gt; ipython[version='&gt;=8.12.2,&lt;9.0.0,!=8.17.1'] -&gt; typing_extensions[version='&gt;=3.10|&gt;=3.10.0|&gt;=3.7|&gt;=3.6.4'] ... The following specifications were found to be incompatible with your system: - feature:/linux-64::__glibc==2.27=0 - feature:/linux-64::__unix==0=0 - feature:|@/linux-64::__glibc==2.27=0 - feature:|@/linux-64::__unix==0=0 - numba -&gt; libgcc-ng[version='&gt;=10.3.0'] -&gt; __glibc[version='&gt;=2.17'] - numpy -&gt; libgcc-ng[version='&gt;=10.3.0'] -&gt; __glibc[version='&gt;=2.17'] - pandas -&gt; libgcc-ng[version='&gt;=10.3.0'] -&gt; __glibc[version='&gt;=2.17'] - python=3.11 -&gt; libgcc-ng[version='&gt;=11.2.0'] -&gt; __glibc[version='&gt;=2.17'] - scipy -&gt; libgfortran-ng -&gt; __glibc[version='&gt;=2.17'] - spyder -&gt; ipython[version='&gt;=8.12.2,&lt;9.0.0,!=8.17.1'] -&gt; __linux - tensorflow-gpu -&gt; tensorflow==2.15.0=cuda118py39h5387621_0 -&gt; __cuda - tensorflow-gpu -&gt; tensorflow==2.6.2=cuda111py37hf54207c_2 -&gt; __glibc[version='&gt;=2.17'] Your installed version is: 2.27 real 32m40.392s I'm not sure how to interpret this, is the installed glibc version an issue here? Most packages seem to be happy with 2.17 or newer. Oddly tensorflow-gpu has dependencies tensorflow==2.15.0 and tensorflow==2.6.2.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Honestly, if you already tried many things, maybe it's better to reinstall your ubuntu and start over? 👀 Anyways The first thing you need is to install nvidia drivers, and you can install it via Software and Updates: Just select the one with (proprietary, tested), in your case it might be higher such as 535 or whatever. You can check if nvidia-drivers are installed by running this command: nvidia-smi Install Anaconda (run these commands): wget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh shasum -a 256 Anaconda3-2023.09-0-Linux-x86_64.sh bash ~/Downloads/Anaconda3-2023.09-0-Linux-x86_64.sh Source for installing Anaconda Create conda environment and activate it by running these commands: conda create -n any_name python=3.9 conda activate any_name Run following comment to install latest pip version: pip install --upgrade pip Lastly, run this: # For installing tensorflow-gpu pip install 'tensorflow[and-cuda]' Source for installing tensorflow-gpu You might check if tensorflow is installed and using gpu by running the following command on your editor: import tensorflow as tf print(&quot;Num GPUs Available: &quot;, len(tf.config.list_physical_devices('GPU'))) Done!",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Maybe try conda install anaconda::tensorflow-gpu, which worked for me when conda-forge did not. Not sure why.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "tensorflow",
        "anaconda"
      ],
      "question_score": 7,
      "answer_score": 4,
      "created": "2023-09-29T13:33:13",
      "question_id": 77202380,
      "answer_id": 77205609
    }
  },
  {
    "question": "Handling Circular Imports in Pydantic models with FastAPI",
    "expected_answer": "I had this same issue and spent hours trying to figure it out, in the end i ended up just not type annotating the specific circular imports and i've lived happily ever after(so far). Maybe you could benefit from doing this same ;) That being said, there are multiple ways of fixing circular imports. As highlighted here What you've tried so far is: Normal typing; doesnt work when a child import a parent. String literals such as toys:[&quot;ToyResponse&quot;], this method still causes circular import errors because you are still importing the class to resolve the type. Conditionally importing using TYPE_CHECK. This method seems promising and i believe you've almost got it but were missing one small detail, the TYPE_CHECK boolean must be checked at every place where the circular import types are being used see below: As per the example you provided, you conditionally import your classes but you dont conditionally do the type checks on the class attributes which results in an undefined error when accessing the type. As highlighted in the mypy docs: The typing module defines a TYPE_CHECKING constant that is False at runtime but treated as True while type checking. Since code inside if TYPE_CHECKING: is not executed at runtime, it provides a convenient way to tell mypy something without the code being evaluated at runtime. This is most useful for resolving import cycles. # my_app.modules.box.schemas.py from pydantic import BaseModel from my_app.modules.toy.schemas import ToyResponse class BoxResponse(BaseModel): id: int toys: list[&quot;ToyResponse&quot;] # Type check not required here since this is the parent class # my_app.modules.toy.schemas.py from typing import TYPE_CHECKING from pydantic import BaseModel if TYPE_CHECKING: from my_app.modules.box.schemas import BoxResponse class ToyResponse(BaseModel): id: int if TYPE_CHECKING: box: &quot;BoxResponse&quot; else: box Personally the above seems hackish. If you have Python 3.7 and up you could also use __future__ import annotations. This will take type hints and treat them as string literals during the initial import. Which should prevent the circular import error.",
    "context_chunks": [
      {
        "text": "I'm developing a FastAPI application organized with the following module structure. ... │ ├── modules │ │ ├── box │ │ │ ├── routes.py │ │ │ ├── services.py │ │ │ ├── models.py # the sqlalchemy classes │ │ │ ├── schemas.py # the pydantic schemas │ │ ├── toy │ │ │ ├── routes.py │ │ │ ├── services.py │ │ │ ├── models.py │ │ │ ├── schemas.py Each module contains SQLAlchemy models, Pydantic models (also called schemas), FastAPI routes, and services that handle the business logic. In this example, I am using two modules that represent boxes and toys. Each toy is stored in one box, and each box contains multiple toys, following a classic 1 x N relationship. With SQLAlchemy everything goes well, defining relationships is straightforward by using TYPE_CHECKING to handle circular dependencies: # my_app.modules.box.models.py from sqlalchemy.orm import Mapped, mapped_column, relationship if TYPE_CHECKING: from my_app.modules.toy.models import Toy class Box(Base): __tablename__ = &quot;box&quot; id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True) toys: Mapped[list[&quot;Toy&quot;]] = relationship(back_populates=&quot;box&quot;) # my_app.modules.toy.models.py from sqlalchemy.orm import Mapped, mapped_column, relationship if TYPE_CHECKING: from my_app.modules.box.models import Box class Toy(Base): __tablename__ = &quot;toy&quot; id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True) box: Mapped[&quot;Box&quot;] = relationship(back_populates=&quot;toys&quot;) This setup works perfectly without raising any circular import errors. However, I encounter issues when defining the same relationships between Pydantic schemas. If I import directly the modules on my schemas.py, # my_app.modules.box.schemas.py from my_app.modules.toy.schemas import ToyBase class BoxBase(BaseModel): id: int class BoxResponse(BoxBase): toys: list[ToyBase] # my_app.modules.toy.schemas.py from my_app.modules.box.schemas import BoxBase class ToyBase(BaseModel): id: int class ToyResponse(ToyBase): box: BoxBase I recieve the circular import error: ImportError: cannot import name 'ToyBase' from partially initialized module 'my_app.modules.toy.schemas' (most likely due to a circular import)... I also try the SQLAlchemy approach of TYPE_CHECKING and string declaration: # my_app.modules.box.schemas.py if TYPE_CHECKING: from my_app.modules.toy.schemas import ToyBase class BoxBase(BaseModel): id: int class BoxResponse(BoxBase): toys: list[&quot;ToyBase&quot;] # my_app.modules.toy.schemas.py if TYPE_CHECKING: from my_app.modules.box.schemas import BoxBase class ToyBase(BaseModel): id: int class ToyResponse(ToyBase): box: &quot;BoxBase&quot; But apparently, pydantic doesn't support this: raise PydanticUndefinedAnnotation.from_name_error(e) from e pydantic.errors.PydanticUndefinedAnnotation: name 'ToyBase' is not defined (Some answers) suggest that the issue comes from a poor module organization. (Others) suggest, too complex and hard to understand solutions. Maybe I'm wrong but I consider the relationship between Box and Toy something trivial and fundamental that should be manageable in any moderately complex project. For example, a straightforward use case would be to request a toy along with its containing box and vice versa, a box with all its toys. Aren't they legitimate requests? So, my question How can I define interrelated Pydantic schemas (BoxResponse and ToyResponse) that reference each other without encountering circular import errors? I'm looking for an clear and maintainable solution that preserves the independence of the box and toy modules, similar to how relationships are handled in SQLAlchemy models. Any suggestions or at least an explanation of why this is so difficult to achieve?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I had this same issue and spent hours trying to figure it out, in the end i ended up just not type annotating the specific circular imports and i've lived happily ever after(so far). Maybe you could benefit from doing this same ;) That being said, there are multiple ways of fixing circular imports. As highlighted here What you've tried so far is: Normal typing; doesnt work when a child import a parent. String literals such as toys:[&quot;ToyResponse&quot;], this method still causes circular import errors because you are still importing the class to resolve the type. Conditionally importing using TYPE_CHECK. This method seems promising and i believe you've almost got it but were missing one small detail, the TYPE_CHECK boolean must be checked at every place where the circular import types are being used see below: As per the example you provided, you conditionally import your classes but you dont conditionally do the type checks on the class attributes which results in an undefined error when accessing the type. As highlighted in the mypy docs: The typing module defines a TYPE_CHECKING constant that is False at runtime but treated as True while type checking. Since code inside if TYPE_CHECKING: is not executed at runtime, it provides a convenient way to tell mypy something without the code being evaluated at runtime. This is most useful for resolving import cycles. # my_app.modules.box.schemas.py from pydantic import BaseModel from my_app.modules.toy.schemas import ToyResponse class BoxResponse(BaseModel): id: int toys: list[&quot;ToyResponse&quot;] # Type check not required here since this is the parent class # my_app.modules.toy.schemas.py from typing import TYPE_CHECKING from pydantic import BaseModel if TYPE_CHECKING: from my_app.modules.box.schemas import BoxResponse class ToyResponse(BaseModel): id: int if TYPE_CHECKING: box: &quot;BoxResponse&quot; else: box Personally the above seems hackish. If you have Python 3.7 and up you could also use __future__ import annotations. This will take type hints and treat them as string literals during the initial import. Which should prevent the circular import error.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Pydantic can't construct infinitely recursive models. AsToyResponse is written the data you receive would have to be infinitely recursive. { id: &quot;x&quot; box: { id: &quot;a&quot; toys: [{ id: &quot;x&quot; box: { id: &quot;a&quot; toys: ... and so on }] } } This seems like a case where SQLAlchemy doesn't attempt to process the type annotations at run time, but Pydantic is when it tries to construct the class objects causing a circular import. One way to break the recursive definition would be to define a Toy model that doesn't have a reference to BoxResponse in the definition and use that in BoxResponse class BoxResponse(BaseModel): id: int toys: list[ToyWithoutNestedBox] EDIT in response to question edits: Now that the models are split into Base and Response classes, you'll remove the circular import if you define the Base and Response classes in separate files from eachother. This is because the Base classes require no imports from other models so the Response classes are free to import them without the risk of circular imports.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "sqlalchemy",
        "fastapi",
        "pydantic",
        "circular-dependency"
      ],
      "question_score": 7,
      "answer_score": 3,
      "created": "2024-09-25T14:33:18",
      "question_id": 79023460,
      "answer_id": 79023826
    }
  },
  {
    "question": "How to choose the best fitting method?",
    "expected_answer": "This answer achieves only moderate speed gains (10%). However, I have removed unnecessary loops and array copies. I have also renamed a few variables if they had different names in other parts of the code (but were otherwise identical), and removed others that were superfluous or even unused. As a result, the code is much shorter and a bit more readable. I hope that it aids others in finding more substantial improvements. import numpy as np def fitfunction(data, maxiter, error_threshold, v=1.4, maximum_shift=2_000): def mse(y_true, y_pred, margin=1000): if margin &gt; 0: return np.sum((y_true[margin:-margin] - y_pred[margin:-margin])**2) else: return np.sum((y_true - y_pred)**2) def fast_roll_add(dst, src, shift): dst[shift:] += src[:-shift] dst[:shift] += src[-shift:] # Main function def fp(x, maximum_shift, v): unitary = x.copy() exponents = unitary / v new_powers = np.exp(-exponents) y = (1 - new_powers) / unitary for k in range(1, maximum_shift+1): fast_roll_add(unitary, x, k) fast_roll_add(unitary, x, -k) exponents += unitary / v old_powers = new_powers new_powers = np.exp(-exponents) y += (old_powers - new_powers) / unitary return y # Fitting function # This is the literal rewrite. # def cfit(y_true, y_pred, x): # x_new = x * (y_pred / y_true)**2 # residuals = np.abs(y_true - y_pred) # return np.where(residuals &lt; .5, x, np.clip(x_new, 1e-20, None)) # However, checking the residuals seems to be unnecessary. def cfit(y_true, y_pred, x): return np.clip(x * (y_pred / y_true)**2, 1e-20, None) err = np.zeros(maxiter) x = np.pi / (4 * v * data**2) y = fp(x, maximum_shift, v) err[0] = mse(data, y) print(&quot;1&quot; + '/' + str(maxiter) + ' err: ' + str('{:.2f}'.format(err[0])), end=&quot;\\r&quot;) for j in range(1, maxiter): if err[j-1] &gt; error_threshold: x = cfit(data, y, x) # Fitting y = fp(x, maximum_shift, v) # Update function values err[j] = mse(data, y) print(str(j+1) + '/' + str(maxiter) + ' err: ' + str('{:.2f}'.format(err[j])), end=&quot;\\r&quot;) else: break print() return x, y, err if __name__ == &quot;__main__&quot;: import time import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1.inset_locator import mark_inset from scipy.ndimage import gaussian_filter1d # Data generation np.random.seed(18) data = np.random.normal(loc=0, scale=10, size=10_000).cumsum() data = (data - data.min()) / (data.max() - data.min()) * 500 # Gaussian smoothing data = gaussian_filter1d(data, sigma=50) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) fig.canvas.draw() ax1_inset = ax1.inset_axes([0.55, 0.05, 0.4, 0.4]) for v in [1.4, 0.05]: tic = time.time() x, y, err = fitfunction(data, 10, 0.1, v=v) toc = time.time() print(f&quot;Time elapsed: {toc-tic:.2f} seconds&quot;) ax1.plot(y, label=f&quot;v = {v:.2f}&quot;) ax1_inset.plot(y, label=f&quot;v = {v:.2f}&quot;) ax2.plot(err, label=f&quot;v = {v:.2f}&quot;) ax1.plot(data, linestyle=':', color=&quot;black&quot;, label=&quot;Original data&quot;) ax1_inset.plot(data, linestyle=':', color=&quot;black&quot;, label=&quot;Original data&quot;) ax1_inset.set_xlim(7_300, 8_000) ax1_inset.set_ylim(330, 410) ax1_inset.set_xticks([]) ax1_inset.set_yticks([]) mark_inset(ax1, ax1_inset, loc1=2, loc2=4, fc=&quot;none&quot;, ec=&quot;0.5&quot;) ax1.set_title(&quot;Data &amp; fits&quot;) ax1.set_xlabel(&quot;Time&quot;) ax2.set_ylabel(&quot;Value&quot;) ax1.legend(loc=&quot;upper left&quot;) ax2.set_title(&quot;Loss improvement&quot;) ax2.set_xlabel(&quot;Iteration&quot;) ax2.set_ylabel(&quot;MSE&quot;) ax2.legend(loc=&quot;upper right&quot;) plt.show() Edit I think it is worth noting that there is a lot of room for improvement in the parameterization of your optimization. For example, your choice of v, which is effectively the inverse of your learning rate, is far too conservative. With v=0.05, the fit converges within 3 iterations, even if the error threshold is reduced to 0.1.",
    "context_chunks": [
      {
        "text": "I want to fit non-negative parameters (xs in the code) to a function fp, given by where v and L are fixed, given parameters. Computing fp can be done relatively quickly by vectorizing it and using some uniform filtering. The following code implements this optimization problem, where the function cfit (through fitf) slowly adjusts the values of xs by calculating fp at each step and comparing it to the real data (list in the code), according to Question: Is there a way of improving this fitting method for this specific function, and the way I am computing it? I tried playing around with scipy.optimize, but everything seems slower than this simple multiplicative gradient method. Here is the code implementation, where the mean-square error is tracked import numpy as np import math def fitfunction(list, maxiter, error_threshold): v = 1.4 st = 5 exp_v = np.exp(-1/v) x00 = np.array([(math.pi/(4*v))*i**(-2) for i in list]) lm = 1000 def mse(y_true, y_pred): mse_value = sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true) return mse_value def fast_roll_add(dst, src, shift): dst[shift:] += src[:-shift] dst[:shift] += src[-shift:] # Main function def fp(x, L, v): n = len(x) y = np.zeros(n) last_exp_2_raw = np.zeros(n) last_exp_2 = np.ones(n) unitary = x.copy() for k in range(L+1): if k != 0: fast_roll_add(unitary, x, k) fast_roll_add(unitary, x, -k) exp_1_raw = last_exp_2_raw exp_1 = last_exp_2 exp_2_raw = exp_1_raw + unitary / v exp_2 = np.exp(-exp_2_raw) y += (exp_1 - exp_2) / unitary last_exp_2_raw = exp_2_raw last_exp_2 = exp_2 return y # Fitting functions def fitf(time, lst, x0, j): return x0[j] * (lst[j] / time[j])**2 def cfit(time, lst, x0): result = np.empty_like(x0) for j in range(len(x0)): if fitf(time, lst, x0, j) &lt; 10**(-20): result[j] = 10**(-20) elif abs(time[j] - lst[j]) &lt; .5: result[j] = x0[j] else: result[j] = fitf(time, lst, x0, j) return result xs = x00 ys = fp(xs, len(xs)//st, v) err = 10**10 for j in range(maxiter): if err &gt; error_threshold: xs = cfit(list, ys, xs) # Fitting ys = fp(xs, len(xs)//st, v) # Update function values err = mse(list[lm:-lm], ys[lm:-lm]) print(str(j+1) + '/' + str(maxiter) + ' err: ' + str('{:.20f}'.format(err)), end=&quot;\\r&quot;) else: break fire_rates = ['{:.20f}'.format(i) for i in xs] time_sim = ys return [fire_rates, time_sim] The efficiency of this method is highly data-dependent, so below is a working example with generated data, illustrating how fitting around local maxima seems suboptimal with my method Minimal working example: from scipy.ndimage import gaussian_filter1d # Data generation np.random.seed(18) data = np.random.normal(loc=0, scale=10, size=10000).cumsum() data = (data - data.min()) / (data.max() - data.min()) * 500 # Gaussian smoothing data = gaussian_filter1d(data, sigma=50) data_fit = fitfunction(data, 100, 2) and to visualize it import matplotlib.pyplot as plt plt.figure(figsize=(10, 6)) plt.plot(data, label='Data') plt.plot(data_fit[1], label='Data Fit') plt.xlabel('Index') plt.ylabel('Value') plt.title('Data and Data Fit') plt.legend() plt.show() Brief comments: My multiplicative method seems to be suboptimal around local maxima, as exemplified in the plot above. However, I want to guarantee the best possible fit. In particular, when using a really large iteration number, the fitting algorithm does not converge at all. This can be seen from around 200 iteration steps, where local values diverge away from the real data value around local maxima. The implementation of fp is periodic in nature, which leads to massive errors in the ends. Hence, the error err is calculated excluding these regions. The fitting function provides an even faster fit def fitf(time, lst, x0, j): return x0[j]**(np.log(time[j]) / np.log(lst[j])) However, when the values come too close to the real value (high iteration number), it is highly unstable, like the multiplicative method.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This answer achieves only moderate speed gains (10%). However, I have removed unnecessary loops and array copies. I have also renamed a few variables if they had different names in other parts of the code (but were otherwise identical), and removed others that were superfluous or even unused. As a result, the code is much shorter and a bit more readable. I hope that it aids others in finding more substantial improvements. import numpy as np def fitfunction(data, maxiter, error_threshold, v=1.4, maximum_shift=2_000): def mse(y_true, y_pred, margin=1000): if margin &gt; 0: return np.sum((y_true[margin:-margin] - y_pred[margin:-margin])**2) else: return np.sum((y_true - y_pred)**2) def fast_roll_add(dst, src, shift): dst[shift:] += src[:-shift] dst[:shift] += src[-shift:] # Main function def fp(x, maximum_shift, v): unitary = x.copy() exponents = unitary / v new_powers = np.exp(-exponents) y = (1 - new_powers) / unitary for k in range(1, maximum_shift+1): fast_roll_add(unitary, x, k) fast_roll_add(unitary, x, -k) exponents += unitary / v old_powers = new_powers new_powers = np.exp(-exponents) y += (old_powers - new_powers) / unitary return y # Fitting function # This is the literal rewrite. # def cfit(y_true, y_pred, x): # x_new = x * (y_pred / y_true)**2 # residuals = np.abs(y_true - y_pred) # return np.where(residuals &lt; .5, x, np.clip(x_new, 1e-20, None)) # However, checking the residuals seems to be unnecessary. def cfit(y_true, y_pred, x): return np.clip(x * (y_pred / y_true)**2, 1e-20, None) err = np.zeros(maxiter) x = np.pi / (4 * v * data**2) y = fp(x, maximum_shift, v) err[0] = mse(data, y) print(&quot;1&quot; + '/' + str(maxiter) + ' err: ' + str('{:.2f}'.format(err[0])), end=&quot;\\r&quot;) for j in range(1, maxiter): if err[j-1] &gt; error_threshold: x = cfit(data, y, x) # Fitting y = fp(x, maximum_shift, v) # Update function values err[j] = mse(data, y) print(str(j+1) + '/' + str(maxiter) + ' err: ' + str('{:.2f}'.format(err[j])), end=&quot;\\r&quot;) else: break print() return x, y, err if __name__ == &quot;__main__&quot;: import time import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1.inset_locator import mark_inset from scipy.ndimage import gaussian_filter1d # Data generation np.random.seed(18) data = np.random.normal(loc=0, scale=10, size=10_000).cumsum() data = (data - data.min()) / (data.max() - data.min()) * 500 # Gaussian smoothing data = gaussian_filter1d(data, sigma=50) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) fig.canvas.draw() ax1_inset = ax1.inset_axes([0.55, 0.05, 0.4, 0.4]) for v in [1.4, 0.05]: tic = time.time() x, y, err = fitfunction(data, 10, 0.1, v=v) toc = time.time() print(f&quot;Time elapsed: {toc-tic:.2f} seconds&quot;) ax1.plot(y, label=f&quot;v = {v:.2f}&quot;) ax1_inset.plot(y, label=f&quot;v = {v:.2f}&quot;) ax2.plot(err, label=f&quot;v = {v:.2f}&quot;) ax1.plot(data, linestyle=':', color=&quot;black&quot;, label=&quot;Original data&quot;) ax1_inset.plot(data, linestyle=':', color=&quot;black&quot;, label=&quot;Original data&quot;) ax1_inset.set_xlim(7_300, 8_000) ax1_inset.set_ylim(330, 410) ax1_inset.set_xticks([]) ax1_inset.set_yticks([]) mark_inset(ax1, ax1_inset, loc1=2, loc2=4, fc=&quot;none&quot;, ec=&quot;0.5&quot;) ax1.set_title(&quot;Data &amp; fits&quot;) ax1.set_xlabel(&quot;Time&quot;) ax2.set_ylabel(&quot;Value&quot;) ax1.legend(loc=&quot;upper left&quot;) ax2.set_title(&quot;Loss improvement&quot;) ax2.set_xlabel(&quot;Iteration&quot;) ax2.set_ylabel(&quot;MSE&quot;) ax2.legend(loc=&quot;upper right&quot;) plt.show() Edit I think it is worth noting that there is a lot of room for improvement in the parameterization of your optimization. For example, your choice of v, which is effectively the inverse of your learning rate, is far too conservative. With v=0.05, the fit converges within 3 iterations, even if the error threshold is reduced to 0.1.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I'd recommend to compute the error of the prediction and compensate it directly into xs with some learning rate. The idea to have more granular control over xs. It is probably something similar to Straight-Through Estimators, in terms of the error is directly back propagated to the variable. So, your fitting functions will be as follows: # # Fitting functions def fitf(time, lst, x0, j): learning_rate = 1e-8 return x0[j] + learning_rate*(lst[j] - time[j]) def cfit(time, lst, x0): result = np.empty_like(x0) for j in range(len(x0)): result[j] = fitf(time, lst, x0, j) return result Please note, you will need to choose correct learning_rate for different data. This will be your hyper parameter. Very small changes in learning rate may cause divergence. Since, the initial xs has the order of 1e-06, I have chosen 1e-8 learning rate, and it worked. But, you will need to increase the number of iterations to be around 400, i.e. maxiter=400, or even higher, because of small learning rate. You will get the following result: With max_iter=1000, the result is even better (note the difference on the left side of the figures): The full code: import numpy as np import math import matplotlib.pyplot as plt from scipy.ndimage import gaussian_filter1d def fitfunction(list, maxiter, error_threshold): v = 1.4 st = 5 exp_v = np.exp(-1/v) x00 = np.array([(math.pi/(4*v))*i**(-2) for i in list]) lm = 1000 def mse(y_true, y_pred): mse_value = sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true) return mse_value def fast_roll_add(dst, src, shift): dst[shift:] += src[:-shift] dst[:shift] += src[-shift:] # Main function def fp(x, L, v): n = len(x) y = np.zeros(n) last_exp_2_raw = np.zeros(n) last_exp_2 = np.ones(n) unitary = x.copy() for k in range(L+1): if k != 0: fast_roll_add(unitary, x, k) fast_roll_add(unitary, x, -k) exp_1_raw = last_exp_2_raw exp_1 = last_exp_2 exp_2_raw = exp_1_raw + unitary / v exp_2 = np.exp(-exp_2_raw) y += (exp_1 - exp_2) / unitary last_exp_2_raw = exp_2_raw last_exp_2 = exp_2 return y # # Fitting functions def fitf(time, lst, x0, j): learning_rate = 1e-8 return x0[j] + learning_rate*(lst[j] - time[j]) def cfit(time, lst, x0): result = np.empty_like(x0) for j in range(len(x0)): result[j] = fitf(time, lst, x0, j) return result xs = x00 ys = fp(xs, len(xs)//st, v) err = 10**10 for j in range(maxiter): if err &gt; error_threshold: xs = cfit(list, ys, xs) # Fitting ys = fp(xs, len(xs)//st, v) # Update function values err = mse(list[lm:-lm], ys[lm:-lm]) print(str(j+1) + '/' + str(maxiter) + ' err: ' + str('{:.20f}'.format(err)), end=&quot;\\r&quot;) else: break fire_rates = ['{:.20f}'.format(i) for i in xs] time_sim = ys return [fire_rates, time_sim] # Data generation np.random.seed(18) data = np.random.normal(loc=0, scale=10, size=10000).cumsum() data = (data - data.min()) / (data.max() - data.min()) * 500 # Gaussian smoothing data = gaussian_filter1d(data, sigma=50) data_fit = fitfunction(data, 500, 0.2) plt.figure(figsize=(10, 6)) plt.plot(data, label='Data') plt.plot(data_fit[1], label='Data Fit') plt.xlabel('Index') plt.ylabel('Value') plt.title('Data and Data Fit') plt.legend() plt.show() And here is the modified code provided by Paul Brodersen: import numpy as np import time import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1.inset_locator import mark_inset from scipy.ndimage import gaussian_filter1d def fitfunction(data, maxiter, error_threshold, v=1.4, maximum_shift=2_000): def mse(y_true, y_pred, margin=1000): if margin &gt; 0: return np.sum((y_true[margin:-margin] - y_pred[margin:-margin])**2) else: return np.sum((y_true - y_pred)**2) def fast_roll_add(dst, src, shift): dst[shift:] += src[:-shift] dst[:shift] += src[-shift:] # Main function def fp(x, maximum_shift, v): unitary = x.copy() exponents = unitary / v new_powers = np.exp(-exponents) y = (1 - new_powers) / unitary for k in range(1, maximum_shift+1): fast_roll_add(unitary, x, k) fast_roll_add(unitary, x, -k) exponents += unitary / v old_powers = new_powers new_powers = np.exp(-exponents) y += (old_powers - new_powers) / unitary return y # Fitting function # This is the literal rewrite. # def cfit(y_true, y_pred, x): # x_new = x * (y_pred / y_true)**2 # residuals = np.abs(y_true - y_pred) # return np.where(residuals &lt; .5, x, np.clip(x_new, 1e-20, None)) # However, checking the residuals seems to be unnecessary. def cfit(y_true, y_pred, x): return x + 1e-8*(y_pred-y_true) err = np.zeros(maxiter) x = np.pi / (4 * v * data**2) y = fp(x, maximum_shift, v) err[0] = mse(data, y) print(&quot;1&quot; + '/' + str(maxiter) + ' err: ' + str('{:.2f}'.format(err[0])), end=&quot;\\r&quot;) for j in range(1, maxiter): if err[j-1] &gt; error_threshold: x = cfit(data, y, x) # Fitting y = fp(x, maximum_shift, v) # Update function values err[j] = mse(data, y) print(str(j+1) + '/' + str(maxiter) + ' err: ' + str('{:.2f}'.format(err[j])), end=&quot;\\r&quot;) else: break print() return x, y, err # Data generation np.random.seed(18) data = np.random.normal(loc=0, scale=10, size=10_000).cumsum() data = (data - data.min()) / (data.max() - data.min()) * 500 # Gaussian smoothing data = gaussian_filter1d(data, sigma=50) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) fig.canvas.draw() ax1_inset = ax1.inset_axes([0.55, 0.05, 0.4, 0.4]) v=1.4 tic = time.time() x, y, err = fitfunction(data, 400, 0.1, v=v) toc = time.time() print(f&quot;Time elapsed: {toc-tic:.2f} seconds&quot;) ax1.plot(y, label=f&quot;v = {v:.2f}&quot;) ax1_inset.plot(y, label=f&quot;v = {v:.2f}&quot;) ax2.plot(err, label=f&quot;v = {v:.2f}&quot;) ax1.plot(data, linestyle=':', color=&quot;black&quot;, label=&quot;Original data&quot;) ax1_inset.plot(data, linestyle=':', color=&quot;black&quot;, label=&quot;Original data&quot;) ax1_inset.set_xlim(7_300, 8_000) ax1_inset.set_ylim(330, 410) ax1_inset.set_xticks([]) ax1_inset.set_yticks([]) mark_inset(ax1, ax1_inset, loc1=2, loc2=4, fc=&quot;none&quot;, ec=&quot;0.5&quot;) ax1.set_title(&quot;Data &amp; fits&quot;) ax1.set_xlabel(&quot;Time&quot;) ax2.set_ylabel(&quot;Value&quot;) ax1.legend(loc=&quot;upper left&quot;) ax2.set_title(&quot;Loss improvement&quot;) ax2.set_xlabel(&quot;Iteration&quot;) ax2.set_ylabel(&quot;MSE&quot;) ax2.legend(loc=&quot;upper right&quot;) plt.show() This will have the following result: Edit: For non-negative combined solution discussed in the comments, you can try the following: # Fitting functions def fitf(time, lst, x0, j): x_new = x0[j] + 1e-8*(lst[j] - time[j]) if x_new &lt; 0: return x0[j] * (lst[j] / time[j])**2 return x_new def cfit(time, lst, x0): result = np.empty_like(x0) for j in range(len(x0)): if fitf(time, lst, x0, j) &lt; 1e-20: result[j] = 1e-20 elif abs(time[j] - lst[j]) &lt; .5: result[j] = x0[j] else: result[j] = fitf(time, lst, x0, j) return result The idea is to use multiplicative approach whenever the item of xs will be negative during the update of my approach. Please, note here you need to use may be around 100 iterations for the best solution. Otherwise, please implement early stopping, i.e. whenever the error starts to increase break the loop. Per me the result is worse than each separate approach, but you can try to adjust. Hope, you will find some better insights. For non-negative solution you can also try to compensate the negative values with positive ones, i.e. the sum of xs during the training should remain constant. Here, I'm checking if there is any negative value in xs, if so I'm trying to add that negative value to the closest positive value, which sum still positive. Probably I haven't done this optimal, for 200 iterations it will take around 20 mins or so. def cfit(y_true, y_pred, x): x_vals = x + 1e-8*(y_pred-y_true) negative_indicies = np.where(x_vals&lt;0)[0] positive_indicies = np.where(x_vals&gt;0)[0] if len(negative_indicies)&gt;0: neg_i = [] pos_j = [] for i, n in enumerate(x_vals[negative_indicies]): for j, p in enumerate(x_vals[positive_indicies]): if p+n&gt;1e-10: neg_i.append(i) pos_j.append(j) x_vals[positive_indicies[pos_j]] = x_vals[positive_indicies[pos_j]] + x_vals[negative_indicies[neg_i]] x_vals[negative_indicies[neg_i]] = 1e-10 return x_vals The result will have slightly improvement with the cost of long training:",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "optimization",
        "scipy",
        "curve-fitting",
        "data-fitting"
      ],
      "question_score": 7,
      "answer_score": 3,
      "created": "2024-06-18T11:05:54",
      "question_id": 78637026,
      "answer_id": 78651952
    }
  },
  {
    "question": "Good way to view matrices and higher dimensional arrays in VScode",
    "expected_answer": "Yes, make sure you have the Jupyter extension installed and then simply right click the variable in the Debug menu and select the View Value in Data Viewer option.",
    "context_chunks": [
      {
        "text": "When working with PyTorch/numpy and similar packages, is there a good way to view matrices (or, in general, arrays with two or more dimensions) in debug mode, similar to the way Matlab (or even pyCharm if I remember correctly) present it? This is, for example, a PyTorch tensor, which is very confusing -- opening H here gives me the same thing again and again. As opposed to Matlab, where I can watch it like that: Would appreciate any help with that!",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Yes, make sure you have the Jupyter extension installed and then simply right click the variable in the Debug menu and select the View Value in Data Viewer option.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "There is a new way to see the arrays using Data Wrangler extension. Here is the link to the extension: https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler Choose View Data from the menu: Toolbar view Select your variable: Selection of the image and then you will see the variable in a nice formatted grid: View of the Data Viewer / Data Wrangler There is no need to use Debug, but this will work in debug as well (debug support is the upcoming release in April). Additionally, you'll see stats of your multidimensional structure. For additional question you can follow: https://github.com/microsoft/vscode-data-wrangler Hope this helps!",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy",
        "matlab",
        "pytorch",
        "pycharm"
      ],
      "question_score": 7,
      "answer_score": 4,
      "created": "2023-08-04T16:12:54",
      "question_id": 76837612,
      "answer_id": 76837686
    }
  },
  {
    "question": "Can&#39;t create Race Condition in Python 3.11 using multiple threads",
    "expected_answer": "Based on https://old.reddit.com/r/learnprogramming/comments/16mlz4h/race_condition_doesnt_happen_from_python_310/k198umz/: In 3.10 an optimisation (introduced by commit https://github.com/python/cpython/commit/4958f5d69dd2bf86866c43491caf72f774ddec97 ) made it instead release and acquire the GIL only at specific bytecode instructions, rather than at any of them. In your example, your code is a loop of [LOAD_GLOBAL counter, LOAD_CONST 1, INPLACE_ADD, STORE_GLOBAL counter], importantly None of these are magic &quot;eval breaking&quot; instructions that check the GIL (the final JUMP_ABSOLUTE however is, and so each iteration of the loop is a potential GIL release+acquire point). That means that the load of counter, adding 1, and storing it back, all happen atomically because there were no bytecode instructions in this sequence that caused the eval breaker to do its periodic GIL release+acquire cycle, meaning the GIL is held the entire time. This explains the behaviour you see. For example, the CALL_FUNCTION bytecode instruction is one of these magic eval breakers, so changing your code to have a def one(): return 1 and counter += one() will cause the original race to return. In this case you can check the bytecode translation using dis and notice the CALL_FUNCTION bytecode instruction: import dis def increment1(): global counter for _ in range(10**6): counter += 1 def increment2(): global counter for _ in range(10**6): counter += int(1) dis.dis(increment1) dis.dis(increment2)",
    "context_chunks": [
      {
        "text": "I believe this is a difference in Python 3.10 and above from older versions. Could someone explain this? import threading import time counter = 0 lock = threading.Lock() def increment(): global counter for _ in range(10**6): counter += 1 threads = [] for i in range(4): x = threading.Thread(target=increment) threads.append(x) for t in threads: t.start() for t in threads: t.join() print(counter) Why does this code does not produce a race condition in Python 3.11? However, when I change this line to counter += int(1) then the race condition occurs.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Based on https://old.reddit.com/r/learnprogramming/comments/16mlz4h/race_condition_doesnt_happen_from_python_310/k198umz/: In 3.10 an optimisation (introduced by commit https://github.com/python/cpython/commit/4958f5d69dd2bf86866c43491caf72f774ddec97 ) made it instead release and acquire the GIL only at specific bytecode instructions, rather than at any of them. In your example, your code is a loop of [LOAD_GLOBAL counter, LOAD_CONST 1, INPLACE_ADD, STORE_GLOBAL counter], importantly None of these are magic &quot;eval breaking&quot; instructions that check the GIL (the final JUMP_ABSOLUTE however is, and so each iteration of the loop is a potential GIL release+acquire point). That means that the load of counter, adding 1, and storing it back, all happen atomically because there were no bytecode instructions in this sequence that caused the eval breaker to do its periodic GIL release+acquire cycle, meaning the GIL is held the entire time. This explains the behaviour you see. For example, the CALL_FUNCTION bytecode instruction is one of these magic eval breakers, so changing your code to have a def one(): return 1 and counter += one() will cause the original race to return. In this case you can check the bytecode translation using dis and notice the CALL_FUNCTION bytecode instruction: import dis def increment1(): global counter for _ in range(10**6): counter += 1 def increment2(): global counter for _ in range(10**6): counter += int(1) dis.dis(increment1) dis.dis(increment2)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "PROBLEM: There are problems loading your counter counter += int(1). The problem is related to bytecode, because there are no bytcode instructions in your sequence to execute the loop, SOLUTION: To create Race Condition in Python 3.11, you need to use a disassembler for Python: dis — Disassembler for Python bytecode. You can import dis, because a general disassembly is necessary, then notice increment_n_1 and increment_n_2 bytecode. Dis is a built-in module that provides tools for disassembling Python bytecode. Bytecode is a low-level intermediate representation of Python code that is produced by the Python compiler and executed by the Python Virtual Machine. The first increment will be: def increment_n_1(): global counter for x in range(10**6): counter += 1 The second increment will be: def increment_n_2(): global counter for x in range(10**6): counter += int(1) Finally you need to use dis: incr1 = dis.dis(increment_n_1) incr2 = dis.dis(increment_n_2) Complete Code import dis #First increment def increment_n_1(): global counter for x in range(10**6): counter += 1 #Second increment def increment_n_2(): global counter for x in range(10**6): counter += int(1) incr1 = dis.dis(increment_n_1) incr2 = dis.dis(increment_n_2) Output: 7 0 LOAD_GLOBAL 0 (range) 2 LOAD_CONST 1 (1000000) 4 CALL_FUNCTION 1 6 GET_ITER &gt;&gt; 8 FOR_ITER 6 (to 22) 10 STORE_FAST 0 (x) 8 12 LOAD_GLOBAL 1 (counter) 14 LOAD_CONST 2 (1) 16 INPLACE_ADD 18 STORE_GLOBAL 1 (counter) 20 JUMP_ABSOLUTE 4 (to 8) 7 &gt;&gt; 22 LOAD_CONST 0 (None) 24 RETURN_VALUE 12 0 LOAD_GLOBAL 0 (range) 2 LOAD_CONST 1 (1000000) 4 CALL_FUNCTION 1 6 GET_ITER &gt;&gt; 8 FOR_ITER 8 (to 26) 10 STORE_FAST 0 (x) 13 12 LOAD_GLOBAL 1 (counter) 14 LOAD_GLOBAL 2 (int) 16 LOAD_CONST 2 (1) 18 CALL_FUNCTION 1 20 INPLACE_ADD 22 STORE_GLOBAL 1 (counter) 24 JUMP_ABSOLUTE 4 (to 8) 12 &gt;&gt; 26 LOAD_CONST 0 (None) 28 RETURN_VALUE",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-3.x",
        "multithreading",
        "python-multithreading",
        "python-internals"
      ],
      "question_score": 7,
      "answer_score": 5,
      "created": "2023-09-13T10:53:26",
      "question_id": 77096404,
      "answer_id": 77513158
    }
  },
  {
    "question": "Separate lru for each argument value?",
    "expected_answer": "An LRU cache is simply a mapping that preserves insertion order so the most recently used item can be moved to the front and the least recently used item can be removed when the maximum size of the cache is reached. In both Python 2 and Python 3, such a data structure can be found in the standard library as collections.OrderedDict with the popitem(last=False) method. To more easily initialize an LRU cache for each column name, you can also use collections.defaultdict: from collections import defaultdict, OrderedDict class Client: def __init__(self, cache_size=3): # a cache size of only 3 for demo purposes self.col_cache = defaultdict(OrderedDict) self.cache_size = cache_size def get_col(self, col_name, time): cache = self.col_cache[col_name] try: cache[time] = cache.pop(time) except KeyError: if len(cache) &gt;= self.cache_size: cache.popitem(last=False) cache[time] = '%s at %s' % (col_name, time) # make DB query here print(cache) return cache[time] so that: c = Client() print(c.get_col('foo', 1)) print(c.get_col('foo', 2)) print(c.get_col('foo', 3)) print(c.get_col('foo', 4)) print(c.get_col('foo', 3)) print(c.get_col('foo', 1)) outputs: OrderedDict([(1, 'foo at 1')]) foo at 1 OrderedDict([(1, 'foo at 1'), (2, 'foo at 2')]) foo at 2 OrderedDict([(1, 'foo at 1'), (2, 'foo at 2'), (3, 'foo at 3')]) foo at 3 OrderedDict([(2, 'foo at 2'), (3, 'foo at 3'), (4, 'foo at 4')]) foo at 4 OrderedDict([(2, 'foo at 2'), (4, 'foo at 4'), (3, 'foo at 3')]) foo at 3 OrderedDict([(4, 'foo at 4'), (3, 'foo at 3'), (1, 'foo at 1')]) foo at 1 Demo: Try it online! Note that since Python 3.2, collections.OrderedDict also has a move_to_end method for you to move an item to the front, so you can change the line: cache[time] = cache.pop(time) to simply: cache.move_to_end(time)",
    "context_chunks": [
      {
        "text": "I need to write a sql database column getter, that takes in a column name and time, and returns the entire column of values for that column corresponding to the input time. This may be a frequent function call with the same arguments, so I would like to use an lru cache. However, I'm not sure if the frequency of the column names is uniformly distributed, so ideally, I would have a separate lru cache for each column name. I previously had it like below, but I would like to separate the lru for each col_name. @lru_cache(...) def get_col(self, col_name, time) # do stuff to get the column and return it How can I achieve this? Also, unfortunately, I have to support py2.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "An LRU cache is simply a mapping that preserves insertion order so the most recently used item can be moved to the front and the least recently used item can be removed when the maximum size of the cache is reached. In both Python 2 and Python 3, such a data structure can be found in the standard library as collections.OrderedDict with the popitem(last=False) method. To more easily initialize an LRU cache for each column name, you can also use collections.defaultdict: from collections import defaultdict, OrderedDict class Client: def __init__(self, cache_size=3): # a cache size of only 3 for demo purposes self.col_cache = defaultdict(OrderedDict) self.cache_size = cache_size def get_col(self, col_name, time): cache = self.col_cache[col_name] try: cache[time] = cache.pop(time) except KeyError: if len(cache) &gt;= self.cache_size: cache.popitem(last=False) cache[time] = '%s at %s' % (col_name, time) # make DB query here print(cache) return cache[time] so that: c = Client() print(c.get_col('foo', 1)) print(c.get_col('foo', 2)) print(c.get_col('foo', 3)) print(c.get_col('foo', 4)) print(c.get_col('foo', 3)) print(c.get_col('foo', 1)) outputs: OrderedDict([(1, 'foo at 1')]) foo at 1 OrderedDict([(1, 'foo at 1'), (2, 'foo at 2')]) foo at 2 OrderedDict([(1, 'foo at 1'), (2, 'foo at 2'), (3, 'foo at 3')]) foo at 3 OrderedDict([(2, 'foo at 2'), (3, 'foo at 3'), (4, 'foo at 4')]) foo at 4 OrderedDict([(2, 'foo at 2'), (4, 'foo at 4'), (3, 'foo at 3')]) foo at 3 OrderedDict([(4, 'foo at 4'), (3, 'foo at 3'), (1, 'foo at 1')]) foo at 1 Demo: Try it online! Note that since Python 3.2, collections.OrderedDict also has a move_to_end method for you to move an item to the front, so you can change the line: cache[time] = cache.pop(time) to simply: cache.move_to_end(time)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "We may preserve the login of an existing implementation of lru_cache but organise a mapping of chosen argument values to separate caches within an outer decorator. Here is a sample implementation: from functools import lru_cache, wraps from inspect import getcallargs def lru_agrument_cache(argument_name): def decorator(function): callable_buckets = {} @wraps(function) def wrapper(*args, **kwargs): inspection = getcallargs(function, *args, **kwargs) # should use functools._make_key() for more general use bucket_key = inspection[argument_name] if bucket_key in callable_buckets: return callable_buckets[bucket_key](*args, **kwargs) callable_buckets[bucket_key] = lru_cache(function) return callable_buckets[bucket_key](*args, **kwargs) # just to demonstrate usage def cache_info(argument_value): try: return callable_buckets[argument_value].cache_info() except KeyError: return None wrapper.cache_info = cache_info return wrapper return decorator Usage: @lru_agrument_cache('key') def my_callable(key, times): return key * times Verify: my_callable('A', 2) Out[16]: 'AA' my_callable('A', 2) Out[17]: 'AA' my_callable('A', 3) Out[18]: 'AAA' my_callable('B', 2) Out[19]: 'BB' my_callable.cache_info('A') Out[20]: CacheInfo(hits=1, misses=2, maxsize=128, currsize=2) my_callable.cache_info('B') Out[21]: CacheInfo(hits=0, misses=1, maxsize=128, currsize=1) inspect is also available in Python 2.*; functools.lru_cache is to replace with that lru_cache currently is use; functools.wraps and it's dependency functools.partial if not available, may than be taken from sources. not sure what to add concerning thread safety though;",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-2.x",
        "lru"
      ],
      "question_score": 7,
      "answer_score": 4,
      "created": "2023-09-08T14:41:52",
      "question_id": 77067833,
      "answer_id": 77078935
    }
  },
  {
    "question": "Why does `strftime(&quot;%Y&quot;)` not yield a 4-digit year for dates &lt; 1000 AD in Python&#39;s datetime module on Linux?",
    "expected_answer": "This is caused by the implementation of .strftime() in the C library in Linux omitting any leading zeros from %Y and %G. The related issue in CPython's issue tracker is here. Thanks to jonrsharpe's comment for the answer, and highlighting this section of the documentation: &quot;The full set of format codes supported varies across platforms, because Python calls the platform C library’s strftime() function, and platform variations are common.&quot;",
    "context_chunks": [
      {
        "text": "I am puzzled by an inconsistency when calling .strftime() for dates which are pre-1000 AD, using Python's datetime module. Take the following example: import datetime old_date = datetime.date(year=33, month=3, day=28) # 28th March 33AD old_date.isoformat() &gt;&gt;&gt; &quot;0033-03-28&quot; # Fine! old_date.strftime(&quot;%Y-%m-%d&quot;) &gt;&gt;&gt; &quot;33-03-28&quot; # Woah - where did my leading zeros go? # And even worse datetime.datetime.strptime(old_date.strftime(&quot;%Y-%m-%d&quot;), &quot;%Y-%m-%d&quot;) &gt;&gt;&gt; ... File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;/usr/lib/python3.12/_strptime.py&quot;, line 554, in _strptime_datetime tt, fraction, gmtoff_fraction = _strptime(data_string, format) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/usr/lib/python3.12/_strptime.py&quot;, line 333, in _strptime raise ValueError(&quot;time data %r does not match format %r&quot; % ValueError: time data '33-03-28' does not match format '%Y-%m-%d' The documentation shows examples of %Y yielding zero-padded years. Even using %G, which is documented to be an ISO-8601 4-digit year, is showing only two digits. This caused a problem in an application where a user can enter a date, and if they type in an old date the exception above would arise when trying to convert a date-string back into a date. Presumably there is something in my local configuration which is causing this, as this seems too obvious to be a bug in Python. I'm using Python 3.12 on Ubuntu 24.04.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This is caused by the implementation of .strftime() in the C library in Linux omitting any leading zeros from %Y and %G. The related issue in CPython's issue tracker is here. Thanks to jonrsharpe's comment for the answer, and highlighting this section of the documentation: &quot;The full set of format codes supported varies across platforms, because Python calls the platform C library’s strftime() function, and platform variations are common.&quot;",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This is an issue that stems from Python (and Linux in general) using GNU libc (glibc) on Linix-based OSes. In glibc, if you need a 4 digit year then you can use %4Y. This is true on a Python that uses glibc too. However, this is a non-portable extension to libc, and so will cause an error on systems that do not use glibc. This includes, but is not limited to, Windows. Examples On Linux &gt;&gt;&gt; old_date.strftime(&quot;%4Y-%m-%d&quot;) '0033-03-28' On Windows &gt;&gt;&gt; old_date.strftime(&quot;%4Y-%m-%d&quot;) Traceback (most recent call last): File &quot;&lt;pyshell#6&gt;&quot;, line 1, in &lt;module&gt; old_date.strftime(&quot;%4Y-%m-%d&quot;) ValueError: Invalid format string Portability If you need portability, either because you need to run on multiple platforms, or do not know which platform your code will be running on, then you can do the following: import datetime import platform # using libc_ver() means you are not using the OS as a proxy for which libc # library Python is using. And so your code will work on any OS, regardless # of which libc library has been used. if platform.libc_ver()[0] == 'glibc': # use a glibc-only format option to get a 4 digit year date_format = '%4Y-%m-%d' else: date_format = '%Y-%m-%d' old_date = datetime.date(year=33, month=3, day=28) assert '0033-03-28' == old_date.strftime(date_format)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "python-datetime"
      ],
      "question_score": 7,
      "answer_score": 1,
      "created": "2025-04-23T08:54:05",
      "question_id": 79588208,
      "answer_id": 79588507
    }
  },
  {
    "question": "function vs class vs module vs package vs session for fixture scopes in Pytest",
    "expected_answer": "The fixture's state is stored within the designated &quot;scope&quot;. The term scope refers to the parameter that determines when the fixture will be invoked. If the scope is set to function, the fixture will be invoked separately for each test as a function. Alternatively, if the scope is set to module, the fixture will only be invoked once for the entire current module. For costly requests giving the same results, this feature optimizes the test duration. Fixtures requiring network access depend on connectivity and are usually time-expensive to create. Extending the previous example, we can add a scope=&quot;module&quot; parameter to the @pytest.fixture invocation to cause a smtp_connection fixture function, responsible to create a connection to a preexisting SMTP server, to only be invoked once per test module (the default is to invoke once per test function). Multiple test functions in a test module will thus each receive the same smtp_connection fixture instance, thus saving time. Possible values for scope are: function, class, module, package or session. Source: https://docs.pytest.org/en/6.2.x/fixture.html",
    "context_chunks": [
      {
        "text": "I set 5 fixtures with function, class, module, package and session scopes to test1() as shown below: import pytest @pytest.fixture(scope='function') def fixture_function(): print('function') @pytest.fixture(scope='class') def fixture_class(): print('class') @pytest.fixture(scope='module') def fixture_module(): print('module') @pytest.fixture(scope='package') def fixture_package(): print('package') @pytest.fixture(scope='session') def fixture_session(): print('session') class Test1: def test1( self, fixture_function, fixture_class, fixture_module, fixture_package, fixture_session ): pass Then, I ran the command below: pytest -q -rP Then, each fixture ran once according to the result below: $ pytest -q -rP . [100%] =============== PASSES =============== ____________ Test1.test1 _____________ _______ Captured stdout setup ________ session package module class function 1 passed in 0.10s My questions: What is the difference between function, class, module, package and session for fixture scopes in Pytest? When should I use fixture scopes in Pytest?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "The fixture's state is stored within the designated &quot;scope&quot;. The term scope refers to the parameter that determines when the fixture will be invoked. If the scope is set to function, the fixture will be invoked separately for each test as a function. Alternatively, if the scope is set to module, the fixture will only be invoked once for the entire current module. For costly requests giving the same results, this feature optimizes the test duration. Fixtures requiring network access depend on connectivity and are usually time-expensive to create. Extending the previous example, we can add a scope=&quot;module&quot; parameter to the @pytest.fixture invocation to cause a smtp_connection fixture function, responsible to create a connection to a preexisting SMTP server, to only be invoked once per test module (the default is to invoke once per test function). Multiple test functions in a test module will thus each receive the same smtp_connection fixture instance, thus saving time. Possible values for scope are: function, class, module, package or session. Source: https://docs.pytest.org/en/6.2.x/fixture.html",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "According to pytest documentation: Fixtures are created when first requested by a test, and are destroyed based on their scope. Scopes: function: the default scope, the fixture is destroyed at the end of the test. class: the fixture is destroyed during teardown of the last test in the class. module: the fixture is destroyed during teardown of the last test in the module. package: the fixture is destroyed during teardown of the last test in the package. session: the fixture is destroyed at the end of the test session. When using parametrized tests, the function scoped fixture is executed for every parameter. The default scope is function so you are always using scopes when using fixtures even when not explicitly defining the scope. Fixtures are useful for doing setup and teardown activities so that these activities can be reusable for mode tests. Example: @pytest.fixture(scope=&quot;module&quot;) def my_fixture(): setup_activities() yield teardown_activities() This code block will be executed after every module (after every test file).",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "pytest",
        "fixtures",
        "pytest-fixtures"
      ],
      "question_score": 7,
      "answer_score": 2,
      "created": "2023-08-08T11:51:31",
      "question_id": 76859293,
      "answer_id": 76859452
    }
  },
  {
    "question": "Identifying Correct JAR Versions for S3 Integration with PySpark 3.5",
    "expected_answer": "Receipe: Scala version - -&gt; 2.13 Spark version - -&gt; 3.5.0 hadoop-client version - -&gt; 3.3.4 hadoop-aws - -&gt; 3.3.4 Delta format support too os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-client:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-spark_2.12:3.1.0 pyspark-shell' conf = SparkConf() conf = conf.set(&quot;spark.sql.extensions&quot;, &quot;io.delta.sql.DeltaSparkSessionExtension&quot;) conf = conf.set(&quot;spark.sql.catalog.spark_catalog&quot;, &quot;org.apache.spark.sql.delta.catalog.DeltaCatalog&quot;) conf = conf.set(&quot;spark.executor.extraJavaOptions&quot;, &quot;-Dcom.amazonaws.services.s3.enableV4=true&quot;) conf = conf.set(&quot;spark.driver.extraJavaOptions&quot;, &quot;-Dcom.amazonaws.services.s3.enableV4=true&quot;) conf = conf.setAppName('pyspark_aws_delta').setMaster('local[*]') conf = conf.set('spark.delta.logStore.class','org.apache.spark.sql.delta.storage.S3SingleDriverLogStore') #DEFINE SPARK CONTEXT sc=SparkContext(conf=conf) hadoopConf = sc._jsc.hadoopConfiguration() hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') hadoopConf.set(&quot;fs.AbstractFileSystem.s3a.impl&quot;, &quot;org.apache.hadoop.fs.s3a.S3A&quot;) hadoopConf.set('fs.s3.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') #define buckets hadoopConf.set('fs.s3a.bucket.{}.access.key'.format(BUCKET_NAME), ACCESS_KEY) hadoopConf.set('fs.s3a.bucket.{}.secret.key'.format(BUCKET_NAME), SECRET) # DEFINE SPARK SESSION spark=SparkSession(sc) references: https://medium.com/@ramachandrankrish/integrating-org-apache-hadoop-fs-s3a-s3afilesystem-to-access-the-aws-s3-bucket-via-spark-java-3744ffadb60d",
    "context_chunks": [
      {
        "text": "I am attempting to set up a PySpark environment to read data from S3 using PySpark 3.5 in a Conda environment (Python 3.12). However, I am facing difficulties in identifying the correct versions of the aws-java-sdk and hadoop-aws JARs to use (needed for reading data from S3 directly). Here are the steps I have followed and the issues I have encountered: conda create --name spark_env conda activate spark_env pip install pyspark Then in my Jupyter notebook (That is using the previously created environment), I'm creating the SparkSession: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(&quot;Read from S3&quot;) \\ .config(&quot;spark.hadoop.fs.s3a.access.key&quot;, &quot;AWS_ACCESS_KEY&quot;) \\ .config(&quot;spark.hadoop.fs.s3a.secret.key&quot;, &quot;AWS_SECRET_KEY&quot;) \\ .config(&quot;spark.hadoop.fs.s3a.impl&quot;, &quot;org.apache.hadoop.fs.s3a.S3AFileSystem&quot;) \\ .getOrCreate() s3_path = &quot;s3a://bucket/file.json&quot; df = spark.read.json(s3_path) df.show() What gaves me the following exception: &quot;java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found&quot; That's why I have tried to download different versions of hadoop-aws aws-java-sdk But after adding these jars to the /jars path that Pyspark is using (I have also tried specifying manually the path to these jars), I keep getting exceptions like: &quot;Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics&quot; or &quot;&quot;&quot; Py4JJavaError: An error occurred while calling o34.json. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 10001) (YYY executor driver): java.lang.NoSuchMethodError: org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(Lorg/apache/hadoop/fs/statistics/DurationTracker;Lorg/apache/hadoop/util/functional/CallableRaisingIOE;)Ljava/lang/Object; or &quot;java.lang.ClassNotFoundException: com.amazonaws.services.s3.model.MultiObjectDeleteException&quot; Can someone guide me on how to identify the correct versions of the aws-java-sdk and hadoop-aws JARs to use with PySpark 3.5 for S3 integration, or provide a reliable source where this information is available? Additionally, if there's a different or more recommended approach to reading data from S3 using PySpark 3.5, I'm open to suggestions. Thanks in advance.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Receipe: Scala version - -&gt; 2.13 Spark version - -&gt; 3.5.0 hadoop-client version - -&gt; 3.3.4 hadoop-aws - -&gt; 3.3.4 Delta format support too os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-client:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-spark_2.12:3.1.0 pyspark-shell' conf = SparkConf() conf = conf.set(&quot;spark.sql.extensions&quot;, &quot;io.delta.sql.DeltaSparkSessionExtension&quot;) conf = conf.set(&quot;spark.sql.catalog.spark_catalog&quot;, &quot;org.apache.spark.sql.delta.catalog.DeltaCatalog&quot;) conf = conf.set(&quot;spark.executor.extraJavaOptions&quot;, &quot;-Dcom.amazonaws.services.s3.enableV4=true&quot;) conf = conf.set(&quot;spark.driver.extraJavaOptions&quot;, &quot;-Dcom.amazonaws.services.s3.enableV4=true&quot;) conf = conf.setAppName('pyspark_aws_delta').setMaster('local[*]') conf = conf.set('spark.delta.logStore.class','org.apache.spark.sql.delta.storage.S3SingleDriverLogStore') #DEFINE SPARK CONTEXT sc=SparkContext(conf=conf) hadoopConf = sc._jsc.hadoopConfiguration() hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') hadoopConf.set(&quot;fs.AbstractFileSystem.s3a.impl&quot;, &quot;org.apache.hadoop.fs.s3a.S3A&quot;) hadoopConf.set('fs.s3.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') #define buckets hadoopConf.set('fs.s3a.bucket.{}.access.key'.format(BUCKET_NAME), ACCESS_KEY) hadoopConf.set('fs.s3a.bucket.{}.secret.key'.format(BUCKET_NAME), SECRET) # DEFINE SPARK SESSION spark=SparkSession(sc) references: https://medium.com/@ramachandrankrish/integrating-org-apache-hadoop-fs-s3a-s3afilesystem-to-access-the-aws-s3-bucket-via-spark-java-3744ffadb60d",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I was facing the same issue and after a lot of search I found a solution and document it in this simple repo https://github.com/sayedabdallah/Read-Write-AWS-S3 In short words you need two jars hadoop-aws-3.3.4 and aws-java-sdk-bundle-1.12.767.jar the versions depends on spark version, you can find which hadoop version from POM.xml for your spark version. this define version for hadoop-aws jar, and in the mvn page for this jar you can find the correct version for aws-java-sdk-bundle in the Compile Dependencies section",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "amazon-web-services",
        "amazon-s3",
        "pyspark"
      ],
      "question_score": 7,
      "answer_score": 1,
      "created": "2023-10-05T13:24:32",
      "question_id": 77237544,
      "answer_id": 78155159
    }
  },
  {
    "question": "How to configure python-lsp-server in NvChad",
    "expected_answer": "Here is the common pattern on configuring lspconfig on NvChad. [Here][1] is the link of documentation on NvChad about setting up the lspconfig. If we take a look step by step, first you need to have this code -- In order to modify the `lspconfig` configuration: { &quot;neovim/nvim-lspconfig&quot;, config = function() require &quot;plugins.configs.lspconfig&quot; require &quot;custom.configs.lspconfig&quot; end, }, inside of your &gt; ~/.config/nvim/lua/custom/plugins.lua file. Second, you have to install the lsp server via mason. Here is how would an example custom/plugins.lua would look like with the previous two steps: local plugins = { { &quot;neovim/nvim-lspconfig&quot;, config = function() require &quot;plugins.configs.lspconfig&quot; require &quot;custom.configs.lspconfig&quot; end, }, { &quot;williamboman/mason.nvim&quot;, opts = { ensure_installed = { &quot;lua-language-server&quot;, &quot;html-lsp&quot;, &quot;prettier&quot;, &quot;stylua&quot;, &quot;gopls&quot; }, }, } } return plugins After you included the mason code as well, you could call MasonInstallAll while in the command mode, :MasonInstallAll Third, we have to configure the lspconfig. Consider custom/plugins.lua as where we declare the plugins we desire to configure. It is recommended to create a file for each plugin configuration. You could do your ~/.config/nvim/lua/custom/configs/luaconfig.lua Here is how the configuration would look like according to the document. local on_attach = require(&quot;plugins.configs.lspconfig&quot;).on_attach local capabilities = require(&quot;plugins.configs.lspconfig&quot;).capabilities local lspconfig = require &quot;lspconfig&quot; local servers = { &quot;gopls&quot;--[[ , &quot;html&quot;, &quot;css&quot; ]]} for _, lsp in ipairs(servers) do lspconfig[lsp].setup { on_attach = on_attach, capabilities = capabilities, } end In this file, you could also configure them according to your liking. Every lsp server have some configurations they would recommend etc. For example here is how gopls lspconfig setup looks like: lspconfig.gopls.setup({ on_attach = on_attach, capabilities = capabilities, settings = { gopls = { analyses = { unusedparams = true, }, staticcheck = true, gofumpt = true, }, }, })",
    "context_chunks": [
      {
        "text": "I have been using the NvChad configuration for neovim for a while after switching from VSCode. As I am new to configuring vim I took help from an youtube video. [This is the video my setup is based upon: ] (https://youtu.be/4BnVeOUeZxc?si=Xatala9e71x0eRtC). But as of now null-ls is archived and some functionalities does not work properly. I have pylsp installed through mason, but I don't know how to configure it and I need some help with that. I have tried to setup the lsp server in my nvim/lua/custom/config/lspconfig.lua file but it does not seem to work properly.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Here is the common pattern on configuring lspconfig on NvChad. [Here][1] is the link of documentation on NvChad about setting up the lspconfig. If we take a look step by step, first you need to have this code -- In order to modify the `lspconfig` configuration: { &quot;neovim/nvim-lspconfig&quot;, config = function() require &quot;plugins.configs.lspconfig&quot; require &quot;custom.configs.lspconfig&quot; end, }, inside of your &gt; ~/.config/nvim/lua/custom/plugins.lua file. Second, you have to install the lsp server via mason. Here is how would an example custom/plugins.lua would look like with the previous two steps: local plugins = { { &quot;neovim/nvim-lspconfig&quot;, config = function() require &quot;plugins.configs.lspconfig&quot; require &quot;custom.configs.lspconfig&quot; end, }, { &quot;williamboman/mason.nvim&quot;, opts = { ensure_installed = { &quot;lua-language-server&quot;, &quot;html-lsp&quot;, &quot;prettier&quot;, &quot;stylua&quot;, &quot;gopls&quot; }, }, } } return plugins After you included the mason code as well, you could call MasonInstallAll while in the command mode, :MasonInstallAll Third, we have to configure the lspconfig. Consider custom/plugins.lua as where we declare the plugins we desire to configure. It is recommended to create a file for each plugin configuration. You could do your ~/.config/nvim/lua/custom/configs/luaconfig.lua Here is how the configuration would look like according to the document. local on_attach = require(&quot;plugins.configs.lspconfig&quot;).on_attach local capabilities = require(&quot;plugins.configs.lspconfig&quot;).capabilities local lspconfig = require &quot;lspconfig&quot; local servers = { &quot;gopls&quot;--[[ , &quot;html&quot;, &quot;css&quot; ]]} for _, lsp in ipairs(servers) do lspconfig[lsp].setup { on_attach = on_attach, capabilities = capabilities, } end In this file, you could also configure them according to your liking. Every lsp server have some configurations they would recommend etc. For example here is how gopls lspconfig setup looks like: lspconfig.gopls.setup({ on_attach = on_attach, capabilities = capabilities, settings = { gopls = { analyses = { unusedparams = true, }, staticcheck = true, gofumpt = true, }, }, })",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "navigate to .config/nvim/lua/custom/configs/lspconfig.lua and find the line that says local servers = {...} in that array just add the string &quot;pylsp&quot;",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "neovim",
        "nvim-lspconfig"
      ],
      "question_score": 7,
      "answer_score": 3,
      "created": "2023-09-06T13:53:23",
      "question_id": 77052587,
      "answer_id": 77879373
    }
  },
  {
    "question": "seleniumbase (undetected Chrome driver): how to set request header?",
    "expected_answer": "I have finally found a simple and extraordinarily effective solution, working correctly with uc=True, via javascript as provided here: https://github.com/ultrafunkamsterdam/undetected-chromedriver/issues/871. Code fragments: from seleniumbase import Driver driver = Driver(uc=True) login() response = driver.execute_async_script(&quot;var callback = arguments[arguments.length - 1]; fetch('&quot; + url + &quot;', {method: 'GET', headers: {'Accept' : 'application/json'}}).then((response) =&gt; response.text().then((text) =&gt; callback({'status': response.status, 'text': text})))&quot;) print(url + ':' + str(response['status'])) if response['status'] == 200: with io.open(outfile, 'w', encoding='utf8', newline='\\n') as f: f.write(response['text']) return response['status'] This works very well for my specific use case, which just involves invoking an API via Get and getting JSON content back (all repeated over and over again). This also allows me to get the response status, which has made the whole thing much more resilient. Finally, the performance is fantastic - not surprisingly, I guess, given the much shorter code path.",
    "context_chunks": [
      {
        "text": "I am using seleniumbase with Driver(uc=True), which works well for my specific scraping use case (and appears to be the only driver that consistently remains undetected for me). It is fine for everything that doesn't need specific header settings. For one particular type of scrape I need to set the Request Header (Accept -&gt; application/json). This works fine, and consistently, done manually in Chrome via the Requestly extension, but I cannot work out how to put it in place for seleniumbase undetected Chrome. I tried using execute_cdp_cmd with Network.setExtraHTTPHeaders (with Network.enable first): this ran without error but the request appeared to ignore it. (I was, tbh, unconvinced that the uc=True support was handling this functionality properly, since it doesn't appear to have full Chromium driver capabilities.) Requestly has a selenium Python mechanism, but this has its own driver and I cannot see how it would integrate with seleniumbase undetected Chrome. The built-in seleniumbase wire=True support won't coexist with uc=True, as far as I can see. selenium-requests has an option to piggyback on an existing driver, but this is (to be honest) beyond my embryonic Python skills (though it does feel like this might be the answer if I knew how to put it in place). My scraping requires initial login, so I can't really swap from one driver to another in the course of the scraping session.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I have finally found a simple and extraordinarily effective solution, working correctly with uc=True, via javascript as provided here: https://github.com/ultrafunkamsterdam/undetected-chromedriver/issues/871. Code fragments: from seleniumbase import Driver driver = Driver(uc=True) login() response = driver.execute_async_script(&quot;var callback = arguments[arguments.length - 1]; fetch('&quot; + url + &quot;', {method: 'GET', headers: {'Accept' : 'application/json'}}).then((response) =&gt; response.text().then((text) =&gt; callback({'status': response.status, 'text': text})))&quot;) print(url + ':' + str(response['status'])) if response['status'] == 200: with io.open(outfile, 'w', encoding='utf8', newline='\\n') as f: f.write(response['text']) return response['status'] This works very well for my specific use case, which just involves invoking an API via Get and getting JSON content back (all repeated over and over again). This also allows me to get the response status, which has made the whole thing much more resilient. Finally, the performance is fantastic - not surprisingly, I guess, given the much shorter code path.",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "My code fragments from second effective solution derived from now deleted bountied answer (the .v2 was the piece I had not seen previously and which I think is what made it work): ... from seleniumwire import webdriver from selenium.webdriver.chrome.options import Options from seleniumwire.undetected_chromedriver.v2 import Chrome, ChromeOptions ... chrome_options = ChromeOptions() driver = Chrome(seleniumwire_options={'options': chrome_options}) driver.header_overrides = { 'Accept': 'application/json', } ...",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "selenium-webdriver",
        "http-headers",
        "seleniumbase"
      ],
      "question_score": 7,
      "answer_score": 2,
      "created": "2023-11-16T11:48:38",
      "question_id": 77494543,
      "answer_id": 77517566
    }
  },
  {
    "question": "Clear all the GPU memory used by pytorch in current python code without exiting python",
    "expected_answer": "Its hard to determine what is causing the memory issue without actually reading the code. But most of the time when empty_cache() can't finish the cleaning is because of some process still running. So, try adding this after empty_cache() import gc gc.collect()",
    "context_chunks": [
      {
        "text": "I am running a modified version of a third-party code which uses pytorch and GPU. I run the same model multiple times by varying the configs, which I am doing within python i.e. I have a wrapper python file which calls the model with different configs. But I am getting out-of-memory errors while running the second or third model. That is to say, the model can run once properly without any memory issues. So, if I end the code after running the first model and then start the second model afresh, the code works fine. However, if I chain the models within python, I'm running into out-of-memory issues. I suspect there are some memory leaks within the third-party code. On googling, I found two suggestions. One is to call torch.cuda.empty_cache(), and the other is to delete the tensors explicitly using del tensor_name. However, empty_cache() command isn't helping free the entire memory, and the third-party code has too many tensors for me to delete all the tensors individually. Is there any way to clear the entire GPU memory used by the current python program within the python code itself?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Its hard to determine what is causing the memory issue without actually reading the code. But most of the time when empty_cache() can't finish the cleaning is because of some process still running. So, try adding this after empty_cache() import gc gc.collect()",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Garbage collector and del directly on the model and training data rarely worked for me when using a model that's within a loop. Usually, each iteration creates a new model without clearing the previous model from memory, making it so the entire loop requires (model_size + training data) * n amount of memory capacity, where n is the number of iterations. This is also a problem when using federated learning tools such as Flower or when using k-fold cross validation. If you want to use a multiprocessing approach which should always work to clear GPU memory used by the child process, this will work: import torch import torch.multiprocessing as mp from torch.multiprocessing import Pool, Manager class nn_model(torch.nn.Module) # define class for model def train_fn(model, train_ds, val_ds, queue) # do training stuff history = # save training metrics into history dictionary queue.put(model, history) if __name__ == &quot;__main__&quot;: train_ds = # load train dataset val_ds = # load val dataset test_ds = # load test dataset model = nn_model(*args) pool = Pool(1) # or replace 1 with however many child processes you want queue = Manager().Queue() # assume training loop of three iterations [pool.apply_async(train_fn, args=(model, train_ds, val_ds, queue)) for _ in range(3)] pool.close() pool.join() # this next line will give a variable queue_results containing a tuple of (model, history) queue_results = [queue.get() for _ in range(queue.qsize())] # assuming queue_results contains only one element model, history = queue_results[0] for i, j in test_ds: with torch.no_grad(): model = model.eval() preds = model(i) If you don't want to use Pool and want to explicitly kill the child process, you can use (instead of Pool): from torch.multiprocessing import Process if __name__ == &quot;__main__&quot;: ... # inside &quot;_main__&quot; replacing Pool, assuming three iterations procs = [mp.Process(target=train_fn, args=(model, train_ds, val_ds, queue)) for _ in range(3)] for p in procs: p.start() p.join() queue_results = [queue.get() for _ in range(queue.qsize())] for p in procs: try: p.close() except ValueError as e: print(f&quot;Couldn't close process: {e}&quot;) del p # assuming queue_results contains only one element model, history = queue_results[0] for i, j in test_ds: with torch.no_grad(): model = model.eval() preds = model(i) I think using Pool is more convenient than Process.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "memory-management",
        "pytorch",
        "memory-leaks",
        "out-of-memory"
      ],
      "question_score": 7,
      "answer_score": 1,
      "created": "2023-11-03T08:50:12",
      "question_id": 77415274,
      "answer_id": 77416015
    }
  },
  {
    "question": "ImportError: cannot import name &#39;Tensor&#39; from &#39;torch&#39; (unknown location)",
    "expected_answer": "Reinstalling torch using the following command (from here) worked for me: (venv) $ pip install --upgrade --no-deps --force-reinstall torch I issued the above command from within my venv, activated via source /path/to/venv/bin/activate. Then, things work again: (venv) $ cat hello.py from torch import Tensor print(&quot;Hello, World!&quot;) (venv) $ python hello.py Hello, World",
    "context_chunks": [
      {
        "text": "I’m trying to import Tensor from PyTorch: from torch import Tensor but I keep getting this error: ImportError: cannot import name 'Tensor' from 'torch' (unknown location) What I’ve Tried: Checked that PyTorch is installed (pip show torch), and I’m using version 2.5.1. Reinstalled PyTorch: pip uninstall torch pip install torch Tested the import in a Python shell, but the error persists. Environment: Python version: 3.10 PyTorch version: 2.5.1 OS: Windows 10 Virtual environment: Yes How can I fix this issue?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Reinstalling torch using the following command (from here) worked for me: (venv) $ pip install --upgrade --no-deps --force-reinstall torch I issued the above command from within my venv, activated via source /path/to/venv/bin/activate. Then, things work again: (venv) $ cat hello.py from torch import Tensor print(&quot;Hello, World!&quot;) (venv) $ python hello.py Hello, World",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "Just use the uv or arkalos instead of pip and manually setting up your project. Create a new python project with uv Inside your projects folder: uv init &lt;project&gt; cd &lt;project&gt; And then to add any packages: uv add &lt;pip package&gt; Then use uv to run scripts uv run scripts/my_script.py Or use Jupyter notebooks: https://arkalos.com/docs/notebooks/ https://code.visualstudio.com/docs/datascience/data-science-tutorial",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "machine-learning",
        "pytorch"
      ],
      "question_score": 7,
      "answer_score": 0,
      "created": "2025-01-18T13:04:35",
      "question_id": 79367182,
      "answer_id": 79541971
    }
  },
  {
    "question": "Python method overriding - more specific arguments in derived than base class",
    "expected_answer": "Turns out this can be solved using generics: from abc import ABC, abstractmethod from typing import Generic, Sequence, TypeVar ParagraphType = TypeVar(&quot;ParagraphType&quot;, bound=&quot;Paragraph&quot;) class Document(ABC, Generic[ParagraphType]): @classmethod @abstractmethod def from_paragraphs(cls, paragraphs: Sequence[ParagraphType]): pass class LegalDocument(Document[&quot;LegalParagraph&quot;]): @classmethod def from_paragraphs(cls, paragraphs): return # some logic here... class AcademicDocument(Document[&quot;AcademicParagraph&quot;]): @classmethod def from_paragraphs(cls, paragraphs): return # some logic here... class Paragraph: text: str class LegalParagraph(Paragraph): pass class AcademicParagraph(Paragraph): pass Saying bound=&quot;Paragraph&quot; guarantees that the ParagraphType represents a (subclass of) Paragraph, but the derived classes are not expected to implement from_paragraphs for all paragraph types, just for the one they choose. The type checker also automatically figures out the type of the argument paragraphs for LegalDocument.from_paragraphs, saving me some work :)",
    "context_chunks": [
      {
        "text": "Let's say I want to create an abstract base class called Document. I want the type checker to guarantee that all its subclasses implement a class method called from_paragraphs, which constructs a document from a sequence of Paragraph objects. However, a LegalDocument should only be constructable from LegalParagraph objects, and an AcademicDocument - only from AcademicParagraph objects. My instinct is to do it like so: from abc import ABC, abstractmethod from typing import Sequence class Document(ABC): @classmethod @abstractmethod def from_paragraphs(cls, paragraphs: Sequence[&quot;Paragraph&quot;]): pass class LegalDocument(Document): @classmethod def from_paragraphs(cls, paragraphs: Sequence[&quot;LegalParagraph&quot;]): return # some logic here... class AcademicDocument(Document): @classmethod def from_paragraphs(cls, paragraphs: Sequence[&quot;AcademicParagraph&quot;]): return # some logic here... class Paragraph: text: str class LegalParagraph(Paragraph): pass class AcademicParagraph(Paragraph): pass However, Pyright complains about this because from_paragraphs on the derived classes violates the Liskov substitution principle. How do I make sure that each derived class implements from_paragraphs for some kind of Paragraph?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "Turns out this can be solved using generics: from abc import ABC, abstractmethod from typing import Generic, Sequence, TypeVar ParagraphType = TypeVar(&quot;ParagraphType&quot;, bound=&quot;Paragraph&quot;) class Document(ABC, Generic[ParagraphType]): @classmethod @abstractmethod def from_paragraphs(cls, paragraphs: Sequence[ParagraphType]): pass class LegalDocument(Document[&quot;LegalParagraph&quot;]): @classmethod def from_paragraphs(cls, paragraphs): return # some logic here... class AcademicDocument(Document[&quot;AcademicParagraph&quot;]): @classmethod def from_paragraphs(cls, paragraphs): return # some logic here... class Paragraph: text: str class LegalParagraph(Paragraph): pass class AcademicParagraph(Paragraph): pass Saying bound=&quot;Paragraph&quot; guarantees that the ParagraphType represents a (subclass of) Paragraph, but the derived classes are not expected to implement from_paragraphs for all paragraph types, just for the one they choose. The type checker also automatically figures out the type of the argument paragraphs for LegalDocument.from_paragraphs, saving me some work :)",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "This pattern is called factory pattern: depends of the input, you get different types of object. What you have there will not work because: # Because Document should not have knowledge of derived class: doc = Document.from_paragraphs(...) # Because type mismatch doc = LegalDocument.from_paragraphs([AcademicParagraphs()]) Here is how I approach this problem: class Document: def __init__(self, paragraphs): self.paragraphs = paragraphs class LegalDocument(Document): pass class AcademicDocument(Document): pass class Paragraph: def __init__(self, text): self.text = text class LegalParagraph(Paragraph): pass class AcademicParagraph(Paragraph): pass def create_document(*paragraphs): # assume that all paragraphs are of the same type if isinstance(paragraphs[0], LegalParagraph): klass = LegalDocument elif isinstance(paragraphs[0], AcademicParagraph): klass = AcademicDocument else: raise TypeError() return klass(paragraphs) d1 = create_document(LegalParagraph(&quot;foo&quot;), LegalParagraph(&quot;bar&quot;)) assert isinstance(d1, LegalDocument) d2 = create_document(AcademicParagraph(&quot;moo&quot;)) assert isinstance(d2, AcademicDocument) Notes I will have a simple set of classes, not messing around with ABC No class methods, the __ini__() will be enough I have a single factory function create_document, which will create a document where the type depends on the input A different approach is to tweak Document.__new__() method, but that requires some knowledge of how __new__() works and not everybody knows that.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "inheritance",
        "overriding",
        "python-typing"
      ],
      "question_score": 7,
      "answer_score": 1,
      "created": "2024-01-26T18:48:39",
      "question_id": 77888435,
      "answer_id": 77893007
    }
  },
  {
    "question": "Why my anaconda keeps showing &quot;Error while loading conda entry point&quot;?",
    "expected_answer": "This solved the issue for me: python -m pip uninstall anaconda-cloud-auth conda update conda conda install “anaconda-cloud-auth&gt;0.5.0”",
    "context_chunks": [
      {
        "text": "I tried to install 'satpy' on anaconda, shows: conda install satpy Error while loading conda entry point: anaconda-cloud-auth (cannot import name 'ChannelAuthBase' from 'conda.plugins.types' (D:\\Anaconda\\Lib\\site-packages\\conda\\plugins\\types.py)) Error while loading conda entry point: anaconda-cloud-auth (cannot import name 'ChannelAuthBase' from 'conda.plugins.types' (D:\\Anaconda\\Lib\\site-packages\\conda\\plugins\\types.py)) and this error seems to accur in recent days,hope u guys can solve this(a possible way:uninstall anaconda &amp; reinstall,i find an answer on Anaconda Community,someone fixed this,but I don't wanna reinstall it...) Same as above: I tried to switch another env, update conda, nothing works",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This solved the issue for me: python -m pip uninstall anaconda-cloud-auth conda update conda conda install “anaconda-cloud-auth&gt;0.5.0”",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "I encountered the same error after trying to install Tensorflow and updating conda using the command. Unfortunately, I tried various methods, but only ended up damaging the existing virtual environments.. I resolved it by reinstalling. I reinstalled Anaconda using an older installation file. Specially the version from 2023.09-0 (I'm not entirely sure, but I believe Updating conda was the problem)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "anaconda",
        "conda"
      ],
      "question_score": 7,
      "answer_score": 4,
      "created": "2024-04-18T13:24:37",
      "question_id": 78347799,
      "answer_id": 78473121
    }
  },
  {
    "question": "How to migrate from a simple Python project : requirements.txt setup.py (setuptools), to a uv project",
    "expected_answer": "This might not be the best solution, but here is what I did. I will accept any better solution as an answer! Initialize uv Being at the root of my_project with the command line, run: $ uv init delete the hello.py file that has been created at the root. Modify the info of the pyproject.toml, you probably should include the info present in your setup.py (description, authors and so on). delete your setup.py file. Add dependencies $ uv add -r requirements.in (or requirements.txt) Install library $ uv pip install -e . Modify the Makefile: Makefile: install_library: uv sync # pip install -r uv pip install -e . # replace pip install -e . requirements.in run_a_script: uv run python ./my_lib/my_script.py # replace python ./my_lib/my_script.py launch_test: uv run pytest -n auto --cov-report=xml # replace uv run pytest -n auto --cov-report=xml Modify a potential CI (example for github action) jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 with: lfs: true - name: Install uv uses: astral-sh/setup-uv@v3 - name: &quot;Set up Python&quot; uses: actions/setup-python@v5 - name: Set up Python environment run: | make install_library - name: Test with pytest run: | make launch_test Deploy on Pypi Deployment does not need to use the --lib/src layout, the app layout allows to deploy a library without issues. $ uv build $ uv publish then you shoud give token as username, and the token from Pypi to publish and make your library installable with pip install my_project. Edit: Setup.py with dependencies and no requirements.in/requirements.txt with pyproject.toml. Some projects (here a dagster university tutorials) may have a setup.py without a requirement.txt and with a pyproject.toml containing: [build-system] requires = [&quot;setuptools&quot;] build-backend = &quot;setuptools.build_meta&quot; [tool.dagster] module_name = &quot;dagster_university&quot; and the requirements.txt containing: from setuptools import find_packages, setup setup( name=&quot;dagster_university&quot;, packages=find_packages(exclude=[&quot;dagster_university_tests&quot;]), install_requires=[ &quot;dagster==1.9.*&quot;, &quot;pandas[parquet]&quot;, &quot;smart_open[s3]&quot;, &quot;pyarrow&quot;, ], extras_require={&quot;dev&quot;: [&quot;pytest&quot;]}, ) To migrate such a project, you can delete the old pyproject.toml, use the new one generated by uv, and paste: dependencies = [ &quot;dagster&gt;=1.9.*&quot;, &quot;pandas[parquet]&quot;, &quot;smart_open[s3]&quot;, &quot;pyarrow&quot;, ] [project.optional-dependencies] dev = [ &quot;pytest&quot; ] Than run uv sync ( and sync --extra dev if you want to install the extra dev dependencies).",
    "context_chunks": [
      {
        "text": "If I want to use uv in an already existing project named my_project with only a requirements.txt (or requirements.in), and simple setup.py(setuptools), that has been installed with pip install -e .. How would I switch from setuptools to uv? If my project has this file configuration: my_project ├── my_project │ └── hello.py ├── tests │ └── test_hello.py ├── Makefile ├── README.md ├── requirements.txt └── setup.py The uv documentation offer two type of projects organisation, the app (uv init --app example-app) that correspond to the one I have, and the library one uv init --lib example-lib, which is based on a src layout organisation. uv allows to publish a library directly on Pypi, do I need to change my layout for a src one if I want to deploy library with uv?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "This might not be the best solution, but here is what I did. I will accept any better solution as an answer! Initialize uv Being at the root of my_project with the command line, run: $ uv init delete the hello.py file that has been created at the root. Modify the info of the pyproject.toml, you probably should include the info present in your setup.py (description, authors and so on). delete your setup.py file. Add dependencies $ uv add -r requirements.in (or requirements.txt) Install library $ uv pip install -e . Modify the Makefile: Makefile: install_library: uv sync # pip install -r uv pip install -e . # replace pip install -e . requirements.in run_a_script: uv run python ./my_lib/my_script.py # replace python ./my_lib/my_script.py launch_test: uv run pytest -n auto --cov-report=xml # replace uv run pytest -n auto --cov-report=xml Modify a potential CI (example for github action) jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 with: lfs: true - name: Install uv uses: astral-sh/setup-uv@v3 - name: &quot;Set up Python&quot; uses: actions/setup-python@v5 - name: Set up Python environment run: | make install_library - name: Test with pytest run: | make launch_test Deploy on Pypi Deployment does not need to use the --lib/src layout, the app layout allows to deploy a library without issues. $ uv build $ uv publish then you shoud give token as username, and the token from Pypi to publish and make your library installable with pip install my_project. Edit: Setup.py with dependencies and no requirements.in/requirements.txt with pyproject.toml. Some projects (here a dagster university tutorials) may have a setup.py without a requirement.txt and with a pyproject.toml containing: [build-system] requires = [&quot;setuptools&quot;] build-backend = &quot;setuptools.build_meta&quot; [tool.dagster] module_name = &quot;dagster_university&quot; and the requirements.txt containing: from setuptools import find_packages, setup setup( name=&quot;dagster_university&quot;, packages=find_packages(exclude=[&quot;dagster_university_tests&quot;]), install_requires=[ &quot;dagster==1.9.*&quot;, &quot;pandas[parquet]&quot;, &quot;smart_open[s3]&quot;, &quot;pyarrow&quot;, ], extras_require={&quot;dev&quot;: [&quot;pytest&quot;]}, ) To migrate such a project, you can delete the old pyproject.toml, use the new one generated by uv, and paste: dependencies = [ &quot;dagster&gt;=1.9.*&quot;, &quot;pandas[parquet]&quot;, &quot;smart_open[s3]&quot;, &quot;pyarrow&quot;, ] [project.optional-dependencies] dev = [ &quot;pytest&quot; ] Than run uv sync ( and sync --extra dev if you want to install the extra dev dependencies).",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "While not yet fully setuptools compatible, migrate-to-uv might be worth trying. At the root of your project, just run uvx migrate-to-uv A pyproject.toml file will be generated with all dependencies from the requirements.txt file. All different CI workflows would still have to be reworked of course. At the time of writing, there is an open issue about setuptools support on migrate-to-uv's side.",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "setuptools",
        "packaging",
        "requirements.txt",
        "uv"
      ],
      "question_score": 6,
      "answer_score": 13,
      "created": "2024-11-04T08:18:08",
      "question_id": 79154674,
      "answer_id": 79155060
    }
  },
  {
    "question": "What exactly is slowing np.sum down?",
    "expected_answer": "When I run np.sum(a) in debug mode on my PC, it steps into the following code. https://github.com/numpy/numpy/blob/v1.26.5/numpy/core/fromnumeric.py#L2178 The following is the part of the code where it is relevant. import numpy as np import types def _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs): passkwargs = {k: v for k, v in kwargs.items() if v is not np._NoValue} if type(obj) is not np.ndarray: raise NotImplementedError return ufunc.reduce(obj, axis, dtype, out, **passkwargs) def copied_np_sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, initial=np._NoValue, where=np._NoValue): if isinstance(a, types.GeneratorType): raise NotImplementedError return _wrapreduction( a, np.add, 'sum', axis, dtype, out, keepdims=keepdims, initial=initial, where=where ) Note that this ends up calling np.add.reduce(a). Benchmark: import timeit def benchmark(setup, stmt, repeat, number): print(f&quot;{stmt:16}: {min(timeit.repeat(setup=setup, stmt=stmt, globals=globals(), repeat=repeat, number=number)) / number}&quot;) n_item = 10 ** 3 n_loop = 1000 n_set = 1000 data_setup = f&quot;&quot;&quot;\\ import numpy as np rng = np.random.default_rng(0) a = rng.random({n_item}) &quot;&quot;&quot; benchmark(setup=data_setup, stmt=&quot;np.sum(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;a.sum()&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;copied_np_sum(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;np.add.reduce(a)&quot;, repeat=n_set, number=n_loop) np.sum(a) : 2.6407251134514808e-06 a.sum() : 1.3474803417921066e-06 copied_np_sum(a): 2.50667380169034e-06 np.add.reduce(a): 1.195137854665518e-06 As you can see, copied_np_sum performs similarly to np.sum, and np.add.reduce is similar to a.sum. So the majority of the difference between np.sum and a.sum is likely due to what copied_np_sum does before calling np.add.reduce. In other words, it's the overhead caused by the dict comprehension and the additional function calls. However, although there is a significant difference in the above benchmark that reproduces the OP's one, as pointed out in the comment, this may be overstated. Because timeit repeatedly executes the code and uses the (best of) average, with a small array like in this benchmark, the array may already be in the CPU cache when it is measured. This is not necessarily an unfair condition. The same thing could happen in actual use. Rather, it should be so whenever possible. That being said, for a canonical answer, we should measure it. Based on @user3666197 advice, we can create a large array immediately after creating a to evicts a from the cache. Note that I decided to use np.arange here, which I confirmed has the same effect but runs faster. import timeit def benchmark(setup, stmt, repeat, number): print(f&quot;{stmt:16}: {min(timeit.repeat(setup=setup, stmt=stmt, globals=globals(), repeat=repeat, number=number)) / number}&quot;) n_item = 10 ** 3 n_loop = 1 n_set = 100 data_setup = f&quot;&quot;&quot;\\ import numpy as np rng = np.random.default_rng(0) a = rng.random({n_item}) _ = np.arange(10 ** 9, dtype=np.uint8) # To evict `a` from the CPU cache. &quot;&quot;&quot; benchmark(setup=data_setup, stmt=&quot;np.sum(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;a.sum()&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;copied_np_sum(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;np.add.reduce(a)&quot;, repeat=n_set, number=n_loop) Without eviction (With cache): np.sum(a) : 2.6407251134514808e-06 a.sum() : 1.3474803417921066e-06 copied_np_sum(a): 2.50667380169034e-06 np.add.reduce(a): 1.195137854665518e-06 With eviction (Without cache): np.sum(a) : 4.916824400424957e-05 a.sum() : 3.245798870921135e-05 copied_np_sum(a): 4.7205016016960144e-05 np.add.reduce(a): 3.0195806175470352e-05 Naturally, the presence or absence of cache makes a huge impact on performance. However, although the difference has become smaller, it can still be said to be a significant difference. Also, since these four relationships remain the same as before, the conclusion also remains the same. There are a few things I should add. Note1 The claim regarding method loading is incorrect. benchmark(setup=f&quot;{data_setup}f = np.sum&quot;, stmt=&quot;f(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=f&quot;{data_setup}f = a.sum&quot;, stmt=&quot;f()&quot;, repeat=n_set, number=n_loop) np.sum(a) : 4.916824400424957e-05 a.sum() : 3.245798870921135e-05 f(a) : 4.6479981392621994e-05 &lt;-- Same as np.sum. f() : 3.27317975461483e-05 &lt;-- Same as a.sum. np.add.reduce(a): 3.0195806175470352e-05 &lt;-- Also, note that this one is fast. Note2 As all benchmarks show, np.add.reduce is the fastest (least overhead). If your actual application also deals only with 1D arrays, and such a small difference is important to you, you should consider using np.add.reduce. Note3 Actually, numba may be the fastest in this case. from numba import njit import numpy as np import math @njit(cache=True) def nb_numpy_sum(a): # This will be a reduce sum. return np.sum(a) @njit(cache=True) def nb_pairwise_sum(a): # https://en.wikipedia.org/wiki/Pairwise_summation N = 2 if len(a) &lt;= N: return np.sum(a) # reduce sum else: m = len(a) // 2 return nb_pairwise_sum(a[:m]) + nb_pairwise_sum(a[m:]) @njit(cache=True) def nb_kahan_sum(a): # https://en.wikipedia.org/wiki/Kahan_summation_algorithm total = a.dtype.type(0.0) c = total for i in range(len(a)): y = a[i] - c t = total + y c = (t - total) - y total = t return total def test(): candidates = [ (&quot;np.sum&quot;, np.sum), (&quot;math.fsum&quot;, math.fsum), (&quot;nb_numpy_sum&quot;, nb_numpy_sum), (&quot;nb_pairwise_sum&quot;, nb_pairwise_sum), (&quot;nb_kahan_sum&quot;, nb_kahan_sum), ] n = 10 ** 7 + 1 a = np.full(n, 0.1, dtype=np.float64) for name, f in candidates: print(f&quot;{name:16}: {f(a)}&quot;) test() Accuracy: np.sum : 1000000.0999999782 math.fsum : 1000000.1000000001 nb_numpy_sum : 1000000.0998389754 nb_pairwise_sum : 1000000.1 nb_kahan_sum : 1000000.1000000001 Timing: np.sum(a) : 4.7777313739061356e-05 a.sum() : 3.219071435928345e-05 np.add.reduce(a) : 2.9000919312238693e-05 nb_numpy_sum(a) : 1.0361894965171814e-05 nb_pairwise_sum(a): 1.4733988791704178e-05 nb_kahan_sum(a) : 1.2937933206558228e-05 Note that although nb_pairwise_sum and nb_kahan_sum have mathematical accuracy comparable to NumPy, neither is intended to be an exact replica of NumPy's implementation. So there is no guarantee that the results will be exactly the same as NumPy's. It should also be clarified that this difference is due to the amount of overhead, and NumPy is significantly faster for large arrays (e.g. &gt;10000). The following section was added after this answer was accepted. Below is an improved version of @JérômeRichard's pairwise sum that sacrifices some accuracy for faster performance on larger arrays. See the comments for more details. import numba as nb import numpy as np # Very fast function which should be inlined by LLVM. # The loop should be completely unrolled and designed so the SLP-vectorizer # could emit SIMD instructions, though in practice it does not... @nb.njit(cache=True) def nb_sum_x16(a): v1 = a[0] v2 = a[1] for i in range(2, 16, 2): v1 += a[i] v2 += a[i+1] return v1 + v2 @nb.njit(cache=True) def nb_pairwise_sum(a): n = len(a) m = n // 2 # Trivial case for tiny arrays if n &lt; 16: return sum(a[:m]) + sum(a[m:]) # Computation of a chunk (of 16~256 items) using an iterative # implementation so to reduce the overhead of function calls. if n &lt;= 256: v = nb_sum_x16(a[0:16]) i = 16 # Main loop iterating on blocks (of exactly 16 items) while i + 15 &lt; n: v += nb_sum_x16(a[i:i+16]) i += 16 return v + sum(a[i:]) # OPTIONAL OPTIMIZATION: only for array with 1_000~100_000 items # Same logic than above but with bigger chunks # It is meant to reduce branch prediction issues with small # chunks by splitting them in equal size. if n &lt;= 4096: v = nb_pairwise_sum(a[:256]) i = 256 while i + 255 &lt; n: v += nb_pairwise_sum(a[i:i+256]) i += 256 return v + nb_pairwise_sum(a[i:]) return nb_pairwise_sum(a[:m]) + nb_pairwise_sum(a[m:])",
    "context_chunks": [
      {
        "text": "It is known that np.sum(arr) is quite a lot slower than arr.sum(). For example: import numpy as np np.random.seed(7) A = np.random.random(1000) %timeit np.sum(A) 2.94 µs ± 13.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) %timeit A.sum() 1.8 µs ± 40.8 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) Can anyone give a detailed code-based explanation of what np.sum(arr) is doing that arr.sum() is not? The difference is insignificant for much longer arrays. But it is relatively significant for arrays of length 1000 or less, for example. In my code I do millions of array sums so the difference is particularly significant.",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "When I run np.sum(a) in debug mode on my PC, it steps into the following code. https://github.com/numpy/numpy/blob/v1.26.5/numpy/core/fromnumeric.py#L2178 The following is the part of the code where it is relevant. import numpy as np import types def _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs): passkwargs = {k: v for k, v in kwargs.items() if v is not np._NoValue} if type(obj) is not np.ndarray: raise NotImplementedError return ufunc.reduce(obj, axis, dtype, out, **passkwargs) def copied_np_sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, initial=np._NoValue, where=np._NoValue): if isinstance(a, types.GeneratorType): raise NotImplementedError return _wrapreduction( a, np.add, 'sum', axis, dtype, out, keepdims=keepdims, initial=initial, where=where ) Note that this ends up calling np.add.reduce(a). Benchmark: import timeit def benchmark(setup, stmt, repeat, number): print(f&quot;{stmt:16}: {min(timeit.repeat(setup=setup, stmt=stmt, globals=globals(), repeat=repeat, number=number)) / number}&quot;) n_item = 10 ** 3 n_loop = 1000 n_set = 1000 data_setup = f&quot;&quot;&quot;\\ import numpy as np rng = np.random.default_rng(0) a = rng.random({n_item}) &quot;&quot;&quot; benchmark(setup=data_setup, stmt=&quot;np.sum(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;a.sum()&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;copied_np_sum(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;np.add.reduce(a)&quot;, repeat=n_set, number=n_loop) np.sum(a) : 2.6407251134514808e-06 a.sum() : 1.3474803417921066e-06 copied_np_sum(a): 2.50667380169034e-06 np.add.reduce(a): 1.195137854665518e-06 As you can see, copied_np_sum performs similarly to np.sum, and np.add.reduce is similar to a.sum. So the majority of the difference between np.sum and a.sum is likely due to what copied_np_sum does before calling np.add.reduce. In other words, it's the overhead caused by the dict comprehension and the additional function calls. However, although there is a significant difference in the above benchmark that reproduces the OP's one, as pointed out in the comment, this may be overstated. Because timeit repeatedly executes the code and uses the (best of) average, with a small array like in this benchmark, the array may already be in the CPU cache when it is measured. This is not necessarily an unfair condition. The same thing could happen in actual use. Rather, it should be so whenever possible. That being said, for a canonical answer, we should measure it. Based on @user3666197 advice, we can create a large array immediately after creating a to evicts a from the cache. Note that I decided to use np.arange here, which I confirmed has the same effect but runs faster. import timeit def benchmark(setup, stmt, repeat, number): print(f&quot;{stmt:16}: {min(timeit.repeat(setup=setup, stmt=stmt, globals=globals(), repeat=repeat, number=number)) / number}&quot;) n_item = 10 ** 3 n_loop = 1 n_set = 100 data_setup = f&quot;&quot;&quot;\\ import numpy as np rng = np.random.default_rng(0) a = rng.random({n_item}) _ = np.arange(10 ** 9, dtype=np.uint8) # To evict `a` from the CPU cache. &quot;&quot;&quot; benchmark(setup=data_setup, stmt=&quot;np.sum(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;a.sum()&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;copied_np_sum(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=data_setup, stmt=&quot;np.add.reduce(a)&quot;, repeat=n_set, number=n_loop) Without eviction (With cache): np.sum(a) : 2.6407251134514808e-06 a.sum() : 1.3474803417921066e-06 copied_np_sum(a): 2.50667380169034e-06 np.add.reduce(a): 1.195137854665518e-06 With eviction (Without cache): np.sum(a) : 4.916824400424957e-05 a.sum() : 3.245798870921135e-05 copied_np_sum(a): 4.7205016016960144e-05 np.add.reduce(a): 3.0195806175470352e-05 Naturally, the presence or absence of cache makes a huge impact on performance. However, although the difference has become smaller, it can still be said to be a significant difference. Also, since these four relationships remain the same as before, the conclusion also remains the same. There are a few things I should add. Note1 The claim regarding method loading is incorrect. benchmark(setup=f&quot;{data_setup}f = np.sum&quot;, stmt=&quot;f(a)&quot;, repeat=n_set, number=n_loop) benchmark(setup=f&quot;{data_setup}f = a.sum&quot;, stmt=&quot;f()&quot;, repeat=n_set, number=n_loop) np.sum(a) : 4.916824400424957e-05 a.sum() : 3.245798870921135e-05 f(a) : 4.6479981392621994e-05 &lt;-- Same as np.sum. f() : 3.27317975461483e-05 &lt;-- Same as a.sum. np.add.reduce(a): 3.0195806175470352e-05 &lt;-- Also, note that this one is fast. Note2 As all benchmarks show, np.add.reduce is the fastest (least overhead). If your actual application also deals only with 1D arrays, and such a small difference is important to you, you should consider using np.add.reduce. Note3 Actually, numba may be the fastest in this case. from numba import njit import numpy as np import math @njit(cache=True) def nb_numpy_sum(a): # This will be a reduce sum. return np.sum(a) @njit(cache=True) def nb_pairwise_sum(a): # https://en.wikipedia.org/wiki/Pairwise_summation N = 2 if len(a) &lt;= N: return np.sum(a) # reduce sum else: m = len(a) // 2 return nb_pairwise_sum(a[:m]) + nb_pairwise_sum(a[m:]) @njit(cache=True) def nb_kahan_sum(a): # https://en.wikipedia.org/wiki/Kahan_summation_algorithm total = a.dtype.type(0.0) c = total for i in range(len(a)): y = a[i] - c t = total + y c = (t - total) - y total = t return total def test(): candidates = [ (&quot;np.sum&quot;, np.sum), (&quot;math.fsum&quot;, math.fsum), (&quot;nb_numpy_sum&quot;, nb_numpy_sum), (&quot;nb_pairwise_sum&quot;, nb_pairwise_sum), (&quot;nb_kahan_sum&quot;, nb_kahan_sum), ] n = 10 ** 7 + 1 a = np.full(n, 0.1, dtype=np.float64) for name, f in candidates: print(f&quot;{name:16}: {f(a)}&quot;) test() Accuracy: np.sum : 1000000.0999999782 math.fsum : 1000000.1000000001 nb_numpy_sum : 1000000.0998389754 nb_pairwise_sum : 1000000.1 nb_kahan_sum : 1000000.1000000001 Timing: np.sum(a) : 4.7777313739061356e-05 a.sum() : 3.219071435928345e-05 np.add.reduce(a) : 2.9000919312238693e-05 nb_numpy_sum(a) : 1.0361894965171814e-05 nb_pairwise_sum(a): 1.4733988791704178e-05 nb_kahan_sum(a) : 1.2937933206558228e-05 Note that although nb_pairwise_sum and nb_kahan_sum have mathematical accuracy comparable to NumPy, neither is intended to be an exact replica of NumPy's implementation. So there is no guarantee that the results will be exactly the same as NumPy's. It should also be clarified that this difference is due to the amount of overhead, and NumPy is significantly faster for large arrays (e.g. &gt;10000). The following section was added after this answer was accepted. Below is an improved version of @JérômeRichard's pairwise sum that sacrifices some accuracy for faster performance on larger arrays. See the comments for more details. import numba as nb import numpy as np # Very fast function which should be inlined by LLVM. # The loop should be completely unrolled and designed so the SLP-vectorizer # could emit SIMD instructions, though in practice it does not... @nb.njit(cache=True) def nb_sum_x16(a): v1 = a[0] v2 = a[1] for i in range(2, 16, 2): v1 += a[i] v2 += a[i+1] return v1 + v2 @nb.njit(cache=True) def nb_pairwise_sum(a): n = len(a) m = n // 2 # Trivial case for tiny arrays if n &lt; 16: return sum(a[:m]) + sum(a[m:]) # Computation of a chunk (of 16~256 items) using an iterative # implementation so to reduce the overhead of function calls. if n &lt;= 256: v = nb_sum_x16(a[0:16]) i = 16 # Main loop iterating on blocks (of exactly 16 items) while i + 15 &lt; n: v += nb_sum_x16(a[i:i+16]) i += 16 return v + sum(a[i:]) # OPTIONAL OPTIMIZATION: only for array with 1_000~100_000 items # Same logic than above but with bigger chunks # It is meant to reduce branch prediction issues with small # chunks by splitting them in equal size. if n &lt;= 4096: v = nb_pairwise_sum(a[:256]) i = 256 while i + 255 &lt; n: v += nb_pairwise_sum(a[i:i+256]) i += 256 return v + nb_pairwise_sum(a[i:]) return nb_pairwise_sum(a[:m]) + nb_pairwise_sum(a[m:])",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "The difference only occurs for small enough arrays, and is really negligible (but significant). If you look at the bytecode, np.sum(A) must fetch the method from the module, which might contribute a tiny bit to the small delay. # np.sum(A) 6 0 LOAD_GLOBAL 0 (np) 2 LOAD_METHOD 1 (sum) 4 LOAD_FAST 0 (A) 6 CALL_METHOD 1 8 RETURN_VALUE # A.sum() 9 0 LOAD_FAST 0 (A) 2 LOAD_METHOD 0 (sum) 4 CALL_METHOD 0 6 RETURN_VALUE In addition, there is also a slightly different route to reach the code that actually performs the computation (for example np.sum checks that the input is not a generator, etc.), which adds a different overhead, but in fine the algorithm/code used is the same. numpy.ndarray.sum actually refers to numpy.sum in the documentation. If you run a profiler on both codes, you can see the difference lies in fromnumeric.py (see @ken's answer that also discusses this): import cProfile A = np.array([1]) cProfile.run('np.sum(A)') # 10 function calls in 0.000 seconds # Ordered by: standard name # ncalls tottime percall cumtime percall filename:lineno(function) # 1 0.000 0.000 0.000 0.000 &lt;string&gt;:1(&lt;module&gt;) # 1 0.000 0.000 0.000 0.000 fromnumeric.py:2172(_sum_dispatcher) # 1 0.000 0.000 0.000 0.000 fromnumeric.py:2177(sum) # 1 0.000 0.000 0.000 0.000 fromnumeric.py:71(_wrapreduction) # 1 0.000 0.000 0.000 0.000 fromnumeric.py:72(&lt;dictcomp&gt;) # 1 0.000 0.000 0.000 0.000 {built-in method builtins.exec} # 1 0.000 0.000 0.000 0.000 {built-in method builtins.isinstance} # 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} # 1 0.000 0.000 0.000 0.000 {method 'items' of 'dict' objects} # 1 0.000 0.000 0.000 0.000 {method 'reduce' of 'numpy.ufunc' objects} cProfile.run('A.sum()') # 6 function calls in 0.000 seconds # Ordered by: standard name # ncalls tottime percall cumtime percall filename:lineno(function) # 1 0.000 0.000 0.000 0.000 &lt;string&gt;:1(&lt;module&gt;) # 1 0.000 0.000 0.000 0.000 _methods.py:47(_sum) # 1 0.000 0.000 0.000 0.000 {built-in method builtins.exec} # 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} # 1 0.000 0.000 0.000 0.000 {method 'reduce' of 'numpy.ufunc' objects} # 1 0.000 0.000 0.000 0.000 {method 'sum' of 'numpy.ndarray' objects} Interestingly, there is the same difference when starting from a list:",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "numpy"
      ],
      "question_score": 6,
      "answer_score": 10,
      "created": "2024-06-15T11:53:44",
      "question_id": 78626515,
      "answer_id": 78626678
    }
  },
  {
    "question": "Where does uvicorn/FastAPI display/log unhandled errors?",
    "expected_answer": "I don't think this is an issue with uvicorn. If there is an error it displays it on terminal by default unless you have changed some configuration. If you haven't changed any configuration and still not seeing the log you can pass the log configuration file during command. filename: log_config.yml { &quot;version&quot;: 1, &quot;disable_existing_loggers&quot;: false, &quot;formatters&quot;: { &quot;verbose&quot;: { &quot;format&quot;: &quot;%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s&quot; } }, &quot;handlers&quot;: { &quot;uvicorn&quot;: { &quot;class&quot;: &quot;logging.handlers.TimedRotatingFileHandler&quot;, &quot;level&quot;: &quot;DEBUG&quot;, &quot;formatter&quot;: &quot;verbose&quot;, &quot;when&quot;: &quot;D&quot;, &quot;backupCount&quot;: 0, &quot;filename&quot;: &quot;uvicorn.log&quot; } }, &quot;loggers&quot;: { &quot;uvicorn&quot;: { &quot;level&quot;: &quot;DEBUG&quot;, &quot;handlers&quot;: [&quot;uvicorn&quot;], &quot;propagate&quot;: true, &quot;qualname&quot;: &quot;uvicorn&quot; } } } command: uvicorn api.main:app --reload --log-config=log_config.yml After this you will see the logs in uvicorn.log file. Note:You may need to create uvicorn.log file beforehand",
    "context_chunks": [
      {
        "text": "I am running a simple FastAPI application under uvicorn. The FastAPI code is this: from fastapi import FastAPI @app.post(&quot;/events&quot;) def create_events(): print(&quot;entering create_events()&quot;) raise Exception(&quot;an error&quot;) I run the app: uvicorn api.main:app --reload --log-level=debug I now post to the endpoint using wget: wget -O- --header='Content-Type:application/json' --post-file=/tmp/data.json http://127.0.0.1:8000/events Not surprisingly, the wget returns a 500 Internal Server Error. In the output of the terminal where I ran uvicorn I see this: entering create_events() In other web application contexts (Perl, Ruby on Rails, Python with Flask) if the server raises an unhandled exception I can see the error message on the server side somewhere: in a log file, on standard output, somewhere. But in this FastAPI/uvicorn application I cannot find any error message. I don't see it in the place where I ran the wget and I don't see it in the uvicorn terminal. Where is the 500 error message logged/displayed?",
        "contains_answer": false,
        "score": 0.3,
        "source": "stackoverflow_question"
      },
      {
        "text": "I don't think this is an issue with uvicorn. If there is an error it displays it on terminal by default unless you have changed some configuration. If you haven't changed any configuration and still not seeing the log you can pass the log configuration file during command. filename: log_config.yml { &quot;version&quot;: 1, &quot;disable_existing_loggers&quot;: false, &quot;formatters&quot;: { &quot;verbose&quot;: { &quot;format&quot;: &quot;%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s&quot; } }, &quot;handlers&quot;: { &quot;uvicorn&quot;: { &quot;class&quot;: &quot;logging.handlers.TimedRotatingFileHandler&quot;, &quot;level&quot;: &quot;DEBUG&quot;, &quot;formatter&quot;: &quot;verbose&quot;, &quot;when&quot;: &quot;D&quot;, &quot;backupCount&quot;: 0, &quot;filename&quot;: &quot;uvicorn.log&quot; } }, &quot;loggers&quot;: { &quot;uvicorn&quot;: { &quot;level&quot;: &quot;DEBUG&quot;, &quot;handlers&quot;: [&quot;uvicorn&quot;], &quot;propagate&quot;: true, &quot;qualname&quot;: &quot;uvicorn&quot; } } } command: uvicorn api.main:app --reload --log-config=log_config.yml After this you will see the logs in uvicorn.log file. Note:You may need to create uvicorn.log file beforehand",
        "contains_answer": true,
        "score": 1.0,
        "source": "stackoverflow"
      },
      {
        "text": "If you want your Fastapi endpoint to return specific Http error you need to use HTTPException. If code executed by endpoint throws any other exception, it will return 500, which is internal server error. example: from fastapi import FastAPI, HTTPException, status @app.post(&quot;/events&quot;) def create_events(): print(&quot;entering create_events()&quot;) raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail=&quot;an error&quot;, ) If you want to log error message, you could add custom handler: from fastapi import Request from fastapi.responses import PlainTextResponse async def unhandled_exception_handler(request: Request, exc: Exception) -&gt; PlainTextResponse: &quot;&quot;&quot; This middleware will log all unhandled exceptions. Unhandled exceptions are all exceptions that are not HTTPExceptions or RequestValidationErrors. &quot;&quot;&quot; logger.debug(&quot;Our custom unhandled_exception_handler was called&quot;) host = getattr(getattr(request, &quot;client&quot;, None), &quot;host&quot;, None) port = getattr(getattr(request, &quot;client&quot;, None), &quot;port&quot;, None) url = f&quot;{request.url.path}?{request.query_params}&quot; if request.query_params else request.url.path exception_type, exception_value, exception_traceback = sys.exc_info() exception_name = getattr(exception_type, &quot;__name__&quot;, None) logger.error( f'{host}:{port} - &quot;{request.method} {url}&quot; 500 Internal Server Error &lt;{exception_name}: {exception_value}&gt;' ) return PlainTextResponse(str(exc), status_code=500) Then you can add the handler to your app: app.add_exception_handler(Exception, unhandled_exception_handler)",
        "contains_answer": false,
        "score": 0.1,
        "source": "stackoverflow"
      }
    ],
    "metadata": {
      "tags": [
        "python",
        "fastapi",
        "http-status-code-500",
        "uvicorn"
      ],
      "question_score": 7,
      "answer_score": 1,
      "created": "2023-07-25T17:32:14",
      "question_id": 76765229,
      "answer_id": 76765660
    }
  }
]